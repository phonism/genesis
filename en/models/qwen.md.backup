# Qwenæ¨¡å‹å®ç°

## æ¦‚è¿°

GenesisåŒ…å«äº†Qwenï¼ˆé€šä¹‰åƒé—®ï¼‰å¤§è¯­è¨€æ¨¡å‹çš„å®Œæ•´å®ç°ï¼Œæ”¯æŒå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†å·¥ä½œæµã€‚

## æ¨¡å‹æ¶æ„

Qwenæ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

- **æ³¨æ„åŠ›æœºåˆ¶**: å¤šå¤´æ³¨æ„åŠ›ä¸RoPEï¼ˆæ—‹è½¬ä½ç½®åµŒå…¥ï¼‰
- **æ¿€æ´»å‡½æ•°**: SwiGLUæ¿€æ´»å‡½æ•°
- **å±‚å½’ä¸€åŒ–**: RMSNorm
- **ä½ç½®ç¼–ç **: æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€æ¨ç†

```python
import genesis
from genesis.models.qwen import QwenModel, QwenConfig

# åˆ›å»ºæ¨¡å‹é…ç½®
config = QwenConfig(
    vocab_size=32000,
    n_layer=24,
    n_head=16,
    n_embd=2048,
    max_seq_len=2048
)

# åˆ›å»ºæ¨¡å‹
model = QwenModel(config)

# æ¨ç†
input_ids = genesis.tensor([[1, 2, 3, 4, 5]])  # [batch_size, seq_len]
output = model(input_ids)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [1, 5, 32000]
```

### è®­ç»ƒç¤ºä¾‹

```python
import genesis.optim as optim
import genesis.nn as nn

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    input_ids, labels = batch
    
    # å‰å‘ä¼ æ’­
    logits = model(input_ids)
    
    # è®¡ç®—æŸå¤±
    loss = nn.functional.cross_entropy(
        logits.view(-1, logits.size(-1)),
        labels.view(-1)
    )
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # å‚æ•°æ›´æ–°
    optimizer.step()
```

## é…ç½®å‚æ•°

### QwenConfig

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|-------|------|
| `vocab_size` | int | 32000 | è¯æ±‡è¡¨å¤§å° |
| `n_layer` | int | 24 | Transformerå±‚æ•° |
| `n_head` | int | 16 | æ³¨æ„åŠ›å¤´æ•° |
| `n_embd` | int | 2048 | éšè—å±‚ç»´åº¦ |
| `max_seq_len` | int | 2048 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `dropout` | float | 0.1 | Dropoutæ¦‚ç‡ |
| `bias` | bool | False | æ˜¯å¦ä½¿ç”¨åç½®é¡¹ |

## æ€§èƒ½ä¼˜åŒ–

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# å¯ç”¨æ··åˆç²¾åº¦
from genesis.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    logits = model(input_ids)
    loss = criterion(logits, labels)

# æ¢¯åº¦ç¼©æ”¾
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
model.enable_gradient_checkpointing()
```

## åº”ç”¨ç¤ºä¾‹

### æ–‡æœ¬ç”Ÿæˆ

```python
def generate_text(model, tokenizer, prompt, max_length=100):
    input_ids = tokenizer.encode(prompt)
    input_tensor = genesis.tensor([input_ids])
    
    with genesis.no_grad():
        for _ in range(max_length):
            logits = model(input_tensor)
            next_token = logits[0, -1].argmax()
            input_tensor = genesis.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)
            
            if next_token.item() == tokenizer.eos_token_id:
                break
    
    return tokenizer.decode(input_tensor[0].tolist())

# ä½¿ç”¨ç¤ºä¾‹
generated = generate_text(model, tokenizer, "ä»Šå¤©çš„å¤©æ°”")
print(generated)
```

### å¾®è°ƒ

å‚è€ƒ `apps/llm/train_sft_qwen.py` è·å¾—å®Œæ•´çš„SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å®ç°ã€‚

```bash
# è¿è¡ŒSFTè®­ç»ƒ
cd apps/llm
python train_sft_qwen.py \
    --model_size 0.5b \
    --data_path /path/to/data \
    --batch_size 4 \
    --learning_rate 5e-5 \
    --num_epochs 3
```

## æ–‡ä»¶ç»“æ„

- `genesis/models/qwen.py` - æ¨¡å‹å®ç°
- `apps/llm/qwen_model.py` - è®­ç»ƒé…ç½®å’Œå·¥å…·
- `apps/llm/train_sft_qwen.py` - SFTè®­ç»ƒè„šæœ¬
- `apps/llm/chat_qwen.py` - æ¨ç†èŠå¤©è„šæœ¬

## æœ€æ–°æ›´æ–° (2025-01)

- âœ… å®Œæ•´çš„Qwenæ¨¡å‹å®ç°ï¼Œæ”¯æŒRoPEå’ŒRMSNorm
- âœ… æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ (FP16/BF16)
- âœ… æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ ç‡è°ƒåº¦
- âœ… SFTè®­ç»ƒè„šæœ¬å’ŒèŠå¤©ç•Œé¢
- âœ… æ¨¡å‹æ£€æŸ¥ç‚¹å’ŒçŠ¶æ€ç®¡ç†
- ğŸš§ æ€§èƒ½ä¼˜åŒ–æŒç»­è¿›è¡Œä¸­

## ç›¸å…³èµ„æº

- [Qwenå®˜æ–¹è®ºæ–‡](https://arxiv.org/abs/2309.16609)
- [é«˜çº§è®­ç»ƒç‰¹æ€§](\../training/advanced-features.md)
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](\../tutorials/performance-tuning.md)
- [Genesis APIå‚è€ƒ](\../api-reference/index.md)