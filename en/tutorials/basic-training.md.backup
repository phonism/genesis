# åŸºç¡€è®­ç»ƒæ•™ç¨‹

æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œä½¿ç”¨Genesisæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºå’Œè®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å›¾åƒåˆ†ç±»é¡¹ç›®æ¥å­¦ä¹ Genesisçš„æ ¸å¿ƒæ¦‚å¿µå’Œç”¨æ³•ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š
- Genesisçš„åŸºæœ¬APIå’Œæ•°æ®ç»“æ„
- å¦‚ä½•å®šä¹‰å’Œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- è®­ç»ƒå¾ªç¯çš„æ„å»ºå’Œä¼˜åŒ–
- æ¨¡å‹è¯„ä¼°å’Œä¿å­˜

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿å·²å®‰è£…Genesis
pip install torch triton numpy matplotlib tqdm
git clone https://github.com/phonism/genesis.git
cd genesis
pip install -e .
```

### éªŒè¯å®‰è£…

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim

# æµ‹è¯•åŸºæœ¬åŠŸèƒ½
x = genesis.randn(2, 3)
print(f"Genesiså¼ é‡å·²åˆ›å»º: {x.shape}")
print(f"Genesisæ¨¡å—å¯ç”¨: {dir(nn)}")
```

## ğŸ“Š é¡¹ç›®ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿï¼Œä½¿ç”¨ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œå’Œåˆæˆæ•°æ®æ¥æ¼”ç¤ºGenesisçš„åŠŸèƒ½ã€‚

### 1. æ•°æ®å‡†å¤‡

ç”±äºGenesisè¿˜æ²¡æœ‰å†…ç½®çš„æ•°æ®åŠ è½½å·¥å…·ï¼Œæˆ‘ä»¬å°†åˆ›å»ºæ¨¡ä»¿MNISTç»“æ„çš„åˆæˆæ•°æ®ï¼š

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim
import numpy as np
import matplotlib.pyplot as plt

class SimpleDataset:
    """æ¼”ç¤ºç”¨çš„ç®€å•æ•°æ®é›†ç±»"""
    
    def __init__(self, num_samples=1000, input_dim=784, num_classes=10):
        # ç”Ÿæˆç±»ä¼¼å±•å¹³MNISTçš„åˆæˆæ•°æ®
        self.data = genesis.randn(num_samples, input_dim)
        
        # åŸºäºæ•°æ®æ¨¡å¼åˆ›å»ºæ ‡ç­¾ï¼ˆåˆæˆï¼‰
        labels = genesis.randn(num_samples, num_classes)
        self.labels = genesis.functional.max(labels, axis=1, keepdims=False)
        
        self.num_samples = num_samples
        
    def __len__(self):
        return self.num_samples
    
    def get_batch(self, batch_size=32, start_idx=0):
        """è·å–ä¸€æ‰¹æ•°æ®"""
        end_idx = min(start_idx + batch_size, self.num_samples)
        return (self.data[start_idx:end_idx], 
                self.labels[start_idx:end_idx])

# åˆ›å»ºæ•°æ®é›†
train_dataset = SimpleDataset(num_samples=800, input_dim=784, num_classes=10)
test_dataset = SimpleDataset(num_samples=200, input_dim=784, num_classes=10)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
print(f"è¾“å…¥ç»´åº¦: 784 (28x28å±•å¹³)")
print(f"ç±»åˆ«æ•°é‡: 10")
```

### 2. æ¨¡å‹å®šä¹‰

æˆ‘ä»¬å°†ä½¿ç”¨Genesisæ¨¡å—æ„å»ºä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼š

```python
class MNISTNet(nn.Module):
    """æ•°å­—è¯†åˆ«çš„ç®€å•å…¨è¿æ¥ç½‘ç»œ"""
    
    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):
        super(MNISTNet, self).__init__()
        
        # ä½¿ç”¨å®é™…çš„Genesisæ¨¡å—å®šä¹‰å±‚
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, num_classes)
        
        # æ¿€æ´»å‡½æ•°å’Œæ­£åˆ™åŒ–
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        # å¦‚æœéœ€è¦ï¼Œå±•å¹³è¾“å…¥
        if len(x.shape) > 2:
            x = x.view(x.shape[0], -1)
        
        # ç¬¬ä¸€ä¸ªéšè—å±‚
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒä¸ªéšè—å±‚
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout(x)
        
        # è¾“å‡ºå±‚
        x = self.fc3(x)
        
        return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)

print("æ¨¡å‹ç»“æ„:")
print(f"å±‚1: {model.fc1}")
print(f"å±‚2: {model.fc2}")
print(f"å±‚3: {model.fc3}")
print(f"å‚æ•°æ€»æ•°: {sum(p.data.size for p in model.parameters())}")
```

### 3. æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

```python
# ä½¿ç”¨Genesiså®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.SoftmaxLoss()  # ä½¿ç”¨Genesisçš„SoftmaxLoss
optimizer = optim.Adam(model.parameters(), lr=0.001)

print(f"æŸå¤±å‡½æ•°: {criterion}")
print(f"ä¼˜åŒ–å™¨: {optimizer}")
print(f"å­¦ä¹ ç‡: 0.001")
```

### 4. è®­ç»ƒå¾ªç¯

```python
def train_epoch(model, dataset, criterion, optimizer, batch_size=32):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    total_loss = 0.0
    num_batches = len(dataset) // batch_size
    
    for i in range(num_batches):
        # è·å–æ‰¹æ•°æ®
        start_idx = i * batch_size
        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)
        
        # å‰å‘ä¼ æ’­
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        
        # åº”ç”¨æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # æ›´æ–°æƒé‡
        optimizer.step()
        
        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)
    
    return total_loss / num_batches

def evaluate(model, dataset, criterion, batch_size=32):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    total_loss = 0.0
    correct = 0
    total = 0
    num_batches = len(dataset) // batch_size
    
    for i in range(num_batches):
        start_idx = i * batch_size
        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)
        
        # å‰å‘ä¼ æ’­ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # è®¡ç®—å‡†ç¡®ç‡
        predicted = genesis.functional.max(outputs, axis=1, keepdims=False)
        total += batch_labels.shape[0]
        correct += (predicted == batch_labels).sum().data
        
        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)
    
    accuracy = correct / total
    avg_loss = total_loss / num_batches
    
    return avg_loss, accuracy

# è®­ç»ƒé…ç½®
num_epochs = 10
batch_size = 32

print("å¼€å§‹è®­ç»ƒ...")
print(f"è½®æ•°: {num_epochs}")
print(f"æ‰¹é‡å¤§å°: {batch_size}")
print("-" * 50)

# è®­ç»ƒå¾ªç¯
train_losses = []
test_losses = []
test_accuracies = []

for epoch in range(num_epochs):
    # è®­ç»ƒä¸€ä¸ªepoch
    train_loss = train_epoch(model, train_dataset, criterion, optimizer, batch_size)
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_loss, test_accuracy = evaluate(model, test_dataset, criterion, batch_size)
    
    # è®°å½•æŒ‡æ ‡
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    test_accuracies.append(test_accuracy)
    
    # æ‰“å°è¿›åº¦
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
    print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print("-" * 30)

print("è®­ç»ƒå®Œæˆï¼")
```

### 5. æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

```python
# ç»˜åˆ¶è®­ç»ƒè¿›åº¦
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

# ç»˜åˆ¶æŸå¤±
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='è®­ç»ƒæŸå¤±')
plt.plot(test_losses, label='æµ‹è¯•æŸå¤±')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('è®­ç»ƒå’Œæµ‹è¯•æŸå¤±')
plt.legend()
plt.grid(True)

# ç»˜åˆ¶å‡†ç¡®ç‡
plt.subplot(1, 2, 2)
plt.plot(test_accuracies, label='æµ‹è¯•å‡†ç¡®ç‡')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('æµ‹è¯•å‡†ç¡®ç‡')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# æœ€ç»ˆè¯„ä¼°
final_test_loss, final_test_accuracy = evaluate(model, test_dataset, criterion, batch_size)
print(f"\næœ€ç»ˆç»“æœ:")
print(f"æµ‹è¯•æŸå¤±: {final_test_loss:.4f}")
print(f"æµ‹è¯•å‡†ç¡®ç‡: {final_test_accuracy:.4f}")
```

### 6. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä½¿ç”¨Genesisåºåˆ—åŒ–ä¿å­˜æ¨¡å‹
model_path = "mnist_model.pkl"
genesis.save(model.state_dict(), model_path)
print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")

# åŠ è½½æ¨¡å‹
model_new = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)
model_new.load_state_dict(genesis.load(model_path))
print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")

# éªŒè¯åŠ è½½çš„æ¨¡å‹æ˜¯å¦å·¥ä½œ
test_loss, test_accuracy = evaluate(model_new, test_dataset, criterion, batch_size)
print(f"åŠ è½½æ¨¡å‹çš„å‡†ç¡®ç‡: {test_accuracy:.4f}")
```

## ğŸ“ å­¦åˆ°çš„å…³é”®æ¦‚å¿µ

### 1. Genesiså¼ é‡æ“ä½œ
- ä½¿ç”¨`genesis.randn()`, `genesis.tensor()`åˆ›å»ºå¼ é‡
- åŸºæœ¬æ“ä½œå¦‚çŸ©é˜µä¹˜æ³•å’Œé€å…ƒç´ æ“ä½œ
- ä½¿ç”¨`requires_grad`è¿›è¡Œè‡ªåŠ¨å¾®åˆ†

### 2. ç¥ç»ç½‘ç»œæ¨¡å—
- é€šè¿‡ç»§æ‰¿`nn.Module`å®šä¹‰æ¨¡å‹
- ä½¿ç”¨å†…ç½®å±‚ï¼š`nn.Linear`, `nn.ReLU`, `nn.Dropout`
- ç†è§£å‰å‘ä¼ æ’­å®ç°

### 3. è®­ç»ƒè¿‡ç¨‹
- è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
- å®ç°è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯
- ä½¿ç”¨æ¢¯åº¦è£å‰ªå’Œæ­£åˆ™åŒ–

### 4. æ¨¡å‹ç®¡ç†
- ä½¿ç”¨Genesisåºåˆ—åŒ–ä¿å­˜å’ŒåŠ è½½æ¨¡å‹çŠ¶æ€
- ç®¡ç†æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–çŠ¶æ€

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å¯ä»¥ï¼š

1. **æ¢ç´¢æ›´å¤æ‚çš„æ¨¡å‹** - å°è¯•å…·æœ‰æ›´å¤šå±‚çš„ä¸åŒæ¶æ„
2. **å­¦ä¹ é«˜çº§ç‰¹æ€§** - æ¢ç´¢æ··åˆç²¾åº¦è®­ç»ƒå’Œå­¦ä¹ ç‡è°ƒåº¦
3. **å¤„ç†çœŸå®æ•°æ®** - å½“æ•°æ®åŠ è½½å·¥å…·å¯ç”¨æ—¶ä¸å®é™…æ•°æ®é›†é›†æˆ
4. **æ€§èƒ½ä¼˜åŒ–** - äº†è§£GPUåŠ é€Ÿå’ŒTritonå†…æ ¸ä½¿ç”¨

## ğŸ“š å…¶ä»–èµ„æº

- [Genesis APIå‚è€ƒ](../api-reference/index.md) - å®Œæ•´çš„APIæ–‡æ¡£
- [é«˜çº§è®­ç»ƒç‰¹æ€§](../training/advanced-features.md) - æ··åˆç²¾åº¦ã€è°ƒåº¦å™¨ç­‰
- [æ€§èƒ½ä¼˜åŒ–](performance-tuning.md) - æ›´å¿«è®­ç»ƒçš„æŠ€å·§

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å¯¼å…¥é”™è¯¯**ï¼šç¡®ä¿ä½¿ç”¨`pip install -e .`æ­£ç¡®å®‰è£…Genesis
2. **å½¢çŠ¶ä¸åŒ¹é…**ï¼šæ£€æŸ¥å‰å‘ä¼ æ’­ä¸­çš„å¼ é‡ç»´åº¦
3. **å†…å­˜é—®é¢˜**ï¼šå¦‚æœé‡åˆ°å†…å­˜ä¸è¶³é”™è¯¯ï¼Œå‡å°‘æ‰¹é‡å¤§å°
4. **è®­ç»ƒç¼“æ…¢**ï¼šåœ¨å¯ç”¨æ—¶å¯ç”¨GPUæ”¯æŒ

### è·å–å¸®åŠ©

- æŸ¥çœ‹[Genesisæ–‡æ¡£](../index.md)
- åœ¨[GitHub Issues](https://github.com/phonism/genesis/issues)æŠ¥å‘Šé—®é¢˜
- åœ¨ç¤¾åŒºè®ºå›åŠ å…¥è®¨è®º