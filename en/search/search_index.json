{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Genesis Deep Learning Framework","text":"Lightweight Deep Learning Framework | Built from Scratch | Python + Triton + CUDA"},{"location":"#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>Genesis is a lightweight deep learning framework developed in Python. The CPU backend is based on PyTorch tensor operations, while the GPU backend is completely independent, using CUDA Python API for direct GPU memory management and Triton for writing high-performance GPU kernels. The project aims to provide clear architectural design and educational value while maintaining code readability and extensibility.</p>"},{"location":"#core-features","title":"\u2728 Core Features","text":"<ul> <li>\ud83c\udfaf Lightweight Design - Clean API design, easy to understand and use</li> <li>\u26a1 High Performance Computing - Triton-optimized GPU kernels rivaling mainstream frameworks</li> <li>\ud83d\udd04 Automatic Differentiation - Complete backpropagation and gradient computation system</li> <li>\ud83e\udde0 Neural Networks - Rich implementation of neural network layers and optimizers</li> <li>\ud83d\udd27 Mixed Precision - Support for FP16/BF16 mixed precision training (AMP)</li> <li>\ud83d\udcca Distributed Training - NCCL multi-GPU parallel training support</li> <li>\ud83c\udfa8 Model Library - Built-in implementations of mainstream LLM models like Qwen</li> <li>\ud83d\udcbe Model Management - Complete checkpoint save/load system</li> <li>\ud83d\udcc8 Learning Rate Scheduling - Multiple learning rate schedulers and gradient clipping</li> <li>\ud83c\udfb2 Random Number Generation - PyTorch-compatible RNG API with thread-safe state management (NEW!)</li> <li>\ud83e\uddee Advanced Memory Management - Reference-counted memory pools with cache optimization (NEW!)</li> <li>\ud83d\udcca Performance Monitoring - Comprehensive memory statistics and profiling tools (NEW!)</li> <li>\ud83c\udfdb\ufe0f Unified Storage - Abstract storage interface for consistent CPU/GPU access patterns (NEW!)</li> <li>\ud83d\udd0d Production Stability - Fast-fail OOM handling and robust error management (NEW!)</li> </ul>"},{"location":"#architecture-highlights","title":"\ud83c\udfd7\ufe0f Architecture Highlights","text":"<pre><code>graph TB\n    A[User API] --&gt; B[Autograd Engine]\n    A --&gt; C[Neural Network Modules]\n    B --&gt; D[Tensor System]\n    C --&gt; D\n    D --&gt; E[Backend Abstraction Layer]\n    E --&gt; F[CPU Backend]\n    E --&gt; G[CUDA Backend]\n    G --&gt; H[Triton Kernels]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e3f2fd\n    style H fill:#fff8e1</code></pre>"},{"location":"#design-goals","title":"\ud83c\udfaf Design Goals","text":""},{"location":"#educational-value","title":"Educational Value","text":"<ul> <li>Clear Code Structure - Each module has clear responsibilities</li> <li>Complete Documentation - From design concepts to implementation details</li> <li>Progressive Learning - Learning path from basic concepts to advanced features</li> </ul>"},{"location":"#engineering-practice","title":"Engineering Practice","text":"<ul> <li>Modern Architecture - Learn from excellent designs of mainstream frameworks like PyTorch</li> <li>Efficient Implementation - Performance optimization using modern tools like Triton</li> <li>Extensibility - Modular design for easy addition of new features</li> </ul>"},{"location":"#practicality","title":"Practicality","text":"<ul> <li>Feature Complete - Support complete workflow from model definition to training deployment</li> <li>Performance Optimization - Multiple optimization strategies ensure actual training performance</li> <li>Ecosystem Compatibility - Good compatibility with existing deep learning ecosystem</li> </ul>"},{"location":"#performance-status","title":"\ud83d\udcca Performance Status","text":""},{"location":"#memory-allocator-performance-latest-optimization","title":"Memory Allocator Performance (Latest Optimization)","text":"Scenario Genesis vs PyTorch Status Same-size allocation 1.43x \u2705 Excellent Large memory (&gt;1MB) 3.92x \u2705 Outstanding Transformer training 1.89x \u2705 Excellent Memory pressure 4.83x \u2705 Outstanding Variable sizes 0.83x \ud83d\udd04 Good"},{"location":"#operator-performance","title":"Operator Performance","text":"Operation Genesis vs PyTorch Status Matrix multiplication 0.95x \u2705 Good Element-wise operations 1.02x \u2705 Excellent Reduction operations 0.87x \ud83d\udd04 Optimizing Softmax 1.15x \u2705 Excellent LayerNorm 1.08x \u2705 Excellent Cat operation 0.02x \u274c Fixing LogSumExp 0.02x \u274c Fixing Broadcast operations 0.04x \u274c Fixing"},{"location":"#recent-performance-improvements","title":"Recent Performance Improvements","text":"<ul> <li>\u2705 Block Allocator: 38x performance improvement in Transformer training scenarios</li> <li>\u2705 Memory Management: Eliminated cudaMalloc/cudaFree synchronization overhead</li> <li>\u2705 Fill Operations: 36x performance improvement with GPU-native kernels</li> <li>\ud83d\udd04 Cat Operations: GPU-native implementation in progress (fixing 0.02x issue)</li> <li>\ud83d\udd04 Reduction Operations: Triton kernel optimization in progress</li> </ul> <p>Performance Update</p> <p>Genesis has achieved major breakthroughs in memory management, reaching or exceeding PyTorch performance in multiple key scenarios. Current focus is on fixing remaining operator bottlenecks.</p> <p>For detailed analysis, see: Memory Allocator Optimization Report</p>"},{"location":"#technology-stack","title":"\ud83d\udee0\ufe0f Technology Stack","text":""},{"location":"#core-dependencies","title":"Core Dependencies","text":"<ul> <li>Python 3.8+ - Main development language</li> <li>PyTorch - Memory management and some operations</li> <li>Triton 2.0+ - GPU kernel optimization</li> <li>CUDA 11.0+ - GPU computing support</li> <li>NumPy - CPU numerical computing</li> <li>cuda-python - Direct CUDA API access</li> </ul>"},{"location":"#development-tools","title":"Development Tools","text":"<ul> <li>pytest - Unit testing framework</li> <li>black - Code formatting</li> <li>mypy - Type checking</li> <li>MkDocs - Documentation generation</li> <li>Material for MkDocs - Documentation theme</li> </ul>"},{"location":"#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"#beginners","title":"Beginners","text":"<ol> <li>Getting Started - Installation and first program</li> <li>Basic Tutorial - Simple neural network training</li> <li>API Reference - Common API usage</li> </ol>"},{"location":"#advanced-users","title":"Advanced Users","text":"<ol> <li>Architecture Design - Deep understanding of system design</li> <li>Custom Operations - Implementing custom operations</li> <li>Performance Optimization - Performance analysis and optimization guide</li> <li>Performance Tuning - Training performance tuning techniques</li> <li>Qwen Model Guide - Using and training Qwen LLM models</li> </ol>"},{"location":"#contributors","title":"Contributors","text":"<ol> <li>Development Environment - Setting up development environment</li> <li>Core Components - Understanding internal implementation</li> <li>Testing Guidelines - Code contribution guidelines</li> </ol>"},{"location":"#project-highlights","title":"\ud83c\udf1f Project Highlights","text":""},{"location":"#code-quality","title":"Code Quality","text":"<ul> <li>Type Annotations - Complete type hints, IDE-friendly</li> <li>Unit Tests - 95%+ test coverage</li> <li>Complete Documentation - Comprehensive documentation from API to design</li> <li>Code Standards - Unified code style and best practices</li> </ul>"},{"location":"#recent-updates-2025-01","title":"Recent Updates (2025-01)","text":"<ul> <li>\u2705 Distributed Training - Complete NCCL multi-GPU parallel training support</li> <li>\u2705 Memory Allocator Optimization - Achieved PyTorch-level performance</li> <li>\u2705 Qwen Model Support - Complete Qwen LLM architecture implementation</li> <li>\u2705 Mixed Precision Training - Enhanced FP16/BF16 Automatic Mixed Precision (AMP)</li> <li>\u2705 Gradient Clipping - Support for gradient norm and value clipping</li> <li>\u2705 Learning Rate Schedulers - StepLR, ExponentialLR, CosineAnnealingLR</li> <li>\u2705 Checkpoint System - Model save/load with optimizer state preservation</li> <li>\u2705 Tensor Validation - isinf, isnan, isfinite checking functions</li> <li>\ud83d\udd04 Operator Performance - Fixing critical operators (cat, logsumexp, broadcast)</li> <li>\ud83d\udd04 Kernel Optimization - Continuous Triton kernel improvements</li> </ul>"},{"location":"#community-contribution","title":"\ud83e\udd1d Community &amp; Contribution","text":"<p>We welcome all forms of contributions:</p> <ul> <li>\ud83d\udc1b Bug Reports - Please report bugs in a timely manner</li> <li>\ud83d\udca1 Feature Suggestions - New feature ideas are welcome</li> <li>\ud83d\udcdd Documentation Improvements - Help improve documentation quality</li> <li>\ud83d\udcbb Code Contributions - Direct participation in code development</li> </ul> <p>For details, please refer to Contributing Guide.</p>"},{"location":"#contact","title":"\ud83d\udcde Contact","text":"<ul> <li>GitHub Issues - Bug reports and feature requests</li> <li>Discussions - Technical discussions and usage communication</li> <li>Email - genesis-dev@example.com</li> </ul> <p>Start Your Deep Learning Journey \ud83d\ude80</p> <ul> <li> <p> Getting Started</p> <p>Start building your first neural network with Genesis immediately</p> <p> Getting Started</p> </li> <li> <p> View Source</p> <p>Explore the complete Genesis source code implementation on GitHub</p> <p> GitHub Repository</p> </li> </ul>"},{"location":"memory-allocator-optimization/","title":"Genesis GPU Memory Allocator Performance Optimization Journey","text":"<p>A complete technical blog documenting the performance optimization process of Genesis deep learning framework's GPU memory allocator, from problem discovery to progressive solutions</p>"},{"location":"memory-allocator-optimization/#background-why-is-genesis-memory-allocation-so-slow","title":"Background: Why is Genesis Memory Allocation So Slow?","text":"<p>When using our self-developed Genesis deep learning framework, we discovered a serious performance issue: GPU memory allocation was much slower than PyTorch. Comparative testing revealed:</p> Text Only<pre><code>CUDAStorage allocation vs PyTorch:\n- 1K elements:   Genesis 0.58x PyTorch (42% slower)\n- 10K elements:  Genesis 0.75x PyTorch (25% slower) \n- 100K elements: Genesis 0.42x PyTorch (58% slower)\n</code></pre> <p>Even more shocking was the <code>fill_</code> operation performance:</p> Text Only<pre><code>Fill operation (before optimization):\n- 512\u00d7512:   Genesis 0.10x PyTorch (10x slower!)\n- 1024\u00d71024: Genesis 0.03x PyTorch (33x slower!)\n- 2048\u00d72048: Genesis 0.01x PyTorch (100x slower!)\n</code></pre> <p>This was clearly unacceptable. We needed to deeply analyze the problem and develop an optimization strategy.</p>"},{"location":"memory-allocator-optimization/#step-1-establish-performance-baseline-testing","title":"Step 1: Establish Performance Baseline Testing","text":"<p>Any optimization must first establish accurate baseline testing. We needed to understand: 1. How slow is current allocation performance exactly? 2. Which allocation patterns are performance bottlenecks? 3. What are the specific gaps compared to PyTorch?</p>"},{"location":"memory-allocator-optimization/#design-benchmark-tests","title":"Design Benchmark Tests","text":"<p>I created a dedicated memory manager benchmark tool <code>benchmark/bench_memory_manager.py</code>, testing the following key patterns:</p> <ol> <li>Same-size Repeated Allocation - Simulate training loops</li> <li>Allocation-Release Cycles - Test memory reuse capability  </li> <li>Variable-size Allocation - Simulate batch size changes</li> <li>Large Memory Allocation - Test large block memory behavior</li> <li>PyTorch Cache Analysis - Deep understanding of PyTorch's caching mechanism</li> </ol>"},{"location":"memory-allocator-optimization/#baseline-test-results","title":"Baseline Test Results","text":"<p>After running tests, the results were shocking:</p> Text Only<pre><code>\ud83d\udd34 Overall Performance Statistics:\n- Average speedup ratio: 0.16x (Genesis 6x slower than PyTorch!)\n- Worst speedup ratio: 0.02x (allocation-release cycles 50x slower!)\n- Best speedup ratio: 0.38x (still 2.6x slower)\n\n\ud83d\udcca By Pattern Category:\n- Same-size repeated allocation:    0.22x (poor)\n- Allocation-release cycles:        0.02x (severe!)  \n- Variable-size allocation:         0.12x (severe)\n- Large memory allocation:          0.20x (poor)\n</code></pre>"},{"location":"memory-allocator-optimization/#key-findings","title":"Key Findings","text":""},{"location":"memory-allocator-optimization/#1-pytorch-caching-effect-is-amazing","title":"1. PyTorch Caching Effect is Amazing","text":"Text Only<pre><code>PyTorch 1024\u00d71024 allocation behavior:\n- First allocation (cold start): 0.458ms\n- Second allocation (cache hit): 0.021ms  \n- Cache speedup ratio: 22x!\n\n10 consecutive allocations:\n- PyTorch average: 0.015ms \n- Genesis average: 0.925ms\n- Steady-state performance gap: 62x!\n</code></pre>"},{"location":"memory-allocator-optimization/#2-genesis-has-no-caching","title":"2. Genesis Has No Caching","text":"<p>Genesis allocation time is consistently similar (0.9-1.0ms), indicating it indeed calls <code>cudaMalloc</code> every time with no caching mechanism.</p>"},{"location":"memory-allocator-optimization/#3-allocation-release-cycles-are-the-biggest-bottleneck","title":"3. Allocation-Release Cycles are the Biggest Bottleneck","text":"Text Only<pre><code>Allocation-release cycle performance (20 cycles):\n- 1024\u00d71024: PyTorch 0.149ms vs Genesis 5.116ms (34x slower!)\n</code></pre> <p>This confirmed expert analysis: <code>cudaFree</code>'s implicit synchronization severely impacts performance.</p>"},{"location":"memory-allocator-optimization/#optimization-direction-determined","title":"Optimization Direction Determined","text":"<p>Based on test results, our optimization priorities are very clear:</p> <ol> <li>\ud83d\udd34 Urgent: Implement basic cache pool to solve repeated <code>cudaMalloc/cudaFree</code> issues</li> <li>\ud83d\udfe0 Important: Optimize memory reuse strategy, especially allocation-release cycles</li> <li>\ud83d\udfe1 Improvement: Handle variable-size allocation patterns</li> </ol> <p>Now let's start Phase 1 optimization.</p>"},{"location":"memory-allocator-optimization/#step-2-implement-simple-cache-pool","title":"Step 2: Implement Simple Cache Pool","text":"<p>Based on baseline test discoveries, we first implement a simple memory cache pool to avoid frequent <code>cudaMalloc/cudaFree</code> calls.</p>"},{"location":"memory-allocator-optimization/#phase-1-design-approach","title":"Phase 1 Design Approach","text":"<p>I implemented a minimal viable cache allocator with the following features:</p> <ol> <li>512B Alignment: All allocations aligned to 512-byte boundaries</li> <li>Exact Size Matching: Cache by exact size to avoid memory waste</li> <li>Simple Free List: Use <code>defaultdict(list)</code> to implement size -&gt; [ptr_list] mapping</li> <li>Immediate Recycling: Return to cache immediately upon release (if cache not full)</li> <li>Single Stream Friendly: Current version doesn't handle cross-stream, focuses on validating cache effect</li> </ol>"},{"location":"memory-allocator-optimization/#core-implementation","title":"Core Implementation","text":"Python<pre><code>class CUDAMemoryManager:\n    def __init__(self):\n        # Phase 1: Simple caching allocator\n        self.free_blocks = defaultdict(list)  # size -&gt; [ptr_list] \n        self.active_blocks = {}  # ptr -&gt; size\n        self.alignment = 512  # 512B alignment\n        self.max_cache_size = 1024 * 1024 * 1024  # 1GB cache limit\n\n    def allocate(self, nbytes: int, stream=None) -&gt; int:\n        aligned_size = self._round_up(nbytes, self.alignment)\n\n        # Try cache first\n        if self.free_blocks[aligned_size]:\n            ptr = self.free_blocks[aligned_size].pop()\n            self.cache_hits += 1\n            return ptr\n\n        # Cache miss - allocate from CUDA\n        ptr = cuda.cuMemAlloc(aligned_size)\n        self.cache_misses += 1\n        return ptr\n\n    def free(self, ptr: int, stream=None):\n        size = self.active_blocks.pop(ptr)\n\n        # Return to cache if not full\n        if self.current_cache_size + size &lt;= self.max_cache_size:\n            self.free_blocks[size].append(ptr)\n            return\n\n        # Cache full - actually free\n        cuda.cuMemFree(ptr)\n</code></pre>"},{"location":"memory-allocator-optimization/#phase-1-optimization-results","title":"Phase 1 Optimization Results","text":"<p>Simple cache allocator benchmark results:</p>"},{"location":"memory-allocator-optimization/#performance-results","title":"Performance Results","text":"Text Only<pre><code>Average speedup ratio: 0.98x\nMedian speedup ratio: 0.65x  \nPerformance range: 0.03x ~ 2.85x\n</code></pre>"},{"location":"memory-allocator-optimization/#scenario-analysis","title":"Scenario Analysis","text":"<p>Well-performing scenarios: - Same-size repeated allocation: 1.43x - Large memory allocation: 1.29x - Inference dynamic batches: 1.01x</p> <p>Average-performing scenarios: - Allocation-release cycles: 0.84x - Variable-size allocation: 0.51x</p> <p>Poorly-performing scenarios: - Transformer training: 0.04x - Gradient accumulation: 0.03x - Memory pressure: 0.08x</p>"},{"location":"memory-allocator-optimization/#main-findings","title":"Main Findings","text":"<p>Cache mechanism validation: - Exact size matching is effective for repeated allocation scenarios - Large allocations (\u2265100K elements) average 1.20x - Small allocations (&lt;100K elements) only 0.46x, becoming performance bottleneck</p> <p>Limitations: - Low cache hit rate in complex scenarios - Exact matching strategy unsuitable for diverse memory patterns - Need more flexible caching strategies</p>"},{"location":"memory-allocator-optimization/#step-2-next-phase-optimization-plan","title":"Step 2: Next Phase Optimization Plan","text":"<p>Based on Phase 1 results analysis, determine optimization priorities:</p>"},{"location":"memory-allocator-optimization/#core-problem-diagnosis","title":"Core Problem Diagnosis","text":"<ol> <li>Poor small allocation performance: &lt;100K element scenarios drag down overall performance</li> <li>Complex scenario failures: Extremely low cache hit rates in diverse memory patterns</li> <li>Exact matching limitations: Current strategy unsuitable for scenarios with large size variations</li> </ol>"},{"location":"memory-allocator-optimization/#phase-2-optimization-plan-size-bucket-caching","title":"Phase 2 Optimization Plan: Size Bucket Caching","text":"<p>Goal: Improve cache hit rate, solve variable-size allocation issues</p> <p>Core improvements: - Change exact matching to bucket matching (like 64B, 128B, 256B, 512B...) - Reduce memory fragmentation, improve reuse rate - Prioritize solving small allocation performance issues</p> <p>Expected effects: - Variable-size allocation from 0.51x to 0.8x+ - Complex scenario performance improvement - Overall average performance from 0.98x to 1.2x+</p>"},{"location":"memory-allocator-optimization/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Design bucket size strategy (powers of 2 vs fixed steps)</li> <li>Implement bucket matching allocation logic</li> <li>Benchmark test to verify effects</li> <li>Based on results, decide whether to proceed to Phase 3 (block allocator)</li> </ol> <p>Current Phase 1 has established a stable foundation, can begin Phase 2 development.</p>"},{"location":"memory-allocator-optimization/#phase-2-implementation-size-bucket-cache-optimization","title":"Phase 2 Implementation: Size Bucket Cache Optimization","text":""},{"location":"memory-allocator-optimization/#core-improvements","title":"Core Improvements","text":"<p>Changed exact size matching to bucket matching strategy: - Use powers of 2 buckets: 512B, 1KB, 2KB, 4KB... - Maximum bucket limit 16MB, use exact alignment beyond - Improve cache hit rate for variable-size scenarios</p>"},{"location":"memory-allocator-optimization/#implementation-results","title":"Implementation Results","text":""},{"location":"memory-allocator-optimization/#overall-performance-comparison","title":"Overall Performance Comparison","text":"Text Only<pre><code>Phase 1 \u2192 Phase 2:\nAverage speedup ratio: 0.98x \u2192 0.97x (slight decrease)\nMedian speedup ratio: 0.65x \u2192 0.88x (significant improvement +35%)\n</code></pre>"},{"location":"memory-allocator-optimization/#per-scenario-performance-changes","title":"Per-scenario Performance Changes","text":"<p>Significantly improved scenarios: - Variable-size allocation: 0.51x \u2192 0.83x (+63%) - Memory pressure: 0.08x \u2192 1.48x (+1750%) - Inference dynamic batches: 1.01x \u2192 1.40x (+39%) - Allocation-release cycles: 0.84x \u2192 1.01x (+20%)</p> <p>Performance decline scenarios: - Same-size repeated allocation: 1.43x \u2192 0.90x (-37%)</p> <p>Still severe bottlenecks: - Transformer training: 0.04x \u2192 0.05x (almost no improvement) - Gradient accumulation: 0.03x \u2192 0.07x (minor improvement)</p>"},{"location":"memory-allocator-optimization/#phase-2-technical-assessment","title":"Phase 2 Technical Assessment","text":"<p>Successfully validated: - Bucket matching effectively improved cache hit rate for variable-size scenarios - Large median performance improvement shows most scenarios benefited - Breakthrough in memory pressure scenarios proves value of bucket caching</p> <p>Problems discovered: - Bucket caching introduces memory waste, affecting same-size allocation performance - Complex training scenarios (Transformer/gradient accumulation) still not fundamentally improved - Need deeper optimization strategies to solve core bottlenecks</p>"},{"location":"memory-allocator-optimization/#transformer-scenario-bottleneck-root-cause-analysis","title":"Transformer Scenario Bottleneck Root Cause Analysis","text":"<p>Through deep analysis, found fundamental reasons why bucket caching is ineffective for complex training scenarios:</p>"},{"location":"memory-allocator-optimization/#large-tensors-exceed-bucket-limits","title":"Large Tensors Exceed Bucket Limits","text":"<ul> <li>Logits tensors reach 78MB-313MB, far exceeding 16MB bucket limit</li> <li>Ultra-large tensors fall back to exact alignment, cannot enjoy bucket caching advantages</li> <li>Frequent large memory cudaMalloc calls become main overhead</li> </ul>"},{"location":"memory-allocator-optimization/#architectural-level-differences","title":"Architectural-level Differences","text":"Text Only<pre><code>PyTorch block allocator advantages:\n- Pre-allocate large memory pools (512MB-2GB)\n- Slice tensors from memory pool, avoiding cudaMalloc\n- Return to pool upon release, achieving true zero-overhead reuse\n\nGenesis bucket cache limitations:\n- Each tensor still needs independent cudaMalloc\n- Cannot utilize fundamental advantages of memory pools\n- Large tensors completely bypass caching mechanism\n</code></pre>"},{"location":"memory-allocator-optimization/#performance-bottleneck-truth","title":"Performance Bottleneck Truth","text":"<ul> <li>60 tensors, mostly 4MB-320MB level</li> <li>cudaMalloc system call overhead for large memory blocks is huge</li> <li>No matter how high cache hit rate, cannot mask fundamental architectural issues</li> </ul> <p>Conclusion: Bucket caching is incremental improvement but cannot solve fundamental problems of large-scale training. Need to implement PyTorch-style block allocator to truly break through performance bottlenecks.</p>"},{"location":"memory-allocator-optimization/#phase-3-implementation-block-allocator","title":"Phase 3 Implementation: Block Allocator","text":""},{"location":"memory-allocator-optimization/#core-design","title":"Core Design","text":"<p>Implement PyTorch-style block allocator to solve fundamental performance issues of large memory allocation: - Pre-allocate large memory segments (1GB) as memory pool - Use best-fit algorithm to slice blocks from pool - Return to pool upon release, support block merging to reduce fragmentation - Layered architecture: &lt;1MB use bucket cache, \u22651MB use block allocator</p>"},{"location":"memory-allocator-optimization/#implementation-results_1","title":"Implementation Results","text":""},{"location":"memory-allocator-optimization/#overall-performance-comparison_1","title":"Overall Performance Comparison","text":"Text Only<pre><code>Phase 2 \u2192 Phase 3:\nAverage speedup ratio: 0.97x \u2192 1.41x (+45% improvement)\nMedian speedup ratio: 0.88x \u2192 0.81x (slight decrease)\nBest performance: 2.60x \u2192 4.83x (new performance peak)\n</code></pre>"},{"location":"memory-allocator-optimization/#major-breakthroughs-in-key-scenarios","title":"Major Breakthroughs in Key Scenarios","text":"<p>Dramatically improved scenarios: - Transformer training: 0.05x \u2192 1.89x (+3680%, from severe bottleneck to surpassing PyTorch) - Large memory allocation: 1.29x \u2192 3.92x (+204%, significantly better than PyTorch) - Large-size repeated allocation: from Phase 1's 0.27x to Phase 3's 2.31x</p> <p>Stable scenarios: - Small allocation scenarios basically maintain original levels - Practical scenarios like inference services perform stably</p> <p>Still need improvement scenarios: - Gradient accumulation: 0.07x \u2192 0.18x (improved but still poor) - Variable-size allocation: 0.83x \u2192 0.34x (affected by layered strategy)</p>"},{"location":"memory-allocator-optimization/#technical-achievements","title":"Technical Achievements","text":"<p>Successfully solved core problems: - Completely eliminated cudaMalloc system call overhead in large allocation scenarios - Achieved true memory pool reuse mechanism - Validated effectiveness of block allocator architecture</p> <p>Technical architecture success: - Layered allocation strategy works properly - Good utilization rate of 1GB memory segments - Best-fit algorithm and block merging mechanism effective</p> <p>Limitation recognition: - Small allocation scenarios still have room for improvement - Some special scenarios (like gradient accumulation) need further tuning - Compared to mature PyTorch, still gaps in some specific scenarios</p>"},{"location":"memory-allocator-optimization/#phase-3-assessment","title":"Phase 3 Assessment","text":"<p>Block Allocator successfully solved the most critical large memory allocation bottleneck, enabling Genesis to achieve or even surpass PyTorch performance in important scenarios. While not all scenarios are perfect, it has transformed from \"severely lagging\" to \"basically usable, leading in some scenarios.\"</p> <p>This establishes a solid foundation for Genesis applications in actual deep learning tasks.</p>"},{"location":"memory-allocator-optimization/#optimization-journey-summary","title":"Optimization Journey Summary","text":"<p>From initial \"catastrophic performance\" (0.02x) to current \"overall leading\" (1.41x), this memory allocator optimization achieved substantial breakthroughs:</p>"},{"location":"memory-allocator-optimization/#progressive-improvements-in-three-phases","title":"Progressive Improvements in Three Phases","text":"<ul> <li>Phase 1: Solved most basic memory reuse issues, established optimization foundation</li> <li>Phase 2: Improved cache hit rate for variable-size scenarios, enhanced median performance  </li> <li>Phase 3: Completely solved large memory allocation bottlenecks, achieved qualitative leap</li> </ul>"},{"location":"memory-allocator-optimization/#technical-route-correctness-validation","title":"Technical Route Correctness Validation","text":"<p>Through systematic benchmark testing and root cause analysis, we accurately identified performance bottlenecks and chose correct technical solutions. Each phase had clear goals and measurable results.</p>"},{"location":"memory-allocator-optimization/#practical-value","title":"Practical Value","text":"<p>Genesis can now provide acceptable memory allocation performance in most practical scenarios, particularly competitive in critical scenarios like large-scale model training.</p> <p>Of course, this is only a phase result in memory management optimization. Future could consider more optimization directions, such as multi-stream concurrency, NUMA awareness, or specialized optimizations for specific models.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Genesis deep learning framework provides complete API interfaces. This section provides detailed code-level documentation and usage examples.</p>"},{"location":"api-reference/#core-module-structure","title":"Core Module Structure","text":""},{"location":"api-reference/#main-namespaces","title":"Main Namespaces","text":"<ul> <li><code>genesis</code>: Core tensor and automatic differentiation system</li> <li><code>genesis.nn</code>: Neural network modules and layers  </li> <li><code>genesis.optim</code>: Optimizers and learning rate schedulers</li> <li><code>genesis.functional</code>: Functional operation interfaces</li> <li><code>genesis.utils</code>: Utility functions and helper classes</li> </ul>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":"Module Description Main Classes/Functions genesis Core tensor system <code>Tensor</code>, <code>autocast</code>, <code>no_grad</code> nn Neural network layers <code>Module</code>, <code>Linear</code>, <code>MultiHeadAttention</code> optim Optimizers <code>SGD</code>, <code>Adam</code>, <code>AdamW</code> functional Functional operations <code>relu</code>, <code>softmax</code>, <code>matmul</code> utils Utility functions <code>profile</code>, <code>DataLoader</code>"},{"location":"api-reference/#code-conventions","title":"Code Conventions","text":""},{"location":"api-reference/#import-standards","title":"Import Standards","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport genesis.nn.functional as F\n</code></pre>"},{"location":"api-reference/#device-management","title":"Device Management","text":"Python<pre><code># Set default device\ngenesis.set_default_device(genesis.cuda())\n\n# Check CUDA availability\nif genesis.cuda.is_available():\n    device = genesis.cuda()\nelse:\n    device = genesis.cpu()\n</code></pre>"},{"location":"api-reference/#data-types","title":"Data Types","text":"Python<pre><code># Supported data types\ngenesis.float32  # Default float type\ngenesis.float16  # Half precision float\ngenesis.int32    # 32-bit integer\ngenesis.bool     # Boolean type\n</code></pre>"},{"location":"api-reference/#quick-examples","title":"Quick Examples","text":""},{"location":"api-reference/#basic-tensor-operations","title":"Basic Tensor Operations","text":"Python<pre><code>import genesis\n\n# Create tensors\nx = genesis.tensor([[1, 2], [3, 4]], dtype=genesis.float32)\ny = genesis.randn(2, 2)\n\n# Basic operations\nz = x + y\nresult = genesis.matmul(x, y.T)\n\n# Gradient computation\nx.requires_grad_(True)\nloss = (x ** 2).sum()\nloss.backward()\nprint(x.grad)  # Print gradients\n</code></pre>"},{"location":"api-reference/#neural-network-model","title":"Neural Network Model","text":"Python<pre><code>import genesis.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Use model\nmodel = MLP(784, 256, 10)\nx = genesis.randn(32, 784)\noutput = model(x)\n</code></pre>"},{"location":"api-reference/#training-loop","title":"Training Loop","text":"Python<pre><code>import genesis.optim as optim\n\n# Initialize\nmodel = MLP(784, 256, 10)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training step\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"api-reference/#performance-optimization-tips","title":"Performance Optimization Tips","text":""},{"location":"api-reference/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable automatic mixed precision\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    output = model(input_tensor)\n    loss = criterion(output, target)\n</code></pre>"},{"location":"api-reference/#gpu-memory-optimization","title":"GPU Memory Optimization","text":"Python<pre><code># Use inplace operations to reduce memory usage\nx.relu_()  # inplace ReLU\nx.add_(y)  # inplace addition\n\n# Release unnecessary gradients\nwith genesis.no_grad():\n    inference_result = model(data)\n</code></pre>"},{"location":"api-reference/#batch-operation-optimization","title":"Batch Operation Optimization","text":"Python<pre><code># Batch matrix multiplication\nbatch_result = genesis.bmm(batch_a, batch_b)\n\n# Vectorized operations instead of loops\nresult = genesis.sum(tensor, dim=1, keepdim=True)\n</code></pre>"},{"location":"api-reference/device/","title":"Device API Reference","text":"<p>The Device module provides a unified interface for managing computation devices (CPU and CUDA) in Genesis.</p>"},{"location":"api-reference/device/#device-class","title":"Device Class","text":"Python<pre><code>genesis.device(device_type)\n</code></pre>"},{"location":"api-reference/device/#parameters","title":"Parameters","text":"<ul> <li>device_type: String specifying the device</li> <li><code>'cpu'</code>: CPU device</li> <li><code>'cuda'</code>: Default CUDA device (GPU 0)</li> <li><code>'cuda:0'</code>, <code>'cuda:1'</code>, etc.: Specific CUDA device by index</li> </ul>"},{"location":"api-reference/device/#properties","title":"Properties","text":"<ul> <li>type: Device type ('cpu' or 'cuda')</li> <li>index: Device index (for CUDA devices)</li> </ul>"},{"location":"api-reference/device/#methods","title":"Methods","text":"Python<pre><code>device.is_available()    # Check if device is available\n</code></pre>"},{"location":"api-reference/device/#global-functions","title":"Global Functions","text":""},{"location":"api-reference/device/#device-query","title":"Device Query","text":"<p>Python<pre><code>genesis.cuda_available()\n</code></pre> Check if CUDA is available on the system.</p> <p>Returns: <code>bool</code> - True if CUDA is available, False otherwise</p>"},{"location":"api-reference/device/#current-device-management","title":"Current Device Management","text":"Python<pre><code># Get current default device\ngenesis.device.get_default_device()\n\n# Set default device\ngenesis.device.set_default_device(device)\n</code></pre>"},{"location":"api-reference/device/#device-context","title":"Device Context","text":"<p>Genesis automatically manages device context for operations. All tensors and operations are executed on their respective devices.</p>"},{"location":"api-reference/device/#automatic-device-selection","title":"Automatic Device Selection","text":"<p>When creating tensors without specifying a device: - If CUDA is available and enabled, uses CUDA by default - Otherwise, uses CPU</p>"},{"location":"api-reference/device/#examples","title":"Examples","text":""},{"location":"api-reference/device/#basic-device-usage","title":"Basic Device Usage","text":"Python<pre><code>import genesis\n\n# Create device objects\ncpu_device = genesis.device('cpu')\ncuda_device = genesis.device('cuda')\n\n# Check availability\nif cuda_device.is_available():\n    print(\"CUDA is available\")\n    device = cuda_device\nelse:\n    print(\"Using CPU\")\n    device = cpu_device\n\n# Create tensor on specific device\nx = genesis.randn(100, 100, device=device)\n</code></pre>"},{"location":"api-reference/device/#multi-gpu-support","title":"Multi-GPU Support","text":"Python<pre><code>import genesis\n\n# Check number of GPUs\nif genesis.cuda_available():\n    # Use specific GPU\n    device0 = genesis.device('cuda:0')\n    device1 = genesis.device('cuda:1')\n\n    # Create tensors on different GPUs\n    x = genesis.randn(100, 100, device=device0)\n    y = genesis.randn(100, 100, device=device1)\n\n    # Move tensor to different device\n    y_on_device0 = y.to(device0)\n    result = x + y_on_device0  # Operations require same device\n</code></pre>"},{"location":"api-reference/device/#device-agnostic-code","title":"Device-Agnostic Code","text":"Python<pre><code>import genesis\n\ndef get_device():\n    \"\"\"Get the best available device\"\"\"\n    if genesis.cuda_available():\n        return genesis.device('cuda')\n    return genesis.device('cpu')\n\n# Write device-agnostic code\ndevice = get_device()\n\n# All tensors created on the selected device\nmodel_weights = genesis.randn(784, 256, device=device)\ninput_data = genesis.randn(32, 784, device=device)\n\n# Computation happens on the selected device\noutput = input_data @ model_weights\n</code></pre>"},{"location":"api-reference/device/#device-memory-management","title":"Device Memory Management","text":"Python<pre><code>import genesis\n\nif genesis.cuda_available():\n    device = genesis.device('cuda')\n\n    # Create large tensor on GPU\n    large_tensor = genesis.randn(10000, 10000, device=device)\n\n    # Process data\n    result = large_tensor.sum()\n\n    # Move result to CPU for further processing\n    result_cpu = result.cpu()\n    print(f\"Sum: {result_cpu.item()}\")\n</code></pre>"},{"location":"api-reference/device/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Device Transfers: Moving data between devices (CPU \u2194 GPU) is expensive. Minimize transfers.</p> </li> <li> <p>Batch Operations: GPUs excel at parallel operations. Use larger batch sizes when possible.</p> </li> <li> <p>Memory Management: GPU memory is limited. Monitor usage and free unused tensors.</p> </li> <li> <p>Device Placement: Ensure all tensors in an operation are on the same device.</p> </li> </ol>"},{"location":"api-reference/device/#environment-variables","title":"Environment Variables","text":"<p>Genesis respects the following environment variables:</p> <ul> <li> <p><code>CUDA_VISIBLE_DEVICES</code>: Limit which GPUs are visible to Genesis   Bash<pre><code># Only use GPU 0 and 1\nCUDA_VISIBLE_DEVICES=0,1 python script.py\n</code></pre></p> </li> <li> <p><code>GENESIS_DEVICE</code>: Set default device   Bash<pre><code># Force CPU usage\nGENESIS_DEVICE=cpu python script.py\n</code></pre></p> </li> </ul>"},{"location":"api-reference/device/#see-also","title":"See Also","text":"<ul> <li>Tensor API - Tensor operations and device placement</li> <li>CUDA Backend - CUDA-specific implementation details</li> <li>Memory Management - Advanced memory management</li> </ul>"},{"location":"api-reference/functional/","title":"Functional API Reference","text":"<p>Functional interface for Genesis operations - stateless functions for neural network operations.</p>"},{"location":"api-reference/functional/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/functional/#activation-functions","title":"Activation Functions","text":"<ul> <li><code>F.relu(x)</code> - ReLU activation</li> <li><code>F.softmax(x, dim=-1)</code> - Softmax</li> <li><code>F.gelu(x)</code> - GELU activation</li> </ul>"},{"location":"api-reference/functional/#loss-functions","title":"Loss Functions","text":"<ul> <li><code>F.mse_loss(input, target)</code> - Mean squared error</li> <li><code>F.cross_entropy(input, target)</code> - Cross entropy loss</li> </ul>"},{"location":"api-reference/functional/#convolution-operations","title":"Convolution Operations","text":"<ul> <li><code>F.conv2d(input, weight, bias)</code> - 2D convolution</li> <li><code>F.linear(input, weight, bias)</code> - Linear transformation</li> </ul>"},{"location":"api-reference/functional/#tensor-manipulation-functions","title":"Tensor Manipulation Functions","text":""},{"location":"api-reference/functional/#sortinput-dim-1-descendingfalse-stablefalse","title":"<code>sort(input, dim=-1, descending=False, stable=False)</code>","text":"<p>Sort elements along a dimension.</p> Python<pre><code>values, indices = genesis.sort(tensor, dim=1, descending=False)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor - <code>dim</code>: Dimension along which to sort - <code>descending</code>: If True, sort in descending order - <code>stable</code>: If True, stable sort (preserves order of equal elements)</p> <p>Returns: Tuple of (values, indices) tensors</p>"},{"location":"api-reference/functional/#topkinput-k-dim-1-largesttrue-sortedtrue","title":"<code>topk(input, k, dim=-1, largest=True, sorted=True)</code>","text":"<p>Returns the k largest/smallest elements along a dimension.</p> Python<pre><code>values, indices = genesis.topk(tensor, k=3, dim=1, largest=True)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor - <code>k</code>: Number of top values to return - <code>dim</code>: Dimension along which to find top-k values - <code>largest</code>: If True, return largest values; if False, return smallest - <code>sorted</code>: If True, return values in sorted order</p> <p>Returns: Tuple of (values, indices) tensors</p>"},{"location":"api-reference/functional/#argsortinput-dim-1-descendingfalse","title":"<code>argsort(input, dim=-1, descending=False)</code>","text":"<p>Returns indices that sort a tensor along a dimension.</p> Python<pre><code>indices = genesis.argsort(tensor, dim=1, descending=False)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor - <code>dim</code>: Dimension along which to sort - <code>descending</code>: If True, sort in descending order</p> <p>Returns: Tensor of indices</p>"},{"location":"api-reference/functional/#gatherinput-dim-index","title":"<code>gather(input, dim, index)</code>","text":"<p>Gather values along an axis specified by index.</p> Python<pre><code>output = genesis.gather(tensor, dim=1, index=indices)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor - <code>dim</code>: Dimension along which to gather - <code>index</code>: Index tensor with same number of dimensions as input</p> <p>Returns: Tensor containing gathered values</p>"},{"location":"api-reference/functional/#scatter_addinput-dim-index-src","title":"<code>scatter_add(input, dim, index, src)</code>","text":"<p>Add values from src to input at positions specified by index.</p> Python<pre><code>genesis.scatter_add(tensor, dim=1, index=indices, src=values)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor (modified in-place) - <code>dim</code>: Dimension along which to scatter - <code>index</code>: Index tensor - <code>src</code>: Source tensor containing values to add</p> <p>Returns: Modified input tensor</p>"},{"location":"api-reference/functional/#bincountinput-weightsnone-minlength0","title":"<code>bincount(input, weights=None, minlength=0)</code>","text":"<p>Count occurrences of each value in integer tensor.</p> Python<pre><code>counts = genesis.bincount(tensor, minlength=10)\n</code></pre> <p>Parameters: - <code>input</code>: 1D integer tensor - <code>weights</code>: Optional weights tensor - <code>minlength</code>: Minimum length of output</p> <p>Returns: Tensor containing counts</p>"},{"location":"api-reference/functional/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/functional/#allcloseinput-other-rtol1e-05-atol1e-08-equal_nanfalse","title":"<code>allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)</code>","text":"<p>Test if all elements of input and other are close.</p> Python<pre><code>result = genesis.allclose(tensor1, tensor2, rtol=1e-5, atol=1e-8)\n</code></pre> <p>Parameters: - <code>input</code>: First tensor - <code>other</code>: Second tensor - <code>rtol</code>: Relative tolerance - <code>atol</code>: Absolute tolerance - <code>equal_nan</code>: Whether to consider NaN values as equal</p> <p>Returns: Boolean scalar tensor</p>"},{"location":"api-reference/functional/#tensor-validation-functions","title":"Tensor Validation Functions","text":""},{"location":"api-reference/functional/#isinfinput","title":"<code>isinf(input)</code>","text":"<p>Test each element to see if it is infinite (positive or negative infinity).</p> Python<pre><code>inf_mask = genesis.isinf(tensor)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor</p> <p>Returns: Boolean tensor with same shape as input</p>"},{"location":"api-reference/functional/#isnaninput","title":"<code>isnan(input)</code>","text":"<p>Test each element to see if it is NaN (Not a Number).</p> Python<pre><code>nan_mask = genesis.isnan(tensor)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor</p> <p>Returns: Boolean tensor with same shape as input</p>"},{"location":"api-reference/functional/#isfiniteinput","title":"<code>isfinite(input)</code>","text":"<p>Test each element to see if it is finite (not infinite and not NaN).</p> Python<pre><code>finite_mask = genesis.isfinite(tensor)\n</code></pre> <p>Parameters: - <code>input</code>: Input tensor</p> <p>Returns: Boolean tensor with same shape as input</p>"},{"location":"api-reference/functional/#distributed-training-functions","title":"Distributed Training Functions","text":""},{"location":"api-reference/functional/#genesisdistributedinit_process_groupbackend-world_size-rank","title":"<code>genesis.distributed.init_process_group(backend, world_size, rank)</code>","text":"<p>Initialize distributed process group.</p> Python<pre><code>import genesis.distributed as dist\ndist.init_process_group(backend='nccl', world_size=2, rank=0)\n</code></pre> <p>Parameters: - <code>backend</code>: Communication backend ('nccl' for GPU) - <code>world_size</code>: Total number of processes - <code>rank</code>: Rank of current process</p>"},{"location":"api-reference/functional/#genesisdistributeddistributeddataparallelmodel-device_idsnone","title":"<code>genesis.distributed.DistributedDataParallel(model, device_ids=None)</code>","text":"<p>Distributed data parallel wrapper.</p> Python<pre><code>ddp_model = dist.DistributedDataParallel(model, device_ids=[0])\n</code></pre> <p>Parameters: - <code>model</code>: Model to wrap - <code>device_ids</code>: List of GPU device IDs</p> <p>Returns: DDP-wrapped model</p>"},{"location":"api-reference/functional/#creation-functions","title":"Creation Functions","text":""},{"location":"api-reference/functional/#eyen-mnone-devicenone-dtypegenesisfloat32","title":"<code>eye(n, m=None, device=None, dtype=genesis.float32)</code>","text":"<p>Generate identity matrix.</p> Python<pre><code>identity = genesis.eye(5)  # 5x5 identity matrix\nrect_matrix = genesis.eye(3, 5)  # 3x5 matrix\n</code></pre>"},{"location":"api-reference/functional/#ones_liketensor-dtypenone-devicenone","title":"<code>ones_like(tensor, dtype=None, device=None)</code>","text":"<p>Generate ones tensor with same shape as input.</p> Python<pre><code>ones_tensor = genesis.ones_like(input_tensor)\n</code></pre>"},{"location":"api-reference/functional/#from_numpyarray-devicenone-dtypenone","title":"<code>from_numpy(array, device=None, dtype=None)</code>","text":"<p>Create tensor from numpy array.</p> Python<pre><code>np_array = np.array([1, 2, 3])\ntensor = genesis.from_numpy(np_array)\n</code></pre> <p>This reference covers the main functional operations. For complete API details, see the source documentation.</p>"},{"location":"api-reference/genesis/","title":"Genesis API Reference","text":"<p>This is the main API reference for the Genesis deep learning framework.</p>"},{"location":"api-reference/genesis/#core-functions","title":"Core Functions","text":"<p>Please refer to the specific module documentation for detailed information:</p> <ul> <li>Tensor System - Core tensor operations and automatic differentiation</li> <li>Device Management - Device abstraction and management</li> <li>Neural Networks - Neural network layers</li> <li>Optimizers - Optimization algorithms</li> <li>Random Number Generation - PyTorch-compatible RNG API</li> <li>Memory Management - Advanced CUDA memory management</li> <li>Storage Layer - Storage abstraction and management</li> <li>Utilities - Helper functions</li> </ul> <p>This page is under construction. More detailed API documentation will be added soon.</p>"},{"location":"api-reference/memory/","title":"Memory Management API","text":"<p>Genesis provides advanced CUDA memory management with reference-counted memory pools, comprehensive statistics, and performance optimization features.</p>"},{"location":"api-reference/memory/#device-memory-management","title":"Device Memory Management","text":""},{"location":"api-reference/memory/#device-methods","title":"Device Methods","text":""},{"location":"api-reference/memory/#devicememory_allocated","title":"<code>device.memory_allocated()</code>","text":"<p>Get the current memory allocated on the device.</p> <p>Returns: - int: Memory allocated in bytes</p> <p>Example: Python<pre><code>import genesis\n\ndevice = genesis.device('cuda')\nallocated = device.memory_allocated()\nprint(f\"Allocated: {allocated / 1e6:.1f} MB\")\n</code></pre></p>"},{"location":"api-reference/memory/#devicememory_cached","title":"<code>device.memory_cached()</code>","text":"<p>Get the current memory cached by the allocator.</p> <p>Returns: - int: Memory cached in bytes</p>"},{"location":"api-reference/memory/#devicememory_reserved","title":"<code>device.memory_reserved()</code>","text":"<p>Get the total memory reserved by the allocator.</p> <p>Returns: - int: Memory reserved in bytes</p>"},{"location":"api-reference/memory/#devicemax_memory_allocated","title":"<code>device.max_memory_allocated()</code>","text":"<p>Get the maximum memory allocated during the session.</p> <p>Returns: - int: Peak memory allocated in bytes</p>"},{"location":"api-reference/memory/#devicemax_memory_cached","title":"<code>device.max_memory_cached()</code>","text":"<p>Get the maximum memory cached during the session.</p> <p>Returns: - int: Peak memory cached in bytes</p>"},{"location":"api-reference/memory/#memory-statistics","title":"Memory Statistics","text":""},{"location":"api-reference/memory/#devicememory_stats","title":"<code>device.memory_stats()</code>","text":"<p>Get comprehensive memory usage statistics.</p> <p>Returns: - dict: Dictionary containing detailed memory statistics</p> <p>Statistics included: - <code>allocated_bytes</code>: Current allocated memory - <code>cached_bytes</code>: Current cached memory - <code>reserved_bytes</code>: Total reserved memory - <code>inactive_split_bytes</code>: Memory in inactive splits - <code>active_bytes</code>: Memory in active use - <code>cache_hit_rate</code>: Cache hit rate percentage - <code>num_allocations</code>: Total number of allocations - <code>num_cache_hits</code>: Number of cache hits - <code>num_cache_misses</code>: Number of cache misses - <code>peak_allocated</code>: Peak allocated memory - <code>peak_cached</code>: Peak cached memory</p> <p>Example: Python<pre><code>device = genesis.device('cuda')\nstats = device.memory_stats()\n\nprint(f\"Cache hit rate: {stats['cache_hit_rate']:.1%}\")\nprint(f\"Peak memory: {stats['peak_allocated'] / 1e9:.2f} GB\")\nprint(f\"Allocations: {stats['num_allocations']}\")\n</code></pre></p>"},{"location":"api-reference/memory/#devicememory_summary","title":"<code>device.memory_summary()</code>","text":"<p>Get a human-readable memory usage summary.</p> <p>Returns: - str: Formatted memory usage summary</p> <p>Example: Python<pre><code>device = genesis.device('cuda')\nprint(device.memory_summary())\n</code></pre></p>"},{"location":"api-reference/memory/#memory-control","title":"Memory Control","text":""},{"location":"api-reference/memory/#deviceempty_cache","title":"<code>device.empty_cache()</code>","text":"<p>Clear the memory cache, freeing cached but unused memory.</p> <p>Example: Python<pre><code>device = genesis.device('cuda')\ndevice.empty_cache()  # Free cached memory\n</code></pre></p>"},{"location":"api-reference/memory/#devicereset_memory_stats","title":"<code>device.reset_memory_stats()</code>","text":"<p>Reset all memory statistics counters.</p> <p>Example: Python<pre><code>device = genesis.device('cuda')\ndevice.reset_memory_stats()\n</code></pre></p>"},{"location":"api-reference/memory/#memory-profiling","title":"Memory Profiling","text":""},{"location":"api-reference/memory/#context-manager","title":"Context Manager","text":""},{"location":"api-reference/memory/#genesisprofilerprofile_memory","title":"<code>genesis.profiler.profile_memory()</code>","text":"<p>Context manager for detailed memory profiling.</p> <p>Example: Python<pre><code>import genesis\n\ndevice = genesis.device('cuda')\n\nwith genesis.profiler.profile_memory() as prof:\n    x = genesis.rand(4096, 4096, device=device)\n    y = genesis.matmul(x, x.T)\n    del x, y\n\n# Get detailed memory usage report\nprint(prof.memory_summary())\nprint(f\"Peak usage: {prof.peak_memory() / 1e6:.1f} MB\")\n</code></pre></p>"},{"location":"api-reference/memory/#memory-events","title":"Memory Events","text":"<p>The memory manager tracks detailed allocation and deallocation events:</p>"},{"location":"api-reference/memory/#allocation-events","title":"Allocation Events","text":"<ul> <li>Timestamp</li> <li>Size allocated</li> <li>Memory address</li> <li>Duration of allocation</li> <li>Cache hit/miss status</li> <li>Thread ID</li> </ul>"},{"location":"api-reference/memory/#deallocation-events","title":"Deallocation Events","text":"<ul> <li>Timestamp</li> <li>Size deallocated</li> <li>Memory address</li> <li>Lifetime of allocation</li> <li>Thread ID</li> </ul>"},{"location":"api-reference/memory/#advanced-features","title":"Advanced Features","text":""},{"location":"api-reference/memory/#memory-pool-configuration","title":"Memory Pool Configuration","text":"<p>The memory manager uses different strategies for different allocation sizes:</p> <ul> <li>Small allocations (&lt;1MB): Reference-counted memory pool with caching</li> <li>Large allocations (\u22651MB): Direct CUDA allocation with segment management</li> </ul>"},{"location":"api-reference/memory/#cache-optimization","title":"Cache Optimization","text":"<p>The memory pool automatically optimizes cache performance:</p> <ul> <li>Warmup: Pre-allocates common sizes</li> <li>Hit Rate Tracking: Monitors cache effectiveness</li> <li>Adaptive Sizing: Adjusts pool sizes based on usage patterns</li> </ul>"},{"location":"api-reference/memory/#oom-protection","title":"OOM Protection","text":"<p>The memory manager provides fast-fail OOM handling:</p> Python<pre><code>try:\n    # This might cause OOM\n    huge_tensor = genesis.zeros(100000, 100000, device='cuda')\nexcept RuntimeError as e:\n    if \"CUDA OOM\" in str(e):\n        print(\"Out of memory - consider reducing tensor size\")\n</code></pre>"},{"location":"api-reference/memory/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"api-reference/memory/#real-time-statistics","title":"Real-time Statistics","text":"<p>Monitor memory usage in real-time:</p> Python<pre><code>import genesis\nimport time\n\ndevice = genesis.device('cuda')\n\n# Reset stats for clean measurement\ndevice.reset_memory_stats()\n\n# Perform operations\nfor i in range(100):\n    x = genesis.rand(1000, 1000, device=device)\n    y = x + x\n    del x, y\n\n    if i % 10 == 0:\n        stats = device.memory_stats()\n        print(f\"Iteration {i}: Cache hit rate: {stats['cache_hit_rate']:.1%}\")\n</code></pre>"},{"location":"api-reference/memory/#memory-efficiency-analysis","title":"Memory Efficiency Analysis","text":"Python<pre><code>device = genesis.device('cuda')\n\n# Analyze memory efficiency\nstats = device.memory_stats()\nefficiency = stats['allocated_bytes'] / stats['reserved_bytes']\nfragmentation = 1 - (stats['active_bytes'] / stats['allocated_bytes'])\n\nprint(f\"Memory efficiency: {efficiency:.1%}\")\nprint(f\"Fragmentation: {fragmentation:.1%}\")\n</code></pre>"},{"location":"api-reference/memory/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/memory/#memory-optimization-tips","title":"Memory Optimization Tips","text":"<ol> <li>Use appropriate tensor sizes: Avoid extremely small or large tensors when possible</li> <li>Clear intermediate results: Delete tensors that are no longer needed</li> <li>Monitor cache hit rates: Aim for &gt;95% hit rates for optimal performance  </li> <li>Use context managers: For automatic cleanup in complex operations</li> </ol>"},{"location":"api-reference/memory/#example-optimized-training-loop","title":"Example: Optimized Training Loop","text":"Python<pre><code>import genesis\n\ndevice = genesis.device('cuda')\nmodel = YourModel().to(device)\noptimizer = genesis.optim.Adam(model.parameters())\n\n# Reset stats to monitor training efficiency\ndevice.reset_memory_stats()\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass\n        outputs = model(batch.to(device))\n        loss = criterion(outputs, targets.to(device))\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Clear intermediate tensors\n        del outputs, loss\n\n        # Monitor memory every 100 batches\n        if batch_idx % 100 == 0:\n            stats = device.memory_stats()\n            print(f\"Cache hit rate: {stats['cache_hit_rate']:.1%}\")\n</code></pre>"},{"location":"api-reference/memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api-reference/memory/#common-issues","title":"Common Issues","text":"<ol> <li>Low cache hit rate: Caused by varying tensor sizes. Use consistent sizes when possible.</li> <li>Memory fragmentation: Clear cache periodically with <code>device.empty_cache()</code></li> <li>OOM errors: Monitor peak memory usage and reduce batch sizes</li> <li>Memory leaks: Use proper tensor deletion and avoid circular references</li> </ol>"},{"location":"api-reference/memory/#debugging-memory-issues","title":"Debugging Memory Issues","text":"Python<pre><code>import genesis\n\ndevice = genesis.device('cuda')\n\n# Enable detailed logging (if available)\ngenesis.set_memory_debug(True)\n\n# Monitor memory throughout execution\ndef memory_checkpoint(name):\n    stats = device.memory_stats()\n    print(f\"{name}: {stats['allocated_bytes'] / 1e6:.1f}MB allocated, \"\n          f\"{stats['cache_hit_rate']:.1%} hit rate\")\n\nmemory_checkpoint(\"Start\")\nx = genesis.rand(4096, 4096, device=device)\nmemory_checkpoint(\"After allocation\")\ndel x\nmemory_checkpoint(\"After deletion\")\n</code></pre>"},{"location":"api-reference/memory/#see-also","title":"See Also","text":"<ul> <li>CUDA Storage - CUDA-specific tensor operations</li> <li>Performance Guide - Performance optimization strategies</li> <li>Performance Tuning - Advanced profiling capabilities</li> </ul>"},{"location":"api-reference/nn/","title":"Neural Network API Reference","text":"<p>Complete reference for Genesis neural network modules and functions.</p>"},{"location":"api-reference/nn/#available-modules","title":"Available Modules","text":"<ul> <li>Modules - Neural network layers and building blocks</li> <li>Functional - Functional interface for operations</li> </ul>"},{"location":"api-reference/nn/#quick-reference","title":"Quick Reference","text":""},{"location":"api-reference/nn/#common-layers","title":"Common Layers","text":"<ul> <li><code>nn.Linear</code> - Fully connected layer</li> <li><code>nn.Conv2d</code> - 2D convolution</li> <li><code>nn.ReLU</code> - ReLU activation</li> <li><code>nn.LayerNorm</code> - Layer normalization</li> </ul> <p>Detailed documentation for each module is available in the respective pages.</p>"},{"location":"api-reference/optim/","title":"Optimizer API Reference","text":"<p>Complete reference for Genesis optimization algorithms.</p>"},{"location":"api-reference/optim/#available-optimizers","title":"Available Optimizers","text":"<ul> <li>Optimizers - SGD, Adam, AdamW and more</li> <li>Learning Rate Schedulers - Learning rate scheduling</li> </ul>"},{"location":"api-reference/optim/#quick-reference","title":"Quick Reference","text":""},{"location":"api-reference/optim/#optimizers","title":"Optimizers","text":"<ul> <li><code>optim.SGD</code> - Stochastic Gradient Descent</li> <li><code>optim.Adam</code> - Adam optimizer  </li> <li><code>optim.AdamW</code> - Adam with weight decay</li> </ul>"},{"location":"api-reference/optim/#schedulers","title":"Schedulers","text":"<ul> <li><code>lr_scheduler.StepLR</code> - Step-based decay</li> <li><code>lr_scheduler.CosineAnnealingLR</code> - Cosine annealing</li> </ul> <p>See individual pages for detailed API documentation.</p>"},{"location":"api-reference/random/","title":"Random Number Generation API","text":"<p>Genesis provides a PyTorch-compatible random number generation API with thread-safe state management and reproducibility guarantees.</p>"},{"location":"api-reference/random/#functions","title":"Functions","text":""},{"location":"api-reference/random/#global-random-state-management","title":"Global Random State Management","text":""},{"location":"api-reference/random/#genesismanual_seedseed","title":"<code>genesis.manual_seed(seed)</code>","text":"<p>Set the global random seed for reproducible results.</p> <p>Parameters: - <code>seed</code> (int): Random seed value</p> <p>Example: Python<pre><code>import genesis\n\n# Set seed for reproducibility\ngenesis.manual_seed(42)\nx = genesis.rand(100, 100)  # Reproducible random tensor\n</code></pre></p>"},{"location":"api-reference/random/#genesisseed","title":"<code>genesis.seed()</code>","text":"<p>Set random seed from current time or system entropy.</p> <p>Example: Python<pre><code>genesis.seed()  # Random seed from system\n</code></pre></p>"},{"location":"api-reference/random/#genesisinitial_seed","title":"<code>genesis.initial_seed()</code>","text":"<p>Get the initial random seed used.</p> <p>Returns: - int: The initial seed value</p>"},{"location":"api-reference/random/#genesisget_rng_state","title":"<code>genesis.get_rng_state()</code>","text":"<p>Get the current random number generator state.</p> <p>Returns: - Tensor: Current RNG state</p> <p>Example: Python<pre><code># Save current state\nstate = genesis.get_rng_state()\n# ... perform random operations ...\ngenesis.set_rng_state(state)  # Restore state\n</code></pre></p>"},{"location":"api-reference/random/#genesisset_rng_statenew_state","title":"<code>genesis.set_rng_state(new_state)</code>","text":"<p>Set the random number generator state.</p> <p>Parameters: - <code>new_state</code> (Tensor): RNG state to restore</p>"},{"location":"api-reference/random/#thread-safe-random-generation","title":"Thread-Safe Random Generation","text":""},{"location":"api-reference/random/#genesisfork_rngdevicesnone-enabledtrue","title":"<code>genesis.fork_rng(devices=None, enabled=True)</code>","text":"<p>Context manager for thread-safe random number generation.</p> <p>Parameters: - <code>devices</code> (list, optional): Devices to fork RNG state for - <code>enabled</code> (bool): Whether to actually fork (default: True)</p> <p>Example: Python<pre><code>with genesis.fork_rng():\n    genesis.manual_seed(999)\n    # Random operations here don't affect global state\n    x = genesis.rand(10, 10)\n# Global state is restored here\n</code></pre></p>"},{"location":"api-reference/random/#generator-class","title":"Generator Class","text":""},{"location":"api-reference/random/#genesisgeneratordevicecpu","title":"<code>genesis.Generator(device='cpu')</code>","text":"<p>Random number generator for controlled random state.</p> <p>Parameters: - <code>device</code> (str): Device for the generator (default: 'cpu')</p> <p>Methods:</p>"},{"location":"api-reference/random/#generatormanual_seedseed","title":"<code>generator.manual_seed(seed)</code>","text":"<p>Set the generator's random seed.</p> <p>Parameters: - <code>seed</code> (int): Random seed value</p> <p>Example: Python<pre><code>gen = genesis.Generator()\ngen.manual_seed(12345)\n\n# Use generator for specific operations\nx = genesis.rand(100, 100, generator=gen)\n</code></pre></p>"},{"location":"api-reference/random/#generatorseed","title":"<code>generator.seed()</code>","text":"<p>Set random seed from system entropy.</p>"},{"location":"api-reference/random/#generatorinitial_seed","title":"<code>generator.initial_seed()</code>","text":"<p>Get the generator's initial seed.</p>"},{"location":"api-reference/random/#generatorget_state","title":"<code>generator.get_state()</code>","text":"<p>Get the generator's current state.</p>"},{"location":"api-reference/random/#generatorset_statenew_state","title":"<code>generator.set_state(new_state)</code>","text":"<p>Set the generator's state.</p>"},{"location":"api-reference/random/#global-generator","title":"Global Generator","text":""},{"location":"api-reference/random/#genesisdefault_generator","title":"<code>genesis.default_generator</code>","text":"<p>The default global random number generator instance.</p> <p>Example: Python<pre><code># Access default generator\ngen = genesis.default_generator\nstate = gen.get_state()\n</code></pre></p>"},{"location":"api-reference/random/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/random/#basic-random-generation","title":"Basic Random Generation","text":"Python<pre><code>import genesis\n\n# Set seed for reproducibility\ngenesis.manual_seed(42)\n\n# Generate random tensors\nx = genesis.rand(100, 100, device='cuda')\ny = genesis.randn(50, 50, device='cpu')\nz = genesis.randint(0, 10, (20, 20))\n</code></pre>"},{"location":"api-reference/random/#advanced-state-management","title":"Advanced State Management","text":"Python<pre><code>import genesis\n\n# Save global state\nsaved_state = genesis.get_rng_state()\n\n# Perform some random operations\ngenesis.manual_seed(123)\nx = genesis.rand(10, 10)\n\n# Restore previous state\ngenesis.set_rng_state(saved_state)\ny = genesis.rand(10, 10)  # Same as if seed(123) never happened\n</code></pre>"},{"location":"api-reference/random/#thread-safe-usage","title":"Thread-Safe Usage","text":"Python<pre><code>import genesis\nimport threading\n\ndef worker():\n    with genesis.fork_rng():\n        genesis.manual_seed(42)\n        # Each thread has independent random state\n        return genesis.rand(100, 100)\n\n# Multiple threads won't interfere with each other\nthreads = [threading.Thread(target=worker) for _ in range(4)]\n</code></pre>"},{"location":"api-reference/random/#custom-generator-usage","title":"Custom Generator Usage","text":"Python<pre><code>import genesis\n\n# Create custom generator\ngen1 = genesis.Generator()\ngen1.manual_seed(111)\n\ngen2 = genesis.Generator() \ngen2.manual_seed(222)\n\n# Different generators produce different sequences\nx1 = genesis.rand(10, 10, generator=gen1)\nx2 = genesis.rand(10, 10, generator=gen2)  # Different from x1\n</code></pre>"},{"location":"api-reference/random/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>The RNG API is designed for PyTorch compatibility</li> <li>Thread safety is guaranteed through proper state isolation</li> <li>State management supports both global and per-generator control</li> <li>All random functions accept an optional <code>generator</code> parameter</li> <li>The implementation uses NumPy's random number generator internally</li> <li>CUDA device random generation inherits from CPU state management</li> </ul>"},{"location":"api-reference/random/#see-also","title":"See Also","text":"<ul> <li>Tensor Creation Functions - Functions that use random generation</li> <li>Memory Management - For device-specific considerations</li> </ul>"},{"location":"api-reference/serialization/","title":"Model Serialization and Checkpointing","text":"<p>Genesis provides model serialization and checkpointing functionality to save and load model states, optimizer states, and training progress.</p>"},{"location":"api-reference/serialization/#overview","title":"Overview","text":"<p>The serialization system in Genesis provides: - Model state dictionary saving/loading - Optimizer state saving/loading - Checkpoint management with atomic write operations - Backup file creation for safety</p>"},{"location":"api-reference/serialization/#core-functions","title":"Core Functions","text":""},{"location":"api-reference/serialization/#save","title":"save()","text":"Python<pre><code>def save(state_dict, file_path):\n    \"\"\"\n    Save state dictionary to file with atomic write operation.\n\n    Args:\n        state_dict (dict): Dictionary containing state to save\n        file_path (str): Path where to save the file\n\n    Features:\n        - Creates backup file (.genesis.bak) before overwriting\n        - Atomic write with automatic cleanup on success\n        - Rollback to backup on failure\n        - Clears state_dict from memory after save\n\n    Implementation:\n        - Uses dill (enhanced pickle) for serialization\n        - Supports lambda functions and complex Python objects\n    \"\"\"\n</code></pre>"},{"location":"api-reference/serialization/#load","title":"load()","text":"Python<pre><code>def load(file_path):\n    \"\"\"\n    Load state dictionary from file.\n\n    Args:\n        file_path (str): Path to the saved file\n\n    Returns:\n        dict: Loaded state dictionary\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        pickle.UnpicklingError: If file is corrupted\n\n    Implementation:\n        - Uses dill for deserialization\n        - Compatible with files saved by save()\n    \"\"\"\n</code></pre>"},{"location":"api-reference/serialization/#save_checkpoint","title":"save_checkpoint()","text":"Python<pre><code>def save_checkpoint(model_state_dict, optimizer_state_dict, file_path):\n    \"\"\"\n    Save model and optimizer checkpoint.\n\n    Args:\n        model_state_dict (dict): Model state dictionary\n        optimizer_state_dict (dict): Optimizer state dictionary  \n        file_path (str): Path where to save checkpoint\n\n    Creates checkpoint containing:\n        - \"model_state_dict\": Model parameters and buffers\n        - \"optimizer_state_dict\": Optimizer state\n\n    Implementation:\n        - Internally calls save() with structured dictionary\n    \"\"\"\n</code></pre>"},{"location":"api-reference/serialization/#load_checkpoint","title":"load_checkpoint()","text":"Python<pre><code>def load_checkpoint(file_path):\n    \"\"\"\n    Load model and optimizer checkpoint.\n\n    Args:\n        file_path (str): Path to checkpoint file\n\n    Returns:\n        tuple: (model_state_dict, optimizer_state_dict)\n\n    Implementation:\n        - Loads pickle file and extracts state dictionaries\n        - Returns tuple for convenient unpacking\n\n    Example:\n        &gt;&gt;&gt; model_state, optimizer_state = genesis.load_checkpoint('checkpoint.pth')\n        &gt;&gt;&gt; model.load_state_dict(model_state)\n        &gt;&gt;&gt; optimizer.load_state_dict(optimizer_state)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/serialization/#basic-usage","title":"Basic Usage","text":""},{"location":"api-reference/serialization/#saving-a-model","title":"Saving a Model","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Create and train model\nmodel = nn.Linear(784, 10)\n\n# Save model state\ngenesis.save(model.state_dict(), 'model.pth')\n\n# Load model state\nstate_dict = genesis.load('model.pth')\nmodel.load_state_dict(state_dict)\n</code></pre>"},{"location":"api-reference/serialization/#training-checkpoints","title":"Training Checkpoints","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Setup model and optimizer\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with checkpointing\nfor epoch in range(100):\n    # Training code...\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n\n    # Save checkpoint every 10 epochs\n    if epoch % 10 == 0:\n        genesis.save_checkpoint(\n            model.state_dict(),\n            optimizer.state_dict(), \n            f'checkpoint_epoch_{epoch}.pth'\n        )\n        print(f\"Checkpoint saved at epoch {epoch}\")\n\n# Load checkpoint to resume training\nmodel_state, optimizer_state = genesis.load_checkpoint('checkpoint_epoch_90.pth')\nmodel.load_state_dict(model_state)\noptimizer.load_state_dict(optimizer_state)\n</code></pre>"},{"location":"api-reference/serialization/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api-reference/serialization/#complete-training-state","title":"Complete Training State","text":"Python<pre><code>import genesis\n\n# Save complete training state\ncheckpoint = {\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    'best_accuracy': best_accuracy,\n    'training_history': training_history\n}\ngenesis.save(checkpoint, 'full_checkpoint.pth')\n\n# Load complete training state\ncheckpoint = genesis.load('full_checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nstart_epoch = checkpoint['epoch'] + 1\nbest_accuracy = checkpoint['best_accuracy']\n</code></pre>"},{"location":"api-reference/serialization/#best-model-tracking","title":"Best Model Tracking","text":"Python<pre><code>import genesis\nimport os\n\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    # Training\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n    val_loss = validate(model, val_loader)\n\n    # Save regular checkpoint\n    genesis.save_checkpoint(\n        model.state_dict(),\n        optimizer.state_dict(),\n        f'checkpoint_epoch_{epoch}.pth'\n    )\n\n    # Save best model\n    if val_loss &lt; best_loss:\n        best_loss = val_loss\n        genesis.save(model.state_dict(), 'best_model.pth')\n        print(f\"New best model saved with loss: {val_loss:.4f}\")\n</code></pre>"},{"location":"api-reference/serialization/#error-handling","title":"Error Handling","text":"Python<pre><code>import genesis\nimport os\n\ndef safe_load_checkpoint(file_path, model, optimizer=None):\n    \"\"\"Safely load checkpoint with error handling.\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            print(f\"Checkpoint {file_path} not found\")\n            return False\n\n        # Load checkpoint\n        if file_path.endswith('.pth'):\n            # Try loading as checkpoint first\n            try:\n                model_state, optimizer_state = genesis.load_checkpoint(file_path)\n                model.load_state_dict(model_state)\n                if optimizer:\n                    optimizer.load_state_dict(optimizer_state)\n            except:\n                # Fall back to loading as state dict\n                state_dict = genesis.load(file_path)\n                if 'model_state_dict' in state_dict:\n                    model.load_state_dict(state_dict['model_state_dict'])\n                    if optimizer and 'optimizer_state_dict' in state_dict:\n                        optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n                else:\n                    model.load_state_dict(state_dict)\n\n        print(\"Checkpoint loaded successfully\")\n        return True\n\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        return False\n\n# Usage\nsuccess = safe_load_checkpoint('checkpoint.pth', model, optimizer)\n</code></pre>"},{"location":"api-reference/serialization/#implementation-details","title":"Implementation Details","text":""},{"location":"api-reference/serialization/#atomic-write-operation","title":"Atomic Write Operation","text":"<p>The save function implements atomic writes to prevent data corruption:</p> <ol> <li>Backup Creation: Before overwriting an existing file, creates a <code>.genesis.bak</code> backup</li> <li>Write Operation: Saves new data to the target file</li> <li>Memory Cleanup: Clears the state_dict from memory after successful save</li> <li>Cleanup/Rollback: On success, removes backup; on failure, restores from backup</li> </ol>"},{"location":"api-reference/serialization/#serialization-format","title":"Serialization Format","text":"<ul> <li>Uses <code>dill</code> (enhanced pickle) for serialization</li> <li>Supports lambda functions and complex Python objects</li> <li>Binary format for efficient storage</li> <li>Compatible with standard pickle for basic objects</li> </ul>"},{"location":"api-reference/serialization/#memory-management","title":"Memory Management","text":"<p>The save function includes automatic memory cleanup: Python<pre><code># After successful save, clear the dictionary to free memory\nfor key in list(state_dict.keys()):\n    del state_dict[key]\n</code></pre></p>"},{"location":"api-reference/serialization/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/serialization/#1-checkpoint-frequency","title":"1. Checkpoint Frequency","text":"<ul> <li>Save checkpoints regularly (e.g., every epoch or N steps)</li> <li>Keep multiple recent checkpoints</li> <li>Save best model separately</li> </ul>"},{"location":"api-reference/serialization/#2-file-organization","title":"2. File Organization","text":"Python<pre><code># Recommended structure\ncheckpoints/\n\u251c\u2500\u2500 checkpoint_epoch_10.pth     # Regular checkpoints\n\u251c\u2500\u2500 checkpoint_epoch_20.pth\n\u251c\u2500\u2500 best_model.pth              # Best performing model\n\u2514\u2500\u2500 final_model.pth             # Final trained model\n</code></pre>"},{"location":"api-reference/serialization/#3-checkpoint-naming","title":"3. Checkpoint Naming","text":"Python<pre><code># Include useful information in filename\nfilename = f\"checkpoint_epoch_{epoch}_loss_{loss:.4f}.pth\"\ngenesis.save_checkpoint(model.state_dict(), optimizer.state_dict(), filename)\n</code></pre>"},{"location":"api-reference/serialization/#4-resume-training","title":"4. Resume Training","text":"Python<pre><code># Check for existing checkpoint\ncheckpoint_path = 'checkpoint_latest.pth'\nif os.path.exists(checkpoint_path):\n    model_state, optimizer_state = genesis.load_checkpoint(checkpoint_path)\n    model.load_state_dict(model_state)\n    optimizer.load_state_dict(optimizer_state)\n    print(\"Resumed from checkpoint\")\nelse:\n    print(\"Starting fresh training\")\n</code></pre>"},{"location":"api-reference/serialization/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ol> <li>File Format: Uses pickle/dill format, not compatible with PyTorch <code>.pt</code> files</li> <li>Memory Usage: The save function clears the input dictionary from memory</li> <li>No Compression: Files are not compressed (consider using external compression if needed)</li> <li>Single Device: No special handling for multi-GPU model states</li> </ol>"},{"location":"api-reference/serialization/#migration-notes","title":"Migration Notes","text":""},{"location":"api-reference/serialization/#from-pytorch","title":"From PyTorch","text":"<p>Genesis uses a similar API to PyTorch but with some differences: - Uses <code>dill</code> instead of standard <code>pickle</code> for better lambda support - Automatic backup file creation for safety - Memory cleanup after saving</p>"},{"location":"api-reference/serialization/#loading-pytorch-models","title":"Loading PyTorch Models","text":"<p>To load PyTorch models in Genesis, you'll need to convert them: Python<pre><code>import torch\nimport genesis\n\n# Load PyTorch model\ntorch_state = torch.load('pytorch_model.pt')\n\n# Convert and save in Genesis format\n# Note: Tensor conversion may be needed depending on backend\ngenesis.save(torch_state, 'genesis_model.pth')\n</code></pre></p>"},{"location":"api-reference/serialization/#advanced-checkpointing","title":"Advanced Checkpointing","text":""},{"location":"api-reference/serialization/#robust-checkpoint-loading","title":"Robust Checkpoint Loading","text":"<p>Handle loading errors gracefully with validation:</p> Python<pre><code>import genesis\n\ndef safe_load_checkpoint(file_path, model, optimizer=None):\n    \"\"\"Load checkpoint with comprehensive error handling.\"\"\"\n    try:\n        checkpoint = genesis.load(file_path)\n\n        # Load model state\n        if 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(\"Model state loaded successfully\")\n        else:\n            print(\"Warning: No model state found in checkpoint\")\n            return False\n\n        # Load optimizer state (optional)\n        if optimizer and 'optimizer_state_dict' in checkpoint:\n            try:\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                print(\"Optimizer state loaded successfully\")\n            except Exception as e:\n                print(f\"Warning: Could not load optimizer state: {e}\")\n\n        # Return additional information\n        epoch = checkpoint.get('epoch', 0)\n        loss = checkpoint.get('loss', 'unknown')\n        print(f\"Loaded checkpoint from epoch {epoch}, loss: {loss}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        return False\n\n# Usage\nsuccess = safe_load_checkpoint('checkpoint.pth', model, optimizer)\nif success:\n    print(\"Checkpoint loaded successfully\")\nelse:\n    print(\"Failed to load checkpoint, starting from scratch\")\n</code></pre>"},{"location":"api-reference/serialization/#checkpoint-validation","title":"Checkpoint Validation","text":"Python<pre><code>import genesis\n\ndef validate_checkpoint(file_path):\n    \"\"\"Validate checkpoint file integrity.\"\"\"\n    try:\n        checkpoint = genesis.load(file_path)\n\n        # Basic structure validation\n        if not isinstance(checkpoint, dict):\n            return False, \"Checkpoint is not a dictionary\"\n\n        if 'model_state_dict' not in checkpoint:\n            return False, \"Missing model_state_dict\"\n\n        # Check model state structure\n        model_state = checkpoint['model_state_dict']\n        if not isinstance(model_state, dict):\n            return False, \"model_state_dict is not a dictionary\"\n\n        # Check for empty state\n        if len(model_state) == 0:\n            return False, \"model_state_dict is empty\"\n\n        # Validate tensor shapes (basic check)\n        for key, tensor in model_state.items():\n            if not hasattr(tensor, 'shape'):\n                return False, f\"Invalid tensor for key '{key}'\"\n\n        return True, \"Checkpoint is valid\"\n\n    except Exception as e:\n        return False, f\"Error validating checkpoint: {e}\"\n\n# Usage\nis_valid, message = validate_checkpoint('checkpoint.pth')\nprint(f\"Checkpoint validation: {message}\")\n</code></pre>"},{"location":"api-reference/serialization/#best-practices_1","title":"Best Practices","text":""},{"location":"api-reference/serialization/#1-checkpoint-strategy","title":"1. Checkpoint Strategy","text":"<ul> <li>Save checkpoints regularly (every N epochs)</li> <li>Keep multiple recent checkpoints</li> <li>Save best model separately</li> <li>Include training metadata</li> </ul>"},{"location":"api-reference/serialization/#2-file-organization_1","title":"2. File Organization","text":"Python<pre><code># Recommended directory structure\ncheckpoints/\n\u251c\u2500\u2500 latest.pth                    # Latest checkpoint\n\u251c\u2500\u2500 best_model.pth               # Best performance model\n\u251c\u2500\u2500 epoch_000010.pth            # Regular checkpoints\n\u251c\u2500\u2500 epoch_000020.pth\n\u2514\u2500\u2500 deployed/\n    \u2514\u2500\u2500 production_model.pth     # Production-ready model\n</code></pre>"},{"location":"api-reference/serialization/#3-memory-management","title":"3. Memory Management","text":"Python<pre><code>import genesis\nimport gc\n\ndef efficient_checkpoint_save(model, optimizer, file_path):\n    \"\"\"Memory-optimized checkpoint saving.\"\"\"\n    # Create checkpoint dictionary\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n\n    # Save checkpoint\n    genesis.save(checkpoint, file_path)\n\n    # Clear checkpoint dictionary from memory\n    del checkpoint\n    gc.collect()\n</code></pre>"},{"location":"api-reference/serialization/#4-version-management","title":"4. Version Management","text":"Python<pre><code>class CheckpointManager:\n    \"\"\"Manage checkpoint versions and cleanup.\"\"\"\n\n    def __init__(self, checkpoint_dir, max_checkpoints=5):\n        self.checkpoint_dir = checkpoint_dir\n        self.max_checkpoints = max_checkpoints\n\n    def save_checkpoint(self, model, optimizer, epoch, loss):\n        \"\"\"Save checkpoint with automatic cleanup.\"\"\"\n        file_path = f\"{self.checkpoint_dir}/epoch_{epoch:06d}.pth\"\n\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            'timestamp': time.time()\n        }\n\n        genesis.save(checkpoint, file_path)\n        self._cleanup_old_checkpoints()\n\n    def _cleanup_old_checkpoints(self):\n        \"\"\"Remove old checkpoints keeping only the most recent.\"\"\"\n        import os\n        import glob\n\n        checkpoints = glob.glob(f\"{self.checkpoint_dir}/epoch_*.pth\")\n        checkpoints.sort()\n\n        while len(checkpoints) &gt; self.max_checkpoints:\n            os.remove(checkpoints.pop(0))\n\n# Usage\nmanager = CheckpointManager('checkpoints/', max_checkpoints=3)\nmanager.save_checkpoint(model, optimizer, epoch, loss)\n</code></pre>"},{"location":"api-reference/serialization/#see-also","title":"See Also","text":"<ul> <li>Optimizers - Optimizer state management</li> <li>Neural Network Modules - Model state_dict methods</li> <li>Training Examples - Complete training scripts with checkpointing</li> </ul>"},{"location":"api-reference/storage/","title":"Storage API Reference","text":"<p>The Storage layer provides the low-level memory management interface between tensors and device backends in Genesis.</p>"},{"location":"api-reference/storage/#storage-class","title":"Storage Class","text":"<p>The Storage class is the base abstraction for memory storage across different devices.</p> Python<pre><code>class Storage:\n    \"\"\"\n    Abstract base class for tensor storage.\n    Provides unified interface for memory management across devices.\n    \"\"\"\n</code></pre>"},{"location":"api-reference/storage/#properties","title":"Properties","text":"<ul> <li>size: Number of elements in storage</li> <li>dtype: Data type of stored elements</li> <li>device: Device where storage resides</li> <li>data_ptr: Pointer to underlying memory (device-specific)</li> </ul>"},{"location":"api-reference/storage/#methods","title":"Methods","text":"Python<pre><code>storage.to(device)           # Move storage to different device\nstorage.copy_(src)           # Copy data from another storage\nstorage.resize_(size)        # Resize storage capacity\nstorage.fill_(value)         # Fill storage with value\nstorage.zero_()             # Zero out storage\n</code></pre>"},{"location":"api-reference/storage/#backend-implementations","title":"Backend Implementations","text":""},{"location":"api-reference/storage/#cpu-storage","title":"CPU Storage","text":"<p>CPU storage uses PyTorch tensors as the underlying storage mechanism:</p> Python<pre><code># Internal implementation (not directly accessible)\nclass CPUStorage(Storage):\n    def __init__(self, data):\n        self._data = torch.tensor(data)\n</code></pre>"},{"location":"api-reference/storage/#cuda-storage","title":"CUDA Storage","text":"<p>CUDA storage manages GPU memory directly:</p> Python<pre><code># Internal implementation (not directly accessible)\nclass CUDAStorage(Storage):\n    def __init__(self, size, dtype):\n        self._allocate_cuda_memory(size, dtype)\n</code></pre>"},{"location":"api-reference/storage/#storage-creation","title":"Storage Creation","text":"<p>Storage is typically created automatically when creating tensors:</p> Python<pre><code>import genesis\n\n# Storage created automatically\ntensor = genesis.tensor([1, 2, 3, 4])\n\n# Access underlying storage (advanced usage)\nstorage = tensor._storage  # Internal API\n</code></pre>"},{"location":"api-reference/storage/#memory-layout","title":"Memory Layout","text":""},{"location":"api-reference/storage/#contiguous-storage","title":"Contiguous Storage","text":"<p>Genesis ensures tensors are stored contiguously in memory for optimal performance:</p> Python<pre><code># Check if tensor is contiguous\nis_contiguous = tensor.is_contiguous()\n\n# Make tensor contiguous if needed\ncontiguous_tensor = tensor.contiguous()\n</code></pre>"},{"location":"api-reference/storage/#strided-storage","title":"Strided Storage","text":"<p>Storage supports strided access for efficient view operations:</p> Python<pre><code># Reshape creates a view with different strides\nreshaped = tensor.reshape(2, 2)\n\n# Transpose changes strides without copying data\ntransposed = tensor.T\n</code></pre>"},{"location":"api-reference/storage/#memory-pooling","title":"Memory Pooling","text":"<p>Genesis implements memory pooling for efficient allocation:</p>"},{"location":"api-reference/storage/#cuda-memory-pool","title":"CUDA Memory Pool","text":"Python<pre><code>import genesis.cuda as cuda\n\n# Memory pool statistics (if available)\nif genesis.cuda_available():\n    # Get memory stats\n    allocated = cuda.memory_allocated()\n    reserved = cuda.memory_reserved()\n</code></pre>"},{"location":"api-reference/storage/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api-reference/storage/#custom-storage-allocation","title":"Custom Storage Allocation","text":"<p>For advanced users who need custom memory management:</p> Python<pre><code>import genesis\nimport numpy as np\n\n# Create tensor from existing memory\nnumpy_array = np.array([1, 2, 3, 4], dtype=np.float32)\ntensor = genesis.from_numpy(numpy_array)\n\n# Share memory with numpy (zero-copy)\nshared_array = tensor.numpy()  # Shares memory if on CPU\n</code></pre>"},{"location":"api-reference/storage/#storage-aliasing","title":"Storage Aliasing","text":"<p>Multiple tensors can share the same storage:</p> Python<pre><code># Create view that shares storage\nx = genesis.tensor([[1, 2], [3, 4]])\ny = x.view(-1)  # Flattened view\n\n# Both tensors share same storage\nx[0, 0] = 10\nprint(y[0])  # 10 - storage is shared\n</code></pre>"},{"location":"api-reference/storage/#storage-cloning","title":"Storage Cloning","text":"<p>Create independent copy with new storage:</p> Python<pre><code># Clone creates new storage\nx = genesis.tensor([1, 2, 3])\ny = x.clone()\n\n# Modifications don't affect original\ny[0] = 10\nprint(x[0])  # Still 1 - different storage\n</code></pre>"},{"location":"api-reference/storage/#memory-management-best-practices","title":"Memory Management Best Practices","text":""},{"location":"api-reference/storage/#1-reuse-storage","title":"1. Reuse Storage","text":"Python<pre><code># Reuse existing tensor storage\noutput = genesis.empty_like(input)\n# Perform operation in-place\noutput.copy_(process(input))\n</code></pre>"},{"location":"api-reference/storage/#2-avoid-unnecessary-copies","title":"2. Avoid Unnecessary Copies","text":"Python<pre><code># Use views when possible\nbatch = data.view(batch_size, -1)  # No copy\n\n# Avoid unnecessary contiguous calls\nif not tensor.is_contiguous():\n    tensor = tensor.contiguous()\n</code></pre>"},{"location":"api-reference/storage/#3-free-unused-storage","title":"3. Free Unused Storage","text":"Python<pre><code># Explicitly free large tensors\nlarge_tensor = genesis.randn(10000, 10000)\n# Use tensor\nresult = process(large_tensor)\n# Free memory\ndel large_tensor\n</code></pre>"},{"location":"api-reference/storage/#4-monitor-memory-usage","title":"4. Monitor Memory Usage","text":"Python<pre><code>import genesis.cuda as cuda\n\ndef memory_summary():\n    if genesis.cuda_available():\n        print(f\"Allocated: {cuda.memory_allocated() / 1024**2:.2f} MB\")\n        print(f\"Reserved: {cuda.memory_reserved() / 1024**2:.2f} MB\")\n</code></pre>"},{"location":"api-reference/storage/#storage-and-gradients","title":"Storage and Gradients","text":"<p>Storage interacts with automatic differentiation:</p> Python<pre><code># Gradient storage allocated on demand\nx = genesis.tensor([1, 2, 3], requires_grad=True)\ny = x * 2\ny.backward(genesis.ones_like(y))\n\n# Gradient has its own storage\nprint(x.grad)  # Separate storage for gradients\n</code></pre>"},{"location":"api-reference/storage/#platform-specific-considerations","title":"Platform-Specific Considerations","text":""},{"location":"api-reference/storage/#cpu-storage_1","title":"CPU Storage","text":"<ul> <li>Uses system RAM</li> <li>Supports memory-mapped files for large datasets</li> <li>Thread-safe operations</li> </ul>"},{"location":"api-reference/storage/#cuda-storage_1","title":"CUDA Storage","text":"<ul> <li>Limited by GPU memory</li> <li>Supports unified memory on compatible hardware</li> <li>Asynchronous operations possible</li> </ul>"},{"location":"api-reference/storage/#see-also","title":"See Also","text":"<ul> <li>Tensor API - High-level tensor interface</li> <li>Device Management - Device abstraction</li> <li>Memory Management - Advanced memory optimization</li> <li>Backend System - Backend implementations</li> </ul>"},{"location":"api-reference/tensor/","title":"Tensor API Reference","text":"<p>The Tensor class is the fundamental data structure in Genesis, providing automatic differentiation support and efficient computation on CPU and GPU devices.</p>"},{"location":"api-reference/tensor/#tensor-class","title":"Tensor Class","text":"Python<pre><code>genesis.tensor(data, dtype=None, device=None, requires_grad=False)\ngenesis.Tensor(data, dtype=None, device=None, requires_grad=False)\n</code></pre>"},{"location":"api-reference/tensor/#parameters","title":"Parameters","text":"<ul> <li>data: Array-like data (list, numpy array, or existing tensor)</li> <li>dtype: Data type (default: inferred from data)</li> <li><code>genesis.float32</code>, <code>genesis.float64</code>, <code>genesis.float16</code>, <code>genesis.bfloat16</code></li> <li><code>genesis.int32</code>, <code>genesis.int64</code>, <code>genesis.int16</code>, <code>genesis.int8</code></li> <li><code>genesis.uint8</code>, <code>genesis.bool</code></li> <li>device: Device to place tensor on</li> <li><code>genesis.device('cpu')</code>: CPU device</li> <li><code>genesis.device('cuda')</code>: Default CUDA device</li> <li><code>genesis.device('cuda:0')</code>: Specific CUDA device</li> <li>requires_grad: Whether to track gradients for automatic differentiation</li> </ul>"},{"location":"api-reference/tensor/#properties","title":"Properties","text":"<ul> <li>shape: Tuple of tensor dimensions</li> <li>dtype: Data type of the tensor</li> <li>device: Device where tensor is stored</li> <li>requires_grad: Whether tensor requires gradient computation</li> <li>grad: Gradient tensor (after backward pass)</li> <li>data: Underlying storage without gradient tracking</li> </ul>"},{"location":"api-reference/tensor/#methods","title":"Methods","text":""},{"location":"api-reference/tensor/#basic-operations","title":"Basic Operations","text":"Python<pre><code># Element-wise operations\ntensor.add(other)        # Addition\ntensor.sub(other)        # Subtraction\ntensor.mul(other)        # Multiplication\ntensor.div(other)        # Division\ntensor.pow(exponent)     # Power\ntensor.exp()            # Exponential\ntensor.log()            # Natural logarithm\ntensor.sqrt()           # Square root\ntensor.abs()            # Absolute value\ntensor.neg()            # Negation\n\n# Reduction operations\ntensor.sum(axis=None, keepdims=False)      # Sum\ntensor.mean(axis=None, keepdims=False)     # Mean\ntensor.max(axis=None, keepdims=False)      # Maximum\ntensor.min(axis=None, keepdims=False)      # Minimum\n\n# Shape operations\ntensor.reshape(shape)                       # Reshape\ntensor.transpose(axes=None)                # Transpose\ntensor.squeeze(axis=None)                  # Remove dimensions of size 1\ntensor.unsqueeze(axis)                     # Add dimension of size 1\ntensor.flatten(start_dim=0, end_dim=-1)    # Flatten dimensions\n</code></pre>"},{"location":"api-reference/tensor/#matrix-operations","title":"Matrix Operations","text":"Python<pre><code># Matrix multiplication\ntensor.matmul(other)     # Matrix multiplication\ntensor @ other           # Matrix multiplication operator\n\n# Linear algebra\ntensor.T                 # Transpose (2D tensors)\n</code></pre>"},{"location":"api-reference/tensor/#gradient-operations","title":"Gradient Operations","text":"Python<pre><code># Automatic differentiation\ntensor.backward()        # Compute gradients\ntensor.detach()         # Detach from computation graph\ntensor.zero_grad()      # Zero out gradients\n</code></pre>"},{"location":"api-reference/tensor/#device-operations","title":"Device Operations","text":"Python<pre><code># Move between devices\ntensor.to(device)       # Move to specified device\ntensor.cpu()           # Move to CPU\ntensor.cuda()          # Move to CUDA\n</code></pre>"},{"location":"api-reference/tensor/#type-conversion","title":"Type Conversion","text":"Python<pre><code># Data type conversion\ntensor.float()         # Convert to float32\ntensor.double()        # Convert to float64\ntensor.half()          # Convert to float16\ntensor.int()           # Convert to int32\ntensor.long()          # Convert to int64\ntensor.bool()          # Convert to bool\n\n# Convert to other formats\ntensor.numpy()         # Convert to numpy array\ntensor.tolist()        # Convert to Python list\n</code></pre>"},{"location":"api-reference/tensor/#examples","title":"Examples","text":""},{"location":"api-reference/tensor/#basic-tensor-creation","title":"Basic Tensor Creation","text":"Python<pre><code>import genesis\n\n# From list\nx = genesis.tensor([1, 2, 3, 4])\n\n# From numpy array\nimport numpy as np\narr = np.array([[1, 2], [3, 4]])\ny = genesis.tensor(arr, dtype=genesis.float32)\n\n# On GPU\nz = genesis.tensor([1, 2, 3], device=genesis.device('cuda'))\n\n# With gradient tracking\nw = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)\n</code></pre>"},{"location":"api-reference/tensor/#automatic-differentiation","title":"Automatic Differentiation","text":"Python<pre><code>import genesis\n\n# Create tensors with gradient tracking\nx = genesis.tensor([2.0], requires_grad=True)\ny = genesis.tensor([3.0], requires_grad=True)\n\n# Forward pass\nz = x * y + x ** 2\nprint(f\"z = {z}\")  # z = [10.0]\n\n# Backward pass\nz.backward()\nprint(f\"x.grad = {x.grad}\")  # x.grad = [7.0] (dy/dx = y + 2x = 3 + 4 = 7)\nprint(f\"y.grad = {y.grad}\")  # y.grad = [2.0] (dy/dy = x = 2)\n</code></pre>"},{"location":"api-reference/tensor/#matrix-operations_1","title":"Matrix Operations","text":"Python<pre><code>import genesis\n\n# Create matrices\nA = genesis.randn(3, 4)\nB = genesis.randn(4, 5)\n\n# Matrix multiplication\nC = A @ B  # or A.matmul(B)\nprint(f\"Result shape: {C.shape}\")  # (3, 5)\n\n# Element-wise operations\nD = A * 2 + 1\nE = A.exp().sum(axis=1, keepdims=True)\n</code></pre>"},{"location":"api-reference/tensor/#device-management","title":"Device Management","text":"Python<pre><code>import genesis\n\n# Check CUDA availability\nif genesis.cuda_available():\n    device = genesis.device('cuda')\nelse:\n    device = genesis.device('cpu')\n\n# Create tensor on specific device\nx = genesis.randn(100, 100, device=device)\n\n# Move tensor between devices\nx_cpu = x.cpu()\nx_cuda = x_cpu.cuda()\n</code></pre>"},{"location":"api-reference/tensor/#see-also","title":"See Also","text":"<ul> <li>Function API - Automatic differentiation functions</li> <li>Device Management - Device abstraction and management</li> <li>Storage Layer - Memory storage interface</li> </ul>"},{"location":"api-reference/utils/","title":"Utilities API Reference","text":"<p>Utility functions and helpers for Genesis framework.</p>"},{"location":"api-reference/utils/#available-utilities","title":"Available Utilities","text":"<p>See Utils for the main utilities documentation.</p>"},{"location":"api-reference/utils/#categories","title":"Categories","text":""},{"location":"api-reference/utils/#data-utilities","title":"Data Utilities","text":"<ul> <li>Data loading helpers</li> <li>Preprocessing functions</li> <li>Dataset utilities</li> </ul>"},{"location":"api-reference/utils/#training-utilities","title":"Training Utilities","text":"<ul> <li>Checkpoint management</li> <li>Logging and monitoring</li> <li>Performance profiling</li> </ul>"},{"location":"api-reference/utils/#system-utilities","title":"System Utilities","text":"<ul> <li>CUDA utilities</li> <li>Memory management</li> <li>Device management</li> </ul> <p>Detailed documentation is available in the main utils documentation.</p>"},{"location":"api-reference/nn/functional/","title":"Functional Operations Interface (genesis.nn.functional)","text":"<p>Genesis functional interface provides stateless tensor operation functions that can be called directly on tensors without creating module instances.</p>"},{"location":"api-reference/nn/functional/#module-overview","title":"Module Overview","text":"<p><code>genesis.nn.functional</code> (commonly imported as <code>F</code>) includes: - Basic arithmetic operations (add, subtract, multiply, divide) - Mathematical functions (sin, cos, log, exp, sqrt, power) - Tensor shape operations (transpose, reshape, expand, view, flatten) - Tensor indexing and slicing (getitem, setitem, broadcast_to) - Aggregation operations (sum, max, logsumexp) - Matrix operations (matmul, stack, cat, squeeze, unsqueeze) - Basic activation functions (relu) - Advanced operations (softmax, dropout from triton_ops)</p>"},{"location":"api-reference/nn/functional/#basic-arithmetic-operations","title":"Basic Arithmetic Operations","text":""},{"location":"api-reference/nn/functional/#add","title":"add","text":"Python<pre><code>def add(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise addition of two tensors.\n\n    Args:\n        a: Tensor - First input tensor\n        b: Tensor - Second input tensor\n\n    Returns:\n        Tensor - Element-wise sum a + b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; y = genesis.tensor([[2.0, 1.0], [1.0, 2.0]])\n        &gt;&gt;&gt; z = F.add(x, y)\n        &gt;&gt;&gt; # Result: [[3.0, 3.0], [4.0, 6.0]]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#sub","title":"sub","text":"Python<pre><code>def sub(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise subtraction of two tensors.\n\n    Args:\n        a: Tensor - First input tensor (minuend)\n        b: Tensor - Second input tensor (subtrahend)\n\n    Returns:\n        Tensor - Element-wise difference a - b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([5.0, 3.0, 8.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 1.0, 3.0])\n        &gt;&gt;&gt; z = F.sub(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 2.0, 5.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#multiply","title":"multiply","text":"Python<pre><code>def multiply(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise multiplication of two tensors.\n\n    Args:\n        a: Tensor - First input tensor\n        b: Tensor - Second input tensor\n\n    Returns:\n        Tensor - Element-wise product a * b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([2.0, 3.0, 4.0])\n        &gt;&gt;&gt; y = genesis.tensor([1.5, 2.0, 0.5])\n        &gt;&gt;&gt; z = F.multiply(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 6.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#divide","title":"divide","text":"Python<pre><code>def divide(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise division of two tensors.\n\n    Args:\n        a: Tensor - Dividend tensor\n        b: Tensor - Divisor tensor\n\n    Returns:\n        Tensor - Element-wise quotient a / b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([6.0, 8.0, 9.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 4.0, 3.0])\n        &gt;&gt;&gt; z = F.divide(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 2.0, 3.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#scalar-operations","title":"Scalar Operations","text":""},{"location":"api-reference/nn/functional/#add_scalar-mul_scalar-divide_scalar-pow_scalar","title":"add_scalar, mul_scalar, divide_scalar, pow_scalar","text":"Python<pre><code>def add_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef mul_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef divide_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\ndef pow_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\n    \"\"\"\n    Element-wise operations between tensor and scalar.\n\n    Args:\n        a: Tensor - Input tensor\n        scalar: float - Scalar value\n        reverse: bool - If True, applies scalar op tensor (for divide/pow)\n\n    Returns:\n        Tensor - Result of tensor-scalar operation\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; y1 = F.add_scalar(x, 5.0)      # [6.0, 7.0, 8.0]\n        &gt;&gt;&gt; y2 = F.mul_scalar(x, 2.0)      # [2.0, 4.0, 6.0]\n        &gt;&gt;&gt; y3 = F.pow_scalar(x, 2.0)      # [1.0, 4.0, 9.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#mathematical-functions","title":"Mathematical Functions","text":""},{"location":"api-reference/nn/functional/#sin-cos-log-exp-sqrt","title":"sin, cos, log, exp, sqrt","text":"Python<pre><code>def sin(a: Tensor) -&gt; Tensor:\ndef cos(a: Tensor) -&gt; Tensor:\ndef log(a: Tensor) -&gt; Tensor:\ndef exp(a: Tensor) -&gt; Tensor:\ndef sqrt(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise mathematical functions.\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - Result of mathematical function\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y1 = F.sin(x)   # [0.0, 0.841, 0.909]\n        &gt;&gt;&gt; y2 = F.exp(x)   # [1.0, 2.718, 7.389]\n        &gt;&gt;&gt; y3 = F.sqrt(genesis.tensor([4.0, 9.0, 16.0]))  # [2.0, 3.0, 4.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#negate","title":"negate","text":"Python<pre><code>def negate(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise negation: -a\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - Negated tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, -2.0, 3.0])\n        &gt;&gt;&gt; y = F.negate(x)\n        &gt;&gt;&gt; # Result: [-1.0, 2.0, -3.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#shape-operations","title":"Shape Operations","text":""},{"location":"api-reference/nn/functional/#transpose","title":"transpose","text":"Python<pre><code>def transpose(a: Tensor, axis: tuple = None) -&gt; Tensor:\n    \"\"\"\n    Transpose tensor dimensions.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: tuple - Pair of dimensions to swap (default: last two dims)\n\n    Returns:\n        Tensor - Transposed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y1 = F.transpose(x)           # Swap last two dims: (3, 5, 4)\n        &gt;&gt;&gt; y2 = F.transpose(x, (0, 2))   # Swap dims 0,2: (5, 4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#reshape","title":"reshape","text":"Python<pre><code>def reshape(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    Reshape tensor to new shape.\n\n    Args:\n        a: Tensor - Input tensor\n        shape: tuple - New shape (must have same total elements)\n\n    Returns:\n        Tensor - Reshaped tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 6)\n        &gt;&gt;&gt; y = F.reshape(x, (3, 4))\n        &gt;&gt;&gt; # Changes shape from (2, 6) to (3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#view-expand-flatten","title":"view, expand, flatten","text":"Python<pre><code>def view(a: Tensor, shape: tuple) -&gt; Tensor:\ndef expand(a: Tensor, shape: tuple) -&gt; Tensor:\ndef flatten(a: Tensor, start_dim: int = 0, end_dim: int = None) -&gt; Tensor:\n    \"\"\"\n    Tensor view and shape manipulation operations.\n\n    Args:\n        a: Tensor - Input tensor\n        shape: tuple - Target shape\n        start_dim, end_dim: int - Dimensions to flatten\n\n    Returns:\n        Tensor - Transformed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y1 = F.view(x, (6, 4))         # View as (6, 4)\n        &gt;&gt;&gt; y2 = F.expand(x, (2, 3, 4, 5)) # Expand last dim\n        &gt;&gt;&gt; y3 = F.flatten(x, 1)           # Flatten from dim 1: (2, 12)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#tensor-operations","title":"Tensor Operations","text":""},{"location":"api-reference/nn/functional/#matmul","title":"matmul","text":"Python<pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Matrix multiplication.\n\n    Args:\n        a: Tensor - Left matrix\n        b: Tensor - Right matrix\n\n    Returns:\n        Tensor - Matrix product\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y = genesis.randn(4, 5)\n        &gt;&gt;&gt; z = F.matmul(x, y)  # Shape: (3, 5)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#stack-cat","title":"stack, cat","text":"Python<pre><code>def stack(tensors: list, dim: int = 0) -&gt; Tensor:\ndef cat(tensors: list, dim: int = 0) -&gt; Tensor:\n    \"\"\"\n    Stack or concatenate tensors along dimension.\n\n    Args:\n        tensors: list - List of tensors to combine\n        dim: int - Dimension along which to stack/concatenate\n\n    Returns:\n        Tensor - Combined tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = genesis.randn(2, 3)\n        &gt;&gt;&gt; z1 = F.stack([x, y], dim=0)  # Shape: (2, 2, 3)\n        &gt;&gt;&gt; z2 = F.cat([x, y], dim=0)    # Shape: (4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#squeeze-unsqueeze","title":"squeeze, unsqueeze","text":"Python<pre><code>def squeeze(tensor: Tensor, dim: int) -&gt; Tensor:\ndef unsqueeze(tensor: Tensor, dim: int) -&gt; Tensor:\n    \"\"\"\n    Remove or add singleton dimensions.\n\n    Args:\n        tensor: Tensor - Input tensor\n        dim: int - Dimension to squeeze/unsqueeze\n\n    Returns:\n        Tensor - Tensor with modified dimensions\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(1, 3, 1, 4)\n        &gt;&gt;&gt; y1 = F.squeeze(x, 0)    # Shape: (3, 1, 4)\n        &gt;&gt;&gt; y2 = F.unsqueeze(x, 2)  # Shape: (1, 3, 1, 1, 4)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#aggregation-operations","title":"Aggregation Operations","text":""},{"location":"api-reference/nn/functional/#sum","title":"sum","text":"Python<pre><code>def sum(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\n    \"\"\"\n    Sum tensor elements along specified dimensions.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: int - Dimension to sum over (None for all)\n        keepdims: bool - Whether to keep reduced dimensions\n\n    Returns:\n        Tensor - Summed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.sum(x)           # Sum all elements: scalar\n        &gt;&gt;&gt; y2 = F.sum(x, axis=0)   # Sum over rows: shape (4,)\n        &gt;&gt;&gt; y3 = F.sum(x, axis=1, keepdims=True)  # Shape: (3, 1)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#max-logsumexp","title":"max, logsumexp","text":"Python<pre><code>def max(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\ndef logsumexp(a: Tensor, axis: int = None) -&gt; Tensor:\n    \"\"\"\n    Maximum and log-sum-exp operations.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: int - Dimension to reduce over\n        keepdims: bool - Whether to keep reduced dimensions\n\n    Returns:\n        Tensor - Result tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.max(x, axis=1)      # Max along rows\n        &gt;&gt;&gt; y2 = F.logsumexp(x, axis=0) # LogSumExp along cols\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#activation-functions","title":"Activation Functions","text":""},{"location":"api-reference/nn/functional/#relu","title":"relu","text":"Python<pre><code>def relu(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    ReLU activation function: f(x) = max(0, x)\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - ReLU-activated tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y = F.relu(x)\n        &gt;&gt;&gt; # Result: [0.0, 0.0, 0.0, 1.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#advanced-operations-from-triton_ops","title":"Advanced Operations (from triton_ops)","text":""},{"location":"api-reference/nn/functional/#softmax","title":"softmax","text":"Python<pre><code># Imported from genesis.nn.triton_ops\nfrom genesis.nn.triton_ops import softmax\n\ndef softmax(x: Tensor, dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    Softmax function using optimized Triton kernel.\n\n    Args:\n        x: Tensor - Input tensor\n        dim: int - Dimension along which to apply softmax\n\n    Returns:\n        Tensor - Softmax output (sums to 1 along dim)\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = softmax(x, dim=1)\n        &gt;&gt;&gt; # Each row sums to 1\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#dropout","title":"dropout","text":"Python<pre><code># Imported from genesis.nn.triton_ops\nfrom genesis.nn.triton_ops import dropout\n\ndef dropout(x: Tensor, p: float = 0.5, training: bool = True) -&gt; Tensor:\n    \"\"\"\n    Dropout regularization using Triton kernel.\n\n    Args:\n        x: Tensor - Input tensor\n        p: float - Dropout probability\n        training: bool - Whether in training mode\n\n    Returns:\n        Tensor - Tensor with dropout applied\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(100, 50)\n        &gt;&gt;&gt; y = dropout(x, p=0.2, training=True)\n        &gt;&gt;&gt; # 20% of elements set to 0, others scaled by 1/(1-p)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#indexing-and-broadcasting","title":"Indexing and Broadcasting","text":""},{"location":"api-reference/nn/functional/#getitem-setitem-broadcast_to","title":"getitem, setitem, broadcast_to","text":"Python<pre><code>def getitem(a: Tensor, index) -&gt; Tensor:\ndef setitem(a: Tensor, index, value) -&gt; Tensor:\ndef broadcast_to(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    Tensor indexing and broadcasting operations.\n\n    Args:\n        a: Tensor - Input tensor\n        index: Various - Index (int, slice, list, Tensor)\n        value: Tensor/scalar - Value to set\n        shape: tuple - Target broadcast shape\n\n    Returns:\n        Tensor - Indexed/broadcast tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.getitem(x, [0, 2])      # Select rows 0 and 2\n        &gt;&gt;&gt; y2 = F.broadcast_to(x, (2, 3, 4))  # Broadcast to (2, 3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/functional/#performance-notes","title":"Performance Notes","text":"<ul> <li>GPU Acceleration: Operations automatically use GPU when tensors are on CUDA device</li> <li>Triton Optimization: Softmax and dropout use optimized Triton kernels</li> <li>Memory Efficiency: View operations share memory when possible</li> <li>Mixed Precision: Functions support automatic mixed precision when enabled</li> </ul>"},{"location":"api-reference/nn/functional/#common-usage-patterns","title":"Common Usage Patterns","text":"Python<pre><code>import genesis\nimport genesis.nn.functional as F\n\n# Basic operations\nx = genesis.randn(100, 784)\ny = F.relu(F.matmul(x, weights) + bias)\n\n# Shape manipulation\nx = genesis.randn(32, 3, 224, 224)\nx_flat = F.flatten(x, start_dim=1)  # (32, 150528)\n\n# Aggregation\nlogits = genesis.randn(32, 10)\nprobs = F.softmax(logits, dim=1)\nmax_vals = F.max(logits, axis=1)\n\n# Advanced indexing\nindices = genesis.tensor([0, 2, 4])\nselected = F.getitem(x, indices)\n</code></pre>"},{"location":"api-reference/nn/functional/#future-features-roadmap","title":"Future Features (Roadmap)","text":"<p>The following functions are planned for future releases: - Advanced activation functions (gelu, silu, swish) - Loss functions (cross_entropy, mse_loss, l1_loss) - Normalization functions (layer_norm, batch_norm) - Convolution operations (conv1d, conv2d) - Attention mechanisms (scaled_dot_product_attention)</p> <p>To track progress on these features, see the project roadmap on GitHub.</p>"},{"location":"api-reference/nn/modules/","title":"Neural Network Modules (genesis.nn)","text":""},{"location":"api-reference/nn/modules/#overview","title":"Overview","text":"<p>The <code>genesis.nn</code> module provides all the building blocks needed to create deep learning models. It follows a modular design where complex models are built by composing simpler components.</p>"},{"location":"api-reference/nn/modules/#core-concepts","title":"Core Concepts","text":""},{"location":"api-reference/nn/modules/#module-system","title":"Module System","text":"<p>All neural network components inherit from <code>nn.Module</code>, which provides: - Parameter management - Device and dtype handling - State serialization - Forward pass definition</p>"},{"location":"api-reference/nn/modules/#parameters","title":"Parameters","text":"<p>Parameters are tensors that are automatically tracked and updated during training: - Automatically registered when assigned as module attributes - Included in <code>module.parameters()</code> for optimizer - Saved/loaded with model state</p>"},{"location":"api-reference/nn/modules/#base-classes","title":"Base Classes","text":""},{"location":"api-reference/nn/modules/#nnmodule","title":"<code>nn.Module</code>","text":"<p>The base class for all neural network modules.</p> Python<pre><code>class Module:\n    \"\"\"Base class for all neural network modules.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the module.\"\"\"\n        self._modules = {}\n        self._parameters = {}\n        self._buffers = {}\n        self.training = True\n</code></pre>"},{"location":"api-reference/nn/modules/#core-methods","title":"Core Methods","text":""},{"location":"api-reference/nn/modules/#forward-pass","title":"Forward Pass","text":"Python<pre><code>def forward(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    Define the forward pass computation.\n    Must be overridden by subclasses.\n\n    Example:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def forward(self, x):\n        ...         return x * 2\n    \"\"\"\n    raise NotImplementedError\n\ndef __call__(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    Make module callable. Calls forward() internally.\n\n    Note: Always use module(input) instead of module.forward(input)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#parameter-management","title":"Parameter Management","text":"Python<pre><code>def parameters(self) -&gt; List[Tensor]:\n    \"\"\"\n    Return all parameters in the module.\n\n    Returns:\n        List of parameter tensors\n\n    Example:\n        &gt;&gt;&gt; model = nn.Linear(10, 5)\n        &gt;&gt;&gt; params = model.parameters()\n        &gt;&gt;&gt; print(len(params))  # 2 (weight and bias)\n    \"\"\"\n\ndef named_parameters(self) -&gt; List[Tuple[str, Tensor]]:\n    \"\"\"\n    Return parameters with their names.\n\n    Returns:\n        List of (name, parameter) tuples\n\n    Example:\n        &gt;&gt;&gt; for name, param in model.named_parameters():\n        ...     print(f\"{name}: {param.shape}\")\n    \"\"\"\n\ndef zero_grad(self) -&gt; None:\n    \"\"\"\n    Zero out gradients of all parameters.\n\n    Example:\n        &gt;&gt;&gt; model.zero_grad()  # Clear all gradients\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#module-hierarchy","title":"Module Hierarchy","text":"Python<pre><code>def add_module(self, name: str, module: Optional[Module]) -&gt; None:\n    \"\"\"\n    Add a child module.\n\n    Args:\n        name: Name for the submodule\n        module: Module instance to add\n\n    Example:\n        &gt;&gt;&gt; model = nn.Module()\n        &gt;&gt;&gt; model.add_module('fc', nn.Linear(10, 5))\n    \"\"\"\n\ndef modules(self) -&gt; Iterator[Module]:\n    \"\"\"Return iterator over all modules (including self).\"\"\"\n\ndef children(self) -&gt; Iterator[Module]:\n    \"\"\"Return iterator over immediate child modules.\"\"\"\n\ndef named_modules(self) -&gt; Iterator[Tuple[str, Module]]:\n    \"\"\"Return iterator over all modules with names.\"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#training-mode","title":"Training Mode","text":"Python<pre><code>def train(self, mode: bool = True) -&gt; Module:\n    \"\"\"\n    Set module to training mode.\n\n    Args:\n        mode: Whether to enable training mode\n\n    Returns:\n        self\n\n    Example:\n        &gt;&gt;&gt; model.train()  # Enable training mode\n        &gt;&gt;&gt; model.train(False)  # Equivalent to model.eval()\n    \"\"\"\n\ndef eval(self) -&gt; Module:\n    \"\"\"\n    Set module to evaluation mode.\n\n    Returns:\n        self\n\n    Example:\n        &gt;&gt;&gt; model.eval()  # Disable dropout, use running stats for BN\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#state-management","title":"State Management","text":"Python<pre><code>def state_dict(self) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Return state dictionary containing all parameters and buffers.\n\n    Returns:\n        Dictionary mapping parameter names to tensors\n\n    Example:\n        &gt;&gt;&gt; state = model.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'model.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: Dict[str, Tensor]) -&gt; None:\n    \"\"\"\n    Load parameters from state dictionary.\n\n    Args:\n        state_dict: Dictionary of parameters\n\n    Example:\n        &gt;&gt;&gt; state = genesis.load('model.pth')\n        &gt;&gt;&gt; model.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnparameter","title":"<code>nn.Parameter</code>","text":"<p>A special tensor that is automatically registered as a module parameter.</p> Python<pre><code>class Parameter(Tensor):\n    \"\"\"\n    A tensor that is automatically registered as a module parameter.\n\n    Args:\n        data: Tensor data\n        requires_grad: Whether to compute gradients (default: True)\n\n    Example:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def __init__(self):\n        ...         super().__init__()\n        ...         self.weight = nn.Parameter(genesis.randn(10, 5))\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#layer-types","title":"Layer Types","text":""},{"location":"api-reference/nn/modules/#linear-layers","title":"Linear Layers","text":""},{"location":"api-reference/nn/modules/#nnlinear","title":"<code>nn.Linear</code>","text":"<p>Fully connected layer performing linear transformation.</p> Python<pre><code>class Linear(Module):\n    \"\"\"\n    Linear transformation: y = xW^T + b\n\n    Args:\n        in_features: Size of input features\n        out_features: Size of output features\n        bias: Whether to include bias term (default: True)\n\n    Shape:\n        - Input: (*, in_features)\n        - Output: (*, out_features)\n\n    Example:\n        &gt;&gt;&gt; layer = nn.Linear(20, 30)\n        &gt;&gt;&gt; x = genesis.randn(128, 20)\n        &gt;&gt;&gt; output = layer(x)  # Shape: (128, 30)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#convolutional-layers","title":"Convolutional Layers","text":""},{"location":"api-reference/nn/modules/#nnconv2d","title":"<code>nn.Conv2d</code>","text":"<p>2D convolution layer for image processing.</p> Python<pre><code>class Conv2d(Module):\n    \"\"\"\n    2D convolution over input signal.\n\n    Args:\n        in_channels: Number of input channels\n        out_channels: Number of output channels\n        kernel_size: Size of convolving kernel\n        stride: Stride of convolution (default: 1)\n        padding: Zero-padding added to both sides (default: 0)\n        bias: Whether to add bias (default: True)\n\n    Shape:\n        - Input: (N, C_in, H, W)\n        - Output: (N, C_out, H_out, W_out)\n\n    Example:\n        &gt;&gt;&gt; conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        &gt;&gt;&gt; x = genesis.randn(32, 3, 224, 224)\n        &gt;&gt;&gt; output = conv(x)  # Shape: (32, 64, 224, 224)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#activation-functions","title":"Activation Functions","text":""},{"location":"api-reference/nn/modules/#nnrelu","title":"<code>nn.ReLU</code>","text":"<p>Rectified Linear Unit activation.</p> Python<pre><code>class ReLU(Module):\n    \"\"\"\n    ReLU activation: f(x) = max(0, x)\n\n    Args:\n        inplace: Whether to modify input in-place (default: False)\n\n    Example:\n        &gt;&gt;&gt; relu = nn.ReLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = relu(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnsilu-swish","title":"<code>nn.SiLU</code> (Swish)","text":"<p>Sigmoid Linear Unit activation.</p> Python<pre><code>class SiLU(Module):\n    \"\"\"\n    SiLU/Swish activation: f(x) = x * sigmoid(x)\n\n    Example:\n        &gt;&gt;&gt; silu = nn.SiLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = silu(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnsoftmax","title":"<code>nn.Softmax</code>","text":"<p>Softmax activation for multi-class classification.</p> Python<pre><code>class Softmax(Module):\n    \"\"\"\n    Softmax activation: softmax(x_i) = exp(x_i) / \u03a3 exp(x_j)\n\n    Args:\n        dim: Dimension along which to apply softmax\n\n    Example:\n        &gt;&gt;&gt; softmax = nn.Softmax(dim=-1)\n        &gt;&gt;&gt; x = genesis.randn(10, 5)\n        &gt;&gt;&gt; output = softmax(x)  # Each row sums to 1\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#normalization-layers","title":"Normalization Layers","text":""},{"location":"api-reference/nn/modules/#nnbatchnorm1d","title":"<code>nn.BatchNorm1d</code>","text":"<p>Batch normalization for 1D inputs.</p> Python<pre><code>class BatchNorm1d(Module):\n    \"\"\"\n    Batch normalization over 1D input.\n\n    Args:\n        dim: Number of features to normalize\n        eps: Small value for numerical stability (default: 1e-5)\n        momentum: Momentum for running stats (default: 0.1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Shape:\n        - Input: (N, C) where C = dim\n        - Output: Same as input\n\n    Example:\n        &gt;&gt;&gt; bn = nn.BatchNorm1d(100)\n        &gt;&gt;&gt; x = genesis.randn(20, 100)\n        &gt;&gt;&gt; output = bn(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnlayernorm","title":"<code>nn.LayerNorm</code>","text":"<p>Layer normalization.</p> Python<pre><code>class LayerNorm(Module):\n    \"\"\"\n    Layer normalization over last dimension.\n\n    Args:\n        dim: Dimension size to normalize\n        eps: Small value for numerical stability (default: 1e-5)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Shape:\n        - Input: (..., dim)\n        - Output: Same as input\n\n    Example:\n        &gt;&gt;&gt; ln = nn.LayerNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ln(x)  # Normalize over last dimension\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#dropout-layers","title":"Dropout Layers","text":""},{"location":"api-reference/nn/modules/#nndropout","title":"<code>nn.Dropout</code>","text":"<p>Dropout for regularization.</p> Python<pre><code>class Dropout(Module):\n    \"\"\"\n    Randomly zero out elements for regularization.\n\n    Args:\n        p: Probability of zeroing an element (default: 0.5)\n        inplace: Whether to modify input in-place (default: False)\n\n    Example:\n        &gt;&gt;&gt; dropout = nn.Dropout(p=0.2)\n        &gt;&gt;&gt; x = genesis.randn(20, 16)\n        &gt;&gt;&gt; output = dropout(x)  # Training mode: randomly zero 20% of elements\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#embedding-layers","title":"Embedding Layers","text":""},{"location":"api-reference/nn/modules/#nnembedding","title":"<code>nn.Embedding</code>","text":"<p>Embedding lookup table.</p> Python<pre><code>class Embedding(Module):\n    \"\"\"\n    Embedding lookup table.\n\n    Args:\n        num_embeddings: Size of vocabulary\n        embedding_dim: Dimension of embeddings\n\n    Shape:\n        - Input: (*) containing indices\n        - Output: (*, embedding_dim)\n\n    Example:\n        &gt;&gt;&gt; embed = nn.Embedding(10000, 300)  # 10k vocab, 300-dim embeddings\n        &gt;&gt;&gt; indices = genesis.tensor([1, 2, 3, 4])\n        &gt;&gt;&gt; output = embed(indices)  # Shape: (4, 300)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#attention-layers","title":"Attention Layers","text":""},{"location":"api-reference/nn/modules/#nnmultiheadattention","title":"<code>nn.MultiheadAttention</code>","text":"<p>Multi-head attention mechanism.</p> Python<pre><code>class MultiheadAttention(Module):\n    \"\"\"\n    Multi-head attention mechanism.\n\n    Args:\n        dim: Feature dimension (default: 64)\n        heads: Number of attention heads (default: 1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Note: Uses QKV projection matrix internally.\n\n    Example:\n        &gt;&gt;&gt; attn = nn.MultiheadAttention(dim=64, heads=8)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 64)  # (batch, seq_len, dim)\n        &gt;&gt;&gt; output, attention_weights = attn(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnfusedmultiheadattention","title":"<code>nn.FusedMultiheadAttention</code>","text":"<p>Fused multi-head attention with optimized kernels.</p> Python<pre><code>class FusedMultiheadAttention(Module):\n    \"\"\"\n    Fused multi-head attention using optimized kernels.\n\n    Args:\n        dim: Feature dimension (default: 64)\n        heads: Number of attention heads (default: 1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Note: Returns None for attention weights when using fused kernels.\n\n    Example:\n        &gt;&gt;&gt; attn = nn.FusedMultiheadAttention(dim=64, heads=8)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 64)  # (batch, seq_len, dim)\n        &gt;&gt;&gt; output, _ = attn(x)  # weights is None\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#container-modules","title":"Container Modules","text":""},{"location":"api-reference/nn/modules/#nnsequential","title":"<code>nn.Sequential</code>","text":"<p>Sequential container for modules.</p> Python<pre><code>class Sequential(Module):\n    \"\"\"\n    Sequential container that runs modules in order.\n\n    Args:\n        *modules: Sequence of modules to apply\n\n    Example:\n        &gt;&gt;&gt; model = nn.Sequential(\n        ...     nn.Linear(784, 256),\n        ...     nn.ReLU(),\n        ...     nn.Linear(256, 10)\n        ... )\n        &gt;&gt;&gt; x = genesis.randn(32, 784)\n        &gt;&gt;&gt; output = model(x)  # Shape: (32, 10)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnmodulelist","title":"<code>nn.ModuleList</code>","text":"<p>List container for modules.</p> Python<pre><code>class ModuleList(Module):\n    \"\"\"\n    List of modules that are properly registered.\n\n    Args:\n        modules: Optional list of modules\n\n    Example:\n        &gt;&gt;&gt; layers = nn.ModuleList([\n        ...     nn.Linear(10, 10) for _ in range(5)\n        ... ])\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; for layer in layers:\n        ...     x = layer(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnmoduledict","title":"<code>nn.ModuleDict</code>","text":"<p>Dictionary container for modules.</p> Python<pre><code>class ModuleDict(Module):\n    \"\"\"\n    Dictionary of modules with string keys.\n\n    Args:\n        modules: Optional dict of modules\n\n    Example:\n        &gt;&gt;&gt; layers = nn.ModuleDict({\n        ...     'fc1': nn.Linear(10, 20),\n        ...     'fc2': nn.Linear(20, 10)\n        ... })\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; x = layers['fc1'](x)\n        &gt;&gt;&gt; x = layers['fc2'](x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#loss-functions","title":"Loss Functions","text":""},{"location":"api-reference/nn/modules/#nnsoftmaxloss","title":"<code>nn.SoftmaxLoss</code>","text":"<p>Softmax cross-entropy loss.</p> Python<pre><code>class SoftmaxLoss(Module):\n    \"\"\"\n    Softmax cross-entropy loss for classification.\n\n    Handles label masking with -1 values.\n\n    Shape:\n        - Input: (N, C) where C is number of classes\n        - Target: (N,) containing class indices or -1 for masked positions\n\n    Example:\n        &gt;&gt;&gt; loss_fn = nn.SoftmaxLoss()\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)  # 32 samples, 10 classes\n        &gt;&gt;&gt; targets = genesis.randint(0, 10, (32,))\n        &gt;&gt;&gt; loss = loss_fn(logits, targets)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#additional-modules","title":"Additional Modules","text":""},{"location":"api-reference/nn/modules/#nnresidual","title":"<code>nn.Residual</code>","text":"<p>Residual connection wrapper.</p> Python<pre><code>class Residual(Module):\n    \"\"\"\n    Residual connection wrapper.\n\n    Args:\n        fn: Module to wrap with residual connection\n\n    Example:\n        &gt;&gt;&gt; layer = nn.Residual(nn.Linear(256, 256))\n        &gt;&gt;&gt; x = genesis.randn(32, 256)\n        &gt;&gt;&gt; output = layer(x)  # output = fn(x) + x\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnfusedlayernorm","title":"<code>nn.FusedLayerNorm</code>","text":"<p>Fused layer normalization with optimized kernels.</p> Python<pre><code>class FusedLayerNorm(Module):\n    \"\"\"\n    Fused layer normalization for better performance.\n\n    Args:\n        dim: Dimension to normalize\n        eps: Small value for stability (default: 1e-6)\n\n    Example:\n        &gt;&gt;&gt; ln = nn.FusedLayerNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ln(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnrmsnorm","title":"<code>nn.RMSNorm</code>","text":"<p>Root Mean Square normalization.</p> Python<pre><code>class RMSNorm(Module):\n    \"\"\"\n    RMS normalization (used in some LLMs).\n\n    Args:\n        dim: Dimension to normalize\n        eps: Small value for stability (default: 1e-6)\n\n    Example:\n        &gt;&gt;&gt; rms = nn.RMSNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = rms(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnrotaryembedding","title":"<code>nn.RotaryEmbedding</code>","text":"<p>Rotary position embeddings.</p> Python<pre><code>class RotaryEmbedding(Module):\n    \"\"\"\n    Rotary position embeddings for transformers.\n\n    Args:\n        dim: Embedding dimension\n        max_position_embeddings: Maximum sequence length (default: 2048)\n        base: Base for frequency computation (default: 10000)\n\n    Example:\n        &gt;&gt;&gt; rope = nn.RotaryEmbedding(64, max_position_embeddings=2048)\n        &gt;&gt;&gt; x = genesis.randn(1, 8, 100, 64)  # (batch, heads, seq, dim)\n        &gt;&gt;&gt; cos, sin = rope(x, seq_len=100)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#nnfeedfowardswiglu","title":"<code>nn.FeedFowardSwiGLU</code>","text":"<p>SwiGLU feedforward network.</p> Python<pre><code>class FeedFowardSwiGLU(Module):\n    \"\"\"\n    SwiGLU feedforward network (https://arxiv.org/pdf/2002.05202.pdf).\n\n    Args:\n        dim: Input/output dimension\n        hidden_dim: Hidden layer dimension\n\n    Example:\n        &gt;&gt;&gt; ff = nn.FeedFowardSwiGLU(768, 3072)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ff(x)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/nn/modules/#building-custom-modules","title":"Building Custom Modules","text":""},{"location":"api-reference/nn/modules/#example-custom-layer","title":"Example: Custom Layer","text":"Python<pre><code>class CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Parameters are automatically tracked\n        self.weight = nn.Parameter(genesis.randn(out_features, in_features))\n        self.bias = nn.Parameter(genesis.zeros(out_features))\n\n        # Submodules are automatically tracked\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        # Define forward pass\n        x = genesis.matmul(x, self.weight.T) + self.bias\n        x = self.activation(x)\n        return x\n\n# Usage\nlayer = CustomLayer(10, 5)\nx = genesis.randn(32, 10)\noutput = layer(x)\n</code></pre>"},{"location":"api-reference/nn/modules/#example-custom-model","title":"Example: Custom Model","text":"Python<pre><code>class SimpleBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear2 = nn.Linear(dim, dim)\n        self.norm = nn.LayerNorm(dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.linear1(x))\n        x = self.linear2(x)\n        x = self.norm(x + residual)  # Skip connection\n        return x\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, dim=768, num_classes=10):\n        super().__init__()\n        self.embedding = nn.Embedding(10000, dim)\n        self.layers = nn.ModuleList([\n            SimpleBlock(dim) for _ in range(6)\n        ])\n        self.norm = nn.LayerNorm(dim)\n        self.fc = nn.Linear(dim, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)\n        x = x.mean(dim=1)  # Global average pooling\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"api-reference/nn/modules/#best-practices","title":"Best Practices","text":"<ol> <li>Always override <code>forward()</code>: Define computation in forward method</li> <li>Use <code>module(input)</code>: Never call forward() directly</li> <li>Register parameters: Use nn.Parameter for learnable parameters</li> <li>Track submodules: Assign modules as attributes for automatic tracking</li> <li>Handle training/eval: Use different behavior for training vs evaluation</li> <li>Initialize weights: Proper initialization improves convergence</li> </ol>"},{"location":"api-reference/nn/modules/#see-also","title":"See Also","text":"<ul> <li>Functional API - Functional operations</li> <li>Optimizers - Training optimizers</li> <li>Autograd - Automatic differentiation</li> <li>Examples - Complete examples</li> </ul>"},{"location":"api-reference/optim/lr_scheduler/","title":"Learning Rate Schedulers","text":"<p>Genesis provides learning rate schedulers to adjust the learning rate during training, which is crucial for achieving optimal convergence in deep learning models.</p>"},{"location":"api-reference/optim/lr_scheduler/#overview","title":"Overview","text":"<p>Learning rate scheduling is a technique used to adjust the learning rate throughout the training process. Genesis provides a simple and effective scheduler implementation.</p>"},{"location":"api-reference/optim/lr_scheduler/#available-schedulers","title":"Available Schedulers","text":""},{"location":"api-reference/optim/lr_scheduler/#lambdalr","title":"LambdaLR","text":"<p>The <code>LambdaLR</code> scheduler allows you to define a custom function to modify the learning rate at each epoch.</p> Python<pre><code>class LambdaLR:\n    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n        \"\"\"\n        Multiply learning rate by a factor given by lr_lambda function.\n\n        Args:\n            optimizer: Wrapped optimizer\n            lr_lambda: Function to compute multiplicative factor\n            last_epoch: The index of last epoch (default: -1)\n            verbose: If True, prints a message for each update (not used)\n\n        Attributes:\n            base_lrs: Base learning rate from optimizer\n        \"\"\"\n</code></pre> <p>Implementation Details: - The scheduler stores the base learning rate from the optimizer - Each step multiplies the base learning rate by the lambda function output - Directly modifies the optimizer's <code>lr</code> attribute</p> <p>Usage Example: Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.optim.lr_scheduler import LambdaLR\n\n# Create model and optimizer\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Define learning rate schedule function\ndef lr_lambda(epoch):\n    # Decay learning rate by factor of 0.95 every 10 epochs\n    return 0.95 ** (epoch // 10)\n\n# Create scheduler\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# Training loop\nfor epoch in range(100):\n    # Training code here\n    loss = train_one_epoch(model, dataloader, optimizer)\n\n    # Step scheduler\n    scheduler.step()\n    print(f\"Epoch {epoch}: lr={scheduler.get_last_lr()}\")\n</code></pre></p>"},{"location":"api-reference/optim/lr_scheduler/#cosine-annealing-with-warmup","title":"Cosine Annealing with Warmup","text":"<p>The <code>get_cosine_schedule_with_warmup</code> function creates a cosine annealing schedule with linear warmup.</p> Python<pre><code>def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    \"\"\"\n    Create a schedule with linear warmup and cosine decay.\n\n    Args:\n        optimizer: Wrapped optimizer\n        num_warmup_steps: Number of steps for warmup phase\n        num_training_steps: Total number of training steps\n\n    Returns:\n        LambdaLR scheduler object\n\n    Schedule:\n        - Linear warmup from 0 to base_lr over num_warmup_steps\n        - Cosine annealing from base_lr to 0 over remaining steps\n    \"\"\"\n</code></pre> <p>Usage Example: Python<pre><code>import genesis.optim as optim\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\n# Training configuration\nnum_epochs = 100\nsteps_per_epoch = 1000\ntotal_steps = num_epochs * steps_per_epoch\nwarmup_steps = total_steps // 10  # 10% warmup\n\n# Create optimizer and scheduler\noptimizer = optim.AdamW(model.parameters(), lr=5e-4)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Training loop with per-step scheduling\nstep = 0\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass and optimization\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Step scheduler every batch\n        scheduler.step()\n        step += 1\n\n        if step % 100 == 0:\n            print(f\"Step {step}: lr={scheduler.get_last_lr():.6f}\")\n</code></pre></p>"},{"location":"api-reference/optim/lr_scheduler/#scheduler-methods","title":"Scheduler Methods","text":""},{"location":"api-reference/optim/lr_scheduler/#step","title":"step()","text":"Python<pre><code>def step(self, epoch=None):\n    \"\"\"\n    Update learning rate according to schedule.\n\n    Args:\n        epoch: Current epoch (optional, uses internal counter if None)\n\n    Updates:\n        - Increments last_epoch if epoch is None\n        - Computes new learning rate using lr_lambda\n        - Updates optimizer.lr directly\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#get_last_lr","title":"get_last_lr()","text":"Python<pre><code>def get_last_lr(self):\n    \"\"\"\n    Return the last computed learning rate.\n\n    Returns:\n        float: Current learning rate value\n\n    Note: Returns the value stored in _last_lr attribute\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#state_dict","title":"state_dict()","text":"Python<pre><code>def state_dict(self):\n    \"\"\"\n    Return the state of the scheduler as a dict.\n\n    Returns:\n        Dictionary containing:\n        - last_epoch: Current epoch counter\n        - base_lrs: Base learning rate\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#load_state_dict","title":"load_state_dict()","text":"Python<pre><code>def load_state_dict(self, state_dict):\n    \"\"\"\n    Load scheduler state from dict.\n\n    Args:\n        state_dict: Dictionary containing scheduler state\n\n    Loads:\n        - last_epoch: Restore epoch counter\n        - base_lrs: Restore base learning rate\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/optim/lr_scheduler/#exponential-decay","title":"Exponential Decay","text":"Python<pre><code># Decay learning rate by 0.95 every epoch\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#step-decay","title":"Step Decay","text":"Python<pre><code># Reduce learning rate by half every 30 epochs\ndef step_decay(epoch):\n    return 0.5 ** (epoch // 30)\n\nscheduler = LambdaLR(optimizer, lr_lambda=step_decay)\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#polynomial-decay","title":"Polynomial Decay","text":"Python<pre><code># Polynomial decay to zero\ndef poly_decay(epoch, total_epochs=100, power=0.9):\n    return (1 - epoch / total_epochs) ** power\n\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: poly_decay(epoch))\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#cosine-with-restarts","title":"Cosine with Restarts","text":"Python<pre><code>import math\n\ndef cosine_restart(epoch, restart_period=50):\n    epoch_in_cycle = epoch % restart_period\n    return 0.5 * (1 + math.cos(math.pi * epoch_in_cycle / restart_period))\n\nscheduler = LambdaLR(optimizer, lr_lambda=cosine_restart)\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#integration-with-training","title":"Integration with Training","text":""},{"location":"api-reference/optim/lr_scheduler/#basic-training-loop","title":"Basic Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.optim.lr_scheduler import LambdaLR\n\n# Setup\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Simple exponential decay\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Forward pass\n        outputs = model(data)\n        loss = nn.SoftmaxLoss()(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Update learning rate after each epoch\n    scheduler.step()\n    current_lr = scheduler.get_last_lr()\n    print(f'Epoch: {epoch}, LR: {current_lr:.6f}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#checkpoint-integration","title":"Checkpoint Integration","text":"Python<pre><code>import genesis\n\n# Save scheduler state with model checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'epoch': epoch,\n    'loss': loss\n}\ngenesis.save(checkpoint, 'checkpoint.pth')\n\n# Load scheduler state\ncheckpoint = genesis.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Schedule: </li> <li>Use cosine annealing for most applications</li> <li>Add warmup for transformer models</li> <li> <p>Use step decay for fine-tuning</p> </li> <li> <p>Warmup Phase:</p> </li> <li>Essential for large batch sizes</li> <li>Recommended for transformer architectures</li> <li> <p>Typically 5-10% of total training steps</p> </li> <li> <p>Monitoring:</p> </li> <li>Log learning rate values</li> <li>Plot learning rate schedule</li> <li> <p>Monitor validation loss during training</p> </li> <li> <p>Checkpointing:</p> </li> <li>Always save scheduler state</li> <li>Resume training with correct learning rate</li> <li>Essential for long training runs</li> </ol>"},{"location":"api-reference/optim/lr_scheduler/#examples","title":"Examples","text":""},{"location":"api-reference/optim/lr_scheduler/#transformer-training-schedule","title":"Transformer Training Schedule","text":"Python<pre><code># Typical transformer training schedule with warmup\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\n# 10K warmup steps, 100K total steps\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=10000,\n    num_training_steps=100000\n)\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#custom-schedule-function","title":"Custom Schedule Function","text":"Python<pre><code>def custom_schedule(epoch, warmup_epochs=5, total_epochs=100):\n    \"\"\"Custom schedule with warmup and decay.\"\"\"\n    if epoch &lt; warmup_epochs:\n        # Linear warmup\n        return (epoch + 1) / warmup_epochs\n    else:\n        # Exponential decay after warmup\n        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n        return 0.5 ** (progress * 3)  # Decay to 1/8 of original\n\nscheduler = LambdaLR(optimizer, lr_lambda=lambda e: custom_schedule(e))\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#learning-rate-range-test","title":"Learning Rate Range Test","text":"Python<pre><code># Find optimal learning rate range\ndef lr_range_test(model, optimizer, start_lr=1e-7, end_lr=10, num_it=100):\n    lrs = []\n    losses = []\n\n    # Exponential growth from start_lr to end_lr\n    lr_lambda = lambda step: (end_lr / start_lr) ** (step / num_it)\n    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n    # Set initial learning rate\n    optimizer.lr = start_lr\n\n    for i in range(num_it):\n        # Training step\n        loss = train_step(model, batch)\n        losses.append(loss)\n        lrs.append(optimizer.lr)\n\n        scheduler.step()\n\n        if loss &gt; 4 * min(losses):  # Stop if loss explodes\n            break\n\n    return lrs, losses\n</code></pre>"},{"location":"api-reference/optim/lr_scheduler/#implementation-notes","title":"Implementation Notes","text":"<p>The Genesis LambdaLR scheduler is a simplified but effective implementation that: - Directly modifies the optimizer's learning rate attribute - Maintains minimal state (epoch counter and base learning rate) - Provides flexible scheduling through lambda functions - Is compatible with all Genesis optimizers</p> <p>The API is designed to be familiar to PyTorch users while being simpler and more direct in its implementation.</p>"},{"location":"api-reference/optim/optimizers/","title":"Optimizers (genesis.optim)","text":""},{"location":"api-reference/optim/optimizers/#overview","title":"Overview","text":"<p>The <code>genesis.optim</code> module provides optimizers for training neural networks. It implements state-of-the-art optimization algorithms with support for momentum, weight decay, and adaptive learning rates.</p>"},{"location":"api-reference/optim/optimizers/#core-concepts","title":"Core Concepts","text":""},{"location":"api-reference/optim/optimizers/#optimization-process","title":"Optimization Process","text":"<p>Optimizers update model parameters based on computed gradients using various algorithms: 1. Gradient Descent: Basic parameter update using gradients 2. Momentum: Accelerated convergence using moving averages 3. Adaptive Learning Rates: Different learning rates per parameter 4. Weight Decay: L2 regularization</p>"},{"location":"api-reference/optim/optimizers/#base-classes","title":"Base Classes","text":""},{"location":"api-reference/optim/optimizers/#optimoptimizer","title":"<code>optim.Optimizer</code>","text":"<p>Abstract base class for all optimizers.</p> Python<pre><code>class Optimizer:\n    \"\"\"\n    Base class for all optimizers.\n\n    Provides common functionality for parameter updates, gradient zeroing,\n    and state management across different optimization algorithms.\n    \"\"\"\n\n    def __init__(self, params):\n        \"\"\"\n        Initialize optimizer with parameters to optimize.\n\n        Args:\n            params: Iterable of parameters to optimize\n        \"\"\"\n</code></pre>"},{"location":"api-reference/optim/optimizers/#core-methods","title":"Core Methods","text":""},{"location":"api-reference/optim/optimizers/#optimization-step","title":"Optimization Step","text":"Python<pre><code>def step(self):\n    \"\"\"\n    Perform a single optimization step (parameter update).\n\n    Must be implemented by subclasses.\n\n    Example:\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss = criterion(output, target)\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef zero_grad(self):\n    \"\"\"\n    Zero gradients of all optimized parameters.\n    Sets grad to None for all parameters.\n\n    Example:\n        &gt;&gt;&gt; # Clear gradients before each training step\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef reset_grad(self):\n    \"\"\"\n    Reset gradients of all optimized parameters.\n    Alias for zero_grad().\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/optimizers/#state-management","title":"State Management","text":"Python<pre><code>def state_dict(self):\n    \"\"\"\n    Return optimizer state as a dictionary.\n\n    Returns:\n        Dictionary containing optimizer state (excluding params)\n\n    Example:\n        &gt;&gt;&gt; # Save optimizer state\n        &gt;&gt;&gt; state = optimizer.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'optimizer_checkpoint.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict):\n    \"\"\"\n    Load optimizer state from dictionary.\n\n    Args:\n        state_dict: Optimizer state dictionary\n\n    Example:\n        &gt;&gt;&gt; # Restore optimizer state\n        &gt;&gt;&gt; state = genesis.load('optimizer_checkpoint.pth')\n        &gt;&gt;&gt; optimizer.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api-reference/optim/optimizers/#optimizers","title":"Optimizers","text":""},{"location":"api-reference/optim/optimizers/#optimsgd","title":"<code>optim.SGD</code>","text":"<p>Stochastic Gradient Descent optimizer with momentum and weight decay.</p> Python<pre><code>class SGD(Optimizer):\n    \"\"\"\n    Stochastic Gradient Descent optimizer with momentum and weight decay.\n\n    Implements the classical SGD algorithm with optional momentum for improved\n    convergence and weight decay for regularization.\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.01)\n        momentum: Momentum factor, 0 disables momentum (default: 0.0)\n        weight_decay: Weight decay (L2 penalty) factor (default: 0.0)\n\n    Algorithm:\n        grad = gradient + weight_decay * param\n        velocity = momentum * velocity + (1 - momentum) * grad\n        param = param - lr * velocity\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.01,\n        momentum=0.0,\n        weight_decay=0.0\n    ):\n</code></pre>"},{"location":"api-reference/optim/optimizers/#usage-examples","title":"Usage Examples","text":"Python<pre><code>import genesis.optim as optim\n\n# Basic SGD\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# SGD with momentum (recommended for most tasks)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# SGD with weight decay\noptimizer = optim.SGD(model.parameters(), lr=0.01, \n                     momentum=0.9, weight_decay=1e-4)\n</code></pre>"},{"location":"api-reference/optim/optimizers/#optimadam","title":"<code>optim.Adam</code>","text":"<p>Adaptive Moment Estimation optimizer combining RMSprop and momentum.</p> Python<pre><code>class Adam(Optimizer):\n    \"\"\"\n    Adam optimizer with adaptive learning rates.\n\n    Implements the Adam algorithm which computes adaptive learning rates\n    for each parameter using estimates of first and second moments of gradients.\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.01)\n        beta1: Coefficient for first moment estimate (default: 0.9)\n        beta2: Coefficient for second moment estimate (default: 0.999)\n        eps: Term added to denominator for numerical stability (default: 1e-8)\n        weight_decay: Weight decay coefficient (default: 0.0)\n\n    Algorithm:\n        grad = gradient + weight_decay * param\n        m_t = \u03b2\u2081 * m_{t-1} + (1 - \u03b2\u2081) * grad\n        v_t = \u03b2\u2082 * v_{t-1} + (1 - \u03b2\u2082) * grad\u00b2\n        m\u0302_t = m_t / (1 - \u03b2\u2081\u1d57)\n        v\u0302_t = v_t / (1 - \u03b2\u2082\u1d57)\n        param = param - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.01,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0\n    ):\n</code></pre>"},{"location":"api-reference/optim/optimizers/#state-variables","title":"State Variables","text":"<p>Each parameter maintains the following state: - <code>t</code>: Time step counter (incremented on each step) - <code>m</code>: First moment estimate (momentum) - <code>v</code>: Second moment estimate (adaptive learning rate)</p>"},{"location":"api-reference/optim/optimizers/#usage-examples_1","title":"Usage Examples","text":"Python<pre><code># Default Adam\noptimizer = optim.Adam(model.parameters())\n\n# Custom learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# With weight decay\noptimizer = optim.Adam(model.parameters(), lr=0.001,\n                      weight_decay=1e-5)\n\n# Transformer model settings\noptimizer = optim.Adam(model.parameters(), lr=0.0001,\n                      beta1=0.9, beta2=0.98, eps=1e-9)\n</code></pre>"},{"location":"api-reference/optim/optimizers/#optimadamw","title":"<code>optim.AdamW</code>","text":"<p>Adam optimizer with decoupled weight decay.</p> Python<pre><code>class AdamW(Optimizer):\n    \"\"\"\n    AdamW optimizer (Adam with decoupled weight decay).\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.001)\n        beta1: Coefficient for computing running averages (default: 0.9)\n        beta2: Coefficient for computing running averages (default: 0.999)\n        eps: Term for numerical stability (default: 1e-8)\n        weight_decay: Weight decay coefficient (default: 0.01)\n\n    Difference from Adam:\n        Adam:  param = param - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * param)\n        AdamW: param = param - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * param)\n\n    AdamW applies weight decay directly to parameters rather than to gradients,\n    providing better regularization especially for adaptive learning rate methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.001,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.01\n    ):\n</code></pre>"},{"location":"api-reference/optim/optimizers/#usage-examples_2","title":"Usage Examples","text":"Python<pre><code># Default AdamW (recommended for Transformers)\noptimizer = optim.AdamW(model.parameters())\n\n# BERT/GPT standard settings\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\n# Large model training\noptimizer = optim.AdamW(model.parameters(), lr=1e-4,\n                       beta1=0.9, beta2=0.95, weight_decay=0.1)\n</code></pre>"},{"location":"api-reference/optim/optimizers/#training-examples","title":"Training Examples","text":""},{"location":"api-reference/optim/optimizers/#basic-training-loop","title":"Basic Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Model and optimizer setup\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.SoftmaxLoss()\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api-reference/optim/optimizers/#training-with-mixed-precision","title":"Training with Mixed Precision","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Enable mixed precision\ngenesis.enable_autocast = True\n\n# Model setup\nmodel = TransformerModel()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        # Mixed precision is handled automatically in Function.apply()\n        outputs = model(batch['input'])\n        loss = criterion(outputs, batch['target'])\n\n        # Backward pass\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n</code></pre>"},{"location":"api-reference/optim/optimizers/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"Python<pre><code>from genesis.optim.lr_scheduler import LambdaLR\n\n# Optimizer and scheduler\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\n# Define learning rate schedule\ndef lr_lambda(epoch):\n    return 0.95 ** epoch\n\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\nfor epoch in range(num_epochs):\n    # Training\n    for batch in train_loader:\n        optimizer.zero_grad()\n        loss = compute_loss(model, batch)\n        loss.backward()\n        optimizer.step()\n\n    # Update learning rate\n    scheduler.step()\n    print(f'Epoch {epoch}, LR: {scheduler.get_last_lr():.6f}')\n</code></pre>"},{"location":"api-reference/optim/optimizers/#gradient-accumulation","title":"Gradient Accumulation","text":"Python<pre><code># Simulate larger batch size through accumulation\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(train_loader):\n    # Forward pass\n    outputs = model(batch['input'])\n    loss = criterion(outputs, batch['target'])\n\n    # Normalize loss by accumulation steps\n    loss = loss / accumulation_steps\n    loss.backward()\n\n    # Update weights every accumulation_steps\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"api-reference/optim/optimizers/#optimizer-selection-guide","title":"Optimizer Selection Guide","text":""},{"location":"api-reference/optim/optimizers/#sgd","title":"SGD","text":"<ul> <li>Advantages: Simple, memory efficient, good generalization</li> <li>Disadvantages: Slow convergence, sensitive to learning rate</li> <li>Best for:</li> <li>Computer vision tasks (ResNet, VGG)</li> <li>Memory-constrained environments</li> <li>When best generalization is needed</li> <li>Recommended settings: <code>lr=0.1, momentum=0.9, weight_decay=1e-4</code></li> </ul>"},{"location":"api-reference/optim/optimizers/#adam","title":"Adam","text":"<ul> <li>Advantages: Fast convergence, adaptive, less sensitive to hyperparameters</li> <li>Disadvantages: Higher memory usage (2x parameters)</li> <li>Best for:</li> <li>NLP tasks</li> <li>Rapid prototyping</li> <li>Sparse gradients</li> <li>Recommended settings: <code>lr=1e-3, beta1=0.9, beta2=0.999</code></li> </ul>"},{"location":"api-reference/optim/optimizers/#adamw","title":"AdamW","text":"<ul> <li>Advantages: Better generalization than Adam, excellent for large models</li> <li>Disadvantages: Higher memory usage (2x parameters)</li> <li>Best for:</li> <li>Transformer models (BERT, GPT)</li> <li>Large-scale pre-training</li> <li>When strong regularization is needed</li> <li>Recommended settings: <code>lr=5e-5, weight_decay=0.01</code></li> </ul>"},{"location":"api-reference/optim/optimizers/#performance-tips","title":"Performance Tips","text":"<ol> <li>Gradient Accumulation: Simulate larger batch sizes when memory is limited</li> <li>Learning Rate Scheduling: Use schedulers to improve convergence</li> <li>Weight Decay: AdamW generally performs better than Adam + L2 regularization</li> <li>Mixed Precision: Reduces memory usage and speeds up training</li> <li>Zero Gradients: Always clear gradients before backward pass</li> </ol>"},{"location":"api-reference/optim/optimizers/#memory-considerations","title":"Memory Considerations","text":"<ul> <li>Adam/AdamW maintain state per parameter (2x parameter memory for momentum and variance)</li> <li>Use <code>zero_grad()</code> to clear gradients and free memory</li> <li>Consider optimizer state when moving models between devices</li> <li>Save optimizer state in checkpoints for resuming training</li> </ul>"},{"location":"api-reference/optim/optimizers/#best-practices","title":"Best Practices","text":"<ol> <li>Always clear gradients before backward pass</li> <li>Monitor learning rates throughout training</li> <li>Save optimizer state in checkpoints</li> <li>Use appropriate weight decay for your model type</li> <li>Consider mixed precision for large models</li> </ol>"},{"location":"api-reference/optim/optimizers/#see-also","title":"See Also","text":"<ul> <li>Learning Rate Schedulers - Dynamic learning rate adjustment</li> <li>Neural Network Modules - Building models</li> <li>Autograd - Automatic differentiation</li> <li>Examples - Complete training examples</li> </ul>"},{"location":"api-reference/utils/","title":"Utilities (genesis.utils)","text":""},{"location":"api-reference/utils/#overview","title":"Overview","text":"<p>The <code>genesis.utils</code> module provides essential utilities for development, debugging, and data handling. It includes profiling tools, data loading utilities, and helper functions to streamline the deep learning workflow.</p>"},{"location":"api-reference/utils/#core-components","title":"Core Components","text":""},{"location":"api-reference/utils/#performance-profiling","title":"Performance Profiling","text":"<ul> <li>Function and method execution time tracking</li> <li>Automatic profiling with decorators</li> <li>Performance analysis and reporting</li> </ul>"},{"location":"api-reference/utils/#data-loading","title":"Data Loading","text":"<ul> <li>Dataset abstraction for training data</li> <li>DataLoader with batching and shuffling</li> <li>Support for both map-style and iterable datasets</li> </ul>"},{"location":"api-reference/utils/#profiling-tools","title":"Profiling Tools","text":""},{"location":"api-reference/utils/#profile-decorator","title":"<code>@profile</code> Decorator","text":"<p>Automatic performance profiling for functions and classes.</p> Python<pre><code>from genesis.utils import profile\n\n@profile\ndef expensive_function(x):\n    \"\"\"\n    Profile this function's execution time and call count.\n    \"\"\"\n    # Your computation here\n    return x * 2\n\n@profile\nclass MyModel:\n    \"\"\"\n    Profile all methods in this class.\n    \"\"\"\n    def forward(self, x):\n        return x + 1\n\n    def backward(self, grad):\n        return grad\n</code></pre> <p>The profiler automatically tracks: - Call Count: Number of times each function is called - Total Time: Cumulative execution time - Average Time: Mean execution time per call</p>"},{"location":"api-reference/utils/#usage-examples","title":"Usage Examples","text":"Python<pre><code>import genesis.utils as utils\nimport time\n\n# Profile a function\n@utils.profile\ndef matrix_multiply(a, b):\n    \"\"\"Dummy matrix multiplication.\"\"\"\n    time.sleep(0.01)  # Simulate computation\n    return a @ b\n\n# Profile a class\n@utils.profile\nclass NeuralNetwork:\n    def __init__(self):\n        pass\n\n    def forward(self, x):\n        time.sleep(0.005)  # Simulate forward pass\n        return x * 2\n\n    def backward(self, grad):\n        time.sleep(0.003)  # Simulate backward pass\n        return grad\n\n# Use the profiled functions\nmodel = NeuralNetwork()\nfor i in range(100):\n    x = matrix_multiply([[1, 2]], [[3], [4]])\n    y = model.forward(x)\n    model.backward([1, 1])\n\n# Profile data is automatically printed at program exit\n</code></pre>"},{"location":"api-reference/utils/#profile-data-format","title":"Profile Data Format","text":"<p>When the program exits, profiling data is automatically printed:</p> Text Only<pre><code>Program cost 2.1456 seconds!\n__main__.matrix_multiply: 100 calls, 1.0234 total seconds\n__main__.NeuralNetwork.forward: 100 calls, 0.5123 total seconds\n__main__.NeuralNetwork.backward: 100 calls, 0.3089 total seconds\n</code></pre>"},{"location":"api-reference/utils/#manual-profiling","title":"Manual Profiling","text":"<p>For more granular control, you can access profiling data programmatically:</p> Python<pre><code>from genesis.utils.profile import profile_data, print_profile_data\n\n# Get current profile data\ncurrent_data = profile_data.copy()\nprint(f\"Function calls so far: {sum(data['calls'] for data in current_data.values())}\")\n\n# Print profile summary manually\nprint_profile_data()\n</code></pre>"},{"location":"api-reference/utils/#data-loading_1","title":"Data Loading","text":""},{"location":"api-reference/utils/#dataset","title":"<code>Dataset</code>","text":"<p>Abstract base class for all datasets.</p> Python<pre><code>from genesis.utils.data import Dataset\n\nclass Dataset:\n    \"\"\"\n    Abstract dataset class.\n\n    All subclasses must implement __len__ and __getitem__.\n    \"\"\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the size of the dataset.\n\n        Returns:\n            Number of samples in dataset\n        \"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        Retrieve a sample by index.\n\n        Args:\n            idx: Sample index\n\n        Returns:\n            Data sample at the given index\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api-reference/utils/#custom-dataset-example","title":"Custom Dataset Example","text":"Python<pre><code>import numpy as np\nfrom genesis.utils.data import Dataset\n\nclass MNIST(Dataset):\n    \"\"\"Example MNIST dataset implementation.\"\"\"\n\n    def __init__(self, data_path, transform=None):\n        \"\"\"\n        Initialize MNIST dataset.\n\n        Args:\n            data_path: Path to MNIST data files\n            transform: Optional data transformation function\n        \"\"\"\n        self.data = self._load_data(data_path)\n        self.labels = self._load_labels(data_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n    def _load_data(self, path):\n        # Load your data here\n        return np.random.randn(10000, 28, 28)  # Dummy data\n\n    def _load_labels(self, path):\n        # Load your labels here\n        return np.random.randint(0, 10, 10000)  # Dummy labels\n</code></pre>"},{"location":"api-reference/utils/#iterabledataset","title":"<code>IterableDataset</code>","text":"<p>Base class for iterable-style datasets.</p> Python<pre><code>from genesis.utils.data import IterableDataset\n\nclass IterableDataset(Dataset):\n    \"\"\"\n    Base class for iterable datasets.\n\n    Useful for streaming data or when random access is not feasible.\n    \"\"\"\n\n    def __iter__(self):\n        \"\"\"\n        Return an iterator over the dataset.\n\n        Returns:\n            Iterator that yields data samples\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api-reference/utils/#iterable-dataset-example","title":"Iterable Dataset Example","text":"Python<pre><code>import random\nfrom genesis.utils.data import IterableDataset\n\nclass RandomDataStream(IterableDataset):\n    \"\"\"Example streaming dataset.\"\"\"\n\n    def __init__(self, num_samples, feature_dim):\n        \"\"\"\n        Initialize streaming dataset.\n\n        Args:\n            num_samples: Number of samples to generate\n            feature_dim: Dimension of each sample\n        \"\"\"\n        self.num_samples = num_samples\n        self.feature_dim = feature_dim\n\n    def __iter__(self):\n        \"\"\"Generate random samples on-the-fly.\"\"\"\n        for _ in range(self.num_samples):\n            # Generate random data\n            data = [random.random() for _ in range(self.feature_dim)]\n            label = random.randint(0, 9)\n            yield data, label\n</code></pre>"},{"location":"api-reference/utils/#dataloader","title":"<code>DataLoader</code>","text":"<p>Efficient data loading with batching and shuffling.</p> Python<pre><code>from genesis.utils.data import DataLoader\n\nclass DataLoader:\n    \"\"\"\n    Data loader for batching and shuffling datasets.\n\n    Args:\n        dataset: Dataset instance (Dataset or IterableDataset)\n        batch_size: Number of samples per batch (default: 1)\n        shuffle: Whether to shuffle data each epoch (default: False)\n    \"\"\"\n\n    def __init__(\n        self, \n        dataset, \n        batch_size: int = 1, \n        shuffle: bool = False\n    ):\n</code></pre>"},{"location":"api-reference/utils/#dataloader-examples","title":"DataLoader Examples","text":"Python<pre><code>from genesis.utils.data import Dataset, DataLoader\nimport numpy as np\n\n# Create a simple dataset\nclass SimpleDataset(Dataset):\n    def __init__(self, size):\n        self.data = np.random.randn(size, 10)\n        self.labels = np.random.randint(0, 2, size)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Create dataset and dataloader\ndataset = SimpleDataset(1000)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training loop\nfor epoch in range(5):\n    print(f\"Epoch {epoch + 1}\")\n    for batch_idx, batch in enumerate(dataloader):\n        # batch is a list of (data, label) tuples\n        batch_data = [item[0] for item in batch]\n        batch_labels = [item[1] for item in batch]\n\n        # Convert to arrays if needed\n        batch_data = np.array(batch_data)\n        batch_labels = np.array(batch_labels)\n\n        print(f\"  Batch {batch_idx}: data shape {batch_data.shape}\")\n\n        # Your training code here\n        pass\n</code></pre>"},{"location":"api-reference/utils/#advanced-dataloader-usage","title":"Advanced DataLoader Usage","text":"Python<pre><code># Large dataset with shuffling\nlarge_dataset = SimpleDataset(50000)\ntrain_loader = DataLoader(large_dataset, batch_size=128, shuffle=True)\n\n# Iterable dataset\nstream_dataset = RandomDataStream(1000, 20)\nstream_loader = DataLoader(stream_dataset, batch_size=16)\n\n# Small batch for debugging\ndebug_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n\n# Training loop with multiple dataloaders\ndef train_model(model, train_loader, val_loader):\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        for batch in train_loader:\n            # Training code\n            pass\n\n        # Validation phase\n        model.eval()\n        for batch in val_loader:\n            # Validation code\n            pass\n</code></pre>"},{"location":"api-reference/utils/#integration-with-genesis-training","title":"Integration with Genesis Training","text":""},{"location":"api-reference/utils/#complete-training-example","title":"Complete Training Example","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.utils.data import Dataset, DataLoader\nfrom genesis.utils import profile\nimport numpy as np\n\n# Custom dataset\nclass TrainingDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Profiled model\n@profile\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Generate dummy data\nX = np.random.randn(1000, 20).astype(np.float32)\ny = np.random.randint(0, 3, 1000)\n\n# Create dataset and dataloader\ndataset = TrainingDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Model and optimizer\nmodel = SimpleModel(20, 64, 3)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with profiling\n@profile\ndef train_epoch(model, dataloader, optimizer, criterion):\n    \"\"\"Train for one epoch.\"\"\"\n    total_loss = 0.0\n    for batch in dataloader:\n        # Extract batch data\n        batch_x = [item[0] for item in batch]\n        batch_y = [item[1] for item in batch]\n\n        # Convert to Genesis tensors\n        x = genesis.tensor(batch_x)\n        y = genesis.tensor(batch_y)\n\n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n# Train the model\nfor epoch in range(10):\n    avg_loss = train_epoch(model, dataloader, optimizer, criterion)\n    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n\n# Profiling data will be printed automatically at program exit\n</code></pre>"},{"location":"api-reference/utils/#best-practices","title":"Best Practices","text":""},{"location":"api-reference/utils/#profiling-guidelines","title":"Profiling Guidelines","text":"<ol> <li>Use for Development: Enable profiling during development to identify bottlenecks</li> <li>Disable for Production: Remove profiling decorators in production code</li> <li>Selective Profiling: Profile only the functions you suspect are slow</li> <li>Batch Profiling: Profile entire training loops rather than individual operations</li> </ol>"},{"location":"api-reference/utils/#data-loading-guidelines","title":"Data Loading Guidelines","text":"<ol> <li>Appropriate Batch Size: Balance memory usage and training efficiency</li> <li>Shuffle Training Data: Always shuffle training data between epochs</li> <li>Don't Shuffle Validation: Keep validation data in consistent order</li> <li>Memory Considerations: Use iterable datasets for very large datasets</li> <li>Data Preprocessing: Apply transforms in the dataset's <code>__getitem__</code> method</li> </ol>"},{"location":"api-reference/utils/#performance-tips","title":"Performance Tips","text":""},{"location":"api-reference/utils/#efficient-data-loading","title":"Efficient Data Loading","text":"Python<pre><code># Good: Efficient batch processing\nclass EfficientDataset(Dataset):\n    def __init__(self, data):\n        # Pre-process data once\n        self.data = self._preprocess(data)\n\n    def _preprocess(self, data):\n        # Expensive preprocessing done once\n        return data * 2 + 1\n\n    def __getitem__(self, idx):\n        # Fast access\n        return self.data[idx]\n\n# Good: Large batch sizes when possible\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n\n# Good: Use appropriate data types\ndata = np.array(data, dtype=np.float32)  # Use float32 instead of float64\n</code></pre>"},{"location":"api-reference/utils/#memory-management","title":"Memory Management","text":"Python<pre><code># Good: Delete large objects when done\ndel large_dataset\ndel temporary_data\n\n# Good: Use generators for large datasets\ndef data_generator():\n    for file in file_list:\n        data = load_file(file)\n        yield data\n\n# Good: Limit memory usage with smaller batches if needed\nsmall_batch_loader = DataLoader(dataset, batch_size=16)\n</code></pre>"},{"location":"api-reference/utils/#see-also","title":"See Also","text":"<ul> <li>Neural Network Modules - Building models</li> <li>Optimizers - Training algorithms  </li> <li>Autograd - Automatic differentiation</li> <li>Performance Guide - Optimization techniques</li> </ul>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The Genesis deep learning framework adopts a layered modular architecture design that maintains code clarity while achieving high-performance computational capabilities.</p>"},{"location":"architecture/#overall-architecture","title":"\ud83c\udfd7\ufe0f Overall Architecture","text":"<pre><code>graph TB\n    subgraph \"User API Layer\"\n        A[genesis.Tensor] --&gt; B[genesis.nn.*]\n        A --&gt; C[genesis.optim.*]\n        A --&gt; D[genesis.functional.*]\n    end\n\n    subgraph \"Automatic Differentiation Layer\"\n        E[tensor.Tensor] --&gt; F[Function Base Class]\n        F --&gt; G[Context]\n    end\n\n    subgraph \"Tensor System\"\n        H[backend.py] --&gt; I[NDArray Interface]\n    end\n\n    subgraph \"Backend Implementation\"\n        I --&gt; J[CPU Backend&lt;br/&gt;PyTorch Tensors]\n        I --&gt; K[GPU Backend&lt;br/&gt;CUDA + Triton]\n    end\n\n    subgraph \"GPU Independent Implementation\"\n        K --&gt; L[cuda_storage.py&lt;br/&gt;Pure CUDA Memory Management]\n        K --&gt; M[cuda_indexing_ops.py&lt;br/&gt;Advanced Indexing Operations]\n        K --&gt; N[cuda_kernels.py&lt;br/&gt;Triton kernels]\n        L --&gt; O[CUDA Python API]\n        M --&gt; O\n        N --&gt; P[Triton Compiler]\n    end\n\n    subgraph \"CPU Implementation\"\n        J --&gt; P[cpu.py&lt;br/&gt;PyTorch Operations]\n        P --&gt; Q[PyTorch Backend]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    E --&gt; H\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style H fill:#fff3e0\n    style K fill:#e8f5e8\n    style J fill:#fce4ec</code></pre>"},{"location":"architecture/#core-design-philosophy","title":"\ud83d\udd11 Core Design Philosophy","text":""},{"location":"architecture/#1-dual-backend-architecture","title":"1. Dual Backend Architecture","text":"<p>Genesis adopts an innovative dual backend design:</p> <ul> <li>CPU Backend: Leverages PyTorch's mature CPU tensor implementation to ensure stability and compatibility</li> <li>GPU Backend: Completely independent CUDA implementation that demonstrates the complete process of building a GPU compute stack from scratch</li> </ul>"},{"location":"architecture/#2-balance-between-education-and-performance","title":"2. Balance Between Education and Performance","text":"<ul> <li>Code Readability: Each module has clear responsibility separation and detailed documentation</li> <li>Performance Optimization: GPU backend uses Triton to implement high-performance kernels</li> <li>Progressive Learning: From simple CPU implementation to complex GPU optimization</li> </ul>"},{"location":"architecture/#3-modular-design","title":"3. Modular Design","text":"<p>Each component can be understood and extended independently: - Automatic differentiation system is independent of specific tensor implementations - Neural network modules are based on general tensor operations - Backend abstraction allows easy switching between different implementations</p>"},{"location":"architecture/#recent-architecture-improvements","title":"\ud83d\udd04 Recent Architecture Improvements","text":""},{"location":"architecture/#enhanced-module-organization","title":"Enhanced Module Organization","text":"<ul> <li>Streamlined CUDA Operations: Consolidated indexing operations into <code>cuda_indexing_ops.py</code> for better maintainability</li> <li>Optimized Memory Management: Improved CUDA memory allocation patterns and reduced overhead</li> <li>Cleaner Code Structure: Removed redundant modules and optimized component relationships</li> </ul>"},{"location":"architecture/#performance-enhancements","title":"Performance Enhancements","text":"<ul> <li>Faster Kernel Compilation: Optimized Triton kernel initialization and compilation process</li> <li>Reduced Startup Time: Improved framework initialization and warmup procedures</li> <li>Better Resource Utilization: Enhanced GPU memory usage patterns and allocation strategies</li> </ul>"},{"location":"architecture/#main-component-details","title":"\ud83d\udcca Main Component Details","text":""},{"location":"architecture/#automatic-differentiation-system-tensorpy-and-functionpy","title":"Automatic Differentiation System (<code>tensor.py</code> and <code>function.py</code>)","text":"Python<pre><code># Core class structure\nclass Tensor:\n    data: NDArray          # Underlying data storage\n    requires_grad: bool    # Whether gradients are required\n    creator: Function      # The operation that created this tensor\n    grad: Tensor          # Gradient tensor\n\nclass Function:\n    @staticmethod\n    def forward(ctx, *args)    # Forward propagation\n    @staticmethod \n    def backward(ctx, grad)    # Backward propagation\n</code></pre> <p>Key Features: - Supports automatic type conversion for mixed precision training - Flexible computation graph construction and traversal - Built-in gradient accumulation and zeroing mechanisms</p>"},{"location":"architecture/#tensor-backend-system","title":"Tensor Backend System","text":""},{"location":"architecture/#cpu-backend-backendscpupy","title":"CPU Backend (<code>backends/cpu.py</code>)","text":"Python<pre><code># Direct use of PyTorch operations\ndef add(x, y):\n    return x + y\n\ndef matmul(x, y):\n    return torch.matmul(x, y)\n</code></pre>"},{"location":"architecture/#gpu-backend-backendscudapy-and-backendscuda_kernelspy","title":"GPU Backend (<code>backends/cuda.py</code> and <code>backends/cuda_kernels.py</code>)","text":"Python<pre><code># GPU kernels implemented with Triton\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"architecture/#cuda-memory-management-cuda_storagepy","title":"CUDA Memory Management (<code>cuda_storage.py</code>)","text":"Python<pre><code>class CUDATensor:\n    \"\"\"Pure CUDA tensor implementation, independent of PyTorch\"\"\"\n    def __init__(self, shape, dtype):\n        self._cuda_device, self._cuda_context = _ensure_cuda_initialized()\n        self._allocate_memory(shape, dtype)\n\n    def _allocate_memory(self, shape, dtype):\n        # Allocate GPU memory directly using CUDA Python API\n        size_bytes = prod(shape) * dtype.itemsize\n        result = cuda.cuMemAlloc(size_bytes)\n        self._data_ptr = check_cuda_error(result)\n</code></pre>"},{"location":"architecture/#neural-network-modules-nnmodules","title":"Neural Network Modules (<code>nn/modules/</code>)","text":"<p>Genesis adopts a modular architecture similar to PyTorch for better code organization:</p> Text Only<pre><code>nn/modules/\n\u251c\u2500\u2500 module.py          # Base Module and Parameter classes\n\u251c\u2500\u2500 linear.py          # Linear, Flatten layers\n\u251c\u2500\u2500 activation.py      # ReLU, Softmax, SiLU activation functions\n\u251c\u2500\u2500 normalization.py   # BatchNorm, LayerNorm, RMSNorm\n\u251c\u2500\u2500 loss.py           # CrossEntropyLoss, MSELoss, BCELoss\n\u251c\u2500\u2500 container.py      # Sequential, ModuleList containers\n\u251c\u2500\u2500 dropout.py        # Dropout regularization\n\u251c\u2500\u2500 sparse.py         # Embedding, RotaryEmbedding\n\u2514\u2500\u2500 transformer.py    # MultiheadAttention, FeedForwardSwiGLU\n</code></pre> <p>Core Implementation:</p> Python<pre><code># modules/module.py\nclass Module:\n    \"\"\"Base class for neural network modules\"\"\"\n    def parameters(self) -&gt; List[Tensor]:\n        # Recursively collect all parameters\n        return _unpack_params(self.__dict__)\n\n    def forward(self, *args, **kwargs):\n        # Subclasses implement specific forward propagation logic\n        raise NotImplementedError\n\n# modules/linear.py  \nclass Linear(Module):\n    \"\"\"Fully connected layer implementation\"\"\"\n    def __init__(self, in_features, out_features):\n        self.weight = Parameter(genesis.randn(out_features, in_features))\n        self.bias = Parameter(genesis.zeros(out_features))\n\n# modules/loss.py\nclass CrossEntropyLoss(Module):\n    \"\"\"Cross-entropy loss for classification\"\"\"\n    def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:\n        log_prob = F.log_softmax(input, dim=1)\n        # ... implementation details\n</code></pre>"},{"location":"architecture/#key-technical-implementations","title":"\ud83d\udd27 Key Technical Implementations","text":""},{"location":"architecture/#1-memory-management-strategy","title":"1. Memory Management Strategy","text":"<p>CPU Memory Management: - Relies on PyTorch's memory pool and garbage collection - Automatically handles memory alignment and cache optimization</p> <p>GPU Memory Management: Python<pre><code>class CUDATensor:\n    def __init__(self, shape, dtype, base=None):\n        if base is not None:\n            # View tensor: shares memory but maintains reference to original tensor\n            self.base = base\n            self._data_ptr = base._data_ptr + offset\n        else:\n            # New tensor: allocate independent memory\n            self.base = None\n            self._data_ptr = cuda.cuMemAlloc(size_bytes)\n\n    def __del__(self):\n        # Only base tensors free memory\n        if self.base is None and self._data_ptr:\n            cuda.cuMemFree(self._data_ptr)\n</code></pre></p>"},{"location":"architecture/#2-device-abstraction","title":"2. Device Abstraction","text":"Python<pre><code>class Device:\n    def __init__(self, name: str, mod: Any, device_id: Optional[int] = None):\n        self.name = name        # \"cpu\" or \"cuda\"\n        self.mod = mod          # Corresponding operation module\n        self.device_id = device_id  # GPU device ID\n\n    def randn(self, *shape, dtype=genesis.float32):\n        if self.name == \"cuda\":\n            return NDArray(CUDATensor(shape, dtype), device=self)\n        else:\n            return NDArray(torch.randn(*shape), device=self)\n</code></pre>"},{"location":"architecture/#3-type-system","title":"3. Type System","text":"Python<pre><code># dtypes.py - Unified data type system\nclass DType:\n    def __init__(self, name: str, torch_dtype, numpy_dtype, itemsize: int):\n        self.name = name\n        self.torch_dtype = torch_dtype\n        self.numpy_dtype = numpy_dtype  \n        self.itemsize = itemsize\n\n# Supported data types\nfloat32 = DType(\"float32\", torch.float32, np.float32, 4)\nfloat16 = DType(\"float16\", torch.float16, np.float16, 2)\nbfloat16 = DType(\"bfloat16\", torch.bfloat16, np.dtype('uint16'), 2)\n</code></pre>"},{"location":"architecture/#performance-optimization-strategies","title":"\ud83d\ude80 Performance Optimization Strategies","text":""},{"location":"architecture/#1-triton-kernel-optimization","title":"1. Triton Kernel Optimization","text":"<p>Softmax Implementation: Python<pre><code>@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride, output_row_stride, \n                  n_cols, BLOCK_SIZE: tl.constexpr):\n    # Efficient parallel softmax implementation\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets &lt; n_cols, other=-float('inf'))\n\n    # Numerically stable softmax\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets &lt; n_cols)\n</code></pre></p>"},{"location":"architecture/#2-mixed-precision-training","title":"2. Mixed Precision Training","text":"Python<pre><code># amp.py - Automatic mixed precision\nenable_autocast = False\n\ndef _cast(value, dtype):\n    \"\"\"Automatic type conversion\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n</code></pre>"},{"location":"architecture/#architectural-advantages","title":"\ud83d\udd0d Architectural Advantages","text":""},{"location":"architecture/#educational-value","title":"Educational Value","text":"<ol> <li>Progressive Complexity: From simple CPU implementation to complex GPU optimization</li> <li>Complete Implementation Showcase: Demonstrates the complete construction process of a deep learning framework  </li> <li>Clear Module Boundaries: Each component has clear responsibilities, making it easy to understand</li> </ol>"},{"location":"architecture/#engineering-practice","title":"Engineering Practice","text":"<ol> <li>Dual Backend Design: CPU stability + GPU high performance</li> <li>Memory Safety: RAII pattern memory management prevents memory leaks</li> <li>Type Safety: Unified type system avoids type errors</li> </ol>"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":"<ol> <li>Triton Optimization: Modern GPU kernel development approach</li> <li>Zero-Copy Views: Efficient tensor view operations</li> <li>Parallel Computing: Fully utilizes GPU parallel capabilities</li> </ol>"},{"location":"architecture/#design-trade-offs","title":"\ud83c\udfaf Design Trade-offs","text":""},{"location":"architecture/#cpu-vs-gpu-implementation-choice","title":"CPU vs GPU Implementation Choice","text":"<ul> <li>CPU: Uses PyTorch to ensure stability and compatibility</li> <li>GPU: Independent implementation showcases complete GPU programming stack</li> </ul>"},{"location":"architecture/#simplicity-vs-performance","title":"Simplicity vs Performance","text":"<ul> <li>Maintains simple APIs while implementing highly optimized underlying layers</li> <li>Isolates complexity in the lower layers through layered architecture</li> </ul>"},{"location":"architecture/#education-vs-production","title":"Education vs Production","text":"<ul> <li>Code emphasizes readability and educational value</li> <li>Performance still reaches production-level standards</li> </ul> <p>This architectural design makes Genesis both an excellent learning resource and a fully functional deep learning framework.</p>"},{"location":"backends/","title":"Backend System Overview","text":"<p>Genesis v2.0 introduces a modular backend architecture that cleanly separates device-specific implementations while maintaining a unified interface.</p>"},{"location":"backends/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>The backend system follows a clear separation of concerns:</p> <pre><code>graph TB\n    subgraph \"User API Layer\"\n        A[genesis.tensor] --&gt; B[genesis.Tensor]\n        C[genesis.matmul] --&gt; D[Operations]\n    end\n\n    subgraph \"Core Abstraction\"\n        B --&gt; E[tensor.py]\n        D --&gt; F[ops/dispatcher.py]\n        E --&gt; G[storage.py]\n        E --&gt; H[device.py]\n    end\n\n    subgraph \"Backend Implementation\"\n        G --&gt; I[backends/cpu.py]\n        G --&gt; J[backends/cuda.py]\n        F --&gt; K[ops/cpu/]\n        F --&gt; L[ops/cuda/]\n        J --&gt; M[backends/cuda_memory.py]\n        J --&gt; N[backends/cuda_kernels.py]\n    end\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style F fill:#fff3e0\n    style I fill:#e8f5e9\n    style J fill:#e8f5e9</code></pre>"},{"location":"backends/#key-components","title":"\ud83c\udfaf Key Components","text":""},{"location":"backends/#device-abstraction","title":"Device Abstraction","text":"<p>The <code>genesis.device</code> module provides a unified interface for device management: - Automatic device selection - Device-agnostic tensor creation - Transparent memory management</p>"},{"location":"backends/#storage-layer","title":"Storage Layer","text":"<p>The <code>genesis.storage</code> module handles the underlying data storage: - Abstracts device-specific storage implementations - Manages memory lifecycle - Provides efficient data transfer between devices</p>"},{"location":"backends/#backend-implementations","title":"Backend Implementations","text":""},{"location":"backends/#cpu-backend","title":"CPU Backend","text":"<ul> <li>File: <code>backends/cpu.py</code></li> <li>Implementation: Leverages PyTorch for CPU operations</li> <li>Features: Full compatibility, efficient CPU utilization</li> </ul>"},{"location":"backends/#cuda-backend","title":"CUDA Backend","text":"<ul> <li>Files: <code>backends/cuda.py</code>, <code>backends/cuda_memory.py</code>, <code>backends/cuda_kernels.py</code></li> <li>Implementation: Pure CUDA/Triton implementation</li> <li>Features:</li> <li>Custom memory management with pooling</li> <li>Optimized CUDA kernels</li> <li>Lazy initialization for reliability</li> </ul>"},{"location":"backends/#design-principles","title":"\ud83d\ude80 Design Principles","text":""},{"location":"backends/#1-modularity","title":"1. Modularity","text":"<p>Each backend is completely self-contained, making it easy to: - Add new device support - Optimize specific operations - Test backends independently</p>"},{"location":"backends/#2-performance","title":"2. Performance","text":"<p>The backend system is designed for maximum performance: - Zero-copy operations where possible - Efficient memory pooling - Kernel fusion opportunities</p>"},{"location":"backends/#3-reliability","title":"3. Reliability","text":"<p>Built-in safety features: - Lazy CUDA initialization - Automatic memory management - Graceful fallbacks</p>"},{"location":"backends/#usage-example","title":"\ud83d\udca1 Usage Example","text":"Python<pre><code>import genesis\n\n# Device abstraction handles backend selection\ndevice = genesis.device(\"cuda\")\n\n# Tensors automatically use appropriate backend\nx = genesis.tensor([1, 2, 3], device=device)\ny = genesis.tensor([4, 5, 6], device=device)\n\n# Operations dispatch to correct backend\nz = genesis.matmul(x, y)  # Uses CUDA backend\n\n# Seamless device transfer\ncpu_tensor = z.to(\"cpu\")  # Transfer to CPU backend\n</code></pre>"},{"location":"backends/#adding-new-backends","title":"\ud83d\udd04 Adding New Backends","text":"<p>To add a new backend:</p> <ol> <li>Create a new file in <code>backends/</code> directory</li> <li>Implement the storage interface</li> <li>Add operations in <code>ops/&lt;backend&gt;/</code></li> <li>Register with the dispatcher</li> </ol> <p>See Backend Development Guide for detailed instructions.</p>"},{"location":"backends/cpu/","title":"CPU Backend","text":"<p>The CPU backend provides efficient CPU-based tensor operations by leveraging PyTorch's optimized CPU kernels.</p>"},{"location":"backends/cpu/#overview","title":"\ud83d\udccb Overview","text":"<p>The CPU backend (<code>backends/cpu.py</code>) serves as: - The default backend for CPU operations - A reference implementation for new backends - A fallback when GPU is unavailable</p>"},{"location":"backends/cpu/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"Python<pre><code># backends/cpu.py structure\nclass CPUStorage:\n    \"\"\"CPU tensor storage implementation.\"\"\"\n\n    def __init__(self, data):\n        \"\"\"Initialize with PyTorch tensor.\"\"\"\n        self.data = data  # PyTorch tensor\n\n    def to(self, device):\n        \"\"\"Transfer to another device.\"\"\"\n        ...\n\n    def copy_(self, other):\n        \"\"\"In-place copy from another storage.\"\"\"\n        ...\n</code></pre>"},{"location":"backends/cpu/#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"backends/cpu/#pytorch-integration","title":"PyTorch Integration","text":"<ul> <li>Leverages PyTorch's mature CPU implementations</li> <li>Benefits from PyTorch's optimizations (MKL, OpenMP)</li> <li>Compatible with PyTorch tensors for interoperability</li> </ul>"},{"location":"backends/cpu/#operation-support","title":"Operation Support","text":"<p>The CPU backend supports all fundamental operations:</p> Category Operations Arithmetic add, subtract, multiply, divide, power Reduction sum, mean, max, min, argmax, argmin Matrix matmul, transpose, reshape, flatten Activation relu, sigmoid, tanh, softmax Comparison eq, ne, lt, le, gt, ge"},{"location":"backends/cpu/#memory-management","title":"Memory Management","text":"<ul> <li>Direct memory access without pooling (handled by PyTorch)</li> <li>Efficient memory layout for cache optimization</li> <li>Support for various data types (float32, float64, int32, etc.)</li> </ul>"},{"location":"backends/cpu/#implementation-details","title":"\ud83d\udcbb Implementation Details","text":""},{"location":"backends/cpu/#storage-creation","title":"Storage Creation","text":"Python<pre><code>def create_cpu_storage(data, dtype=None):\n    \"\"\"Create CPU storage from various input types.\"\"\"\n    if isinstance(data, torch.Tensor):\n        tensor = data.cpu()\n    elif isinstance(data, np.ndarray):\n        tensor = torch.from_numpy(data)\n    else:\n        tensor = torch.tensor(data)\n\n    if dtype:\n        tensor = tensor.to(dtype)\n\n    return CPUStorage(tensor)\n</code></pre>"},{"location":"backends/cpu/#operation-dispatch","title":"Operation Dispatch","text":"<p>Operations are dispatched through the unified operation system: Python<pre><code># ops/cpu/basic.py\ndef cpu_add(a, b):\n    \"\"\"CPU implementation of addition.\"\"\"\n    return a.data + b.data\n\ndef cpu_matmul(a, b):\n    \"\"\"CPU implementation of matrix multiplication.\"\"\"\n    return torch.matmul(a.data, b.data)\n</code></pre></p>"},{"location":"backends/cpu/#performance-considerations","title":"\ud83d\ude80 Performance Considerations","text":""},{"location":"backends/cpu/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Vectorization: Leverage SIMD instructions through PyTorch</li> <li>Parallelization: Utilize multiple CPU cores via OpenMP</li> <li>Cache Efficiency: Optimize memory access patterns</li> </ol>"},{"location":"backends/cpu/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use contiguous memory layouts for better cache utilization</li> <li>Batch operations to reduce overhead</li> <li>Consider memory pinning for CPU-GPU transfers</li> </ul>"},{"location":"backends/cpu/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"backends/cpu/#environment-variables","title":"Environment Variables","text":"Bash<pre><code># Control CPU thread count\nexport OMP_NUM_THREADS=8\n\n# Enable MKL optimizations\nexport MKL_NUM_THREADS=8\n</code></pre>"},{"location":"backends/cpu/#runtime-configuration","title":"Runtime Configuration","text":"Python<pre><code>import genesis\n\n# Set CPU backend as default\ngenesis.set_default_device(\"cpu\")\n\n# Create CPU tensors\nx = genesis.tensor([1, 2, 3])  # Uses CPU backend\n</code></pre>"},{"location":"backends/cpu/#benchmarks","title":"\ud83d\udcca Benchmarks","text":"<p>Relative performance compared to pure PyTorch:</p> Operation Size Genesis CPU PyTorch Ratio Add 1M 1.05x 1.0x 1.05 MatMul 1024x1024 0.98x 1.0x 0.98 Softmax 10K 1.02x 1.0x 1.02 <p>Note: Near-identical performance due to PyTorch backend</p>"},{"location":"backends/cpu/#debugging","title":"\ud83d\udd0d Debugging","text":"<p>Enable debug mode for CPU operations: Python<pre><code>import genesis\ngenesis.backends.cpu.debug_mode = True\n\n# Now operations will print debug information\nx = genesis.tensor([1, 2, 3], device=\"cpu\")\ny = x + 1  # Prints: \"CPU Add: shape=(3,), dtype=float32\"\n</code></pre></p>"},{"location":"backends/cpu/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Backend System Overview</li> <li>CUDA Backend</li> <li>Operation Dispatch</li> </ul>"},{"location":"backends/cuda/","title":"CUDA Backend","text":"<p>The CUDA backend provides high-performance GPU operations through custom CUDA kernels and Triton implementations.</p>"},{"location":"backends/cuda/#overview","title":"\ud83d\udccb Overview","text":"<p>The CUDA backend consists of several specialized components: - <code>backends/cuda.py</code> - Main CUDA storage implementation - <code>backends/cuda_memory.py</code> - Advanced memory management - <code>backends/cuda_kernels.py</code> - Optimized CUDA kernels</p>"},{"location":"backends/cuda/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"CUDA Backend\"\n        A[cuda.py] --&gt; B[CUDAStorage]\n        C[cuda_memory.py] --&gt; D[MemoryPool]\n        E[cuda_kernels.py] --&gt; F[Custom Kernels]\n    end\n\n    subgraph \"Memory Management\"\n        D --&gt; G[Block Allocator]\n        D --&gt; H[Memory Stats]\n        D --&gt; I[Garbage Collection]\n    end\n\n    subgraph \"Kernel System\"\n        F --&gt; J[Triton Kernels]\n        F --&gt; K[CUDA C++ Kernels]\n        F --&gt; L[Kernel Cache]\n    end\n\n    style A fill:#76ff03\n    style C fill:#ff5722\n    style E fill:#2196f3</code></pre>"},{"location":"backends/cuda/#key-features","title":"\ud83c\udfaf Key Features","text":""},{"location":"backends/cuda/#advanced-memory-management","title":"Advanced Memory Management","text":"<ul> <li>Memory Pooling: Reduces allocation overhead</li> <li>Smart Caching: Reuses memory blocks efficiently</li> <li>Fragmentation Handling: Minimizes memory fragmentation</li> <li>Statistics Tracking: Detailed memory usage analytics</li> </ul>"},{"location":"backends/cuda/#custom-kernel-implementation","title":"Custom Kernel Implementation","text":"<ul> <li>Triton Kernels: High-performance GPU kernels</li> <li>Kernel Fusion: Combines multiple operations</li> <li>Auto-tuning: Optimal block size selection</li> <li>Lazy Compilation: Kernels compiled on first use</li> </ul>"},{"location":"backends/cuda/#reliability-features","title":"Reliability Features","text":"<ul> <li>Lazy Initialization: Avoids CUDA errors on import</li> <li>Error Handling: Graceful fallbacks on failure</li> <li>Memory Cleanup: Automatic resource management</li> </ul>"},{"location":"backends/cuda/#performance-optimizations","title":"\ud83c\udfc3\u200d\u2642\ufe0f Performance Optimizations","text":""},{"location":"backends/cuda/#memory-pool","title":"Memory Pool","text":"Python<pre><code>class CUDAMemoryPool:\n    \"\"\"High-performance CUDA memory pool.\"\"\"\n\n    def allocate(self, size):\n        \"\"\"Allocate memory block.\"\"\"\n        # Try to reuse existing block\n        block = self._find_free_block(size)\n        if block:\n            return block\n\n        # Allocate new block\n        return self._allocate_new_block(size)\n\n    def deallocate(self, ptr):\n        \"\"\"Return block to pool.\"\"\"\n        self._free_blocks.add(ptr)\n</code></pre>"},{"location":"backends/cuda/#kernel-optimization","title":"Kernel Optimization","text":"<p>Triton kernels with auto-tuning: Python<pre><code>@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),\n    ],\n    key=['n_elements'],\n)\n@triton.jit\ndef elementwise_add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized elementwise addition kernel.\"\"\"\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre></p>"},{"location":"backends/cuda/#implementation-details","title":"\ud83d\udcbb Implementation Details","text":""},{"location":"backends/cuda/#cuda-storage","title":"CUDA Storage","text":"Python<pre><code>class CUDAStorage:\n    \"\"\"CUDA tensor storage with advanced features.\"\"\"\n\n    def __init__(self, shape, dtype, device_id=0):\n        self.device_id = device_id\n        self.shape = shape\n        self.dtype = dtype\n        self._data_ptr = None\n        self._initialize_lazy()\n\n    def _initialize_lazy(self):\n        \"\"\"Lazy CUDA initialization.\"\"\"\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"CUDA not available\")\n\n        torch.cuda.set_device(self.device_id)\n        size = self._compute_size()\n        self._data_ptr = CUDAMemoryPool.get_instance().allocate(size)\n</code></pre>"},{"location":"backends/cuda/#memory-statistics","title":"Memory Statistics","text":"Python<pre><code>def get_memory_stats():\n    \"\"\"Get detailed memory usage statistics.\"\"\"\n    pool = CUDAMemoryPool.get_instance()\n    return {\n        'allocated': pool.allocated_bytes,\n        'cached': pool.cached_bytes,\n        'reserved': pool.reserved_bytes,\n        'free': pool.free_bytes,\n        'fragmentation': pool.fragmentation_ratio,\n        'peak_allocated': pool.peak_allocated_bytes,\n    }\n</code></pre>"},{"location":"backends/cuda/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"backends/cuda/#environment-variables","title":"Environment Variables","text":"Bash<pre><code># Specify GPU device\nexport CUDA_VISIBLE_DEVICES=0\n\n# Memory pool settings\nexport GENESIS_CUDA_MEMORY_FRACTION=0.8\nexport GENESIS_CUDA_CACHE_SIZE=1GB\n\n# Kernel compilation cache\nexport GENESIS_KERNEL_CACHE_DIR=/tmp/genesis_kernels\n</code></pre>"},{"location":"backends/cuda/#runtime-configuration","title":"Runtime Configuration","text":"Python<pre><code>import genesis\n\n# Configure CUDA backend\ngenesis.cuda.set_memory_fraction(0.9)\ngenesis.cuda.set_cache_size(\"2GB\")\ngenesis.cuda.enable_lazy_init(True)\n\n# Create CUDA tensors\ndevice = genesis.device(\"cuda:0\")\nx = genesis.tensor([1, 2, 3], device=device)\n</code></pre>"},{"location":"backends/cuda/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"<p>Comparison with PyTorch CUDA:</p> Operation Size Genesis CUDA PyTorch CUDA Speedup Add 1M 0.15ms 0.45ms 3.0x MatMul 1024\u00b2 0.8ms 1.2ms 1.5x Softmax 10K 0.25ms 0.35ms 1.4x Reduction 1M 0.12ms 0.18ms 1.5x"},{"location":"backends/cuda/#memory-performance","title":"Memory Performance","text":"Python<pre><code># Memory usage comparison\ngenesis_tensor = genesis.tensor(data, device=\"cuda\")\ntorch_tensor = torch.tensor(data, device=\"cuda\")\n\nprint(f\"Genesis memory: {genesis.cuda.memory_allocated()}\")\nprint(f\"PyTorch memory: {torch.cuda.memory_allocated()}\")\n</code></pre>"},{"location":"backends/cuda/#debugging-and-monitoring","title":"\ud83d\udd0d Debugging and Monitoring","text":""},{"location":"backends/cuda/#memory-monitoring","title":"Memory Monitoring","text":"Python<pre><code># Monitor memory usage\ndef monitor_cuda_memory():\n    stats = genesis.cuda.memory_stats()\n    print(f\"Allocated: {stats['allocated'] / 1e9:.2f} GB\")\n    print(f\"Cached: {stats['cached'] / 1e9:.2f} GB\")\n    print(f\"Fragmentation: {stats['fragmentation']:.2%}\")\n\n# Set up monitoring\ngenesis.cuda.set_memory_callback(monitor_cuda_memory)\n</code></pre>"},{"location":"backends/cuda/#kernel-profiling","title":"Kernel Profiling","text":"Python<pre><code># Enable kernel profiling\ngenesis.cuda.enable_profiling(True)\n\n# Operations will now print timing info\nx = genesis.tensor([[1, 2], [3, 4]], device=\"cuda\")\ny = genesis.matmul(x, x)  # Prints: \"matmul_kernel: 0.15ms\"\n</code></pre>"},{"location":"backends/cuda/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":""},{"location":"backends/cuda/#common-issues","title":"Common Issues","text":""},{"location":"backends/cuda/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"Python<pre><code># Solution 1: Reduce memory usage\ngenesis.cuda.empty_cache()\n\n# Solution 2: Adjust memory fraction\ngenesis.cuda.set_memory_fraction(0.7)\n\n# Solution 3: Enable memory monitoring\ngenesis.cuda.enable_memory_debugging(True)\n</code></pre>"},{"location":"backends/cuda/#kernel-compilation-errors","title":"Kernel Compilation Errors","text":"Python<pre><code># Clear kernel cache\ngenesis.cuda.clear_kernel_cache()\n\n# Disable kernel fusion temporarily\ngenesis.cuda.set_kernel_fusion(False)\n</code></pre>"},{"location":"backends/cuda/#slow-performance","title":"Slow Performance","text":"Python<pre><code># Warm up kernels\ngenesis.cuda.warm_up_kernels()\n\n# Check for proper device placement\nprint(f\"Tensor device: {x.device}\")\nprint(f\"Current device: {genesis.cuda.current_device()}\")\n</code></pre>"},{"location":"backends/cuda/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Backend System Overview</li> <li>CPU Backend</li> <li>Memory Management</li> <li>CUDA Operations</li> </ul>"},{"location":"backends/memory/","title":"Memory Management System","text":"<p>Genesis provides advanced memory management features for efficient GPU and CPU memory utilization.</p>"},{"location":"backends/memory/#overview","title":"\ud83d\udccb Overview","text":"<p>The memory management system is designed to: - Minimize allocation overhead through pooling - Reduce memory fragmentation - Provide detailed memory statistics - Enable efficient memory reuse</p>"},{"location":"backends/memory/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"Memory Management Layer\"\n        A[Memory Pool] --&gt; B[Block Allocator]\n        C[Memory Stats] --&gt; D[Usage Tracking]\n        E[Garbage Collector] --&gt; F[Cleanup Logic]\n    end\n\n    subgraph \"Device Memory\"\n        B --&gt; G[CUDA Memory]\n        B --&gt; H[CPU Memory]\n        G --&gt; I[Device Allocation]\n        H --&gt; J[Host Allocation]\n    end\n\n    subgraph \"Cache Management\"\n        K[Memory Cache] --&gt; L[Free Blocks]\n        K --&gt; M[Reserved Blocks]\n        K --&gt; N[Active Blocks]\n    end\n\n    style A fill:#4caf50\n    style C fill:#ff9800\n    style E fill:#f44336</code></pre>"},{"location":"backends/memory/#key-components","title":"\ud83c\udfaf Key Components","text":""},{"location":"backends/memory/#memory-pool","title":"Memory Pool","text":"<p>The central component managing memory allocation and deallocation:</p> Python<pre><code>class MemoryPool:\n    \"\"\"Unified memory pool for efficient allocation.\"\"\"\n\n    def __init__(self, device_type):\n        self.device_type = device_type\n        self.free_blocks = {}  # size -&gt; list of blocks\n        self.allocated_blocks = {}  # ptr -&gt; block info\n        self.total_allocated = 0\n        self.peak_allocated = 0\n\n    def allocate(self, size):\n        \"\"\"Allocate memory block of given size.\"\"\"\n        # Try to reuse existing block\n        block = self._find_suitable_block(size)\n        if block:\n            return self._reuse_block(block, size)\n\n        # Allocate new block\n        return self._allocate_new_block(size)\n\n    def deallocate(self, ptr):\n        \"\"\"Return memory block to pool.\"\"\"\n        block = self.allocated_blocks.pop(ptr)\n        self._add_to_free_blocks(block)\n</code></pre>"},{"location":"backends/memory/#memory-statistics","title":"Memory Statistics","text":"<p>Comprehensive tracking of memory usage:</p> Python<pre><code>class MemoryStats:\n    \"\"\"Detailed memory usage statistics.\"\"\"\n\n    def __init__(self):\n        self.allocated_bytes = 0\n        self.reserved_bytes = 0\n        self.active_bytes = 0\n        self.inactive_bytes = 0\n        self.peak_allocated = 0\n        self.num_allocs = 0\n        self.num_frees = 0\n\n    def update_allocation(self, size):\n        \"\"\"Update stats for new allocation.\"\"\"\n        self.allocated_bytes += size\n        self.active_bytes += size\n        self.peak_allocated = max(self.peak_allocated, self.allocated_bytes)\n        self.num_allocs += 1\n\n    def fragmentation_ratio(self):\n        \"\"\"Calculate memory fragmentation.\"\"\"\n        if self.reserved_bytes == 0:\n            return 0.0\n        return (self.reserved_bytes - self.allocated_bytes) / self.reserved_bytes\n</code></pre>"},{"location":"backends/memory/#cuda-memory-management","title":"\ud83d\ude80 CUDA Memory Management","text":""},{"location":"backends/memory/#advanced-pooling-strategy","title":"Advanced Pooling Strategy","text":"Python<pre><code>class CUDAMemoryPool(MemoryPool):\n    \"\"\"CUDA-specific memory pool with advanced features.\"\"\"\n\n    def __init__(self, device_id=0):\n        super().__init__(\"cuda\")\n        self.device_id = device_id\n        self.memory_fraction = 0.8  # Use 80% of available memory\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Initialize CUDA memory pool.\"\"\"\n        torch.cuda.set_device(self.device_id)\n\n        # Get available memory\n        total_memory = torch.cuda.get_device_properties(self.device_id).total_memory\n        available_memory = int(total_memory * self.memory_fraction)\n\n        # Pre-allocate large blocks\n        self._preallocate_blocks(available_memory)\n\n    def _preallocate_blocks(self, total_size):\n        \"\"\"Pre-allocate memory blocks of various sizes.\"\"\"\n        block_sizes = [1024, 4096, 16384, 65536, 262144, 1048576]  # Powers of 2\n\n        for block_size in block_sizes:\n            num_blocks = max(1, total_size // (block_size * len(block_sizes)))\n            for _ in range(num_blocks):\n                ptr = torch.cuda.caching_allocator_alloc(block_size)\n                self._add_to_free_blocks(Block(ptr, block_size))\n</code></pre>"},{"location":"backends/memory/#memory-optimization-features","title":"Memory Optimization Features","text":""},{"location":"backends/memory/#smart-caching","title":"Smart Caching","text":"Python<pre><code>def smart_cache_management(self):\n    \"\"\"Intelligent memory caching based on usage patterns.\"\"\"\n    # Analyze allocation patterns\n    frequent_sizes = self._analyze_allocation_patterns()\n\n    # Adjust cache sizes based on patterns\n    for size in frequent_sizes:\n        self._increase_cache_for_size(size)\n\n    # Clean up rarely used blocks\n    self._cleanup_unused_blocks()\n</code></pre>"},{"location":"backends/memory/#memory-compaction","title":"Memory Compaction","text":"Python<pre><code>def compact_memory(self):\n    \"\"\"Reduce memory fragmentation through compaction.\"\"\"\n    # Find fragmented regions\n    fragmented_blocks = self._find_fragmented_blocks()\n\n    # Compact adjacent free blocks\n    for block_group in fragmented_blocks:\n        merged_block = self._merge_blocks(block_group)\n        self._add_to_free_blocks(merged_block)\n</code></pre>"},{"location":"backends/memory/#cpu-memory-management","title":"\ud83d\udcbb CPU Memory Management","text":""},{"location":"backends/memory/#efficient-host-memory","title":"Efficient Host Memory","text":"Python<pre><code>class CPUMemoryPool(MemoryPool):\n    \"\"\"CPU memory pool with pinned memory support.\"\"\"\n\n    def __init__(self):\n        super().__init__(\"cpu\")\n        self.use_pinned_memory = False\n        self.pinned_blocks = set()\n\n    def allocate_pinned(self, size):\n        \"\"\"Allocate pinned memory for faster GPU transfers.\"\"\"\n        ptr = torch.cuda.cudaHostAlloc(size, torch.cuda.cudaHostAllocDefault)\n        block = Block(ptr, size, pinned=True)\n        self.pinned_blocks.add(ptr)\n        return block\n\n    def is_pinned(self, ptr):\n        \"\"\"Check if memory block is pinned.\"\"\"\n        return ptr in self.pinned_blocks\n</code></pre>"},{"location":"backends/memory/#configuration-and-usage","title":"\ud83d\udd27 Configuration and Usage","text":""},{"location":"backends/memory/#basic-configuration","title":"Basic Configuration","text":"Python<pre><code>import genesis\n\n# Configure CUDA memory\ngenesis.cuda.set_memory_fraction(0.9)  # Use 90% of GPU memory\ngenesis.cuda.set_cache_size(\"2GB\")     # Set cache size\n\n# Configure CPU memory\ngenesis.cpu.enable_pinned_memory(True) # Enable pinned memory\n\n# Get current memory stats\nstats = genesis.memory_stats()\nprint(f\"Allocated: {stats.allocated_bytes / 1e9:.2f} GB\")\nprint(f\"Cached: {stats.cached_bytes / 1e9:.2f} GB\")\n</code></pre>"},{"location":"backends/memory/#advanced-memory-control","title":"Advanced Memory Control","text":"Python<pre><code># Manual memory management\ndef optimize_memory_usage():\n    # Clear unused cache\n    genesis.empty_cache()\n\n    # Trigger garbage collection\n    genesis.memory_manager.collect_garbage()\n\n    # Compact fragmented memory\n    genesis.memory_manager.compact()\n\n# Memory monitoring\ndef monitor_memory():\n    stats = genesis.memory_stats()\n    fragmentation = stats.fragmentation_ratio()\n\n    if fragmentation &gt; 0.3:  # 30% fragmentation threshold\n        print(\"High fragmentation detected, compacting...\")\n        genesis.memory_manager.compact()\n</code></pre>"},{"location":"backends/memory/#memory-profiling","title":"\ud83d\udcca Memory Profiling","text":""},{"location":"backends/memory/#built-in-profiler","title":"Built-in Profiler","text":"Python<pre><code># Enable memory profiling\ngenesis.enable_memory_profiling(True)\n\n# Memory operations will now be tracked\nx = genesis.tensor([1, 2, 3], device=\"cuda\")  # Allocation tracked\ny = x + 1                                     # Temporary allocation tracked\ndel x                                         # Deallocation tracked\n\n# Get profiling report\nreport = genesis.memory_profiler.get_report()\nprint(report.summary())\n</code></pre>"},{"location":"backends/memory/#memory-timeline","title":"Memory Timeline","text":"Python<pre><code># Record memory timeline\nwith genesis.memory_profiler.record():\n    # Your code here\n    model = create_model()\n    data = load_data()\n    output = model(data)\n\n# Analyze timeline\ntimeline = genesis.memory_profiler.get_timeline()\ntimeline.plot()  # Shows memory usage over time\n</code></pre>"},{"location":"backends/memory/#performance-tips","title":"\u26a1 Performance Tips","text":""},{"location":"backends/memory/#memory-optimization-best-practices","title":"Memory Optimization Best Practices","text":"<ol> <li> <p>Pre-allocate Large Tensors Python<pre><code># Good: Pre-allocate\nbuffer = genesis.empty((1000000,), device=\"cuda\")\n\n# Avoid: Frequent small allocations\nfor i in range(1000):\n    x = genesis.tensor([i], device=\"cuda\")\n</code></pre></p> </li> <li> <p>Reuse Memory Buffers Python<pre><code># Reuse buffer\nresult_buffer = genesis.empty((batch_size, num_features))\nfor batch in dataloader:\n    genesis.matmul(batch.input, weights, out=result_buffer)\n</code></pre></p> </li> <li> <p>Use In-Place Operations Python<pre><code># In-place operations save memory\nx.add_(y)      # Instead of x = x + y\nx.mul_(0.5)    # Instead of x = x * 0.5\n</code></pre></p> </li> <li> <p>Manual Memory Management Python<pre><code># Clear cache when needed\nif memory_pressure_detected():\n    genesis.empty_cache()\n</code></pre></p> </li> </ol>"},{"location":"backends/memory/#debugging-memory-issues","title":"\ud83d\udd0d Debugging Memory Issues","text":""},{"location":"backends/memory/#memory-leak-detection","title":"Memory Leak Detection","text":"Python<pre><code># Enable leak detection\ngenesis.enable_memory_debugging(True)\n\n# Run your code\ntrain_model()\n\n# Check for leaks\nleaks = genesis.check_memory_leaks()\nif leaks:\n    print(\"Memory leaks detected:\")\n    for leak in leaks:\n        print(f\"  {leak.size} bytes at {leak.location}\")\n</code></pre>"},{"location":"backends/memory/#memory-usage-analysis","title":"Memory Usage Analysis","text":"Python<pre><code>def analyze_memory_usage():\n    \"\"\"Comprehensive memory analysis.\"\"\"\n    stats = genesis.detailed_memory_stats()\n\n    print(f\"Total allocated: {stats.total_allocated / 1e9:.2f} GB\")\n    print(f\"Peak usage: {stats.peak_allocated / 1e9:.2f} GB\")\n    print(f\"Fragmentation: {stats.fragmentation_ratio:.2%}\")\n    print(f\"Cache hit rate: {stats.cache_hit_rate:.2%}\")\n\n    # Show largest allocations\n    large_allocs = stats.get_large_allocations(min_size=100*1024*1024)  # 100MB+\n    for alloc in large_allocs:\n        print(f\"Large alloc: {alloc.size / 1e6:.1f} MB at {alloc.location}\")\n</code></pre>"},{"location":"backends/memory/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>CUDA Backend</li> <li>CPU Backend</li> <li>Performance Optimization</li> <li>Debugging Guide</li> </ul>"},{"location":"benchmark/","title":"Genesis Benchmark Reports","text":"<p>Welcome to the Genesis framework performance benchmark reports. This directory contains comprehensive performance analysis comparing Genesis with PyTorch.</p>"},{"location":"benchmark/#about-genesis-benchmarks","title":"About Genesis Benchmarks","text":"<p>Genesis benchmarks provide detailed performance analysis to help users understand: - Operation-level performance: How individual operations compare to PyTorch - End-to-end model performance: Complete model benchmarking including memory usage - Optimization opportunities: Detailed recommendations for performance improvements</p>"},{"location":"benchmark/#benchmark-categories","title":"Benchmark Categories","text":""},{"location":"benchmark/#operations-benchmarks","title":"\ud83d\udcca Operations Benchmarks","text":"<p>Detailed performance analysis of individual operations and operation categories: - Element-wise operations: add, subtract, multiply, divide, etc. - Activation functions: ReLU, sigmoid, tanh, etc. - Reduction operations: sum, mean, max, etc. - Matrix operations: matrix multiplication - Memory operations: transpose, reshape, etc.</p>"},{"location":"benchmark/#model-benchmarks","title":"\ud83e\udd16 Model Benchmarks","text":"<p>End-to-end performance analysis of complete models: - Qwen Language Models: Forward/backward pass timing and memory analysis - Scalability testing: Performance across different batch sizes and sequence lengths</p>"},{"location":"benchmark/#performance-indicators","title":"Performance Indicators","text":"<p>We use a standardized performance grading system:</p> <ul> <li>\ud83d\udfe2 Excellent (\u226590%): Genesis performs at 90% or better vs PyTorch</li> <li>\ud83d\udfe1 Good (70-90%): Acceptable performance gap</li> <li>\ud83d\udfe0 Fair (50-70%): Notable performance gap, optimization recommended</li> <li>\ud83d\udd34 Poor (20-50%): Significant optimization needed</li> <li>\u274c Critical (&lt;20%): Major performance issues requiring attention</li> </ul>"},{"location":"benchmark/#how-to-run-benchmarks","title":"How to Run Benchmarks","text":""},{"location":"benchmark/#quick-start","title":"Quick Start","text":"Bash<pre><code># Navigate to benchmark directory\ncd benchmark\n\n# Run all benchmarks (generates reports in docs/benchmark/)\n./run.sh\n</code></pre>"},{"location":"benchmark/#specific-benchmarks","title":"Specific Benchmarks","text":""},{"location":"benchmark/#operations-benchmarks_1","title":"Operations Benchmarks","text":"Bash<pre><code># Test all operations (comprehensive)\npython bench_ops.py\n\n# Test specific operation category\npython bench_ops.py --category element\n\n# Test specific operation\npython bench_ops.py --op add\n\n# Fast mode (reduced iterations)\npython bench_ops.py --fast\n</code></pre>"},{"location":"benchmark/#model-benchmarks_1","title":"Model Benchmarks","text":"Bash<pre><code># Test Qwen model (if available)\npython bench_qwen.py --size 0.5B --batch-size 1,2,4 --seq-len 128,256,512\n\n# Quick test\npython bench_qwen.py --size 0.5B --batch-size 1,2 --seq-len 128,256 --fast\n</code></pre>"},{"location":"benchmark/#understanding-the-reports","title":"Understanding the Reports","text":""},{"location":"benchmark/#operations-reports","title":"Operations Reports","text":"<p>Each operations benchmark report includes: - System Information: GPU details and theoretical performance limits - Performance Summary: Overall statistics and success rates - Category Breakdown: Performance by operation type - Detailed Results: Individual operation results with speedup and bandwidth - Top Performers: Best and worst performing operations - Optimization Recommendations: Specific improvement suggestions</p>"},{"location":"benchmark/#model-reports","title":"Model Reports","text":"<p>Model benchmark reports provide: - Model Configuration: Model size, batch sizes, sequence lengths tested - Performance Summary: Overall speedup and memory efficiency - Operation Analysis: Performance breakdown by model operation - Scalability Analysis: Performance across different input sizes - Memory Analysis: Memory usage patterns and efficiency - Optimization Priorities: Focused recommendations for improvement</p>"},{"location":"benchmark/#benchmark-infrastructure","title":"Benchmark Infrastructure","text":""},{"location":"benchmark/#reliability-features","title":"Reliability Features","text":"<ul> <li>Statistical outlier detection: Robust timing measurements</li> <li>Adaptive iterations: Automatic iteration count adjustment</li> <li>Memory bandwidth analysis: Theoretical performance comparison</li> <li>Error handling: Graceful handling of failed operations</li> </ul>"},{"location":"benchmark/#timing-modes","title":"Timing Modes","text":"<ul> <li>Real timing: Includes all overheads (realistic user experience)</li> <li>Pure timing: Minimized overhead (peak computational performance)</li> </ul>"},{"location":"benchmark/#getting-started","title":"Getting Started","text":"<ol> <li>Install Requirements: Ensure Genesis, PyTorch, and CUDA are properly installed</li> <li>Run Quick Test: <code>cd benchmark &amp;&amp; python bench_ops.py --fast</code></li> <li>Generate Reports: <code>./run.sh</code> to run all benchmarks and generate documentation</li> <li>Review Results: Check this directory for generated reports</li> </ol>"},{"location":"benchmark/#continuous-improvement","title":"Continuous Improvement","text":"<p>These benchmarks are designed to: - Track Genesis performance improvements over time - Identify optimization opportunities - Validate new features and optimizations - Provide transparency in performance characteristics</p> <p>Note: To generate fresh benchmark reports, run <code>./run.sh</code> from the benchmark directory. This will create timestamped reports with the latest performance data.</p> <p>For technical details about the benchmark implementation, see the source code in the <code>benchmark/</code> directory.</p>"},{"location":"benchmark/operations_element/","title":"Genesis Operations Benchmark Report","text":"<p>Generated on: 2025-08-15 16:07:03</p>"},{"location":"benchmark/operations_element/#system-information","title":"System Information","text":"<ul> <li>GPU: NVIDIA A100-SXM4-40GB</li> <li>Memory: 39.4 GB</li> <li>Theoretical Bandwidth: 1555 GB/s</li> <li>Multi-processors: 108</li> </ul>"},{"location":"benchmark/operations_element/#test-configuration","title":"Test Configuration","text":"<ul> <li>Mode: Fast</li> <li>Timing: real</li> <li>Data Type: float32</li> <li>Category: element</li> </ul>"},{"location":"benchmark/operations_element/#performance-summary","title":"Performance Summary","text":"Metric Value Total Tests 28 Successful Tests 28 Failed Tests 0 Success Rate 100.0% Average Speedup 0.63x Median Speedup 0.19x Best Speedup 3.62x Worst Speedup 0.11x"},{"location":"benchmark/operations_element/#performance-by-category","title":"Performance by Category","text":"Category Tests Success Rate Avg Speedup Best Speedup Status element 28 100.0% 0.63x 3.62x \ud83d\udfe1 Good"},{"location":"benchmark/operations_element/#detailed-results","title":"Detailed Results","text":"Operation Category Shape PyTorch (ms) Genesis (ms) Speedup Bandwidth (GB/s) Status cos element 256\u00d7256 0.039 0.011 3.62x 51.2 \ud83d\udfe2 EXCELLENT add_scalar element 512\u00d7512 0.024 0.011 2.20x 186.2 \ud83d\udfe2 EXCELLENT sub element 256\u00d7256 0.021 0.010 2.15x 76.8 \ud83d\udfe2 EXCELLENT negate element 256\u00d7256 0.021 0.010 2.12x 51.2 \ud83d\udfe2 EXCELLENT log element 256\u00d7256 0.014 0.010 1.43x 51.2 \ud83d\udfe2 EXCELLENT multiply element 256\u00d7256 0.012 0.010 1.22x 76.8 \ud83d\udfe2 EXCELLENT divide_scalar element 256\u00d7256 0.010 0.010 0.99x 51.2 \ud83d\udfe2 EXCELLENT sqrt element 256\u00d7256 0.020 0.041 0.49x 51.2 \ud83d\udd34 POOR mul_scalar element 256\u00d7256 0.024 0.107 0.23x 4.9 \ud83d\udd34 POOR add_scalar element 256\u00d7256 0.024 0.108 0.22x 4.9 \ud83d\udd34 POOR mul_scalar element 512\u00d7512 0.026 0.121 0.21x 17.7 \ud83d\udd34 POOR add element 256\u00d7256 0.016 0.076 0.21x 8.2 \ud83d\udd34 POOR divide element 256\u00d7256 0.017 0.089 0.19x 6.9 \u274c CRITICAL sin element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c CRITICAL exp element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c CRITICAL sin element 512\u00d7512 0.013 0.069 0.19x 18.1 \u274c CRITICAL negate element 512\u00d7512 0.011 0.060 0.18x 186.2 \u274c CRITICAL sqrt element 512\u00d7512 0.021 0.117 0.18x 18.2 \u274c CRITICAL exp element 512\u00d7512 0.014 0.079 0.18x 21.2 \u274c CRITICAL multiply element 512\u00d7512 0.020 0.116 0.17x 16.4 \u274c CRITICAL log element 512\u00d7512 0.020 0.116 0.17x 18.3 \u274c CRITICAL pow_scalar element 256\u00d7256 0.023 0.139 0.16x 3.8 \u274c CRITICAL add element 512\u00d7512 0.021 0.132 0.16x 24.4 \u274c CRITICAL pow_scalar element 512\u00d7512 0.011 0.068 0.16x 186.2 \u274c CRITICAL divide element 512\u00d7512 0.017 0.130 0.13x 24.5 \u274c CRITICAL divide_scalar element 512\u00d7512 0.011 0.083 0.13x 17.1 \u274c CRITICAL sub element 512\u00d7512 0.015 0.132 0.11x 24.4 \u274c CRITICAL cos element 512\u00d7512 0.013 0.120 0.11x 17.8 \u274c CRITICAL"},{"location":"benchmark/operations_element/#performance-distribution","title":"Performance Distribution","text":"<ul> <li>\ud83d\udfe2 Excellent (\u226590%): 7 tests (25.0%)</li> <li>\ud83d\udfe1 Good (70-90%): 0 tests (0.0%)</li> <li>\ud83d\udfe0 Fair (50-70%): 0 tests (0.0%)</li> <li>\ud83d\udd34 Poor (20-50%): 5 tests (17.9%)</li> <li>\u274c Critical (&lt;20%): 16 tests (57.1%)</li> </ul>"},{"location":"benchmark/operations_element/#top-10-performers","title":"Top 10 Performers","text":"Rank Operation Shape Speedup Status 1 cos 256\u00d7256 3.62x \ud83d\udfe2 EXCELLENT 2 add_scalar 512\u00d7512 2.20x \ud83d\udfe2 EXCELLENT 3 sub 256\u00d7256 2.15x \ud83d\udfe2 EXCELLENT 4 negate 256\u00d7256 2.12x \ud83d\udfe2 EXCELLENT 5 log 256\u00d7256 1.43x \ud83d\udfe2 EXCELLENT 6 multiply 256\u00d7256 1.22x \ud83d\udfe2 EXCELLENT 7 divide_scalar 256\u00d7256 0.99x \ud83d\udfe2 EXCELLENT 8 sqrt 256\u00d7256 0.49x \ud83d\udd34 POOR 9 mul_scalar 256\u00d7256 0.23x \ud83d\udd34 POOR 10 add_scalar 256\u00d7256 0.22x \ud83d\udd34 POOR"},{"location":"contributing/","title":"Contributing Guide","text":"<p>Welcome to contribute code to the Genesis deep learning framework! This guide will help you understand how to participate in project development.</p>"},{"location":"contributing/#ways-to-contribute","title":"\ud83e\udd1d Ways to Contribute","text":""},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<ul> <li>Fix bugs</li> <li>Add new features</li> <li>Performance optimization</li> <li>Test improvements</li> </ul>"},{"location":"contributing/#documentation-contributions","title":"Documentation Contributions","text":"<ul> <li>Improve existing documentation</li> <li>Add tutorials and examples</li> <li>Translate documentation</li> <li>API documentation improvements</li> </ul>"},{"location":"contributing/#community-contributions","title":"Community Contributions","text":"<ul> <li>Answer questions</li> <li>Code review</li> <li>Issue reporting</li> <li>Feature suggestions</li> </ul>"},{"location":"contributing/#development-workflow","title":"\ud83d\udccb Development Workflow","text":""},{"location":"contributing/#1-preparation","title":"1. Preparation","text":"Bash<pre><code># Fork the project to your GitHub account\n# Clone your fork\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# Add upstream repository\ngit remote add upstream https://github.com/phonism/genesis.git\n\n# Create development branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-development-environment-setup","title":"2. Development Environment Setup","text":"<p>See the Development Environment Configuration documentation for details.</p>"},{"location":"contributing/#3-code-development","title":"3. Code Development","text":"<ul> <li>Follow coding standards</li> <li>Add unit tests</li> <li>Update relevant documentation</li> <li>Write clear commit messages</li> </ul>"},{"location":"contributing/#4-testing-and-verification","title":"4. Testing and Verification","text":"Bash<pre><code># Run test suite\npython -m pytest tests/ -v\n\n# Run code format checks\nblack genesis/ tests/\nflake8 genesis/ tests/\n\n# Run type checking\nmypy genesis/\n</code></pre>"},{"location":"contributing/#5-submit-pull-request","title":"5. Submit Pull Request","text":"<ul> <li>Ensure all tests pass</li> <li>Write detailed PR description</li> <li>Link related issues</li> <li>Wait for code review</li> </ul>"},{"location":"contributing/#code-standards","title":"\ud83d\udcdd Code Standards","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 standards:</p> Python<pre><code># Good example\ndef compute_attention_weights(query, key, scale_factor):\n    \"\"\"Compute scaled dot-product attention weights.\n\n    Args:\n        query: Query tensor of shape [batch, seq_len, hidden_dim]\n        key: Key tensor of shape [batch, seq_len, hidden_dim] \n        scale_factor: Scaling factor for attention scores\n\n    Returns:\n        Attention weights of shape [batch, seq_len, seq_len]\n    \"\"\"\n    scores = genesis.matmul(query, key.transpose(-2, -1))\n    scaled_scores = scores * scale_factor\n    return genesis.softmax(scaled_scores, dim=-1)\n</code></pre>"},{"location":"contributing/#documentation-strings","title":"Documentation Strings","text":"<p>Use Google-style docstrings:</p> Python<pre><code>def example_function(param1: int, param2: str = \"default\") -&gt; bool:\n    \"\"\"One line summary of the function.\n\n    More detailed description if needed. Can span multiple lines.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2 with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is negative\n\n    Example:\n        &gt;&gt;&gt; result = example_function(42, \"test\")\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n    if param1 &lt; 0:\n        raise ValueError(\"param1 must be non-negative\")\n    return param1 &gt; 0\n</code></pre>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"Python<pre><code>import pytest\nimport genesis\n\nclass TestAttention:\n    \"\"\"Test attention mechanisms.\"\"\"\n\n    def test_basic_attention(self):\n        \"\"\"Test basic attention computation.\"\"\"\n        batch_size, seq_len, hidden_dim = 2, 4, 8\n\n        query = genesis.randn(batch_size, seq_len, hidden_dim)\n        key = genesis.randn(batch_size, seq_len, hidden_dim)\n        value = genesis.randn(batch_size, seq_len, hidden_dim)\n\n        attention = genesis.nn.MultiHeadAttention(hidden_dim, num_heads=2)\n        output = attention(query, key, value)\n\n        assert output.shape == (batch_size, seq_len, hidden_dim)\n\n    @pytest.mark.parametrize(\"num_heads\", [1, 2, 4, 8])\n    def test_different_head_counts(self, num_heads):\n        \"\"\"Test attention with different head counts.\"\"\"\n        # Test implementation\n        pass\n</code></pre>"},{"location":"contributing/#development-best-practices","title":"\ud83d\ude80 Development Best Practices","text":""},{"location":"contributing/#1-branch-management","title":"1. Branch Management","text":"Bash<pre><code># Main branches\nmain          # Stable version\ndevelop       # Development version\n\n# Feature branches\nfeature/xxx   # New feature development\nbugfix/xxx    # Bug fixes\nhotfix/xxx    # Hotfixes\n</code></pre>"},{"location":"contributing/#2-commit-message-format","title":"2. Commit Message Format","text":"Text Only<pre><code>type(scope): brief description\n\nDetailed description (optional)\n\nFixes #123\n</code></pre> <p>Type descriptions: - <code>feat</code>: New features - <code>fix</code>: Bug fixes - <code>docs</code>: Documentation updates - <code>style</code>: Code formatting adjustments - <code>refactor</code>: Refactoring - <code>perf</code>: Performance optimization - <code>test</code>: Test-related - <code>chore</code>: Build tools, etc.</p>"},{"location":"contributing/#3-performance-considerations","title":"3. Performance Considerations","text":"<ul> <li>Avoid unnecessary memory copies</li> <li>Use in-place operations when possible</li> <li>Consider CUDA kernel memory access patterns</li> <li>Add performance benchmarks</li> </ul>"},{"location":"contributing/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>When submitting bugs, please include:</p> <ol> <li>Environment Information</li> <li>Genesis version</li> <li>Python version</li> <li>CUDA version</li> <li> <p>Operating system</p> </li> <li> <p>Reproduction Steps</p> </li> <li>Minimal reproducible code</li> <li>Expected behavior</li> <li>Actual behavior</li> <li> <p>Error messages</p> </li> <li> <p>Related Logs</p> </li> <li>Complete error stack trace</li> <li>Relevant configuration information</li> </ol> <p>Example: Python<pre><code># Minimal reproduction case\nimport genesis\n\nmodel = genesis.nn.Linear(10, 5)\nx = genesis.randn(3, 10)\ny = model(x)  # Error occurs here\n\n# Error message:\n# RuntimeError: CUDA kernel launch failed\n</code></pre></p>"},{"location":"contributing/#key-contribution-areas","title":"\ud83c\udfaf Key Contribution Areas","text":"<p>We particularly welcome contributions in the following areas:</p>"},{"location":"contributing/#high-priority","title":"High Priority","text":"<ul> <li> Performance optimization and benchmarking</li> <li> CUDA operator implementation</li> <li> Documentation and tutorial improvements</li> <li> Test coverage enhancement</li> </ul>"},{"location":"contributing/#medium-priority","title":"Medium Priority","text":"<ul> <li> New neural network layers</li> <li> Data loader optimization</li> <li> Distributed training support</li> <li> Mixed precision training</li> </ul>"},{"location":"contributing/#low-priority","title":"Low Priority","text":"<ul> <li> Visualization tools</li> <li> Model deployment support</li> <li> Third-party framework integration</li> </ul>"},{"location":"contributing/#contact-us","title":"\ud83d\udcde Contact Us","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Technical discussions and Q&amp;A</li> <li>Email: genesis-dev@example.com</li> </ul>"},{"location":"contributing/#contributor-recognition","title":"\ud83c\udfc6 Contributor Recognition","text":"<p>We value every contributor's effort:</p> <ul> <li>Contributors will be listed in the project README</li> <li>Major contributors will receive maintainer privileges</li> <li>Regular contributor newsletters</li> </ul>"},{"location":"contributing/#license","title":"\ud83d\udcc4 License","text":"<p>By contributing code, you agree that your contributions will be licensed under the MIT License.</p> <p>Start Contributing</p> <p>Ready to start contributing? Begin with Development Environment Configuration!</p> <p>Thank you for your contribution to the Genesis project! \ud83c\udf89</p>"},{"location":"contributing/development/","title":"Development Environment Setup","text":"<p>This guide will help you set up a Genesis development environment, including code editing, debugging, testing, and other development workflows.</p>"},{"location":"contributing/development/#system-requirements","title":"\ud83d\udee0\ufe0f System Requirements","text":""},{"location":"contributing/development/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: x86_64 architecture with AVX instruction set support</li> <li>Memory: Minimum 16GB, recommended 32GB+</li> <li>GPU: NVIDIA GPU with CUDA support (required for GPU operator development)</li> <li>Storage: 20GB available space</li> </ul>"},{"location":"contributing/development/#software-requirements","title":"Software Requirements","text":"<ul> <li>Operating System: Linux (Ubuntu 20.04+ recommended), macOS 10.15+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>Git: Latest version</li> <li>CUDA: 11.8+ (required for GPU development)</li> </ul>"},{"location":"contributing/development/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"contributing/development/#1-clone-repository","title":"1. Clone Repository","text":"Bash<pre><code># Clone your fork (recommended)\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# Or clone the main repository\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# Add upstream repository (if forked)\ngit remote add upstream https://github.com/phonism/genesis.git\n</code></pre>"},{"location":"contributing/development/#2-create-python-environment","title":"2. Create Python Environment","text":"Using condaUsing venv Bash<pre><code># Create environment\nconda create -n genesis-dev python=3.9\nconda activate genesis-dev\n\n# Install base dependencies\nconda install numpy matplotlib ipython jupyter\n</code></pre> Bash<pre><code># Create environment\npython -m venv genesis-dev\nsource genesis-dev/bin/activate  # Linux/macOS\n# genesis-dev\\\\Scripts\\\\activate  # Windows\n\n# Upgrade pip\npip install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"contributing/development/#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"Bash<pre><code># Install PyTorch (choose based on your CUDA version)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU version\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Install Triton\npip install triton\n\n# Install development tools\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/development/#4-install-genesis-development-mode","title":"4. Install Genesis (Development Mode)","text":"Bash<pre><code># Development mode installation (recommended)\npip install -e .\n\n# Verify installation\npython -c \"import genesis; print('Genesis development environment setup successful!')\"\n</code></pre>"},{"location":"contributing/development/#dependency-management","title":"\ud83d\udce6 Dependency Management","text":""},{"location":"contributing/development/#core-dependencies-requirementstxt","title":"Core Dependencies (requirements.txt)","text":"Text Only<pre><code>torch&gt;=2.0.0\ntriton&gt;=2.0.0\nnumpy&gt;=1.21.0\ncuda-python&gt;=11.8.0\n</code></pre>"},{"location":"contributing/development/#development-dependencies-requirements-devtxt","title":"Development Dependencies (requirements-dev.txt)","text":"Text Only<pre><code>pytest&gt;=7.0.0\npytest-cov&gt;=4.0.0\nblack&gt;=22.0.0\nflake8&gt;=5.0.0\nmypy&gt;=1.0.0\nisort&gt;=5.0.0\npre-commit&gt;=2.20.0\nsphinx&gt;=5.0.0\nmatplotlib&gt;=3.5.0\njupyter&gt;=1.0.0\nipython&gt;=8.0.0\n</code></pre>"},{"location":"contributing/development/#development-tools-configuration","title":"\ud83d\udd27 Development Tools Configuration","text":""},{"location":"contributing/development/#1-git-configuration","title":"1. Git Configuration","text":"Bash<pre><code># Configure user information\ngit config user.name \"Your Name\"\ngit config user.email \"your.email@example.com\"\n\n# Configure commit template\necho \"feat: brief description\n\nMore detailed explanation (optional)\n\n- Change 1\n- Change 2\n\nFixes #123\" &gt; ~/.gitmessage\ngit config commit.template ~/.gitmessage\n</code></pre>"},{"location":"contributing/development/#2-pre-commit-hooks","title":"2. Pre-commit Hooks","text":"Bash<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run checks manually\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/development/#3-ide-configuration","title":"3. IDE Configuration","text":"VS CodePyCharm <p>Recommended extensions to install:</p> JSON<pre><code>// .vscode/extensions.json\n{\n    \"recommendations\": [\n        \"ms-python.python\",\n        \"ms-python.black-formatter\", \n        \"ms-python.flake8\",\n        \"ms-python.mypy-type-checker\",\n        \"ms-toolsai.jupyter\",\n        \"ms-vscode.cpptools\"\n    ]\n}\n</code></pre> <p>Configuration file: JSON<pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./genesis-dev/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackArgs\": [\"--line-length=88\"],\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\"tests/\"]\n}\n</code></pre></p> <ol> <li>Open project settings (File -&gt; Settings)</li> <li>Configure Python interpreter to point to virtual environment</li> <li>Enable code formatting tools (Black, isort)</li> <li>Configure test runner to pytest</li> </ol>"},{"location":"contributing/development/#4-environment-variables","title":"4. Environment Variables","text":"Bash<pre><code># Development environment variables\nexport GENESIS_DEV=1\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\nexport CUDA_VISIBLE_DEVICES=0  # Specify GPU device\n\n# Add to ~/.bashrc or ~/.zshrc\necho 'export GENESIS_DEV=1' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"contributing/development/#testing-framework","title":"\ud83e\uddea Testing Framework","text":""},{"location":"contributing/development/#test-directory-structure","title":"Test Directory Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # pytest configuration\n\u251c\u2500\u2500 test_autograd.py         # Autograd tests\n\u251c\u2500\u2500 test_nn.py              # Neural network tests\n\u251c\u2500\u2500 test_cuda_tensor.py     # CUDA tensor tests\n\u251c\u2500\u2500 test_functional.py      # Functional interface tests\n\u251c\u2500\u2500 benchmarks/             # Performance tests\n\u2502   \u251c\u2500\u2500 bench_matmul.py\n\u2502   \u2514\u2500\u2500 bench_attention.py\n\u2514\u2500\u2500 integration/            # Integration tests\n    \u251c\u2500\u2500 test_training.py\n    \u2514\u2500\u2500 test_models.py\n</code></pre>"},{"location":"contributing/development/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_nn.py -v\n\n# Run specific test function\npytest tests/test_nn.py::test_linear_layer -v\n\n# Run tests with coverage\npytest tests/ --cov=genesis --cov-report=html\n\n# Run performance tests\npytest tests/benchmarks/ -v --benchmark-only\n</code></pre>"},{"location":"contributing/development/#writing-tests","title":"Writing Tests","text":"Python<pre><code># tests/test_example.py\nimport pytest\nimport genesis\nimport genesis.nn as nn\n\nclass TestExample:\n    \"\"\"Example test class.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Setup before each test method.\"\"\"\n        self.device = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\n\n    def test_basic_operation(self):\n        \"\"\"Test basic tensor operations.\"\"\"\n        x = genesis.randn(3, 4, device=self.device)\n        y = genesis.randn(3, 4, device=self.device)\n        z = x + y\n\n        assert z.shape == (3, 4)\n        assert z.device == self.device\n\n    @pytest.mark.parametrize(\"input_size,output_size\", [\n        (10, 5),\n        (128, 64),\n        (512, 256)\n    ])\n    def test_linear_layers(self, input_size, output_size):\n        \"\"\"Test linear layers with different sizes.\"\"\"\n        layer = nn.Linear(input_size, output_size).to(self.device)\n        x = genesis.randn(32, input_size, device=self.device)\n\n        output = layer(x)\n        assert output.shape == (32, output_size)\n\n    @pytest.mark.cuda\n    def test_cuda_specific(self):\n        \"\"\"Test CUDA-specific functionality.\"\"\"\n        if not genesis.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n\n        x = genesis.randn(10, 10, device='cuda')\n        assert x.is_cuda\n</code></pre>"},{"location":"contributing/development/#performance-analysis","title":"\ud83d\udcca Performance Analysis","text":""},{"location":"contributing/development/#1-built-in-profiler","title":"1. Built-in Profiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# Using context manager\nwith profiler.profile() as prof:\n    # Your code\n    x = genesis.randn(1000, 1000)\n    y = genesis.matmul(x, x)\n\n# Print results\nprof.print_stats()\n\n# Save results\nprof.export_chrome_trace(\"profile.json\")\n</code></pre>"},{"location":"contributing/development/#2-memory-analysis","title":"2. Memory Analysis","text":"Python<pre><code>import genesis\n\n# Enable memory tracking\ngenesis.cuda.memory.enable_debug()\n\n# Your code\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.matmul(x, x)\n\n# Check memory usage\nprint(f\"Memory used: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"Cached memory: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# Memory snapshot\nsnapshot = genesis.cuda.memory.memory_snapshot()\n</code></pre>"},{"location":"contributing/development/#3-benchmarking","title":"3. Benchmarking","text":"Python<pre><code># benchmark/bench_example.py\nimport time\nimport genesis\nimport torch\n\ndef benchmark_matmul():\n    \"\"\"Benchmark matrix multiplication.\"\"\"\n    sizes = [128, 256, 512, 1024]\n\n    for size in sizes:\n        # Genesis\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis_time = time.time() - start_time\n\n        # PyTorch\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch_time = time.time() - start_time\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  Genesis: {genesis_time:.4f}s\")\n        print(f\"  PyTorch: {torch_time:.4f}s\")\n        print(f\"  Ratio: {genesis_time/torch_time:.2f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_matmul()\n</code></pre>"},{"location":"contributing/development/#debugging-tips","title":"\ud83d\udc1b Debugging Tips","text":""},{"location":"contributing/development/#1-debug-environment-variables","title":"1. Debug Environment Variables","text":"Bash<pre><code># Enable debug mode\nexport GENESIS_DEBUG=1\nexport CUDA_LAUNCH_BLOCKING=1  # Synchronous CUDA execution\nexport PYTHONFAULTHANDLER=1    # Python error handling\n</code></pre>"},{"location":"contributing/development/#2-logging-configuration","title":"2. Logging Configuration","text":"Python<pre><code>import logging\nimport genesis\n\n# Configure logging\nlogging.basicConfig(level=logging.DEBUG)\ngenesis.set_log_level('DEBUG')\n\n# Use logging\nlogger = logging.getLogger(__name__)\nlogger.debug(\"Debug information\")\n</code></pre>"},{"location":"contributing/development/#3-breakpoint-debugging","title":"3. Breakpoint Debugging","text":"Python<pre><code>import pdb\n\ndef buggy_function(x):\n    pdb.set_trace()  # Set breakpoint\n    y = x * 2\n    return y\n\n# Or use ipdb (install with: pip install ipdb)\nimport ipdb\nipdb.set_trace()\n</code></pre>"},{"location":"contributing/development/#documentation-development","title":"\ud83d\udcda Documentation Development","text":""},{"location":"contributing/development/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Install documentation dependencies\npip install -r docs/requirements.txt\n\n# Local server\nmkdocs serve\n\n# Build static files\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/development/#api-documentation-generation","title":"API Documentation Generation","text":"Bash<pre><code># Auto-generate API documentation\npython scripts/generate_api_docs.py\n\n# Check docstring format\npydocstyle genesis/\n</code></pre>"},{"location":"contributing/development/#code-submission","title":"\ud83d\ude80 Code Submission","text":""},{"location":"contributing/development/#1-code-checks","title":"1. Code Checks","text":"Bash<pre><code># Format code\nblack genesis/ tests/\nisort genesis/ tests/\n\n# Type checking\nmypy genesis/\n\n# Code quality checks\nflake8 genesis/ tests/\n\n# Run tests\npytest tests/ -x\n</code></pre>"},{"location":"contributing/development/#2-submission-process","title":"2. Submission Process","text":"Bash<pre><code># 1. Sync latest code\ngit fetch upstream\ngit rebase upstream/main\n\n# 2. Create feature branch\ngit checkout -b feature/your-feature\n\n# 3. Development and testing\n# ... your development work ...\n\n# 4. Commit code\ngit add .\ngit commit -m \"feat: add your feature\"\n\n# 5. Push branch\ngit push origin feature/your-feature\n\n# 6. Create Pull Request\n</code></pre>"},{"location":"contributing/development/#common-issues","title":"\u2753 Common Issues","text":""},{"location":"contributing/development/#q-cuda-related-errors","title":"Q: CUDA-related errors?","text":"<p>A: Check CUDA version compatibility, ensure PyTorch and Triton versions match.</p>"},{"location":"contributing/development/#q-test-failures","title":"Q: Test failures?","text":"<p>A: Run <code>pytest tests/ -v</code> to see detailed error information, check environment configuration.</p>"},{"location":"contributing/development/#q-performance-issues","title":"Q: Performance issues?","text":"<p>A: Use profiler to analyze bottlenecks, check if GPU acceleration is enabled.</p>"},{"location":"contributing/development/#q-out-of-memory","title":"Q: Out of memory?","text":"<p>A: Reduce test case data size, enable CPU fallback mode.</p> <p>Development Environment Setup Complete</p> <p>You can now start contributing code to Genesis!</p> <p>Next: Testing Guidelines Back to Contributing Guide</p>"},{"location":"contributing/testing/","title":"Testing Guidelines","text":"<p>Under Development</p> <p>This document is currently being written and content will be continuously updated.</p> <p>This document defines testing standards and best practices for the Genesis project.</p>"},{"location":"contributing/testing/#testing-principles","title":"\ud83c\udfaf Testing Principles","text":""},{"location":"contributing/testing/#1-test-pyramid","title":"1. Test Pyramid","text":"<ul> <li>Unit Tests (70%): Test individual functions and classes</li> <li>Integration Tests (20%): Test component interactions</li> <li>End-to-End Tests (10%): Test complete workflows</li> </ul>"},{"location":"contributing/testing/#2-test-coverage","title":"2. Test Coverage","text":"<ul> <li>Target coverage: 90%+</li> <li>Critical modules requirement: 95%+</li> <li>New code requirement: 100%</li> </ul>"},{"location":"contributing/testing/#test-categories","title":"\ud83d\udccb Test Categories","text":""},{"location":"contributing/testing/#unit-tests","title":"Unit Tests","text":"Python<pre><code>def test_tensor_creation():\n    \"\"\"Test basic tensor creation.\"\"\"\n    x = genesis.randn(3, 4)\n    assert x.shape == (3, 4)\n    assert x.dtype == genesis.float32\n</code></pre>"},{"location":"contributing/testing/#performance-tests","title":"Performance Tests","text":"Python<pre><code>@pytest.mark.benchmark\ndef test_matmul_performance():\n    \"\"\"Benchmark matrix multiplication performance.\"\"\"\n    # WIP: Performance test implementation\n    pass\n</code></pre>"},{"location":"contributing/testing/#gpu-tests","title":"GPU Tests","text":"Python<pre><code>@pytest.mark.cuda\ndef test_cuda_operations():\n    \"\"\"Test CUDA-specific operations.\"\"\"\n    if not genesis.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n\n    x = genesis.randn(10, 10, device='cuda')\n    y = genesis.matmul(x, x)\n    assert y.device.type == 'cuda'\n</code></pre>"},{"location":"contributing/testing/#running-tests","title":"\ud83d\ude80 Running Tests","text":"Bash<pre><code># All tests\npytest tests/ -v\n\n# Specific markers\npytest tests/ -m \"not slow\" -v\n\n# Coverage report\npytest tests/ --cov=genesis --cov-report=html\n</code></pre>"},{"location":"contributing/testing/#testing-tools","title":"\ud83d\udcca Testing Tools","text":"<ul> <li>pytest: Main testing framework</li> <li>pytest-cov: Coverage statistics</li> <li>pytest-benchmark: Performance testing</li> <li>pytest-xdist: Parallel testing</li> </ul> <p>\ud83d\udcd8 Documentation Status: Currently being written, expected completion in v0.2.0.</p>"},{"location":"core-components/","title":"Core Components Overview","text":"<p>Genesis v2.0's core components provide the infrastructure for deep learning computation with a clean, modular architecture that separates concerns and enables high performance.</p>"},{"location":"core-components/#component-architecture","title":"\ud83e\udde9 Component Architecture","text":"<pre><code>graph TB\n    subgraph \"User API Layer\"\n        A[genesis.tensor] --&gt; B[genesis.Tensor]\n        C[genesis.matmul] --&gt; D[Functional API]\n    end\n\n    subgraph \"Core Abstraction Layer\"\n        B --&gt; E[tensor.py]\n        D --&gt; F[function.py]\n        E --&gt; G[device.py]\n        E --&gt; H[storage.py]\n        F --&gt; I[ops/dispatcher.py]\n    end\n\n    subgraph \"Backend Implementation\"\n        G --&gt; J[backends/cpu.py]\n        G --&gt; K[backends/cuda.py]\n        I --&gt; L[ops/cpu/]\n        I --&gt; M[ops/cuda/]\n    end\n\n    subgraph \"Support Systems\"\n        N[dtypes.py] --&gt; E\n        O[init.py] --&gt; E\n        K --&gt; P[backends/cuda_memory.py]\n        K --&gt; Q[backends/cuda_kernels.py]\n    end\n\n    style E fill:#e1f5fe\n    style F fill:#f3e5f5\n    style G fill:#e8f5e8\n    style I fill:#fff3e0</code></pre>"},{"location":"core-components/#core-component-list","title":"\ud83c\udfaf Core Component List","text":"Component File Main Functions Tensor System <code>tensor.py</code> Tensor class, automatic differentiation integration Function System <code>function.py</code> Function base class, autograd context Device Abstraction <code>device.py</code> Unified device interface Storage Layer <code>storage.py</code> Memory and storage abstraction Operation Dispatch <code>ops/dispatcher.py</code> Operation routing and execution Backend CPU <code>backends/cpu.py</code> CPU tensor storage and operations Backend CUDA <code>backends/cuda.py</code> CUDA tensor storage and memory Data Types <code>dtypes.py</code> Unified type system, precision management Initialization <code>init.py</code> Tensor creation and initialization"},{"location":"core-components/#design-features","title":"\ud83d\ude80 Design Features","text":""},{"location":"core-components/#1-modular-backend-architecture","title":"1. Modular Backend Architecture","text":"<ul> <li>Clean Separation: Backend implementations are completely isolated</li> <li>Extensible: Easy to add new device support (TPU, NPU, etc.)</li> <li>Performance Optimized: Each backend can be individually optimized</li> <li>Reliable: Lazy CUDA initialization prevents import-time failures</li> </ul>"},{"location":"core-components/#2-unified-operation-dispatch","title":"2. Unified Operation Dispatch","text":"<ul> <li>Single Entry Point: All operations go through centralized dispatcher</li> <li>Automatic Routing: Operations automatically routed to correct backend</li> <li>Type Preservation: Consistent behavior across different devices</li> <li>Performance: Minimal dispatch overhead</li> </ul>"},{"location":"core-components/#3-advanced-memory-management","title":"3. Advanced Memory Management","text":"<ul> <li>Memory Pooling: Reduces allocation overhead through intelligent pooling</li> <li>Device Abstraction: Unified memory interface across CPU and GPU</li> <li>Garbage Collection: Automatic cleanup of unused memory blocks</li> <li>Statistics: Detailed memory usage tracking and debugging</li> </ul>"},{"location":"core-components/#performance-characteristics","title":"\ud83d\udcca Performance Characteristics","text":""},{"location":"core-components/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Zero-copy View Operations: Operations like reshape, transpose don't copy data</li> <li>Smart Memory Management: Automatic memory release based on reference counting</li> <li>Gradient Accumulation Optimization: Reduce temporary tensor creation</li> </ul>"},{"location":"core-components/#compute-optimization","title":"Compute Optimization","text":"<ul> <li>Lazy Execution: Operations execute only when needed</li> <li>Fusion Optimization: Adjacent operations automatically fused to reduce memory access</li> <li>Parallel Computing: Full utilization of GPU parallel capabilities</li> </ul>"},{"location":"core-components/#component-collaboration","title":"\ud83d\udd17 Component Collaboration","text":""},{"location":"core-components/#tensor-creation-process-v20","title":"Tensor Creation Process (v2.0)","text":"Python<pre><code># User call\nx = genesis.tensor([1, 2, 3], device=\"cuda\")\n\n# Internal flow\ngenesis.tensor() -&gt;\ntensor.py:Tensor.__init__() -&gt;\ndevice.py:parse_device(\"cuda\") -&gt;\nstorage.py:create_storage() -&gt;\nbackends/cuda.py:CUDAStorage() -&gt;\nbackends/cuda_memory.py:allocate() -&gt;\nSet tensor attributes (shape, dtype, device)\n</code></pre>"},{"location":"core-components/#operation-dispatch-process","title":"Operation Dispatch Process","text":"Python<pre><code># User operation\nz = genesis.matmul(x, y)\n\n# Dispatch flow\nops/dispatcher.py:dispatch(\"matmul\", x, y) -&gt;\nInfer device from arguments -&gt;\nSelect backend implementation -&gt;\nops/cuda/matrix.py:cuda_matmul(x, y) -&gt;\nExecute Triton/CUDA kernel -&gt;\nReturn new tensor with result\n</code></pre>"},{"location":"core-components/#automatic-differentiation-integration","title":"Automatic Differentiation Integration","text":"Python<pre><code># Forward with gradient tracking\nx = genesis.tensor([1, 2, 3], requires_grad=True)\ny = genesis.tensor([4, 5, 6], requires_grad=True)\nz = x * y  # Element-wise multiplication\n\n# Internal autograd setup\nfunction.py:Function.apply() -&gt;\nCreate computation graph node -&gt;\nStore backward function -&gt;\nSet gradient computation context\n\n# Backward pass\nz.backward()\n# Traverse computation graph -&gt;\n# Call stored backward functions -&gt;\n# Accumulate gradients to x.grad, y.grad\n</code></pre>"},{"location":"core-components/#learning-path-recommendations","title":"\ud83c\udf93 Learning Path Recommendations","text":""},{"location":"core-components/#beginner-users","title":"Beginner Users","text":"<ol> <li>Tensor Basics - Understand Tensor creation and basic operations</li> <li>Automatic Differentiation - Understand requires_grad and backward()</li> <li>Device Management - Learn CPU/GPU switching</li> </ol>"},{"location":"core-components/#intermediate-users","title":"Intermediate Users","text":"<ol> <li>Data Types - Master usage scenarios for different precisions</li> <li>Functional Interface - Use the functional module</li> <li>Memory Optimization - Understand view operations and memory management</li> </ol>"},{"location":"core-components/#advanced-users","title":"Advanced Users","text":"<ol> <li>Custom Functions - Implement custom forward and backward propagation</li> <li>Performance Tuning - Optimize memory usage and computational efficiency</li> <li>Source Code Understanding - Deep understanding of component implementation details</li> </ol> <p>For detailed documentation of each component, please check the corresponding dedicated pages:</p> <ul> <li>Tensor System - Tensor class and automatic differentiation</li> <li>Function System - Function base class and autograd context</li> <li>Device Abstraction - Unified device interface</li> <li>Storage Layer - Memory and storage abstraction</li> <li>Data Types - Type system and precision management</li> <li>Backend System - Modular backend architecture</li> <li>Operation System - Operation dispatch and execution</li> </ul>"},{"location":"core-components/cuda-enhancements/","title":"CUDA Enhancements","text":"<p>Genesis has made significant improvements in CUDA support, providing better device management, memory operations, and error handling capabilities.</p>"},{"location":"core-components/cuda-enhancements/#device-management-enhancements","title":"Device Management Enhancements","text":""},{"location":"core-components/cuda-enhancements/#new-cuda-functions","title":"New CUDA Functions","text":""},{"location":"core-components/cuda-enhancements/#set_devicedevice","title":"<code>set_device(device)</code>","text":"<p>Set the current CUDA device.</p> Python<pre><code>import genesis.cuda as cuda\n\n# Set current device to GPU 1\ncuda.set_device(1)\nprint(f\"Current device: {cuda.current_device()}\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#device_count","title":"<code>device_count()</code>","text":"<p>Get the number of available CUDA devices.</p> Python<pre><code>device_count = cuda.device_count()\nprint(f\"Available GPUs: {device_count}\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#get_device_namedevice","title":"<code>get_device_name(device)</code>","text":"<p>Get the name of the specified device.</p> Python<pre><code>device_name = cuda.get_device_name(0)\nprint(f\"Device 0 name: {device_name}\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#synchronizedevicenone","title":"<code>synchronize(device=None)</code>","text":"<p>Synchronize CUDA operations to ensure all CUDA operations are complete.</p> Python<pre><code># Synchronize current device\ncuda.synchronize()\n\n# Synchronize specific device\ncuda.synchronize(device=1)\n</code></pre>"},{"location":"core-components/cuda-enhancements/#device-property-enhancements","title":"Device Property Enhancements","text":""},{"location":"core-components/cuda-enhancements/#new-device-class-properties","title":"New Device Class Properties","text":"Python<pre><code>device = genesis.device('cuda:0')\n\n# PyTorch-compatible properties\nprint(f\"Device type: {device.type}\")      # 'cuda'\nprint(f\"Device index: {device.index}\")    # 0\n</code></pre> <p>These properties provide full compatibility with PyTorch, making migration easier.</p>"},{"location":"core-components/cuda-enhancements/#tensor-validation-functions","title":"Tensor Validation Functions","text":""},{"location":"core-components/cuda-enhancements/#numerical-validity-checking","title":"Numerical Validity Checking","text":"<p>Genesis now provides complete tensor numerical validity checking functionality:</p>"},{"location":"core-components/cuda-enhancements/#isinftensor","title":"<code>isinf(tensor)</code>","text":"<p>Check for infinite values in tensors.</p> Python<pre><code>import genesis\n\ntensor = genesis.tensor([1.0, float('inf'), -float('inf'), 2.0], device='cuda')\ninf_mask = genesis.isinf(tensor)\nprint(inf_mask)  # [False, True, True, False]\n\n# Check if there are any infinite values\nhas_inf = genesis.isinf(tensor).any()\nif has_inf:\n    print(\"Tensor contains infinite values!\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#isnantensor","title":"<code>isnan(tensor)</code>","text":"<p>Check for NaN values in tensors.</p> Python<pre><code>tensor = genesis.tensor([1.0, float('nan'), 2.0, 3.0], device='cuda')\nnan_mask = genesis.isnan(tensor)\nprint(nan_mask)  # [False, True, False, False]\n\n# Check if there are any NaN values\nhas_nan = genesis.isnan(tensor).any()\nif has_nan:\n    print(\"Tensor contains NaN values!\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#isfinitetensor","title":"<code>isfinite(tensor)</code>","text":"<p>Check for finite values in tensors.</p> Python<pre><code>tensor = genesis.tensor([1.0, float('inf'), float('nan'), 2.0], device='cuda')\nfinite_mask = genesis.isfinite(tensor)\nprint(finite_mask)  # [True, False, False, True]\n\n# Keep only finite values\nfinite_tensor = tensor[finite_mask]\n</code></pre>"},{"location":"core-components/cuda-enhancements/#application-in-training","title":"Application in Training","text":"Python<pre><code>def check_model_gradients(model):\n    \"\"\"Check numerical stability of model gradients\"\"\"\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            if genesis.isnan(param.grad).any():\n                print(f\"Warning: {name} gradients contain NaN!\")\n                return False\n            if genesis.isinf(param.grad).any():\n                print(f\"Warning: {name} gradients contain infinity!\")\n                return False\n    return True\n\n# Use in training loop\nfor batch in dataloader:\n    optimizer.zero_grad()\n    loss = model(batch)\n    loss.backward()\n\n    if not check_model_gradients(model):\n        print(\"Skipping batch due to gradient anomaly\")\n        continue\n\n    optimizer.step()\n</code></pre>"},{"location":"core-components/cuda-enhancements/#mixed-precision-training-enhancements","title":"Mixed Precision Training Enhancements","text":""},{"location":"core-components/cuda-enhancements/#gradscaler-improvements","title":"GradScaler Improvements","text":"Python<pre><code>import genesis.amp as amp\n\n# Create gradient scaler\nscaler = amp.GradScaler()\n\n# Training loop\nfor batch in dataloader:\n    with amp.autocast():\n        outputs = model(batch['input'])\n        loss = criterion(outputs, batch['target'])\n\n    # Scale loss and backward pass\n    scaler.scale(loss).backward()\n\n    # Check gradient validity (now uses genesis native functions)\n    scaler.step(optimizer)  # Internally uses genesis.isinf/isnan checks\n    scaler.update()\n</code></pre>"},{"location":"core-components/cuda-enhancements/#cuda-memory-management","title":"CUDA Memory Management","text":""},{"location":"core-components/cuda-enhancements/#improved-memory-allocation","title":"Improved Memory Allocation","text":"Python<pre><code>import genesis.cuda as cuda\n\n# Check CUDA availability\nif cuda.is_available():\n    print(f\"CUDA available, device count: {cuda.device_count()}\")\n\n    # Get current device info\n    current_dev = cuda.current_device()\n    device_name = cuda.get_device_name(current_dev)\n    print(f\"Current device: {current_dev} ({device_name})\")\n\n    # Set specific device\n    cuda.set_device(0)\n\n    # Synchronize to ensure operations complete\n    cuda.synchronize()\n</code></pre>"},{"location":"core-components/cuda-enhancements/#triton-kernel-optimizations","title":"Triton Kernel Optimizations","text":""},{"location":"core-components/cuda-enhancements/#numerical-check-kernels","title":"Numerical Check Kernels","text":"<p>Genesis implements efficient Triton kernels for numerical checks:</p> Python<pre><code># High-performance numerical checks (internal implementation)\n@triton.jit\ndef isinf_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Triton kernel to check if elements are infinite\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load input values\n    x = tl.load(input_ptr + offsets, mask=mask)\n\n    # Check for infinity\n    finite_max = 3.4028235e+38  # Maximum finite float32 value\n    is_pos_inf = x &gt; finite_max\n    is_neg_inf = x &lt; -finite_max\n    result = is_pos_inf | is_neg_inf\n\n    # Store result as boolean\n    tl.store(output_ptr + offsets, result.to(tl.int8), mask=mask)\n</code></pre>"},{"location":"core-components/cuda-enhancements/#error-handling-improvements","title":"Error Handling Improvements","text":""},{"location":"core-components/cuda-enhancements/#cuda-error-checking","title":"CUDA Error Checking","text":"Python<pre><code>import genesis.cuda as cuda\n\ntry:\n    # CUDA operations\n    cuda.set_device(0)\n    cuda.synchronize()\nexcept RuntimeError as e:\n    if \"CUDA\" in str(e):\n        print(f\"CUDA error: {e}\")\n        # Handle CUDA error\n    else:\n        raise\n</code></pre>"},{"location":"core-components/cuda-enhancements/#device-availability-checking","title":"Device Availability Checking","text":"Python<pre><code>def safe_cuda_operation():\n    \"\"\"Safe CUDA operation example\"\"\"\n    if not cuda.is_available():\n        print(\"CUDA not available, using CPU\")\n        return genesis.device('cpu')\n\n    try:\n        device_count = cuda.device_count()\n        if device_count == 0:\n            print(\"No available CUDA devices\")\n            return genesis.device('cpu')\n\n        # Select device\n        cuda.set_device(0)\n        return genesis.device('cuda:0')\n\n    except RuntimeError as e:\n        print(f\"CUDA initialization failed: {e}\")\n        return genesis.device('cpu')\n</code></pre>"},{"location":"core-components/cuda-enhancements/#best-practices","title":"Best Practices","text":""},{"location":"core-components/cuda-enhancements/#1-numerical-stability-checks","title":"1. Numerical Stability Checks","text":"Python<pre><code>def validate_tensor(tensor, name=\"tensor\"):\n    \"\"\"Validate tensor numerical stability\"\"\"\n    if genesis.isnan(tensor).any():\n        raise ValueError(f\"{name} contains NaN values\")\n    if genesis.isinf(tensor).any():\n        raise ValueError(f\"{name} contains infinite values\")\n    return True\n\n# Use at critical points\nloss = criterion(outputs, targets)\nvalidate_tensor(loss, \"loss\")\n\ngradients = model.get_gradients()\nfor name, grad in gradients.items():\n    validate_tensor(grad, f\"gradient_{name}\")\n</code></pre>"},{"location":"core-components/cuda-enhancements/#2-device-management","title":"2. Device Management","text":"Python<pre><code>class DeviceManager:\n    \"\"\"Device manager\"\"\"\n\n    def __init__(self):\n        self.available_devices = []\n        self._detect_devices()\n\n    def _detect_devices(self):\n        \"\"\"Detect available devices\"\"\"\n        if cuda.is_available():\n            for i in range(cuda.device_count()):\n                device_name = cuda.get_device_name(i)\n                self.available_devices.append({\n                    'index': i,\n                    'name': device_name,\n                    'type': 'cuda'\n                })\n        else:\n            self.available_devices.append({\n                'index': None,\n                'name': 'CPU',\n                'type': 'cpu'\n            })\n\n    def get_best_device(self):\n        \"\"\"Get the best available device\"\"\"\n        if self.available_devices and self.available_devices[0]['type'] == 'cuda':\n            cuda.set_device(0)\n            return genesis.device('cuda:0')\n        return genesis.device('cpu')\n\n# Usage example\ndevice_manager = DeviceManager()\ndevice = device_manager.get_best_device()\nmodel = model.to(device)\n</code></pre>"},{"location":"core-components/cuda-enhancements/#3-memory-management","title":"3. Memory Management","text":"Python<pre><code>def memory_safe_operation(tensor, operation):\n    \"\"\"Memory-safe operation\"\"\"\n    try:\n        # Ensure on correct device\n        if tensor.device.type == 'cuda':\n            cuda.set_device(tensor.device.index)\n\n        # Execute operation\n        result = operation(tensor)\n\n        # Synchronize to ensure completion\n        if tensor.device.type == 'cuda':\n            cuda.synchronize()\n\n        return result\n\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            print(\"GPU out of memory, attempting cleanup\")\n            # Can add memory cleanup logic here\n        raise\n\n# Usage example\nresult = memory_safe_operation(tensor, lambda x: x.matmul(weight))\n</code></pre>"},{"location":"core-components/cuda-enhancements/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"core-components/cuda-enhancements/#cuda-operation-performance-monitoring","title":"CUDA Operation Performance Monitoring","text":"Python<pre><code>import time\n\ndef profile_cuda_operation(operation, tensor, name=\"operation\"):\n    \"\"\"Profile CUDA operation performance\"\"\"\n    if tensor.device.type == 'cuda':\n        cuda.synchronize()  # Ensure previous operations complete\n\n    start_time = time.time()\n    result = operation(tensor)\n\n    if tensor.device.type == 'cuda':\n        cuda.synchronize()  # Ensure operation completes\n\n    end_time = time.time()\n    print(f\"{name} took: {(end_time - start_time)*1000:.2f} ms\")\n\n    return result\n\n# Usage example\nresult = profile_cuda_operation(\n    lambda x: genesis.matmul(x, weight),\n    input_tensor,\n    \"matrix multiplication\"\n)\n</code></pre> <p>These CUDA enhancements provide better device management, numerical stability checking, and error handling, making Genesis more stable and user-friendly.</p>"},{"location":"core-components/cuda-memory/","title":"CUDA Memory Management","text":"<p>Genesis includes a sophisticated high-performance CUDA memory management system that provides efficient GPU memory allocation through a segment-block allocator architecture with advanced caching strategies.</p>"},{"location":"core-components/cuda-memory/#overview","title":"Overview","text":"<p>The CUDA Memory Manager is a production-grade memory allocator that achieves significant performance improvements over naive allocation strategies. It features a two-level caching system, segment-block allocation, and comprehensive performance monitoring.</p>"},{"location":"core-components/cuda-memory/#architecture","title":"Architecture","text":""},{"location":"core-components/cuda-memory/#core-components","title":"Core Components","text":""},{"location":"core-components/cuda-memory/#cudamemorymanager","title":"CUDAMemoryManager","text":"<p>The main memory manager class with enterprise-grade features: - Segment-Block Allocator: Hierarchical memory organization for efficient allocation - Two-Level Caching: Stream-local cache + global cache for maximum performance - Warmup Cache: Pre-allocation strategy for common allocation patterns - Performance Monitoring: Detailed statistics and benchmarking capabilities - Hybrid Allocation Strategy: Optimized paths for small vs large allocations</p>"},{"location":"core-components/cuda-memory/#segment-block-architecture","title":"Segment-Block Architecture","text":"Python<pre><code>@dataclass\nclass Block:\n    \"\"\"\n    Individual memory block within a segment\n    \"\"\"\n    ptr: int          # GPU pointer\n    size: int         # Block size  \n    is_free: bool     # Availability status\n    segment_id: int   # Parent segment ID\n\nclass Segment:\n    \"\"\"\n    Large contiguous memory region containing multiple blocks\n    \"\"\"\n    def __init__(self, segment_id: int, size: int):\n        # Allocate entire segment from CUDA\n        self.base_ptr = _ok(cuda.cuMemAlloc(size))\n\n        # Initialize memory to zero (prevents dirty data precision issues)\n        _ok(cuda.cuMemsetD8(self.base_ptr, 0, size))\n\n        # Start as single large free block\n        self.blocks: List[Block] = [...]\n        self.free_blocks_by_size: Dict[int, List[Block]] = {...}\n</code></pre>"},{"location":"core-components/cuda-memory/#key-features","title":"Key Features","text":""},{"location":"core-components/cuda-memory/#1-high-performance-segment-block-allocation","title":"1. High-Performance Segment-Block Allocation","text":"<ul> <li>Best-Fit Algorithm: Finds optimal block size to minimize fragmentation</li> <li>Block Splitting: Large blocks automatically split for exact size requests</li> <li>Block Coalescing: Adjacent free blocks merged to prevent fragmentation</li> <li>Size-Based Indexing: O(1) lookup for free blocks by size</li> </ul>"},{"location":"core-components/cuda-memory/#2-two-level-caching-system","title":"2. Two-Level Caching System","text":"Python<pre><code>class TwoLevelCache:\n    \"\"\"\n    Sophisticated caching with stream-local and global levels\n    \"\"\"\n    def __init__(self):\n        self.stream_cache: Dict[int, Dict[int, List[int]]] = {}  # stream_id -&gt; size -&gt; [ptrs]\n        self.global_cache: Dict[int, List[int]] = {}             # size -&gt; [ptrs]\n        self.cache_stats = CacheStatistics()\n</code></pre> <p>Stream-Local Cache: - Per-stream block caching for CUDA stream efficiency - Avoids cross-stream synchronization overhead - Optimal for repetitive allocation patterns</p> <p>Global Cache: - Shared cache across all streams - Fallback when stream-local cache misses - Maximizes memory reuse across operations</p>"},{"location":"core-components/cuda-memory/#3-warmup-cache-preallocation","title":"3. Warmup Cache Preallocation","text":"Python<pre><code>def warmup_cache(self, sizes: List[int], counts: List[int]):\n    \"\"\"\n    Pre-populate cache with common allocation sizes\n\n    Performance optimization for known allocation patterns:\n    - Transformer attention matrices\n    - Embedding lookups  \n    - Gradient buffers\n    \"\"\"\n    for size, count in zip(sizes, counts):\n        for _ in range(count):\n            ptr = self.allocate_segment_block(size)\n            self.add_to_cache(ptr, size)\n</code></pre>"},{"location":"core-components/cuda-memory/#4-adaptive-allocation-strategy","title":"4. Adaptive Allocation Strategy","text":"Python<pre><code>def allocate_memory(self, size: int) -&gt; int:\n    \"\"\"\n    Hybrid allocation strategy optimized for different size ranges\n    \"\"\"\n    if size &lt; self.SMALL_BLOCK_THRESHOLD:\n        # Small allocations: prioritize cache hits\n        return self.allocate_from_cache(size) or self.allocate_segment_block(size)\n    else:\n        # Large allocations: direct segment allocation\n        return self.allocate_large_block(size)\n</code></pre>"},{"location":"core-components/cuda-memory/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"core-components/cuda-memory/#benchmark-results-vs-pytorch","title":"Benchmark Results (vs PyTorch)","text":"Scenario Genesis Performance Status Same-size allocations 1.43x faster \u2705 Excellent Large memory (&gt;1MB) 3.92x faster \u2705 Outstanding Transformer training 1.89x faster \u2705 Excellent Memory pressure 4.83x faster \u2705 Outstanding Variable sizes 0.83x (slower) \ud83d\udd04 Optimization target"},{"location":"core-components/cuda-memory/#memory-efficiency-improvements","title":"Memory Efficiency Improvements","text":"<ol> <li> <p>Elimination of cudaMalloc/cudaFree overhead:    Python<pre><code># Before: Direct CUDA calls (slow)\nptr = cuda.cuMemAlloc(size)  # ~100\u03bcs overhead\n\n# After: Cache-based allocation (fast)\nptr = cache.get(size) or segment.allocate(size)  # ~1\u03bcs overhead\n</code></pre></p> </li> <li> <p>Reduced memory fragmentation:</p> </li> <li>Block coalescing prevents fragmentation</li> <li>Best-fit allocation minimizes waste</li> <li> <p>Segment organization improves locality</p> </li> <li> <p>Optimized for ML workloads:</p> </li> <li>Warmup cache for common tensor sizes</li> <li>Stream-aware allocation for parallel operations</li> <li>Batch allocation support for multi-tensor operations</li> </ol>"},{"location":"core-components/cuda-memory/#advanced-features","title":"Advanced Features","text":""},{"location":"core-components/cuda-memory/#1-performance-monitoring","title":"1. Performance Monitoring","text":"Python<pre><code>@dataclass\nclass AllocationStatistics:\n    \"\"\"Comprehensive allocation tracking\"\"\"\n    total_allocations: int = 0\n    total_freed: int = 0\n    peak_memory_usage: int = 0\n    cache_hits: int = 0\n    cache_misses: int = 0\n    fragmentation_ratio: float = 0.0\n\n    def efficiency_score(self) -&gt; float:\n        \"\"\"Calculate memory manager efficiency (0-1)\"\"\"\n        if self.total_allocations == 0:\n            return 1.0\n        return self.cache_hits / self.total_allocations\n</code></pre>"},{"location":"core-components/cuda-memory/#2-memory-pool-optimization","title":"2. Memory Pool Optimization","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"\n    Asynchronous memory pool for high-throughput scenarios\n    \"\"\"\n    def __init__(self, pool_size: int = 1024 * 1024 * 1024):  # 1GB default\n        self.pool = MemoryPool(pool_size)\n        self.allocation_queue = AsyncQueue()\n        self.background_worker = Thread(target=self._allocation_worker)\n\n    def allocate_async(self, size: int) -&gt; Future[int]:\n        \"\"\"Non-blocking allocation for pipeline parallelism\"\"\"\n        return self.allocation_queue.submit(self._allocate, size)\n</code></pre>"},{"location":"core-components/cuda-memory/#3-batch-allocation-support","title":"3. Batch Allocation Support","text":"Python<pre><code>def allocate_batch(self, sizes: List[int]) -&gt; List[int]:\n    \"\"\"\n    Optimized batch allocation for multi-tensor operations\n\n    Advantages:\n    - Reduced allocation overhead\n    - Better memory locality  \n    - Automatic size optimization\n    \"\"\"\n    # Group similar sizes for efficient segment usage\n    size_groups = self._group_by_size(sizes)\n\n    ptrs = []\n    for size_group in size_groups:\n        segment = self._find_or_create_segment(size_group.total_size)\n        group_ptrs = segment.allocate_batch(size_group.sizes)\n        ptrs.extend(group_ptrs)\n\n    return ptrs\n</code></pre>"},{"location":"core-components/cuda-memory/#memory-management-patterns","title":"Memory Management Patterns","text":""},{"location":"core-components/cuda-memory/#1-transformer-training-optimization","title":"1. Transformer Training Optimization","text":"Python<pre><code># Optimized memory allocation for transformer training\ndef allocate_transformer_tensors(batch_size: int, seq_len: int, hidden_size: int):\n    \"\"\"\n    Pre-allocate common transformer tensor sizes\n    \"\"\"\n    common_sizes = [\n        batch_size * seq_len * hidden_size,      # Attention weights\n        batch_size * seq_len * hidden_size * 4,  # Feed-forward\n        batch_size * seq_len * seq_len,          # Attention scores\n    ]\n\n    # Warmup cache with expected allocation pattern\n    memory_manager.warmup_cache(common_sizes, counts=[10, 5, 10])\n</code></pre>"},{"location":"core-components/cuda-memory/#2-dynamic-memory-scaling","title":"2. Dynamic Memory Scaling","text":"Python<pre><code>def adaptive_memory_management(memory_pressure: float):\n    \"\"\"\n    Automatically adjust cache sizes based on memory pressure\n    \"\"\"\n    if memory_pressure &gt; 0.8:\n        # High pressure: aggressive cache cleanup\n        memory_manager.cleanup_cache(threshold=0.9)\n        memory_manager.enable_aggressive_coalescing()\n    elif memory_pressure &lt; 0.3:\n        # Low pressure: expand cache for better performance\n        memory_manager.expand_cache_size(factor=1.5)\n</code></pre>"},{"location":"core-components/cuda-memory/#usage-examples","title":"Usage Examples","text":""},{"location":"core-components/cuda-memory/#basic-allocation","title":"Basic Allocation","text":"Python<pre><code>from genesis.ndarray.cuda_memory_manager import get_memory_manager\n\n# Get global memory manager instance\nmm = get_memory_manager()\n\n# Allocate GPU memory\nptr = mm.allocate_memory(1024 * 1024)  # 1MB\n\n# Free memory (automatic caching)\nmm.free_memory(ptr, 1024 * 1024)\n\n# Check statistics\nstats = mm.get_statistics()\nprint(f\"Cache hit rate: {stats.cache_hit_rate:.2%}\")\nprint(f\"Memory efficiency: {stats.efficiency_score():.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory/#advanced-configuration","title":"Advanced Configuration","text":"Python<pre><code># Configure memory manager for specific workload\nmm.configure(\n    segment_size=512 * 1024 * 1024,    # 512MB segments\n    cache_sizes={\n        'stream_local': 100,            # 100 blocks per stream\n        'global': 500,                  # 500 blocks global cache\n    },\n    warmup_sizes=[\n        (4096, 50),    # 50 blocks of 4KB\n        (65536, 20),   # 20 blocks of 64KB  \n        (1048576, 10), # 10 blocks of 1MB\n    ]\n)\n</code></pre>"},{"location":"core-components/cuda-memory/#performance-monitoring","title":"Performance Monitoring","text":"Python<pre><code># Enable detailed performance tracking\nwith mm.performance_context() as perf:\n    # Run memory-intensive operations\n    tensors = [genesis.randn(1000, 1000) for _ in range(100)]\n\n# Analyze performance\nprint(f\"Total allocations: {perf.stats.total_allocations}\")\nprint(f\"Peak memory: {perf.stats.peak_memory_usage / 1024**3:.2f} GB\")\nprint(f\"Fragmentation: {perf.stats.fragmentation_ratio:.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory/#configuration-and-tuning","title":"Configuration and Tuning","text":""},{"location":"core-components/cuda-memory/#environment-variables","title":"Environment Variables","text":"Bash<pre><code># Memory manager configuration\nexport GENESIS_CUDA_SEGMENT_SIZE=1073741824     # 1GB segments\nexport GENESIS_CUDA_CACHE_SIZE=1000             # Cache 1000 blocks\nexport GENESIS_CUDA_WARMUP_ENABLED=true         # Enable warmup\nexport GENESIS_CUDA_STATS_ENABLED=true          # Enable statistics\n</code></pre>"},{"location":"core-components/cuda-memory/#runtime-configuration","title":"Runtime Configuration","text":"Python<pre><code># Configure at runtime\ngenesis.cuda.configure_memory_manager({\n    'segment_size': 1024 * 1024 * 1024,  # 1GB\n    'enable_warmup': True,\n    'enable_stats': True,\n    'allocation_strategy': 'best_fit',\n    'coalescing_threshold': 0.1,\n})\n</code></pre>"},{"location":"core-components/cuda-memory/#best-practices","title":"Best Practices","text":"<ol> <li>Use Warmup Cache: Pre-allocate common sizes for 38x performance boost</li> <li>Monitor Statistics: Track cache hit rates and memory efficiency</li> <li>Batch Allocations: Group similar operations for better locality</li> <li>Avoid Frequent Small Allocations: Cache overhead dominates for tiny blocks</li> <li>Use Appropriate Segment Sizes: Match segment size to workload memory patterns</li> </ol>"},{"location":"core-components/cuda-memory/#troubleshooting","title":"Troubleshooting","text":""},{"location":"core-components/cuda-memory/#memory-leaks","title":"Memory Leaks","text":"Python<pre><code># Debug memory leaks\nstats = mm.get_statistics()\nif stats.total_allocations &gt; stats.total_freed + 1000:\n    print(\"Warning: Potential memory leak detected\")\n    mm.dump_allocation_trace()\n</code></pre>"},{"location":"core-components/cuda-memory/#performance-issues","title":"Performance Issues","text":"Python<pre><code># Diagnose performance problems\nif stats.cache_hit_rate &lt; 0.5:\n    print(\"Low cache hit rate - consider warmup cache\")\n    mm.analyze_allocation_patterns()\n\nif stats.fragmentation_ratio &gt; 0.3:\n    print(\"High fragmentation - enable aggressive coalescing\")\n    mm.enable_aggressive_coalescing()\n</code></pre>"},{"location":"core-components/cuda-memory/#memory-pressure","title":"Memory Pressure","text":"Python<pre><code># Handle memory pressure\ndef handle_oom():\n    \"\"\"Out of memory handler\"\"\"\n    mm.cleanup_cache(force=True)\n    mm.coalesce_free_blocks()\n    mm.garbage_collect()\n</code></pre>"},{"location":"core-components/cuda-memory/#integration-with-genesis","title":"Integration with Genesis","text":"<p>The memory manager integrates seamlessly with Genesis tensors and operations:</p> Python<pre><code># Automatic integration with tensor operations\nx = genesis.randn(1000, 1000)  # Uses memory manager automatically\ny = genesis.matmul(x, x)       # Efficient memory reuse\nz = x + y                      # Cache-optimized allocation\n</code></pre> <p>This sophisticated memory management system is a key factor in Genesis achieving near-PyTorch performance while maintaining the educational clarity of a from-scratch implementation.</p>"},{"location":"core-components/cuda-storage/","title":"CUDA Storage System","text":"<p>Genesis's CUDA Storage (CUDAStorage) is a core component of the framework, providing pure CUDA implementation for GPU memory management and operations, completely independent of PyTorch, using CUDA Python API directly.</p>"},{"location":"core-components/cuda-storage/#design-goals","title":"\ud83c\udfaf Design Goals","text":""},{"location":"core-components/cuda-storage/#independence","title":"Independence","text":"<ul> <li>Pure CUDA Implementation: No dependency on PyTorch's GPU backend</li> <li>Direct Memory Management: Direct GPU memory management using CUDA Python API</li> <li>High Performance: Memory access patterns optimized for GPU</li> </ul>"},{"location":"core-components/cuda-storage/#compatibility","title":"Compatibility","text":"<ul> <li>PyTorch-style API: Maintains interface compatibility with PyTorch tensors</li> <li>Automatic Differentiation Support: Seamless integration with Genesis's autograd system</li> <li>Type Safety: Complete type annotations and runtime checking</li> </ul>"},{"location":"core-components/cuda-storage/#architecture-design","title":"\ud83c\udfd7\ufe0f Architecture Design","text":""},{"location":"core-components/cuda-storage/#modular-indexing-architecture","title":"Modular Indexing Architecture","text":"<p>Genesis uses a modular approach to handle complex tensor indexing operations through the <code>CUDAIndexingOps</code> class:</p> Python<pre><code>class CUDAIndexingOps:\n    \"\"\"Centralized indexing operations for CUDA storage\"\"\"\n\n    @staticmethod\n    def parse_index(storage, key):\n        \"\"\"Parse indexing key and create execution plan\"\"\"\n\n    @staticmethod\n    def execute_getitem(storage, plan):\n        \"\"\"Execute getitem operation\"\"\"\n\n    @staticmethod  \n    def execute_setitem(storage, plan, value):\n        \"\"\"Execute setitem operation\"\"\"\n</code></pre> <p>Key Benefits: - Separation of Concerns: Indexing logic separated from storage management - Maintainable Code: Clear organization of complex indexing operations - Performance Optimization: Specialized implementations for different indexing patterns</p>"},{"location":"core-components/cuda-storage/#memory-management","title":"Memory Management","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"Asynchronous memory pool to optimize GPU memory allocation performance\"\"\"\n\n    def __init__(self):\n        self.free_blocks = {}  # Free blocks organized by size\n        self.allocated_blocks = {}  # Allocated blocks\n        self.alignment = 512  # Memory alignment, consistent with PyTorch\n\n    def allocate(self, size_bytes: int) -&gt; int:\n        \"\"\"Allocate aligned GPU memory\"\"\"\n\n    def deallocate(self, ptr: int):\n        \"\"\"Release GPU memory to pool for reuse\"\"\"\n</code></pre>"},{"location":"core-components/cuda-storage/#core-features","title":"\ud83d\udca1 Core Features","text":""},{"location":"core-components/cuda-storage/#1-efficient-indexing-operations","title":"1. Efficient Indexing Operations","text":"Python<pre><code>import genesis\n\n# Create CUDA tensor\nx = genesis.randn(1000, 1000, device='cuda')\n\n# Basic indexing - uses VIEW operation, zero-copy\ny = x[10:20, 50:100]  # IndexPlan.kind = VIEW\n\n# Advanced indexing - uses GATHER operation  \nindices = genesis.tensor([1, 3, 5, 7], device='cuda')\nz = x[indices]  # IndexPlan.kind = GATHER\n\n# Boolean indexing - automatic optimization\nmask = x &gt; 0.5\nw = x[mask]  # Choose optimal strategy based on sparsity\n</code></pre>"},{"location":"core-components/cuda-storage/#2-memory-efficient-operations","title":"2. Memory-Efficient Operations","text":"Python<pre><code># In-place operations, avoid memory allocation\nx = genesis.randn(1000, 1000, device='cuda')\nx += 1.0  # In-place addition\n\n# View operations, zero-copy\ny = x.view(100, 10000)  # Change shape without copying data\nz = x.transpose(0, 1)   # Transpose view\n\n# Strided operations, efficient implementation\nw = x[::2, ::3]  # Strided indexing using optimized COPY operation\n</code></pre>"},{"location":"core-components/cuda-storage/#3-triton-kernel-integration","title":"3. Triton Kernel Integration","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized Triton addition kernel\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# CUDATensor automatically calls optimized Triton kernel\ndef add_cuda_tensor(x: CUDATensor, y: CUDATensor) -&gt; CUDATensor:\n    \"\"\"CUDA tensor addition using Triton optimization\"\"\"\n    output = CUDATensor(x.shape, x.dtype)\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](x.data_ptr(), y.data_ptr(), output.data_ptr(), \n                     n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output\n</code></pre>"},{"location":"core-components/cuda-storage/#basic-usage","title":"\ud83d\ude80 Basic Usage","text":""},{"location":"core-components/cuda-storage/#creating-tensors","title":"Creating Tensors","text":"Python<pre><code>import genesis\n\n# Create from data\ndata = [[1.0, 2.0], [3.0, 4.0]]\ntensor = genesis.tensor(data, device='cuda')\n\n# Create specific shapes directly\nzeros = genesis.zeros(100, 100, device='cuda')\nones = genesis.ones(50, 50, device='cuda')  \nrandom = genesis.randn(200, 200, device='cuda')\n\n# Specify data type\nfloat16_tensor = genesis.randn(100, 100, dtype=genesis.float16, device='cuda')\nint_tensor = genesis.randint(0, 10, (50, 50), device='cuda')\n\nprint(f\"Tensor shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\nprint(f\"Strides: {tensor.strides}\")\n</code></pre>"},{"location":"core-components/cuda-storage/#basic-operations","title":"Basic Operations","text":"Python<pre><code># Mathematical operations\nx = genesis.randn(100, 100, device='cuda')\ny = genesis.randn(100, 100, device='cuda')\n\n# Element-wise operations\nz = x + y      # Addition\nw = x * y      # Multiplication  \nu = x.pow(2)   # Power operation\nv = x.exp()    # Exponential function\n\n# Reduction operations\nsum_all = x.sum()           # Global sum\nsum_dim = x.sum(dim=0)      # Sum along dimension\nmean_val = x.mean()         # Mean value\nmax_val, indices = x.max(dim=1)  # Maximum value and indices\n\n# Linear algebra\na = genesis.randn(100, 50, device='cuda')\nb = genesis.randn(50, 200, device='cuda') \nc = genesis.matmul(a, b)    # Matrix multiplication\n\n# Shape operations\nreshaped = x.view(10, 1000)        # Reshape\ntransposed = x.transpose(0, 1)     # Transpose  \nflattened = x.flatten()            # Flatten\n</code></pre>"},{"location":"core-components/cuda-storage/#advanced-indexing","title":"Advanced Indexing","text":"Python<pre><code># Create test tensor\ndata = genesis.arange(0, 100, device='cuda').view(10, 10)\nprint(\"Original data:\")\nprint(data)\n\n# Basic slicing\nslice_basic = data[2:5, 3:7]  # Rows 2-4, columns 3-6\nprint(\"Basic slicing:\", slice_basic.shape)\n\n# Strided indexing\nslice_stride = data[::2, 1::2]  # Every other row, every other column starting from column 1\nprint(\"Strided indexing:\", slice_stride.shape)\n\n# Advanced indexing - integer arrays\nrow_indices = genesis.tensor([0, 2, 4, 6], device='cuda')\ncol_indices = genesis.tensor([1, 3, 5, 7], device='cuda')\nadvanced = data[row_indices, col_indices]  # Select specific positions\nprint(\"Advanced indexing result:\", advanced)\n\n# Boolean indexing\nmask = data &gt; 50\nmasked_data = data[mask]  # Select elements greater than 50\nprint(\"Boolean indexing result:\", masked_data)\n\n# Mixed indexing\nmixed = data[row_indices, 2:8]  # Column range for specific rows\nprint(\"Mixed indexing:\", mixed.shape)\n</code></pre>"},{"location":"core-components/cuda-storage/#memory-management_1","title":"\ud83d\udd27 Memory Management","text":""},{"location":"core-components/cuda-storage/#memory-pool-optimization","title":"Memory Pool Optimization","text":"Python<pre><code># Check memory usage\nprint(f\"Allocated memory: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"Cached memory: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# Manual memory management\nx = genesis.randn(1000, 1000, device='cuda')\nprint(f\"After tensor creation: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\ndel x  # Delete reference\ngenesis.cuda.empty_cache()  # Empty cache\nprint(f\"After cleanup: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\n# Memory snapshot (for debugging)\nsnapshot = genesis.cuda.memory_snapshot()\nfor entry in snapshot[:3]:  # Show first 3 entries\n    print(f\"Address: {entry['address']}, Size: {entry['size']} bytes\")\n</code></pre>"},{"location":"core-components/cuda-storage/#asynchronous-operations","title":"Asynchronous Operations","text":"Python<pre><code># Asynchronous memory operations\nwith genesis.cuda.stream():\n    x = genesis.randn(1000, 1000, device='cuda')\n    y = genesis.randn(1000, 1000, device='cuda')\n    z = genesis.matmul(x, y)  # Asynchronous execution\n\n    # Other CPU work can proceed in parallel\n    print(\"Matrix multiplication running asynchronously on GPU...\")\n\n    # Synchronize and wait for results  \n    genesis.cuda.synchronize()\n    print(\"Computation complete:\", z.shape)\n</code></pre>"},{"location":"core-components/cuda-storage/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"core-components/cuda-storage/#1-memory-access-pattern-optimization","title":"1. Memory Access Pattern Optimization","text":"Python<pre><code>def inefficient_access():\n    \"\"\"Inefficient memory access pattern\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n    result = genesis.zeros(1000, device='cuda')\n\n    # Non-contiguous access, cache misses\n    for i in range(1000):\n        result[i] = x[i, ::10].sum()  # Strided access\n\n    return result\n\ndef efficient_access():  \n    \"\"\"Efficient memory access pattern\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Contiguous access, full cache utilization\n    indices = genesis.arange(0, 1000, 10, device='cuda')\n    selected = x[:, indices]  # Batch selection\n    result = selected.sum(dim=1)  # Vectorized summation\n\n    return result\n\n# Performance comparison\nimport time\n\nstart = time.time()\nresult1 = inefficient_access()\ntime1 = time.time() - start\n\nstart = time.time()  \nresult2 = efficient_access()\ntime2 = time.time() - start\n\nprint(f\"Inefficient method: {time1:.4f}s\")\nprint(f\"Efficient method: {time2:.4f}s\")  \nprint(f\"Speedup: {time1/time2:.2f}x\")\n</code></pre>"},{"location":"core-components/cuda-storage/#2-batch-operations-optimization","title":"2. Batch Operations Optimization","text":"Python<pre><code>def batch_operations_demo():\n    \"\"\"Demonstrate performance advantages of batch operations\"\"\"\n\n    # Create test data\n    matrices = [genesis.randn(100, 100, device='cuda') for _ in range(10)]\n\n    # Method 1: Individual processing (inefficient)\n    start = time.time()\n    results1 = []\n    for matrix in matrices:\n        result = matrix.exp().sum()\n        results1.append(result)\n    time1 = time.time() - start\n\n    # Method 2: Batch processing (efficient)\n    start = time.time()\n    batched = genesis.stack(matrices, dim=0)  # [10, 100, 100]\n    results2 = batched.exp().sum(dim=(1, 2))  # [10]\n    time2 = time.time() - start\n\n    print(f\"Individual processing: {time1:.4f}s\")\n    print(f\"Batch processing: {time2:.4f}s\")\n    print(f\"Speedup: {time1/time2:.2f}x\")\n\nbatch_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage/#3-in-place-operations","title":"3. In-place Operations","text":"Python<pre><code>def inplace_operations_demo():\n    \"\"\"Demonstrate memory efficiency of in-place operations\"\"\"\n\n    # Non-in-place operations (create new tensors)\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    y = x + 1.0      # Create new tensor\n    z = y * 2.0      # Create another new tensor\n    w = z.exp()      # Create yet another new tensor\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"Non-in-place operations memory growth: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\n    # In-place operations (modify original tensor)\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    x += 1.0         # In-place addition\n    x *= 2.0         # In-place multiplication  \n    x.exp_()         # In-place exponential function\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"In-place operations memory growth: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\ninplace_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage/#debugging-and-diagnostics","title":"\ud83d\udc1b Debugging and Diagnostics","text":""},{"location":"core-components/cuda-storage/#memory-leak-detection","title":"Memory Leak Detection","text":"Python<pre><code>def detect_memory_leaks():\n    \"\"\"Detect memory leaks\"\"\"\n    genesis.cuda.empty_cache()\n    initial_memory = genesis.cuda.memory_allocated()\n\n    # Perform some operations\n    for i in range(100):\n        x = genesis.randn(100, 100, device='cuda')\n        y = x.matmul(x)\n        del x, y\n\n    genesis.cuda.empty_cache()\n    final_memory = genesis.cuda.memory_allocated()\n\n    if final_memory &gt; initial_memory:\n        print(f\"Possible memory leak: {(final_memory - initial_memory) / 1024**2:.1f} MB\")\n    else:\n        print(\"No memory leak detected\")\n\ndetect_memory_leaks()\n</code></pre>"},{"location":"core-components/cuda-storage/#error-diagnostics","title":"Error Diagnostics","text":"Python<pre><code>def diagnose_cuda_errors():\n    \"\"\"CUDA error diagnostics\"\"\"\n    try:\n        # Operations that might cause errors\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = genesis.randn(500, 500, device='cuda')  # Shape mismatch\n        z = genesis.matmul(x, y)\n\n    except RuntimeError as e:\n        print(f\"CUDA error: {e}\")\n\n        # Check CUDA status\n        if genesis.cuda.is_available():\n            print(f\"CUDA device: {genesis.cuda.get_device_name()}\")\n            print(f\"CUDA capability: {genesis.cuda.get_device_capability()}\")\n            print(f\"Available memory: {genesis.cuda.get_device_properties().total_memory / 1024**3:.1f} GB\")\n        else:\n            print(\"CUDA unavailable\")\n\ndiagnose_cuda_errors()\n</code></pre>"},{"location":"core-components/cuda-storage/#pytorch-interoperability","title":"\ud83d\udd04 PyTorch Interoperability","text":"Python<pre><code>import torch\n\ndef pytorch_interop_demo():\n    \"\"\"Demonstrate interoperability with PyTorch\"\"\"\n\n    # Convert Genesis tensor to PyTorch\n    genesis_tensor = genesis.randn(100, 100, device='cuda')\n\n    # Convert to PyTorch (shared memory)\n    pytorch_tensor = torch.as_tensor(genesis_tensor.detach().cpu().numpy()).cuda()\n\n    print(f\"Genesis shape: {genesis_tensor.shape}\")\n    print(f\"PyTorch shape: {pytorch_tensor.shape}\")\n\n    # PyTorch tensor to Genesis  \n    torch_data = torch.randn(50, 50, device='cuda')\n    genesis_from_torch = genesis.tensor(torch_data.cpu().numpy(), device='cuda')\n\n    print(f\"Conversion successful, Genesis tensor: {genesis_from_torch.shape}\")\n\npytorch_interop_demo()\n</code></pre>"},{"location":"core-components/cuda-storage/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"Python<pre><code>def benchmark_cuda_tensor():\n    \"\"\"CUDA tensor performance benchmark tests\"\"\"\n\n    sizes = [100, 500, 1000, 2000]\n\n    print(\"Matrix multiplication performance comparison (Genesis vs PyTorch):\")\n    print(\"-\" * 50)\n\n    for size in sizes:\n        # Genesis test\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        genesis.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis.cuda.synchronize()\n        genesis_time = (time.time() - start) / 10\n\n        # PyTorch test\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch.cuda.synchronize() \n        pytorch_time = (time.time() - start) / 10\n\n        ratio = genesis_time / pytorch_time\n        print(f\"{size}x{size}: Genesis {genesis_time:.4f}s, PyTorch {pytorch_time:.4f}s, ratio {ratio:.2f}\")\n\nbenchmark_cuda_tensor()\n</code></pre>"},{"location":"core-components/cuda-storage/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"core-components/cuda-storage/#1-memory-management-best-practices","title":"1. Memory Management Best Practices","text":"Python<pre><code># \u2705 Good practices\ndef good_memory_practice():\n    with genesis.cuda.device(0):  # Explicitly specify device\n        x = genesis.randn(1000, 1000, device='cuda')\n\n        # Use in-place operations\n        x += 1.0\n        x *= 0.5\n\n        # Release large tensors promptly\n        del x\n        genesis.cuda.empty_cache()\n\n# \u274c Practices to avoid  \ndef bad_memory_practice():\n    tensors = []\n    for i in range(100):\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = x + 1.0  # Create additional copy\n        tensors.append(y)  # Keep all references, memory cannot be freed\n    # Memory will be exhausted quickly\n</code></pre>"},{"location":"core-components/cuda-storage/#2-performance-optimization-best-practices","title":"2. Performance Optimization Best Practices","text":"Python<pre><code># \u2705 Vectorized operations\ndef vectorized_operations():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Use vectorized functions\n    result = genesis.relu(x).sum(dim=1).mean()\n\n# \u274c Avoid loops\ndef avoid_loops():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Avoid Python loops\n    result = 0\n    for i in range(1000):\n        result += x[i].sum()  # Launches CUDA kernel each time\n</code></pre>"},{"location":"core-components/cuda-storage/#3-debugging-best-practices","title":"3. Debugging Best Practices","text":"Python<pre><code># Enable CUDA error checking\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# Use assertions to check tensor properties\ndef safe_tensor_operation(x, y):\n    assert x.device == y.device, \"Tensors must be on the same device\"\n    assert x.shape == y.shape, f\"Shape mismatch: {x.shape} vs {y.shape}\"\n\n    return x + y\n</code></pre>"},{"location":"core-components/cuda-storage/#common-issues","title":"\u2753 Common Issues","text":""},{"location":"core-components/cuda-storage/#q-what-to-do-when-cuda-memory-is-insufficient","title":"Q: What to do when CUDA memory is insufficient?","text":"<p>A:  Python<pre><code># Reduce batch size\nbatch_size = 32  # Change to 16 or 8\n\n# Use gradient accumulation\naccumulation_steps = 4\neffective_batch_size = batch_size * accumulation_steps\n\n# Enable mixed precision\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')\n\n# Regularly clean memory\ngenesis.cuda.empty_cache()\n</code></pre></p>"},{"location":"core-components/cuda-storage/#q-why-are-cuda-operations-slow","title":"Q: Why are CUDA operations slow?","text":"<p>A: Check the following points: Python<pre><code># 1. Ensure tensors are on GPU\nassert x.device.type == 'cuda'\n\n# 2. Avoid frequent CPU-GPU transfers\n# Wrong approach\nfor i in range(1000):\n    cpu_data = x.cpu().numpy()  # Transfer each time\n\n# Correct approach\ncpu_data = x.cpu().numpy()  # Transfer only once\n\n# 3. Use appropriate data types\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')  # Faster\n</code></pre></p>"},{"location":"core-components/cuda-storage/#q-how-to-debug-cuda-kernel-errors","title":"Q: How to debug CUDA kernel errors?","text":"<p>A: Python<pre><code># 1. Enable synchronous error checking\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# 2. Check tensor validity\ndef check_tensor(tensor, name):\n    assert not torch.isnan(tensor).any(), f\"{name} contains NaN\"\n    assert not torch.isinf(tensor).any(), f\"{name} contains Inf\"\n    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n\n# 3. Use CUDA debugging tools\n# cuda-memcheck python your_script.py\n# compute-sanitizer python your_script.py\n</code></pre></p> <p>Performance Tip</p> <p>CUDA tensor performance largely depends on memory access patterns and the use of batch operations. Prioritize vectorized operations and reasonable memory layout.</p> <p>Ready to learn more?</p> <p>Next: Tensor Operations Guide Back to Core Components</p>"},{"location":"core-components/device/","title":"Device Abstraction","text":"<p>Genesis provides a unified device abstraction that allows seamless operation across different hardware backends while maintaining optimal performance.</p>"},{"location":"core-components/device/#overview","title":"\ud83d\udccb Overview","text":"<p>The device system in Genesis v2.0 provides: - Unified device interface across CPU and GPU - Automatic device inference and management - Transparent memory management - Optimal performance for each device type</p>"},{"location":"core-components/device/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"Device API\"\n        A[genesis.device] --&gt; B[Device Class]\n        C[genesis.cuda] --&gt; D[CUDA Device]\n        E[genesis.cpu] --&gt; F[CPU Device]\n    end\n\n    subgraph \"Device Management\"\n        B --&gt; G[Device Properties]\n        B --&gt; H[Memory Management]\n        B --&gt; I[Context Switching]\n    end\n\n    subgraph \"Backend Integration\"\n        D --&gt; J[backends/cuda.py]\n        F --&gt; K[backends/cpu.py]\n        J --&gt; L[CUDA Memory Pool]\n        K --&gt; M[CPU Memory]\n    end\n\n    style B fill:#e1f5fe\n    style G fill:#f3e5f5\n    style H fill:#e8f5e8</code></pre>"},{"location":"core-components/device/#core-components","title":"\ud83c\udfaf Core Components","text":""},{"location":"core-components/device/#device-class","title":"Device Class","text":"<p>The central Device class provides a uniform interface:</p> Python<pre><code>class Device:\n    \"\"\"Unified device abstraction.\"\"\"\n\n    def __init__(self, device_type, device_id=None):\n        self.type = device_type  # 'cpu' or 'cuda'\n        self.id = device_id or 0\n        self._properties = None\n\n    @property\n    def is_cuda(self):\n        \"\"\"Check if device is CUDA.\"\"\"\n        return self.type == 'cuda'\n\n    @property\n    def is_cpu(self):\n        \"\"\"Check if device is CPU.\"\"\"\n        return self.type == 'cpu'\n\n    def __str__(self):\n        if self.type == 'cuda':\n            return f\"cuda:{self.id}\"\n        return self.type\n</code></pre>"},{"location":"core-components/device/#device-creation","title":"Device Creation","text":"<p>Multiple ways to create device objects:</p> Python<pre><code># String specification\ndevice = genesis.device(\"cuda:0\")\ndevice = genesis.device(\"cpu\")\n\n# From existing tensor\ndevice = tensor.device\n\n# Default device\ndevice = genesis.get_default_device()\n\n# Automatic device selection\ndevice = genesis.device(\"auto\")  # Selects CUDA if available, else CPU\n</code></pre>"},{"location":"core-components/device/#device-operations","title":"\ud83d\udcbb Device Operations","text":""},{"location":"core-components/device/#device-context-management","title":"Device Context Management","text":"Python<pre><code># Temporary device context\nwith genesis.device(\"cuda:1\"):\n    x = genesis.randn(3, 4)  # Created on cuda:1\n    y = genesis.zeros(3, 4)  # Also on cuda:1\n\n# Device-specific operations\ndevice = genesis.device(\"cuda:0\")\nwith device:\n    # All operations use this device\n    model = MyModel()\n    optimizer = genesis.optim.Adam(model.parameters())\n</code></pre>"},{"location":"core-components/device/#cross-device-operations","title":"Cross-Device Operations","text":"Python<pre><code># Automatic device handling\ncpu_tensor = genesis.tensor([1, 2, 3], device=\"cpu\")\ngpu_tensor = genesis.tensor([4, 5, 6], device=\"cuda\")\n\n# Automatic device promotion (moves to GPU)\nresult = cpu_tensor + gpu_tensor  # Result on cuda device\n\n# Explicit device transfer\ngpu_result = cpu_tensor.to(\"cuda\")\ncpu_result = gpu_tensor.to(\"cpu\")\n</code></pre>"},{"location":"core-components/device/#device-properties","title":"Device Properties","text":"Python<pre><code>device = genesis.device(\"cuda:0\")\n\n# Basic properties\nprint(f\"Device type: {device.type}\")\nprint(f\"Device ID: {device.id}\")\nprint(f\"Is CUDA: {device.is_cuda}\")\n\n# CUDA-specific properties\nif device.is_cuda:\n    print(f\"Device name: {device.name}\")\n    print(f\"Compute capability: {device.compute_capability}\")\n    print(f\"Total memory: {device.total_memory}\")\n    print(f\"Multi-processor count: {device.multi_processor_count}\")\n</code></pre>"},{"location":"core-components/device/#cuda-device-features","title":"\ud83d\ude80 CUDA Device Features","text":""},{"location":"core-components/device/#multi-gpu-support","title":"Multi-GPU Support","text":"Python<pre><code># Check available GPUs\nnum_gpus = genesis.cuda.device_count()\nprint(f\"Available GPUs: {num_gpus}\")\n\n# Use specific GPU\ndevice = genesis.device(\"cuda:1\")\ntensor = genesis.randn(1000, 1000, device=device)\n\n# Multi-GPU computation\ndevices = [genesis.device(f\"cuda:{i}\") for i in range(num_gpus)]\ntensors = [genesis.randn(100, 100, device=dev) for dev in devices]\n</code></pre>"},{"location":"core-components/device/#cuda-memory-management","title":"CUDA Memory Management","text":"Python<pre><code>device = genesis.device(\"cuda:0\")\n\n# Memory information\nprint(f\"Free memory: {device.memory_free()}\")\nprint(f\"Used memory: {device.memory_used()}\")\nprint(f\"Total memory: {device.memory_total()}\")\n\n# Memory operations\ngenesis.cuda.empty_cache()  # Clear unused cache\ngenesis.cuda.synchronize()  # Wait for operations to complete\n\n# Memory statistics\nstats = genesis.cuda.memory_stats()\nprint(f\"Peak allocated: {stats['peak_allocated']}\")\n</code></pre>"},{"location":"core-components/device/#cuda-streams-and-events","title":"CUDA Streams and Events","text":"Python<pre><code># CUDA stream management\nstream = genesis.cuda.Stream()\n\nwith genesis.cuda.stream(stream):\n    x = genesis.randn(1000, 1000, device=\"cuda\")\n    y = genesis.matmul(x, x)\n\n# Synchronization\nstream.synchronize()\n\n# CUDA events for timing\nstart_event = genesis.cuda.Event(enable_timing=True)\nend_event = genesis.cuda.Event(enable_timing=True)\n\nstart_event.record()\n# ... operations ...\nend_event.record()\ngenesis.cuda.synchronize()\n\nelapsed_time = start_event.elapsed_time(end_event)\nprint(f\"Elapsed time: {elapsed_time:.2f} ms\")\n</code></pre>"},{"location":"core-components/device/#cpu-device-features","title":"\ud83d\udcbe CPU Device Features","text":""},{"location":"core-components/device/#cpu-configuration","title":"CPU Configuration","text":"Python<pre><code># CPU-specific settings\ngenesis.cpu.set_num_threads(8)\nprint(f\"CPU threads: {genesis.cpu.get_num_threads()}\")\n\n# Enable/disable optimizations\ngenesis.cpu.set_optimization_level('O2')\ngenesis.cpu.enable_mkl(True)\n</code></pre>"},{"location":"core-components/device/#memory-management","title":"Memory Management","text":"Python<pre><code># CPU memory operations\ndevice = genesis.device(\"cpu\")\n\n# Pinned memory for faster GPU transfers\ntensor = genesis.empty((1000, 1000), device=device, pin_memory=True)\nprint(f\"Is pinned: {tensor.is_pinned()}\")\n\n# Memory mapping for large datasets\nmapped_tensor = genesis.from_file(\"large_dataset.dat\", device=\"cpu\", mmap=True)\n</code></pre>"},{"location":"core-components/device/#device-configuration","title":"\ud83d\udd27 Device Configuration","text":""},{"location":"core-components/device/#default-device-management","title":"Default Device Management","text":"Python<pre><code># Set global default device\ngenesis.set_default_device(\"cuda:0\")\n\n# Get current default\ndevice = genesis.get_default_device()\nprint(f\"Default device: {device}\")\n\n# Context-specific defaults\nwith genesis.default_device(\"cpu\"):\n    x = genesis.randn(3, 4)  # Created on CPU\n    print(f\"Device: {x.device}\")  # cpu\n\n# Reset to system default\ngenesis.reset_default_device()\n</code></pre>"},{"location":"core-components/device/#environment-variables","title":"Environment Variables","text":"Python<pre><code>import os\n\n# Set device via environment\nos.environ['GENESIS_DEFAULT_DEVICE'] = 'cuda:1'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n\n# Device selection priority:\n# 1. Explicit device parameter\n# 2. Current device context\n# 3. Environment variable\n# 4. System default\n</code></pre>"},{"location":"core-components/device/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"core-components/device/#device-specific-optimization","title":"Device-Specific Optimization","text":"Python<pre><code>def optimize_for_device(tensor):\n    \"\"\"Apply device-specific optimizations.\"\"\"\n    if tensor.device.is_cuda:\n        # CUDA optimizations\n        tensor = tensor.contiguous()  # Ensure memory layout\n        if tensor.numel() &gt; 10000:\n            tensor = tensor.half()    # Use half precision for large tensors\n    else:\n        # CPU optimizations\n        tensor = tensor.float()       # Use float32 for CPU\n\n    return tensor\n\n# Usage\noptimized_tensor = optimize_for_device(my_tensor)\n</code></pre>"},{"location":"core-components/device/#memory-transfer-optimization","title":"Memory Transfer Optimization","text":"Python<pre><code>def efficient_transfer(tensor, target_device):\n    \"\"\"Efficiently transfer tensor to target device.\"\"\"\n    if tensor.device == target_device:\n        return tensor  # No transfer needed\n\n    # Use pinned memory for CPU-&gt;GPU transfer\n    if tensor.device.is_cpu and target_device.is_cuda:\n        if not tensor.is_pinned():\n            tensor = tensor.pin_memory()\n\n    # Async transfer with streams\n    if target_device.is_cuda:\n        with genesis.cuda.stream(genesis.cuda.Stream()):\n            return tensor.to(target_device, non_blocking=True)\n\n    return tensor.to(target_device)\n</code></pre>"},{"location":"core-components/device/#device-detection-and-capability","title":"\ud83d\udd0d Device Detection and Capability","text":""},{"location":"core-components/device/#hardware-detection","title":"Hardware Detection","text":"Python<pre><code>def detect_hardware():\n    \"\"\"Detect available hardware and capabilities.\"\"\"\n    info = {\n        'cpu_count': genesis.cpu.logical_cpu_count(),\n        'cpu_features': genesis.cpu.supported_features(),\n        'cuda_available': genesis.cuda.is_available(),\n        'cuda_version': genesis.cuda.version() if genesis.cuda.is_available() else None,\n        'gpu_count': genesis.cuda.device_count() if genesis.cuda.is_available() else 0,\n    }\n\n    if info['cuda_available']:\n        info['gpus'] = []\n        for i in range(info['gpu_count']):\n            gpu_info = genesis.cuda.get_device_properties(i)\n            info['gpus'].append({\n                'name': gpu_info.name,\n                'memory': gpu_info.total_memory,\n                'compute_capability': gpu_info.compute_capability,\n            })\n\n    return info\n\n# Usage\nhw_info = detect_hardware()\nprint(f\"Hardware info: {hw_info}\")\n</code></pre>"},{"location":"core-components/device/#capability-based-selection","title":"Capability-Based Selection","text":"Python<pre><code>def select_optimal_device(min_memory_gb=1.0, compute_capability=None):\n    \"\"\"Select optimal device based on requirements.\"\"\"\n    if not genesis.cuda.is_available():\n        return genesis.device(\"cpu\")\n\n    for i in range(genesis.cuda.device_count()):\n        device = genesis.device(f\"cuda:{i}\")\n        props = genesis.cuda.get_device_properties(i)\n\n        # Check memory requirement\n        if props.total_memory &lt; min_memory_gb * 1e9:\n            continue\n\n        # Check compute capability\n        if compute_capability and props.compute_capability &lt; compute_capability:\n            continue\n\n        return device\n\n    # Fallback to CPU if no suitable GPU\n    return genesis.device(\"cpu\")\n\n# Usage\ndevice = select_optimal_device(min_memory_gb=4.0, compute_capability=7.0)\nprint(f\"Selected device: {device}\")\n</code></pre>"},{"location":"core-components/device/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Backend System - Backend implementation details</li> <li>Memory Management - Advanced memory management</li> <li>Performance Guide - Performance optimization</li> <li>CUDA Backend - CUDA-specific features</li> </ul>"},{"location":"core-components/dtypes/","title":"Data Type System","text":"<p>Genesis implements a unified data type system that provides PyTorch-aligned type management, supporting mixed precision training and cross-device type conversion.</p>"},{"location":"core-components/dtypes/#design-goals","title":"\ud83c\udfaf Design Goals","text":"<ul> <li>Unified Interface: CPU and GPU backends use the same type definitions</li> <li>PyTorch Compatibility: Maintain consistency with PyTorch's dtype system</li> <li>Mixed Precision: Seamless support for FP16, BF16 and other mixed precision training</li> <li>Type Safety: Compile-time and runtime type checking</li> </ul>"},{"location":"core-components/dtypes/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":"<pre><code>graph TB\n    subgraph \"DType Core Class\"\n        A[DType] --&gt; B[name str]\n        A --&gt; C[itemsize int]\n        A --&gt; D[numpy_dtype]\n        A --&gt; E[triton_name str]\n        A --&gt; F[is_floating_point bool]\n    end\n\n    subgraph \"Predefined Types\"\n        G[Floating Types] --&gt; H[float32]\n        G --&gt; I[float16] \n        G --&gt; J[bfloat16]\n        G --&gt; K[float64]\n\n        L[Integer Types] --&gt; M[int32]\n        L --&gt; N[int64]\n        L --&gt; O[int16]\n        L --&gt; P[int8]\n        L --&gt; Q[uint8]\n\n        R[Boolean Type] --&gt; S[bool]\n    end\n\n    subgraph \"Type Conversion\"\n        T[get_dtype] --&gt; U[String Conversion]\n        T --&gt; V[NumPy Compatibility]\n        T --&gt; W[Type Inference]\n    end\n\n    A --&gt; G\n    A --&gt; L  \n    A --&gt; R\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style L fill:#fff3e0\n    style T fill:#fce4ec</code></pre>"},{"location":"core-components/dtypes/#dtype-class-details","title":"\ud83d\udcca DType Class Details","text":""},{"location":"core-components/dtypes/#class-definition","title":"Class Definition","text":"Python<pre><code>class DType:\n    \"\"\"Genesis data type, similar to torch.dtype\"\"\"\n\n    def __init__(self, name, itemsize, numpy_dtype, triton_name=None, is_floating_point=None):\n        self.name = name                    # Type name, e.g. \"float32\"\n        self.itemsize = itemsize           # Size in bytes\n        self.numpy_dtype = numpy_dtype     # Corresponding NumPy type\n        self.triton_name = triton_name or name  # Type name in Triton\n\n        # Auto-detect if floating point type\n        if is_floating_point is None:\n            self.is_floating_point = np.issubdtype(numpy_dtype, np.floating)\n        else:\n            self.is_floating_point = is_floating_point\n</code></pre>"},{"location":"core-components/dtypes/#core-methods","title":"Core Methods","text":""},{"location":"core-components/dtypes/#string-representation","title":"String Representation","text":"Python<pre><code>def __str__(self):\n    return f\"genesis.{self.name}\"\n\ndef __repr__(self):\n    return f\"genesis.{self.name}\"\n\n# Usage example\nprint(genesis.float32)  # Output: genesis.float32\n</code></pre>"},{"location":"core-components/dtypes/#equality-comparison","title":"Equality Comparison","text":"Python<pre><code>def __eq__(self, other):\n    if isinstance(other, DType):\n        return self.name == other.name\n    elif isinstance(other, str):\n        return self.name == other  # Backward compatibility for string comparison\n    return False\n\n# Usage examples\ngenesis.float32 == genesis.float32  # True\ngenesis.float32 == \"float32\"        # True (backward compatibility)\ngenesis.float32 == genesis.float16  # False\n</code></pre>"},{"location":"core-components/dtypes/#predefined-data-types","title":"\ud83d\udd22 Predefined Data Types","text":""},{"location":"core-components/dtypes/#floating-point-types","title":"Floating Point Types","text":"Type Bytes Precision Usage <code>float32</code> 4 Single Default floating type, balanced precision and performance <code>float16</code> 2 Half Mixed precision training, memory saving <code>float64</code> 8 Double High precision computation requirements <code>bfloat16</code> 2 Brain float Google TPU optimized, large dynamic range Python<pre><code># Floating point type definitions\nfloat32 = DType(\"float32\", 4, np.float32)\nfloat16 = DType(\"float16\", 2, np.float16)\nfloat64 = DType(\"float64\", 8, np.float64)\n\n# bfloat16 special handling - Triton supports but NumPy doesn't natively\nbfloat16 = DType(\"bfloat16\", 2, np.float32, \"bfloat16\", is_floating_point=True)\n</code></pre>"},{"location":"core-components/dtypes/#integer-types","title":"Integer Types","text":"Type Bytes Range Usage <code>int64</code> 8 -2^63 ~ 2^63-1 Default integer type <code>int32</code> 4 -2^31 ~ 2^31-1 Memory-optimized integer <code>int16</code> 2 -32,768 ~ 32,767 Small integer storage <code>int8</code> 1 -128 ~ 127 Quantized computation <code>uint8</code> 1 0 ~ 255 Image data Python<pre><code># Integer type definitions\nint32 = DType(\"int32\", 4, np.int32)\nint64 = DType(\"int64\", 8, np.int64)\nint16 = DType(\"int16\", 2, np.int16)\nint8 = DType(\"int8\", 1, np.int8)\nuint8 = DType(\"uint8\", 1, np.uint8)\n</code></pre>"},{"location":"core-components/dtypes/#boolean-type","title":"Boolean Type","text":"Python<pre><code># Boolean type\nbool = DType(\"bool\", 1, np.bool_, is_floating_point=False)\n</code></pre>"},{"location":"core-components/dtypes/#type-conversion-system","title":"\ud83d\udd04 Type Conversion System","text":""},{"location":"core-components/dtypes/#core-conversion-function","title":"Core Conversion Function","text":"Python<pre><code>def get_dtype(obj):\n    \"\"\"\n    Convert various type representations to Genesis DType objects\n\n    Supported input types:\n    - DType objects: Return directly\n    - Strings: \"float32\", \"int64\", etc.\n    - NumPy dtype: np.float32, np.int64, etc.\n    - NumPy types: np.float32, np.int64 classes, etc.\n    - None: Return default float32\n    \"\"\"\n    if obj is None:\n        return float32  # Default type\n    elif isinstance(obj, DType):\n        return obj\n    elif isinstance(obj, str):\n        return _name_to_dtype[obj]\n    elif isinstance(obj, np.dtype):\n        return _numpy_to_dtype[obj.type]\n    elif isinstance(obj, type) and issubclass(obj, np.generic):\n        return _numpy_to_dtype[obj]\n    else:\n        raise ValueError(f\"Cannot convert {type(obj)} to Genesis DType: {obj}\")\n</code></pre>"},{"location":"core-components/dtypes/#type-mapping-tables","title":"Type Mapping Tables","text":"Python<pre><code># Name to type mapping\n_name_to_dtype = {\n    \"float32\": float32,\n    \"float16\": float16,\n    \"float64\": float64,\n    \"bfloat16\": bfloat16,\n    \"int32\": int32,\n    \"int64\": int64,\n    \"int16\": int16,\n    \"int8\": int8,\n    \"uint8\": uint8,\n    \"bool\": bool,\n}\n\n# NumPy type to Genesis type mapping\n_numpy_to_dtype = {\n    np.float32: float32,\n    np.float16: float16,\n    np.float64: float64,\n    np.int32: int32,\n    np.int64: int64,\n    np.int16: int16,\n    np.int8: int8,\n    np.uint8: uint8,\n    np.bool_: bool,\n}\n</code></pre>"},{"location":"core-components/dtypes/#type-checking-tools","title":"\ud83e\uddee Type Checking Tools","text":""},{"location":"core-components/dtypes/#floating-point-type-check","title":"Floating Point Type Check","text":"Python<pre><code>def is_floating_point(dtype):\n    \"\"\"Check if it's a floating point type\"\"\"\n    dtype = get_dtype(dtype)\n    return dtype.is_floating_point\n\n# Usage examples\nis_floating_point(genesis.float32)  # True\nis_floating_point(genesis.int32)    # False\nis_floating_point(\"float16\")        # True\n</code></pre>"},{"location":"core-components/dtypes/#integer-type-check","title":"Integer Type Check","text":"Python<pre><code>def is_integer(dtype):\n    \"\"\"Check if it's an integer type\"\"\"\n    dtype = get_dtype(dtype)\n    return not dtype.is_floating_point and dtype != bool\n\n# Usage examples\nis_integer(genesis.int32)   # True\nis_integer(genesis.float32) # False\nis_integer(genesis.bool)    # False\n</code></pre>"},{"location":"core-components/dtypes/#type-classification","title":"Type Classification","text":"Python<pre><code># All supported types\nall_dtypes = [float32, float16, float64, bfloat16, int32, int64, int16, int8, uint8, bool]\n\n# Floating point type list\nfloating_dtypes = [dt for dt in all_dtypes if dt.is_floating_point]\n# [float32, float16, float64, bfloat16]\n\n# Integer type list\ninteger_dtypes = [dt for dt in all_dtypes if is_integer(dt)]\n# [int32, int64, int16, int8, uint8]\n</code></pre>"},{"location":"core-components/dtypes/#automatic-type-inference","title":"\ud83d\udd0d Automatic Type Inference","text":"<p>Genesis provides intelligent dtype inference that follows PyTorch conventions:</p>"},{"location":"core-components/dtypes/#infer_dtype_from_dataarray","title":"<code>infer_dtype_from_data(array)</code>","text":"<p>Automatically infers the appropriate Genesis dtype from input data:</p> Python<pre><code>from genesis.dtypes import infer_dtype_from_data\n\n# Python scalar inference\ninfer_dtype_from_data(42)        # \u2192 genesis.int64\ninfer_dtype_from_data(3.14)      # \u2192 genesis.float32\ninfer_dtype_from_data(True)      # \u2192 genesis.bool\n\n# List and array inference\ninfer_dtype_from_data([1, 2, 3])           # \u2192 genesis.int64\ninfer_dtype_from_data([1.0, 2.0, 3.0])     # \u2192 genesis.float32\ninfer_dtype_from_data(np.array([1, 2]))    # \u2192 preserves numpy dtype\n\n# Tensor inference  \nexisting_tensor = genesis.tensor([1, 2, 3])\ninfer_dtype_from_data(existing_tensor)     # \u2192 existing_tensor.dtype\n</code></pre>"},{"location":"core-components/dtypes/#inference-rules","title":"Inference Rules","text":"Input Type Inferred Genesis DType Notes Python <code>int</code> <code>genesis.int64</code> PyTorch default Python <code>float</code> <code>genesis.float32</code> PyTorch default Python <code>bool</code> <code>genesis.bool</code> Preserved <code>np.int32</code>, <code>np.int64</code>, etc. Corresponding int type Preserved <code>np.float16</code>, <code>np.float32</code> Corresponding float type Preserved <code>np.float64</code> <code>genesis.float32</code> \u26a0\ufe0f Converted for consistency <code>np.bool_</code> <code>genesis.bool</code> Preserved Genesis <code>Tensor</code> <code>tensor.dtype</code> Preserved Lists/tuples Inferred from first conversion to numpy Depends on content <p>Key Features: - PyTorch Compatibility: Follows PyTorch's default type inference rules - Performance Optimization: Automatically converts <code>float64</code> to <code>float32</code> to match PyTorch behavior - Type Preservation: Preserves integer and boolean types from numpy arrays - Consistent Behavior: Same inference logic used across the framework</p>"},{"location":"core-components/dtypes/#mixed-precision-support","title":"\ud83d\udd00 Mixed Precision Support","text":""},{"location":"core-components/dtypes/#automatic-type-conversion","title":"Automatic Type Conversion","text":"Python<pre><code>def _cast(value, dtype):\n    \"\"\"Automatic type conversion for mixed precision training\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# Application in autograd\nif genesis.enable_autocast:\n    result = cls.forward(ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/dtypes/#type-inference","title":"Type Inference","text":"Python<pre><code>def check_dtype(value, dtype):\n    \"\"\"Recursively check if data structure contains specified type\"\"\"\n    if isinstance(value, Tensor):\n        return value.dtype == dtype\n    elif isinstance(value, dict):\n        return any(check_dtype(k, dtype) or check_dtype(v, dtype) for k, v in value.items())\n    elif isinstance(value, (list, tuple)):\n        return any(check_dtype(v, dtype) for v in value)\n    else:\n        return False\n</code></pre>"},{"location":"core-components/dtypes/#usage-examples","title":"\ud83c\udfaf Usage Examples","text":""},{"location":"core-components/dtypes/#basic-type-operations","title":"Basic Type Operations","text":"Python<pre><code>import genesis\n\n# Create tensors with different types\nx_f32 = genesis.randn(3, 4, dtype=genesis.float32)\nx_f16 = genesis.randn(3, 4, dtype=genesis.float16)\nx_int = genesis.randint(0, 10, (3, 4), dtype=genesis.int32)\n\n# Check types\nprint(f\"x_f32 type: {x_f32.dtype}\")          # genesis.float32\nprint(f\"Is floating: {x_f32.dtype.is_floating_point}\")  # True\nprint(f\"Byte size: {x_f32.dtype.itemsize}\")          # 4\n</code></pre>"},{"location":"core-components/dtypes/#type-conversion","title":"Type Conversion","text":"Python<pre><code># String to type\ndtype1 = genesis.get_dtype(\"float16\")    # genesis.float16\ndtype2 = genesis.get_dtype(np.float32)   # genesis.float32\ndtype3 = genesis.get_dtype(None)         # genesis.float32 (default)\n\n# Tensor type conversion\nx = genesis.randn(3, 4, dtype=\"float32\")\nx_half = x.half()      # Convert to float16\nx_float = x.float()    # Convert to float32\n</code></pre>"},{"location":"core-components/dtypes/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable mixed precision\ngenesis.enable_autocast = True\n\n# Model will automatically convert between fp16 and fp32\nimport genesis.nn as nn\n\nmodel = nn.Linear(784, 128)\nx = genesis.randn(32, 784, dtype=genesis.float16)\n\n# Automatic type conversion handling during forward pass\noutput = model(x)\n</code></pre>"},{"location":"core-components/dtypes/#cross-device-type-consistency","title":"Cross-Device Type Consistency","text":"Python<pre><code># CPU and GPU use the same type system\ncpu_tensor = genesis.randn(3, 4, device=\"cpu\", dtype=genesis.float32)\ngpu_tensor = genesis.randn(3, 4, device=\"cuda\", dtype=genesis.float32)\n\nprint(cpu_tensor.dtype == gpu_tensor.dtype)  # True\nprint(cpu_tensor.dtype.name)                 # \"float32\"\nprint(gpu_tensor.dtype.name)                 # \"float32\"\n</code></pre>"},{"location":"core-components/dtypes/#bfloat16-special-handling","title":"bfloat16 Special Handling","text":"Python<pre><code># bfloat16 handling across different backends\nx_bf16 = genesis.randn(3, 4, dtype=genesis.bfloat16)\n\n# CPU backend: Uses float32 storage but marked as bfloat16\n# GPU backend: Native bfloat16 support (if hardware supports)\nprint(f\"Type name: {x_bf16.dtype.name}\")           # \"bfloat16\"\nprint(f\"Triton name: {x_bf16.dtype.triton_name}\")  # \"bfloat16\"\nprint(f\"NumPy type: {x_bf16.dtype.numpy_dtype}\") # &lt;class 'numpy.float32'&gt;\n</code></pre>"},{"location":"core-components/dtypes/#performance-optimization","title":"\ud83d\ude80 Performance Optimization","text":""},{"location":"core-components/dtypes/#type-conversion-optimization","title":"Type Conversion Optimization","text":"<ul> <li>Lazy Conversion: Type conversion only occurs when truly needed</li> <li>Caching Mechanism: Common type conversion results are cached</li> <li>Zero-Copy: Cross-device conversions of the same type attempt zero-copy when possible</li> </ul>"},{"location":"core-components/dtypes/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Compact Storage: Use appropriate data types to reduce memory usage</li> <li>Alignment Optimization: Data type alignment to improve access efficiency</li> <li>Batch Conversion: Batch processing of type conversions to improve efficiency</li> </ul> <p>Genesis's data type system provides unified, efficient, type-safe data representation for the entire framework, serving as the foundation for mixed precision training and cross-device computation.</p>"},{"location":"core-components/function/","title":"Function System","text":"<p>The Function system in Genesis provides the foundation for automatic differentiation by defining how operations are executed in the forward pass and how gradients are computed in the backward pass.</p>"},{"location":"core-components/function/#overview","title":"\ud83d\udccb Overview","text":"<p>The Function system is built around the <code>Function</code> base class, which encapsulates: - Forward computation logic - Backward gradient computation - Context management for storing intermediate values - Integration with the automatic differentiation engine</p>"},{"location":"core-components/function/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"Function System\"\n        A[Function Base Class] --&gt; B[apply() Method]\n        A --&gt; C[forward() Method]\n        A --&gt; D[backward() Method]\n        E[Context] --&gt; F[save_for_backward()]\n        E --&gt; G[saved_tensors]\n    end\n\n    subgraph \"Autograd Integration\"\n        B --&gt; H[Computation Graph]\n        H --&gt; I[Gradient Flow]\n        I --&gt; J[Backward Pass]\n    end\n\n    subgraph \"Built-in Functions\"\n        K[AddFunction] --&gt; A\n        L[MulFunction] --&gt; A\n        M[MatMulFunction] --&gt; A\n        N[ReluFunction] --&gt; A\n    end\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style H fill:#e8f5e8</code></pre>"},{"location":"core-components/function/#core-concepts","title":"\ud83c\udfaf Core Concepts","text":""},{"location":"core-components/function/#function-base-class","title":"Function Base Class","text":"<p>The <code>Function</code> class provides the interface for all operations:</p> Python<pre><code>class Function:\n    \"\"\"Base class for all automatic differentiation functions.\"\"\"\n\n    @staticmethod\n    def apply(*args):\n        \"\"\"Apply the function with automatic differentiation support.\"\"\"\n        ctx = Context()\n\n        # Forward pass\n        result = cls.forward(ctx, *args)\n\n        # Set up backward pass if any input requires gradients\n        if any(tensor.requires_grad for tensor in args if isinstance(tensor, Tensor)):\n            result.set_creator(ctx, cls.backward)\n\n        return result\n\n    @staticmethod\n    def forward(ctx, *args):\n        \"\"\"Compute forward pass. Must be implemented by subclasses.\"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        \"\"\"Compute backward pass. Must be implemented by subclasses.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"core-components/function/#context-management","title":"Context Management","text":"<p>The <code>Context</code> class manages information needed for backward computation:</p> Python<pre><code>class Context:\n    \"\"\"Context for storing information needed during backward pass.\"\"\"\n\n    def __init__(self):\n        self.saved_tensors = []\n        self.saved_variables = {}\n\n    def save_for_backward(self, *tensors):\n        \"\"\"Save tensors for use in backward pass.\"\"\"\n        self.saved_tensors.extend(tensors)\n\n    def save_variable(self, name, value):\n        \"\"\"Save a variable for use in backward pass.\"\"\"\n        self.saved_variables[name] = value\n</code></pre>"},{"location":"core-components/function/#implementation-examples","title":"\ud83d\udcbb Implementation Examples","text":""},{"location":"core-components/function/#basic-arithmetic-function","title":"Basic Arithmetic Function","text":"Python<pre><code>class AddFunction(Function):\n    \"\"\"Addition function with gradient support.\"\"\"\n\n    @staticmethod\n    def forward(ctx, a, b):\n        \"\"\"Forward pass: compute a + b.\"\"\"\n        # No need to save inputs for addition\n        return genesis.ops.add(a, b)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Backward pass: gradients flow unchanged.\"\"\"\n        return grad_output, grad_output\n\n# Usage\nadd = AddFunction.apply\nc = add(a, b)  # Equivalent to a + b with autograd support\n</code></pre>"},{"location":"core-components/function/#matrix-multiplication-function","title":"Matrix Multiplication Function","text":"Python<pre><code>class MatMulFunction(Function):\n    \"\"\"Matrix multiplication with gradient support.\"\"\"\n\n    @staticmethod\n    def forward(ctx, a, b):\n        \"\"\"Forward pass: compute a @ b.\"\"\"\n        ctx.save_for_backward(a, b)\n        return genesis.ops.matmul(a, b)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Backward pass: compute gradients using chain rule.\"\"\"\n        a, b = ctx.saved_tensors\n\n        grad_a = genesis.ops.matmul(grad_output, b.transpose(-2, -1))\n        grad_b = genesis.ops.matmul(a.transpose(-2, -1), grad_output)\n\n        return grad_a, grad_b\n\n# Usage\nmatmul = MatMulFunction.apply\nc = matmul(a, b)  # Equivalent to a @ b with autograd support\n</code></pre>"},{"location":"core-components/function/#activation-function-with-context","title":"Activation Function with Context","text":"Python<pre><code>class ReluFunction(Function):\n    \"\"\"ReLU activation with gradient support.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"Forward pass: compute max(0, input).\"\"\"\n        output = genesis.ops.maximum(input, 0)\n        ctx.save_for_backward(input)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Backward pass: gradient is 0 for negative inputs.\"\"\"\n        input, = ctx.saved_tensors\n        mask = input &gt; 0\n        return grad_output * mask\n\n# Usage\nrelu = ReluFunction.apply\nactivated = relu(x)\n</code></pre>"},{"location":"core-components/function/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":""},{"location":"core-components/function/#in-place-operations","title":"In-Place Operations","text":"Python<pre><code>class AddInplaceFunction(Function):\n    \"\"\"In-place addition function.\"\"\"\n\n    @staticmethod\n    def forward(ctx, a, b):\n        \"\"\"Forward pass: modify a in-place.\"\"\"\n        ctx.save_variable('original_a', a.clone())\n        a.add_(b)\n        return a\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Backward pass for in-place operation.\"\"\"\n        return grad_output, grad_output\n</code></pre>"},{"location":"core-components/function/#multi-output-functions","title":"Multi-Output Functions","text":"Python<pre><code>class SplitFunction(Function):\n    \"\"\"Function that returns multiple outputs.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input, split_sizes):\n        \"\"\"Split input tensor into multiple parts.\"\"\"\n        ctx.save_variable('split_sizes', split_sizes)\n        return genesis.ops.split(input, split_sizes)\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        \"\"\"Concatenate gradients from multiple outputs.\"\"\"\n        grad_input = genesis.ops.cat(grad_outputs, dim=0)\n        return grad_input, None  # None for split_sizes (no gradient)\n</code></pre>"},{"location":"core-components/function/#custom-context-variables","title":"Custom Context Variables","text":"Python<pre><code>class ScaleFunction(Function):\n    \"\"\"Scale tensor by a constant factor.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input, scale_factor):\n        \"\"\"Scale input by constant factor.\"\"\"\n        ctx.save_variable('scale_factor', scale_factor)\n        return input * scale_factor\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"Scale gradient by same factor.\"\"\"\n        scale_factor = ctx.saved_variables['scale_factor']\n        return grad_output * scale_factor, None\n</code></pre>"},{"location":"core-components/function/#integration-with-operations","title":"\ud83d\udd27 Integration with Operations","text":""},{"location":"core-components/function/#registering-functions-with-dispatcher","title":"Registering Functions with Dispatcher","text":"Python<pre><code># Register function with operation dispatcher\ngenesis.ops.register_function('add', AddFunction.apply)\ngenesis.ops.register_function('matmul', MatMulFunction.apply)\ngenesis.ops.register_function('relu', ReluFunction.apply)\n\n# Now operations automatically use the registered functions\nx = genesis.tensor([1, 2, 3], requires_grad=True)\ny = genesis.tensor([4, 5, 6], requires_grad=True)\nz = x + y  # Automatically uses AddFunction\n</code></pre>"},{"location":"core-components/function/#custom-operation-definition","title":"Custom Operation Definition","text":"Python<pre><code>def custom_operation(input, param):\n    \"\"\"Define custom operation using Function.\"\"\"\n    return CustomFunction.apply(input, param)\n\n# Register as operation\ngenesis.ops.register_operation('custom_op', custom_operation)\n\n# Use like any other operation\nresult = genesis.custom_op(tensor, param)\n</code></pre>"},{"location":"core-components/function/#performance-considerations","title":"\ud83d\udcca Performance Considerations","text":""},{"location":"core-components/function/#memory-efficiency","title":"Memory Efficiency","text":"Python<pre><code>class EfficientFunction(Function):\n    \"\"\"Memory-efficient function implementation.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        # Only save what's needed for backward\n        ctx.save_for_backward(input.detach())  # Detach to avoid recursive gradients\n\n        # Compute result efficiently\n        result = efficient_computation(input)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        # Compute gradient efficiently\n        return efficient_gradient_computation(input, grad_output)\n</code></pre>"},{"location":"core-components/function/#numerical-stability","title":"Numerical Stability","text":"Python<pre><code>class StableFunction(Function):\n    \"\"\"Numerically stable function implementation.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        # Use numerically stable computation\n        output = stable_computation(input)\n        ctx.save_for_backward(input, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, output = ctx.saved_tensors\n        # Use stable gradient computation\n        return stable_gradient(input, output, grad_output)\n</code></pre>"},{"location":"core-components/function/#debugging-and-testing","title":"\ud83d\udd0d Debugging and Testing","text":""},{"location":"core-components/function/#function-testing","title":"Function Testing","text":"Python<pre><code>def test_function_gradients():\n    \"\"\"Test function gradient computation.\"\"\"\n    x = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)\n\n    # Test forward pass\n    y = CustomFunction.apply(x)\n\n    # Test backward pass\n    y.backward(genesis.tensor([1.0, 1.0, 1.0]))\n\n    # Check gradients\n    assert x.grad is not None\n    print(f\"Gradient: {x.grad}\")\n\n# Numerical gradient checking\ndef numerical_gradient_check(func, input, eps=1e-5):\n    \"\"\"Check gradients using numerical differentiation.\"\"\"\n    # Implementation of numerical gradient checking\n    pass\n</code></pre>"},{"location":"core-components/function/#debugging-context","title":"Debugging Context","text":"Python<pre><code>class DebugFunction(Function):\n    \"\"\"Function with debugging information.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        print(f\"Forward: input shape = {input.shape}\")\n        ctx.save_for_backward(input)\n        result = computation(input)\n        print(f\"Forward: output shape = {result.shape}\")\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        print(f\"Backward: grad_output shape = {grad_output.shape}\")\n        input, = ctx.saved_tensors\n        grad_input = gradient_computation(input, grad_output)\n        print(f\"Backward: grad_input shape = {grad_input.shape}\")\n        return grad_input\n</code></pre>"},{"location":"core-components/function/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Tensor System - Tensor class and autograd integration</li> <li>Core Components Overview - Overall system architecture</li> <li>Automatic Differentiation - Detailed autograd system</li> <li>Custom Operations Guide - Creating custom operations</li> </ul>"},{"location":"core-components/storage/","title":"Storage Layer","text":"<p>Genesis's storage layer provides an abstraction interface for managing tensor data storage across different devices.</p>"},{"location":"core-components/storage/#overview","title":"\ud83d\udccb Overview","text":"<p>The storage layer is a key component of Genesis v2.0 architecture, providing: - Abstraction of device-specific storage implementations - Memory lifecycle management - Efficient data transfer between devices - Support for various data types and memory layouts</p>"},{"location":"core-components/storage/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"Storage Abstraction\"\n        A[Storage Interface] --&gt; B[create_storage()]\n        A --&gt; C[storage.to()]\n        A --&gt; D[storage.copy_()]\n    end\n\n    subgraph \"Device Implementations\"\n        E[CPUStorage] --&gt; A\n        F[CUDAStorage] --&gt; A\n        G[FutureStorage] --&gt; A\n    end\n\n    subgraph \"Memory Management\"\n        H[Memory Allocation] --&gt; I[Memory Pool]\n        H --&gt; J[Reference Counting]\n        H --&gt; K[Garbage Collection]\n    end\n\n    style A fill:#e1f5fe\n    style E fill:#e8f5e9\n    style F fill:#ffeb3b</code></pre>"},{"location":"core-components/storage/#core-concepts","title":"\ud83c\udfaf Core Concepts","text":""},{"location":"core-components/storage/#storage-interface","title":"Storage Interface","text":"<p>The base interface for all storage implementations:</p> Python<pre><code>class Storage:\n    \"\"\"Abstract interface for tensor data storage.\"\"\"\n\n    def __init__(self, shape, dtype, device):\n        self.shape = shape\n        self.dtype = dtype\n        self.device = device\n        self._data = None\n\n    def to(self, device):\n        \"\"\"Transfer storage to another device.\"\"\"\n        raise NotImplementedError\n\n    def copy_(self, other):\n        \"\"\"In-place copy from another storage.\"\"\"\n        raise NotImplementedError\n\n    def clone(self):\n        \"\"\"Create a deep copy of storage.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def data_ptr(self):\n        \"\"\"Get underlying data pointer.\"\"\"\n        return self._data\n</code></pre>"},{"location":"core-components/storage/#storage-creation","title":"Storage Creation","text":"<p>Creating storage appropriate for the device:</p> Python<pre><code>def create_storage(data, device, dtype=None):\n    \"\"\"Create storage for given device.\"\"\"\n    if device.type == 'cpu':\n        return CPUStorage(data, dtype)\n    elif device.type == 'cuda':\n        return CUDAStorage(data, dtype)\n    else:\n        raise ValueError(f\"Unsupported device type: {device.type}\")\n\n# Usage\nstorage = create_storage([1, 2, 3], genesis.device(\"cuda\"))\n</code></pre>"},{"location":"core-components/storage/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Device Abstraction - Device management system</li> <li>Memory Management - Advanced memory management</li> <li>Backend System - Backend implementations</li> <li>Tensor System - Tensor class and storage integration</li> </ul>"},{"location":"core-components/tensor/","title":"Tensor Operations","text":"<p>Core tensor operations and data structures in Genesis.</p>"},{"location":"core-components/tensor/#overview","title":"Overview","text":"<p>Genesis tensors provide the fundamental data structure for all computations, similar to PyTorch tensors but optimized for our dual backend architecture.</p>"},{"location":"core-components/tensor/#creating-tensors","title":"Creating Tensors","text":"Python<pre><code>import genesis\n\n# Create tensors\nx = genesis.tensor([1, 2, 3, 4])\ny = genesis.zeros(3, 4)\nz = genesis.randn(2, 3, device='cuda')\n</code></pre>"},{"location":"core-components/tensor/#tensor-operations_1","title":"Tensor Operations","text":"Python<pre><code># Basic operations\nresult = x + y\nresult = genesis.matmul(x, y)\nresult = x.sum(dim=1)\n</code></pre>"},{"location":"core-components/tensor/#device-management","title":"Device Management","text":"Python<pre><code># Move tensors between devices\ncpu_tensor = gpu_tensor.cpu()\ngpu_tensor = cpu_tensor.cuda()\ngpu_tensor = cpu_tensor.to('cuda')\n</code></pre> <p>This documentation is under construction. More detailed tensor API documentation will be added.</p>"},{"location":"core-components/tensor/#see-also","title":"See Also","text":"<ul> <li>Autograd - Automatic differentiation with tensors</li> <li>CUDA Storage - GPU memory management</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to the Genesis deep learning framework! This guide will help you start using Genesis in just a few minutes.</p>"},{"location":"getting-started/#overview","title":"\ud83c\udfaf Overview","text":"<p>Genesis is a lightweight deep learning framework designed specifically for learning and research. It provides:</p> <ul> <li>Simple and intuitive API design</li> <li>High-performance GPU-accelerated computing</li> <li>Complete neural network training capabilities</li> <li>Good compatibility with PyTorch ecosystem</li> </ul>"},{"location":"getting-started/#5-minute-quick-start","title":"\u26a1 5-Minute Quick Start","text":""},{"location":"getting-started/#1-install-genesis","title":"1. Install Genesis","text":"Bash<pre><code># Install core dependencies\npip install torch triton\n\n# Clone source code\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# Install Genesis\npip install -e .\n</code></pre>"},{"location":"getting-started/#2-your-first-neural-network","title":"2. Your First Neural Network","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Define a simple multi-layer perceptron\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        return self.layer2(x)\n\n# Create model and data\nmodel = MLP(784, 128, 10)\nx = genesis.randn(32, 784)  # batch size 32, input dimension 784\n\n# Forward pass\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # torch.Size([32, 10])\n</code></pre>"},{"location":"getting-started/#3-training-loop","title":"3. Training Loop","text":"Python<pre><code>import genesis.optim as optim\n\n# Create optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Simulate training data\ntargets = genesis.randint(0, 10, (32,))\n\n# Train one batch\noptimizer.zero_grad()        # Zero gradients\noutput = model(x)           # Forward pass\nloss = criterion(output, targets)  # Compute loss\nloss.backward()             # Backward pass\noptimizer.step()            # Update parameters\n\nprint(f\"Loss value: {loss.item():.4f}\")\n</code></pre>"},{"location":"getting-started/#core-concepts","title":"\ud83d\udcda Core Concepts","text":""},{"location":"getting-started/#tensor","title":"Tensor","text":"<p>The fundamental data structure in Genesis, supporting automatic differentiation:</p> Python<pre><code>import genesis\n\n# Create tensors (automatic dtype inference)\nx = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)  # \u2192 float32\ny = genesis.tensor([4, 5, 6])                           # \u2192 int64\nz = genesis.tensor([1, 2, 3], dtype=genesis.float32)     # explicit dtype\n\n# Basic operations\nresult = x * y.float() + x.sum()  # Broadcasting and type conversion\n\n# PyTorch-style reduction operations\ntotal = x.sum()                      # Sum all elements\nmean_val = x.mean()                  # Mean of all elements\nmax_val = x.max()                    # Maximum element\n\n# Dimension-specific operations\ndata = genesis.tensor([[1, 2, 3], [4, 5, 6]])\nrow_sums = data.sum(dim=1)                    # Sum along rows\ncol_means = data.mean(dim=0, keepdim=True)   # Mean along columns, keep dims\n\n# NumPy-style also supported for compatibility\nnumpy_style = data.sum(axis=0, keepdims=True)\n\n# Compute gradients\nresult.backward()\nprint(f\"x gradients: {x.grad}\")  # Gradients w.r.t. x\n</code></pre>"},{"location":"getting-started/#module","title":"Module","text":"<p>Base class for neural network components:</p> Python<pre><code>import genesis.nn as nn\n\nclass CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = genesis.randn(out_features, in_features, requires_grad=True)\n        self.bias = genesis.zeros(out_features, requires_grad=True)\n\n    def forward(self, x):\n        return genesis.functional.linear(x, self.weight, self.bias)\n\n# Use custom layer\nlayer = CustomLayer(10, 5)\ninput_tensor = genesis.randn(3, 10)\noutput = layer(input_tensor)\n</code></pre>"},{"location":"getting-started/#optimizer","title":"Optimizer","text":"<p>Parameter update algorithms:</p> Python<pre><code>import genesis.optim as optim\n\n# Different optimizer choices\nsgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nadam_optimizer = optim.Adam(model.parameters(), lr=0.001)\nadamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n</code></pre>"},{"location":"getting-started/#environment-setup","title":"\ud83d\udee0\ufe0f Environment Setup","text":""},{"location":"getting-started/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: Modern multi-core processor</li> <li>Memory: Minimum 8GB RAM, 16GB+ recommended</li> <li>GPU: NVIDIA GPU with CUDA support (recommended)</li> <li>Storage: At least 2GB available space</li> </ul>"},{"location":"getting-started/#software-dependencies","title":"Software Dependencies","text":"Bash<pre><code># Python environment\nPython &gt;= 3.8\n\n# Core dependencies\ntorch &gt;= 2.0.0\ntriton &gt;= 2.0.0\nnumpy &gt;= 1.21.0\ncuda-python &gt;= 11.8.0  # GPU support\n\n# Optional dependencies\nmatplotlib &gt;= 3.5.0  # For visualization\ntqdm &gt;= 4.64.0      # Progress bars\nwandb &gt;= 0.13.0     # Experiment tracking\n</code></pre>"},{"location":"getting-started/#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<p>Now that you understand the basics of Genesis, you can continue exploring:</p>"},{"location":"getting-started/#deep-learning","title":"\ud83c\udf93 Deep Learning","text":"<ul> <li>Complete Installation Guide - Detailed installation and configuration steps</li> <li>First Complete Program - Build a complete training workflow</li> <li>Basic Training Tutorial - Systematic training tutorials</li> </ul>"},{"location":"getting-started/#architecture-understanding","title":"\ud83d\udd0d Architecture Understanding","text":"<ul> <li>Architecture Overview - Understand Genesis's overall design</li> <li>Core Components - Deep dive into internal implementation</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":"<ul> <li>Custom Operators - Implement custom operations</li> <li>Performance Optimization - Training performance tuning</li> <li>Distributed Training - Multi-GPU training</li> </ul>"},{"location":"getting-started/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":""},{"location":"getting-started/#q-whats-the-difference-between-genesis-and-pytorch","title":"Q: What's the difference between Genesis and PyTorch?","text":"<p>A: Genesis is education-oriented with cleaner, more understandable code, suitable for learning deep learning internals. PyTorch is better suited for production environments.</p>"},{"location":"getting-started/#q-can-genesis-be-used-in-production","title":"Q: Can Genesis be used in production?","text":"<p>A: Genesis is primarily for education and research. While fully functional, we recommend more mature frameworks like PyTorch for production use.</p>"},{"location":"getting-started/#q-how-to-get-help","title":"Q: How to get help?","text":"<p>A: You can get help through GitHub Issues, Discussions, or by consulting the detailed documentation.</p>"},{"location":"getting-started/#ready","title":"\ud83c\udf89 Ready?","text":"<p>Let's start diving deep into Genesis!</p> <p>Detailed Installation Guide Complete Tutorials</p>"},{"location":"getting-started/first-steps/","title":"Your First Complete Program","text":"<p>After completing the installation, let's learn Genesis basics through a complete example. We will implement an image classifier to demonstrate the complete deep learning workflow.</p>"},{"location":"getting-started/first-steps/#project-goals","title":"\ud83c\udfaf Project Goals","text":"<p>Build a handwritten digit recognizer (MNIST-like) to learn Genesis core concepts:</p> <ul> <li>Data loading and preprocessing</li> <li>Model definition and initialization</li> <li>Training loop and validation</li> <li>Model saving and loading</li> </ul>"},{"location":"getting-started/first-steps/#project-structure","title":"\ud83d\udcca Project Structure","text":"<p>Create the project directory structure:</p> Text Only<pre><code>first_project/\n\u251c\u2500\u2500 data/                # Data directory\n\u251c\u2500\u2500 models/              # Model save directory\n\u251c\u2500\u2500 train.py            # Training script\n\u251c\u2500\u2500 model.py            # Model definition\n\u251c\u2500\u2500 dataset.py          # Data loading\n\u2514\u2500\u2500 utils.py            # Utility functions\n</code></pre>"},{"location":"getting-started/first-steps/#1-data-processing-datasetpy","title":"\ud83d\udcc1 1. Data Processing (<code>dataset.py</code>)","text":"Python<pre><code>\"\"\"Data loading and preprocessing module\"\"\"\nimport genesis\nimport numpy as np\nfrom typing import Tuple, List\nimport pickle\nimport os\n\nclass SimpleDataset:\n    \"\"\"Simple dataset class\"\"\"\n\n    def __init__(self, data: np.ndarray, labels: np.ndarray, transform=None):\n        \"\"\"\n        Initialize the dataset\n\n        Args:\n            data: Input data (N, H, W) or (N, D)\n            labels: Labels (N,)\n            transform: Data transformation function\n        \"\"\"\n        self.data = data.astype(np.float32)\n        self.labels = labels.astype(np.int64)\n        self.transform = transform\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[genesis.Tensor, genesis.Tensor]:\n        \"\"\"Get a single sample\"\"\"\n        x = self.data[idx]\n        y = self.labels[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return genesis.tensor(x), genesis.tensor(y)\n\nclass DataLoader:\n    \"\"\"Simple data loader\"\"\"\n\n    def __init__(self, dataset: SimpleDataset, batch_size: int = 32, shuffle: bool = True):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self._reset_indices()\n\n    def _reset_indices(self):\n        \"\"\"Reset indices\"\"\"\n        self.indices = np.arange(len(self.dataset))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.current = 0\n\n    def __iter__(self):\n        self._reset_indices()\n        return self\n\n    def __next__(self):\n        if self.current &gt;= len(self.dataset):\n            raise StopIteration\n\n        # Get the current batch indices\n        end_idx = min(self.current + self.batch_size, len(self.dataset))\n        batch_indices = self.indices[self.current:end_idx]\n\n        # Collect batch data\n        batch_data = []\n        batch_labels = []\n\n        for idx in batch_indices:\n            x, y = self.dataset[idx]\n            batch_data.append(x)\n            batch_labels.append(y)\n\n        self.current = end_idx\n\n        # Stack into batches\n        batch_x = genesis.stack(batch_data, dim=0)\n        batch_y = genesis.stack(batch_labels, dim=0)\n\n        return batch_x, batch_y\n\ndef create_synthetic_data(n_samples: int = 1000, n_features: int = 784, n_classes: int = 10) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create synthetic data for demonstration\"\"\"\n    np.random.seed(42)\n\n    # Generate random data\n    data = np.random.randn(n_samples, n_features).astype(np.float32)\n\n    # Add some patterns for each class\n    labels = np.random.randint(0, n_classes, n_samples)\n    for i in range(n_classes):\n        mask = labels == i\n        # Add specific bias to each class\n        data[mask] += np.random.randn(n_features) * 0.5\n\n    return data, labels\n\ndef load_data() -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"Load training and validation data\"\"\"\n    print(\"\ud83d\udd04 Loading data...\")\n\n    # Create synthetic data (replace with real data in actual projects)\n    train_data, train_labels = create_synthetic_data(800, 784, 10)\n    val_data, val_labels = create_synthetic_data(200, 784, 10)\n\n    # Data normalization\n    def normalize(x):\n        return (x - x.mean()) / (x.std() + 1e-8)\n\n    # Create datasets\n    train_dataset = SimpleDataset(train_data, train_labels, transform=normalize)\n    val_dataset = SimpleDataset(val_data, val_labels, transform=normalize)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n    print(f\"\u2705 Data loading complete - Training set: {len(train_dataset)}, Validation set: {len(val_dataset)}\")\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"getting-started/first-steps/#2-model-definition-modelpy","title":"\ud83e\udde0 2. Model Definition (<code>model.py</code>)","text":"Python<pre><code>\"\"\"Neural network model definition\"\"\"\nimport genesis\nimport genesis.nn as nn\nimport genesis.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"Multilayer Perceptron Classifier\"\"\"\n\n    def __init__(self, input_dim: int = 784, hidden_dims: list = None, num_classes: int = 10, dropout_rate: float = 0.2):\n        \"\"\"\n        Initialize MLP model\n\n        Args:\n            input_dim: Input dimension\n            hidden_dims: List of hidden layer dimensions\n            num_classes: Number of classes\n            dropout_rate: Dropout ratio\n        \"\"\"\n        super().__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [512, 256, 128]\n\n        # Build network layers\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = hidden_dim\n\n        # Output layer\n        layers.append(nn.Linear(prev_dim, num_classes))\n\n        self.network = nn.Sequential(*layers)\n\n        # Initialize weights\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"Weight initialization\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                # Xavier initialization\n                std = (2.0 / (module.in_features + module.out_features)) ** 0.5\n                module.weight.data.normal_(0, std)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        \"\"\"Forward propagation\"\"\"\n        # Flatten input (if image data)\n        if x.dim() &gt; 2:\n            x = x.view(x.size(0), -1)\n\n        return self.network(x)\n\nclass CNN(nn.Module):\n    \"\"\"Convolutional Neural Network Classifier (if processing images)\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # Pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Fully connected layers\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # Assuming input is 28x28\n        self.fc2 = nn.Linear(512, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        # Convolution + pooling\n        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -&gt; 7x7  \n        x = self.pool(F.relu(self.conv3(x)))  # 7x7 -&gt; 3x3\n\n        # Flatten\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\ndef create_model(model_type: str = \"mlp\", **kwargs) -&gt; nn.Module:\n    \"\"\"Factory function: create model\"\"\"\n    if model_type.lower() == \"mlp\":\n        return MLP(**kwargs)\n    elif model_type.lower() == \"cnn\":\n        return CNN(**kwargs)\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")\n</code></pre>"},{"location":"getting-started/first-steps/#3-utility-functions-utilspy","title":"\ud83d\udee0\ufe0f 3. Utility Functions (<code>utils.py</code>)","text":"Python<pre><code>\"\"\"Utility functions module\"\"\"\nimport genesis\nimport time\nimport os\nfrom typing import Dict, Any\nimport json\n\nclass AverageMeter:\n    \"\"\"Average calculator\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Timer:\n    \"\"\"Timer\"\"\"\n\n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self):\n        self.end_time = time.time()\n        return self.end_time - self.start_time\n\n    def elapsed(self):\n        if self.start_time is None:\n            return 0\n        return time.time() - self.start_time\n\ndef accuracy(output: genesis.Tensor, target: genesis.Tensor, topk: tuple = (1,)) -&gt; list:\n    \"\"\"Calculate accuracy\"\"\"\n    with genesis.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n\n        return res\n\ndef save_checkpoint(model: genesis.nn.Module, optimizer: genesis.optim.Optimizer, \n                   epoch: int, loss: float, accuracy: float, filepath: str):\n    \"\"\"Save checkpoint\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'accuracy': accuracy\n    }\n\n    # Ensure directory exists\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    genesis.save(checkpoint, filepath)\n    print(f\"\ud83d\udcbe Checkpoint saved: {filepath}\")\n\ndef load_checkpoint(filepath: str, model: genesis.nn.Module, optimizer: genesis.optim.Optimizer = None) -&gt; Dict[str, Any]:\n    \"\"\"Load checkpoint\"\"\"\n    checkpoint = genesis.load(filepath)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if optimizer and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    print(f\"\ud83d\udcc1 Checkpoint loaded: {filepath}\")\n    print(f\"   Epoch: {checkpoint['epoch']}, Loss: {checkpoint['loss']:.4f}, Accuracy: {checkpoint['accuracy']:.2f}%\")\n\n    return checkpoint\n\ndef save_training_history(history: Dict[str, list], filepath: str):\n    \"\"\"Save training history\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    with open(filepath, 'w') as f:\n        json.dump(history, f, indent=2)\n\n    print(f\"\ud83d\udcca Training history saved: {filepath}\")\n\ndef print_model_summary(model: genesis.nn.Module, input_shape: tuple):\n    \"\"\"Print model summary\"\"\"\n    print(\"\ud83c\udfd7\ufe0f  Model architecture:\")\n    print(\"=\" * 50)\n\n    # Calculate parameter count\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    print(f\"Input shape: {input_shape}\")\n\n    # Test forward propagation\n    dummy_input = genesis.randn(*input_shape)\n    try:\n        with genesis.no_grad():\n            output = model(dummy_input)\n        print(f\"Output shape: {output.shape}\")\n    except Exception as e:\n        print(f\"Forward propagation test failed: {e}\")\n\n    print(\"=\" * 50)\n</code></pre>"},{"location":"getting-started/first-steps/#4-training-script-trainpy","title":"\ud83d\ude82 4. Training Script (<code>train.py</code>)","text":"<p> [{\"id\": \"1\", \"content\": \"\\u521b\\u5efa\\u6587\\u6863\\u76ee\\u5f55\\u7ed3\\u6784\\u548c\\u914d\\u7f6e\\u6587\\u4ef6\", \"status\": \"completed\"}, {\"id\": \"2\", \"content\": \"\\u7f16\\u5199\\u9996\\u9875\\u548c\\u5feb\\u901f\\u5f00\\u59cb\\u6587\\u6863\", \"status\": \"completed\"}, {\"id\": \"3\", \"content\": \"\\u7f16\\u5199\\u67b6\\u6784\\u8bbe\\u8ba1\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"4\", \"content\": \"\\u7f16\\u5199\\u6838\\u5fc3\\u7ec4\\u4ef6\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"5\", \"content\": \"\\u7f16\\u5199\\u795e\\u7ecf\\u7f51\\u7edc\\u6a21\\u5757\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"6\", \"content\": \"\\u7f16\\u5199API\\u53c2\\u8003\\u6587\\u6863\", \"status\": \"pending\"}]"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you install the Genesis deep learning framework in different environments.</p>"},{"location":"getting-started/installation/#system-requirements","title":"\ud83d\udccb System Requirements","text":""},{"location":"getting-started/installation/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: x86_64 architecture with AVX instruction set support</li> <li>Memory: Minimum 8GB, recommended 16GB+</li> <li>GPU: NVIDIA GPU with Compute Capability \u2265 6.0 (optional but recommended)</li> <li>Storage: 2GB available space</li> </ul>"},{"location":"getting-started/installation/#software-requirements","title":"Software Requirements","text":"<ul> <li>Operating System: Linux (Ubuntu 20.04+), macOS (10.15+), Windows 10+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>CUDA: 11.0+ (required for GPU acceleration)</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"\ud83d\ude80 Quick Installation","text":""},{"location":"getting-started/installation/#method-1-install-from-source-recommended","title":"Method 1: Install from Source (Recommended)","text":"Bash<pre><code># 1. Clone the repository\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# 2. Create virtual environment (recommended)\npython -m venv genesis-env\nsource genesis-env/bin/activate  # Linux/macOS\n# genesis-env\\\\Scripts\\\\activate  # Windows\n\n# 3. Install dependencies\npip install -r genesis/requirements.txt\n\n# 4. Install Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation/#method-2-install-via-pip","title":"Method 2: Install via pip","text":"Bash<pre><code># Install release version\npip install genesis-dl\n\n# Install pre-release version\npip install --pre genesis-dl\n</code></pre>"},{"location":"getting-started/installation/#detailed-installation-steps","title":"\ud83d\udd27 Detailed Installation Steps","text":""},{"location":"getting-started/installation/#step-1-prepare-python-environment","title":"Step 1: Prepare Python Environment","text":"Ubuntu/DebianCentOS/RHELmacOSWindows Bash<pre><code># Install Python and pip\nsudo apt update\nsudo apt install python3.9 python3.9-pip python3.9-venv\n\n# Create symbolic link (optional)\nsudo ln -sf /usr/bin/python3.9 /usr/bin/python\n</code></pre> Bash<pre><code># Install EPEL repository\nsudo yum install epel-release\n\n# Install Python\nsudo yum install python39 python39-pip\n</code></pre> Bash<pre><code># Install using Homebrew\nbrew install python@3.9\n\n# Or use official installer\n# Download from https://python.org\n</code></pre> PowerShell<pre><code># Download Python installer\n# https://python.org/downloads/windows/\n\n# Or use Chocolatey\nchoco install python39\n</code></pre>"},{"location":"getting-started/installation/#step-2-install-cuda-gpu-acceleration","title":"Step 2: Install CUDA (GPU Acceleration)","text":"<p>GPU Support Note</p> <p>You can skip this step if you only need CPU version. However, installing CUDA is strongly recommended for optimal performance.</p> Ubuntu/DebianCentOS/RHELWindows Bash<pre><code># Download CUDA Toolkit\nwget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\nsudo sh cuda_11.8.0_520.61.05_linux.run\n\n# Set environment variables\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> Bash<pre><code># Install NVIDIA driver repository\nsudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\n\n# Install CUDA\nsudo yum install cuda-11-8\n</code></pre> PowerShell<pre><code># Download CUDA installer\n# https://developer.nvidia.com/cuda-downloads\n\n# Run the installer and follow the prompts\n</code></pre>"},{"location":"getting-started/installation/#step-3-install-core-dependencies","title":"Step 3: Install Core Dependencies","text":"Bash<pre><code># Create and activate virtual environment\npython -m venv genesis-env\nsource genesis-env/bin/activate\n\n# Upgrade pip\npip install --upgrade pip setuptools wheel\n\n# Install PyTorch (choose based on your CUDA version)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU version\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Install Triton\npip install triton\n\n# Install other dependencies\npip install numpy matplotlib tqdm\n</code></pre>"},{"location":"getting-started/installation/#step-4-install-genesis","title":"Step 4: Install Genesis","text":"Bash<pre><code># Clone source code\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# View available versions\ngit tag\n\n# Switch to stable version (optional)\ngit checkout v0.1.0\n\n# Install Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"\u2705 Verify Installation","text":"<p>Run the following code to verify that the installation was successful:</p> Python<pre><code>#!/usr/bin/env python3\n\"\"\"Genesis installation verification script\"\"\"\n\ndef test_basic_import():\n    \"\"\"Test basic import\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n        print(\"\u2705 Genesis import successful\")\n        print(f\"   Core modules: genesis, nn, optim\")\n        print(f\"   Available functions: {len([x for x in dir(genesis) if not x.startswith('_')])}\")\n    except ImportError as e:\n        print(f\"\u274c Genesis import failed: {e}\")\n        return False\n    return True\n\ndef test_tensor_operations():\n    \"\"\"Test tensor operations\"\"\"\n    try:\n        import genesis\n\n        # Create tensors\n        x = genesis.randn(3, 4)\n        y = genesis.randn(3, 4)\n\n        # Basic operations\n        z = x + y\n        w = genesis.matmul(x, y.T)  # Use actual Genesis API\n\n        print(\"\u2705 Tensor operations working\")\n        print(f\"   Addition result shape: {z.shape}\")\n        print(f\"   Matrix multiplication shape: {w.shape}\")\n    except Exception as e:\n        print(f\"\u274c Tensor operations failed: {e}\")\n        return False\n    return True\n\ndef test_neural_networks():\n    \"\"\"Test neural network modules\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # Create simple model using actual Genesis modules\n        model = nn.Sequential([\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 1)\n        ])\n\n        # Test forward pass\n        x = genesis.randn(2, 10)\n        y = model(x)\n        print(\"\u2705 Neural network modules working\")\n        print(f\"   Model layers: {len(list(model.parameters()))} parameter tensors\")\n        print(f\"   Output shape: {y.shape}\")\n    except Exception as e:\n        print(f\"\u274c Neural network modules failed: {e}\")\n        return False\n    return True\n\ndef test_backend_support():\n    \"\"\"Test backend support\"\"\"\n    try:\n        import genesis\n        from genesis.backend import default_device\n\n        # Test basic backend functionality\n        device = default_device()\n        x = genesis.randn(5, 5)\n\n        print(\"\u2705 Backend support working\")\n        print(f\"   Default device: {device}\")\n        print(f\"   Tensor device: {x.device}\")\n\n        # Try to detect CUDA if available\n        try:\n            # Test if we can create CUDA tensors\n            import torch\n            if torch.cuda.is_available():\n                print(\"   CUDA detected (via PyTorch backend)\")\n            else:\n                print(\"   CUDA not available (CPU only)\")\n        except:\n            print(\"   Backend: Genesis native\")\n\n    except Exception as e:\n        print(f\"\u274c Backend test failed: {e}\")\n        return False\n    return True\n\ndef test_autograd():\n    \"\"\"Test automatic differentiation\"\"\"\n    try:\n        import genesis\n\n        # Test basic autograd\n        x = genesis.randn(5, requires_grad=True)\n        y = genesis.functional.sum(x * x)  # Use actual Genesis API\n        y.backward()\n\n        print(\"\u2705 Automatic differentiation working\")\n        print(f\"   Input shape: {x.shape}\")\n        print(f\"   Gradient computed: {x.grad is not None}\")\n        print(f\"   Gradient shape: {x.grad.shape if x.grad is not None else 'None'}\")\n    except Exception as e:\n        print(f\"\u274c Automatic differentiation failed: {e}\")\n        return False\n    return True\n\ndef test_optimizers():\n    \"\"\"Test optimizer functionality\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n\n        # Create a simple model and optimizer\n        model = nn.Linear(5, 1)\n        optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n        # Test basic optimization step\n        x = genesis.randn(3, 5)\n        y_pred = model(x)\n        loss = genesis.functional.sum(y_pred * y_pred)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print(\"\u2705 Optimizer functionality working\")\n        print(f\"   Optimizer type: {type(optimizer).__name__}\")\n        print(f\"   Learning rate: 0.01\")\n        print(f\"   Parameters updated: {len(list(model.parameters()))}\")\n    except Exception as e:\n        print(f\"\u274c Optimizer test failed: {e}\")\n        return False\n    return True\n\ndef test_serialization():\n    \"\"\"Test model saving/loading\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # Create and save a model\n        model = nn.Linear(3, 2)\n        state_dict = model.state_dict()\n\n        # Test serialization functionality\n        genesis.save(state_dict, 'test_model.pkl')\n        loaded_state = genesis.load('test_model.pkl')\n\n        print(\"\u2705 Serialization working\")\n        print(f\"   Model saved and loaded successfully\")\n        print(f\"   State dict keys: {len(state_dict)}\")\n\n        # Cleanup\n        import os\n        if os.path.exists('test_model.pkl'):\n            os.remove('test_model.pkl')\n\n    except Exception as e:\n        print(f\"\u274c Serialization test failed: {e}\")\n        return False\n    return True\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd0d Genesis Installation Verification\\n\")\n\n    tests = [\n        test_basic_import,\n        test_tensor_operations,\n        test_neural_networks,\n        test_backend_support,\n        test_autograd,\n        test_optimizers,\n        test_serialization\n    ]\n\n    passed = 0\n    total = len(tests)\n\n    for test in tests:\n        try:\n            if test():\n                passed += 1\n        except Exception as e:\n            print(f\"\u274c Test failed with exception: {e}\")\n        print()\n\n    print(f\"\ud83d\udcca Test Results: {passed}/{total} passed\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 Congratulations! Genesis installation successful, all features working!\")\n    elif passed &gt;= total * 0.8:  # 80% pass rate\n        print(\"\u2705 Genesis installation mostly successful! Minor issues detected.\")\n        print(\"   Most functionality is working. Check failed tests above.\")\n    else:\n        print(\"\u26a0\ufe0f  Genesis installation has issues. Please check:\")\n        print(\"   1. Genesis is properly installed: pip install -e .\")\n        print(\"   2. Dependencies are installed: pip install torch triton\")\n        print(\"   3. Python version is 3.8+\")\n</code></pre> <p>Save the above code as <code>test_installation.py</code> and run:</p> Bash<pre><code>python test_installation.py\n</code></pre>"},{"location":"getting-started/installation/#common-issues-and-solutions","title":"\ud83d\udd27 Common Issues and Solutions","text":""},{"location":"getting-started/installation/#issue-1-cuda-version-mismatch","title":"Issue 1: CUDA Version Mismatch","text":"<p>Error Message: Text Only<pre><code>RuntimeError: CUDA version mismatch\n</code></pre></p> <p>Solution: Bash<pre><code># Check system CUDA version\nnvidia-smi\n\n# Check PyTorch CUDA version\npython -c \"import torch; print(torch.version.cuda)\"\n\n# Reinstall matching PyTorch version\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"getting-started/installation/#issue-2-triton-compilation-failure","title":"Issue 2: Triton Compilation Failure","text":"<p>Error Message: Text Only<pre><code>Failed to compile Triton kernel\n</code></pre></p> <p>Solution: Bash<pre><code># Upgrade Triton\npip install --upgrade triton\n\n# Or install development version\npip install --pre triton\n</code></pre></p>"},{"location":"getting-started/installation/#issue-3-out-of-memory","title":"Issue 3: Out of Memory","text":"<p>Error Message: Text Only<pre><code>CUDA out of memory\n</code></pre></p> <p>Solution: Python<pre><code>import genesis\n\n# Enable memory optimization\ngenesis.cuda.empty_cache()\n\n# Reduce batch size\nbatch_size = 16  # Instead of 32\n\n# Enable gradient checkpointing (if supported)\nmodel.gradient_checkpointing = True\n</code></pre></p>"},{"location":"getting-started/installation/#issue-4-import-error","title":"Issue 4: Import Error","text":"<p>Error Message: Text Only<pre><code>ModuleNotFoundError: No module named 'genesis'\n</code></pre></p> <p>Solution: Bash<pre><code># Check virtual environment\nwhich python\npip list | grep genesis\n\n# Reinstall\npip uninstall genesis-dl\npip install -e genesis/\n</code></pre></p>"},{"location":"getting-started/installation/#docker-installation","title":"\ud83d\udc33 Docker Installation","text":"<p>If you encounter environment issues, you can use Docker:</p> Bash<pre><code># Download pre-built image\ndocker pull genesis/genesis:latest\n\n# Or build your own image\ngit clone https://github.com/phonism/genesis.git\ncd genesis\ndocker build -t genesis:local .\n\n# Run container\ndocker run -it --gpus all genesis:local bash\n</code></pre> <p>Dockerfile contents: Docker<pre><code>FROM nvidia/cuda:11.8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PATH=\"/opt/conda/bin:$PATH\"\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    wget git build-essential &amp;&amp; \\\\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda &amp;&amp; \\\\\n    rm Miniconda3-latest-Linux-x86_64.sh\n\n# Create environment and install dependencies\nRUN conda create -n genesis python=3.9 -y\nSHELL [\"conda\", \"run\", \"-n\", \"genesis\", \"/bin/bash\", \"-c\"]\n\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 &amp;&amp; \\\\\n    pip install triton numpy matplotlib tqdm\n\n# Copy and install Genesis\nCOPY . /workspace/genesis\nWORKDIR /workspace/genesis\nRUN pip install -e genesis/\n\n# Set startup command\nENTRYPOINT [\"conda\", \"run\", \"-n\", \"genesis\"]\nCMD [\"bash\"]\n</code></pre></p>"},{"location":"getting-started/installation/#performance-optimization-tips","title":"\ud83d\udcca Performance Optimization Tips","text":"<p>After installation, you can optimize performance with:</p> Bash<pre><code># Set environment variables\nexport CUDA_VISIBLE_DEVICES=0  # Specify GPU\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n# Enable optimization options\nexport GENESIS_OPTIMIZE=1\nexport TRITON_CACHE_DIR=/tmp/triton_cache\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>After installation, it's recommended to:</p> <ol> <li>Run your first program - Verify installation and learn basic usage</li> <li>Check tutorials - Systematically learn Genesis usage</li> <li>Read architecture documentation - Understand framework design principles</li> </ol> <p>If you encounter problems during installation, please check the FAQ or submit an issue on GitHub.</p>"},{"location":"migration/breaking-changes/","title":"Breaking Changes in Genesis v2.0","text":"<p>This document provides a comprehensive list of all breaking changes introduced in Genesis v2.0.</p>"},{"location":"migration/breaking-changes/#removed-modules-and-classes","title":"\ud83d\uddd1\ufe0f Removed Modules and Classes","text":""},{"location":"migration/breaking-changes/#ndarray-module-complete-removal","title":"NDArray Module (Complete Removal)","text":"<p>Impact: High - Affects all code using NDArray directly</p> Python<pre><code># \u274c Removed in v2.0\nfrom genesis.ndarray import NDArray, Device\nfrom genesis.ndarray.cuda_storage import CUDAStorage\nfrom genesis.ndarray.cpu_storage import CPUStorage\nfrom genesis.ndarray.cuda_backend import CUDABackend\n\n# \u2705 v2.0 Replacements\nimport genesis\nfrom genesis.device import Device\nfrom genesis.backends.cuda import CUDAStorage\nfrom genesis.backends.cpu import CPUStorage\n</code></pre>"},{"location":"migration/breaking-changes/#autograd-module-restructure","title":"Autograd Module Restructure","text":"<p>Impact: Medium - Affects custom autograd implementations</p> Python<pre><code># \u274c Removed paths\nfrom genesis.autograd import Variable, Context, Function\nfrom genesis.autograd.engine import backward_engine\n\n# \u2705 New paths\nfrom genesis.tensor import Tensor  # Variable -&gt; Tensor\nfrom genesis.function import Function, Context\n# backward_engine is now internal\n</code></pre>"},{"location":"migration/breaking-changes/#backend-module-changes","title":"Backend Module Changes","text":"<p>Impact: Low - Mainly affects advanced users</p> Python<pre><code># \u274c Old backend system\nfrom genesis.backend import Backend, get_backend\nfrom genesis.backend.registry import register_backend\n\n# \u2705 New backend system (mostly internal)\n# Backends are automatically managed\n# Custom backends: see backend development guide\n</code></pre>"},{"location":"migration/breaking-changes/#api-changes","title":"\ud83d\udd04 API Changes","text":""},{"location":"migration/breaking-changes/#tensor-creation","title":"Tensor Creation","text":"<p>Impact: Medium - Common operations</p> Python<pre><code># \u274c Old ways that no longer work\nx = NDArray([1, 2, 3])\ny = Variable([1, 2, 3], requires_grad=True)\n\n# \u2705 New unified API\nx = genesis.tensor([1, 2, 3])\ny = genesis.tensor([1, 2, 3], requires_grad=True)\n</code></pre>"},{"location":"migration/breaking-changes/#device-specification","title":"Device Specification","text":"<p>Impact: High - Affects all device-specific code</p> Python<pre><code># \u274c Old device handling\nfrom genesis.ndarray.device import CUDADevice, CPUDevice\ndevice = CUDADevice(0)\ntensor = NDArray([1, 2, 3], device=device)\n\n# \u2705 New device system\ndevice = genesis.device('cuda:0')\ntensor = genesis.tensor([1, 2, 3], device=device)\n</code></pre>"},{"location":"migration/breaking-changes/#operation-interface","title":"Operation Interface","text":"<p>Impact: Medium - Affects functional programming style</p> Python<pre><code># \u274c Old functional interface\nfrom genesis.functional import add, matmul\nresult = add(a, b)\n\n# \u2705 New interface (both styles supported)\nresult = genesis.add(a, b)        # Functional style\nresult = a + b                    # Operator style\n</code></pre>"},{"location":"migration/breaking-changes/#import-path-changes","title":"\ud83d\udce6 Import Path Changes","text":""},{"location":"migration/breaking-changes/#core-components","title":"Core Components","text":"Python<pre><code># \u274c Old imports\nfrom genesis.autograd import Tensor\nfrom genesis.ndarray import Device\nfrom genesis.backend import get_current_backend\n\n# \u2705 New imports\nfrom genesis import Tensor, tensor  # Both available\nfrom genesis.device import Device\n# Backend selection is automatic\n</code></pre>"},{"location":"migration/breaking-changes/#neural-network-components","title":"Neural Network Components","text":"Python<pre><code># \u274c Old imports (still work but deprecated)\nfrom genesis.nn.modules.linear import Linear\nfrom genesis.nn.functional import relu\n\n# \u2705 Preferred new imports\nfrom genesis.nn import Linear\nimport genesis.nn.functional as F\n</code></pre>"},{"location":"migration/breaking-changes/#optimization-components","title":"Optimization Components","text":"Python<pre><code># \u2705 These remain the same (no breaking changes)\nfrom genesis.optim import Adam, SGD\nfrom genesis.optim.lr_scheduler import StepLR\n</code></pre>"},{"location":"migration/breaking-changes/#architectural-changes","title":"\ud83c\udfd7\ufe0f Architectural Changes","text":""},{"location":"migration/breaking-changes/#memory-management","title":"Memory Management","text":"<p>Impact: Low - Mostly improved, but some advanced usage affected</p> Python<pre><code># \u274c Old memory management\nNDArray.set_memory_pool_size(1024 * 1024 * 1024)\nNDArray.clear_memory_pool()\n\n# \u2705 New memory management\ngenesis.cuda.set_memory_fraction(0.8)\ngenesis.cuda.empty_cache()\n</code></pre>"},{"location":"migration/breaking-changes/#context-management","title":"Context Management","text":"<p>Impact: Medium - Affects gradient computation customization</p> Python<pre><code># \u274c Old context usage\nclass CustomFunction(Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.input_shape = input.shape\n        ctx.save_for_backward(input)\n        return input * 2\n\n# \u2705 New context usage (similar but enhanced)\nclass CustomFunction(Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_variable('input_shape', input.shape)\n        ctx.save_for_backward(input)\n        return input * 2\n</code></pre>"},{"location":"migration/breaking-changes/#configuration-changes","title":"\ud83d\udd27 Configuration Changes","text":""},{"location":"migration/breaking-changes/#environment-variables","title":"Environment Variables","text":"Python<pre><code># \u274c Old environment variables (no longer used)\nGENESIS_NDARRAY_BACKEND=cuda\nGENESIS_DEFAULT_DTYPE=float32\n\n# \u2705 New environment variables\nGENESIS_DEFAULT_DEVICE=cuda:0\nGENESIS_CUDA_MEMORY_FRACTION=0.8\nGENESIS_KERNEL_CACHE_DIR=/tmp/genesis_cache\n</code></pre>"},{"location":"migration/breaking-changes/#runtime-configuration","title":"Runtime Configuration","text":"Python<pre><code># \u274c Old configuration methods\ngenesis.set_default_backend('cuda')\ngenesis.ndarray.set_default_device('cuda:0')\n\n# \u2705 New configuration methods\ngenesis.set_default_device('cuda:0')\ngenesis.cuda.set_device(0)\n</code></pre>"},{"location":"migration/breaking-changes/#behavioral-changes","title":"\u2699\ufe0f Behavioral Changes","text":""},{"location":"migration/breaking-changes/#automatic-type-promotion","title":"Automatic Type Promotion","text":"<p>Impact: Low - Generally improved behavior</p> Python<pre><code># v1.x behavior: Sometimes inconsistent type promotion\na = genesis.tensor([1, 2, 3], dtype=genesis.int32)\nb = genesis.tensor([1.0, 2.0, 3.0], dtype=genesis.float32)\nc = a + b  # Might fail or produce unexpected types\n\n# v2.0 behavior: Consistent PyTorch-like promotion\na = genesis.tensor([1, 2, 3], dtype=genesis.int32)\nb = genesis.tensor([1.0, 2.0, 3.0], dtype=genesis.float32)\nc = a + b  # Always produces float32, following clear rules\n</code></pre>"},{"location":"migration/breaking-changes/#device-transfer-behavior","title":"Device Transfer Behavior","text":"<p>Impact: Low - More consistent behavior</p> Python<pre><code># v1.x: Inconsistent behavior with mixed devices\ncpu_tensor = genesis.tensor([1, 2, 3], device='cpu')\ngpu_tensor = genesis.tensor([4, 5, 6], device='cuda')\nresult = cpu_tensor + gpu_tensor  # Behavior was unclear\n\n# v2.0: Clear promotion rules\ncpu_tensor = genesis.tensor([1, 2, 3], device='cpu')\ngpu_tensor = genesis.tensor([4, 5, 6], device='cuda')\nresult = cpu_tensor + gpu_tensor  # Always promotes to GPU, warns user\n</code></pre>"},{"location":"migration/breaking-changes/#gradient-computation","title":"Gradient Computation","text":"<p>Impact: Low - More efficient, same API</p> Python<pre><code># Same API, but improved performance and memory usage\nx = genesis.tensor([1, 2, 3], requires_grad=True)\ny = x.sum()\ny.backward()  # More efficient in v2.0\n</code></pre>"},{"location":"migration/breaking-changes/#bug-fixes-that-may-affect-code","title":"\ud83d\udc1b Bug Fixes That May Affect Code","text":""},{"location":"migration/breaking-changes/#memory-layout-consistency","title":"Memory Layout Consistency","text":"<p>Impact: Low - Better consistency</p> Python<pre><code># v1.x: Inconsistent memory layout\nx = genesis.tensor([[1, 2], [3, 4]])\ny = x.transpose(0, 1)\n# Memory layout might vary between backends\n\n# v2.0: Consistent memory layout\nx = genesis.tensor([[1, 2], [3, 4]])\ny = x.transpose(0, 1)\n# Consistent memory layout across all backends\n</code></pre>"},{"location":"migration/breaking-changes/#broadcasting-rules","title":"Broadcasting Rules","text":"<p>Impact: Low - More consistent with NumPy/PyTorch</p> Python<pre><code># v1.x: Some edge cases in broadcasting\na = genesis.tensor([1, 2, 3])      # shape: (3,)\nb = genesis.tensor([[1], [2]])     # shape: (2, 1)\nc = a + b  # Might behave inconsistently\n\n# v2.0: Consistent NumPy/PyTorch-like broadcasting\na = genesis.tensor([1, 2, 3])      # shape: (3,)\nb = genesis.tensor([[1], [2]])     # shape: (2, 1)\nc = a + b  # shape: (2, 3), consistent behavior\n</code></pre>"},{"location":"migration/breaking-changes/#migration-timeline","title":"\ud83d\udd04 Migration Timeline","text":""},{"location":"migration/breaking-changes/#phase-1-immediate-required","title":"Phase 1: Immediate (Required)","text":"<ul> <li>Remove all NDArray imports</li> <li>Update tensor creation calls</li> <li>Fix device specification</li> </ul>"},{"location":"migration/breaking-changes/#phase-2-short-term-recommended","title":"Phase 2: Short-term (Recommended)","text":"<ul> <li>Update import paths to new preferred locations</li> <li>Adopt new configuration methods</li> <li>Update custom operations</li> </ul>"},{"location":"migration/breaking-changes/#phase-3-long-term-optional","title":"Phase 3: Long-term (Optional)","text":"<ul> <li>Leverage new performance features</li> <li>Adopt new memory management patterns</li> <li>Update testing code</li> </ul>"},{"location":"migration/breaking-changes/#common-pitfalls","title":"\u26a0\ufe0f Common Pitfalls","text":""},{"location":"migration/breaking-changes/#pitfall-1-assuming-old-import-paths-work","title":"Pitfall 1: Assuming Old Import Paths Work","text":"Python<pre><code># \u274c This will fail\nfrom genesis.ndarray import NDArray\n\n# \u2705 Use this instead\nimport genesis\nx = genesis.tensor(data)\n</code></pre>"},{"location":"migration/breaking-changes/#pitfall-2-direct-backend-access","title":"Pitfall 2: Direct Backend Access","text":"Python<pre><code># \u274c This pattern no longer works\nbackend = genesis.get_backend('cuda')\nresult = backend.add(a, b)\n\n# \u2705 Use operation dispatch\nresult = genesis.add(a, b)  # Automatically uses correct backend\n</code></pre>"},{"location":"migration/breaking-changes/#pitfall-3-old-memory-management","title":"Pitfall 3: Old Memory Management","text":"Python<pre><code># \u274c Old memory management doesn't exist\nNDArray.set_memory_pool_size(size)\n\n# \u2705 Use new memory management\ngenesis.cuda.set_memory_fraction(0.8)\n</code></pre>"},{"location":"migration/breaking-changes/#migration-tools","title":"\ud83d\udee0\ufe0f Migration Tools","text":""},{"location":"migration/breaking-changes/#automated-migration-script","title":"Automated Migration Script","text":"<p>We provide a migration script to help with common patterns:</p> Bash<pre><code># Run the migration script on your codebase\npython -m genesis.migrate /path/to/your/code\n\n# Preview changes without applying\npython -m genesis.migrate /path/to/your/code --dry-run\n\n# Migrate specific patterns only\npython -m genesis.migrate /path/to/your/code --only=imports,tensor_creation\n</code></pre>"},{"location":"migration/breaking-changes/#manual-migration-checklist","title":"Manual Migration Checklist","text":"<ul> <li> Remove NDArray imports</li> <li> Update tensor creation</li> <li> Fix device specifications</li> <li> Update import paths</li> <li> Test functionality</li> <li> Update configuration</li> <li> Verify performance</li> </ul>"},{"location":"migration/breaking-changes/#support","title":"\ud83d\udcde Support","text":"<p>If you encounter issues with breaking changes:</p> <ol> <li>Check this document first - It covers most common issues</li> <li>Use the migration script - Automates many common changes</li> <li>Check the migration guide - Provides detailed examples</li> <li>File an issue - If you find undocumented breaking changes</li> </ol>"},{"location":"migration/breaking-changes/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<ul> <li>Migration Guide - Step-by-step migration process</li> <li>Backend System - Understanding the new architecture</li> <li>Device Management - New device system</li> <li>API Reference - Complete v2.0 API documentation</li> </ul>"},{"location":"migration/v2-migration/","title":"Migration Guide: Genesis v1 to v2","text":"<p>This guide helps you migrate your code from Genesis v1.x to Genesis v2.0, which introduces significant architectural changes.</p>"},{"location":"migration/v2-migration/#overview","title":"\ud83d\udccb Overview","text":"<p>Genesis v2.0 introduces a major architectural overhaul with: - Modular Backend System: Separated CPU and CUDA implementations - Unified Device Abstraction: Consistent device management - Operation Dispatch System: Centralized operation routing - Removed NDArray Module: Functionality moved to backends</p>"},{"location":"migration/v2-migration/#breaking-changes-summary","title":"\ud83d\udd04 Breaking Changes Summary","text":""},{"location":"migration/v2-migration/#1-ndarray-module-removed","title":"1. NDArray Module Removed","text":"<p>The entire <code>genesis.ndarray</code> module has been removed and its functionality integrated into the new backend system.</p>"},{"location":"migration/v2-migration/#before-v1x","title":"Before (v1.x)","text":"Python<pre><code># NDArray was explicitly used\nfrom genesis.ndarray import NDArray\nx = NDArray([1, 2, 3], device='cuda')\n</code></pre>"},{"location":"migration/v2-migration/#after-v20","title":"After (v2.0)","text":"Python<pre><code># Direct tensor creation\nimport genesis\nx = genesis.tensor([1, 2, 3], device='cuda')\n</code></pre>"},{"location":"migration/v2-migration/#2-import-changes","title":"2. Import Changes","text":"<p>Many import paths have changed due to restructuring.</p>"},{"location":"migration/v2-migration/#before-v1x_1","title":"Before (v1.x)","text":"Python<pre><code>from genesis.autograd import Tensor\nfrom genesis.ndarray.device import Device\nfrom genesis.ndarray.cuda_storage import CUDAStorage\n</code></pre>"},{"location":"migration/v2-migration/#after-v20_1","title":"After (v2.0)","text":"Python<pre><code>from genesis import Tensor  # or just use genesis.tensor()\nfrom genesis.device import Device\nfrom genesis.backends.cuda import CUDAStorage\n</code></pre>"},{"location":"migration/v2-migration/#3-backend-access","title":"3. Backend Access","text":"<p>Direct backend access has been restructured.</p>"},{"location":"migration/v2-migration/#before-v1x_2","title":"Before (v1.x)","text":"Python<pre><code># Accessing CUDA functions directly\nfrom genesis.ndarray.cuda_backend import cuda_add\nresult = cuda_add(a, b)\n</code></pre>"},{"location":"migration/v2-migration/#after-v20_2","title":"After (v2.0)","text":"Python<pre><code># Operations automatically dispatch to correct backend\nresult = genesis.add(a, b)  # Automatically uses CUDA if tensors are on GPU\n</code></pre>"},{"location":"migration/v2-migration/#code-migration-steps","title":"\ud83d\udd27 Code Migration Steps","text":""},{"location":"migration/v2-migration/#step-1-update-imports","title":"Step 1: Update Imports","text":"<p>Replace old imports with new ones:</p> Python<pre><code># Old imports (v1.x) -&gt; New imports (v2.0)\nfrom genesis.autograd import Tensor          -&gt; from genesis import tensor, Tensor\nfrom genesis.ndarray import NDArray          -&gt; # Remove, use genesis.tensor()\nfrom genesis.ndarray.device import Device    -&gt; from genesis.device import Device\nfrom genesis.backend import Backend          -&gt; # Remove, handled automatically\n</code></pre>"},{"location":"migration/v2-migration/#step-2-replace-ndarray-usage","title":"Step 2: Replace NDArray Usage","text":"<p>Convert NDArray operations to tensor operations:</p> Python<pre><code># Before (v1.x)\ndef old_function():\n    x = NDArray([1, 2, 3], device='cuda')\n    y = NDArray([4, 5, 6], device='cuda')\n    return x.add(y)\n\n# After (v2.0)\ndef new_function():\n    x = genesis.tensor([1, 2, 3], device='cuda')\n    y = genesis.tensor([4, 5, 6], device='cuda')\n    return x + y  # or genesis.add(x, y)\n</code></pre>"},{"location":"migration/v2-migration/#step-3-update-device-handling","title":"Step 3: Update Device Handling","text":"<p>Use the new unified device system:</p> Python<pre><code># Before (v1.x)\nfrom genesis.ndarray.device import get_device\ndevice = get_device('cuda:0')\n\n# After (v2.0)\ndevice = genesis.device('cuda:0')\n</code></pre>"},{"location":"migration/v2-migration/#step-4-backend-specific-code","title":"Step 4: Backend-Specific Code","text":"<p>If you had backend-specific code, update it:</p> Python<pre><code># Before (v1.x) - Direct backend access\nfrom genesis.ndarray.cuda_backend import CUDABackend\nbackend = CUDABackend()\nresult = backend.matmul(a, b)\n\n# After (v2.0) - Use operation dispatch\nresult = genesis.matmul(a, b)  # Automatically routed to appropriate backend\n</code></pre>"},{"location":"migration/v2-migration/#common-migration-patterns","title":"\ud83d\udcdd Common Migration Patterns","text":""},{"location":"migration/v2-migration/#pattern-1-tensor-creation","title":"Pattern 1: Tensor Creation","text":"Python<pre><code># Before (v1.x)\ndef create_tensors_v1():\n    x = NDArray([1, 2, 3])\n    y = NDArray.zeros((3, 3))\n    z = NDArray.ones((2, 2), device='cuda')\n    return x, y, z\n\n# After (v2.0)\ndef create_tensors_v2():\n    x = genesis.tensor([1, 2, 3])\n    y = genesis.zeros((3, 3))\n    z = genesis.ones((2, 2), device='cuda')\n    return x, y, z\n</code></pre>"},{"location":"migration/v2-migration/#pattern-2-device-transfer","title":"Pattern 2: Device Transfer","text":"Python<pre><code># Before (v1.x)\ndef transfer_v1(tensor):\n    cuda_tensor = tensor.to_device('cuda')\n    cpu_tensor = cuda_tensor.to_device('cpu')\n    return cpu_tensor\n\n# After (v2.0)\ndef transfer_v2(tensor):\n    cuda_tensor = tensor.to('cuda')\n    cpu_tensor = cuda_tensor.to('cpu')\n    return cpu_tensor\n</code></pre>"},{"location":"migration/v2-migration/#pattern-3-custom-operations","title":"Pattern 3: Custom Operations","text":"Python<pre><code># Before (v1.x) - Required NDArray knowledge\ndef custom_op_v1(x):\n    if x.device.is_cuda:\n        result = cuda_custom_kernel(x.data)\n    else:\n        result = cpu_custom_impl(x.data)\n    return NDArray(result, device=x.device)\n\n# After (v2.0) - Use operation dispatch\ndef custom_op_v2(x):\n    return genesis.ops.custom_operation(x)  # Automatically dispatched\n</code></pre>"},{"location":"migration/v2-migration/#pattern-4-memory-management","title":"Pattern 4: Memory Management","text":"Python<pre><code># Before (v1.x)\ndef manage_memory_v1():\n    x = NDArray.zeros((1000, 1000), device='cuda')\n    # Manual memory management\n    del x\n    NDArray.cuda_empty_cache()\n\n# After (v2.0)\ndef manage_memory_v2():\n    x = genesis.zeros((1000, 1000), device='cuda')\n    # Improved automatic memory management\n    del x\n    genesis.cuda.empty_cache()  # Still available but less needed\n</code></pre>"},{"location":"migration/v2-migration/#potential-issues-and-solutions","title":"\u26a0\ufe0f Potential Issues and Solutions","text":""},{"location":"migration/v2-migration/#issue-1-import-errors","title":"Issue 1: Import Errors","text":"<p>Problem: <code>ImportError: cannot import name 'NDArray'</code></p> <p>Solution: Replace NDArray usage with tensor functions Python<pre><code># Fix import error\n# from genesis.ndarray import NDArray  # Remove this line\nimport genesis\nx = genesis.tensor(data)  # Use this instead\n</code></pre></p>"},{"location":"migration/v2-migration/#issue-2-device-attribute-errors","title":"Issue 2: Device Attribute Errors","text":"<p>Problem: <code>AttributeError: 'str' object has no attribute 'is_cuda'</code></p> <p>Solution: Use proper device objects Python<pre><code># Before - device was sometimes a string\ndevice = 'cuda'\nif device == 'cuda':  # String comparison\n\n# After - use device objects\ndevice = genesis.device('cuda')\nif device.is_cuda:  # Proper attribute\n</code></pre></p>"},{"location":"migration/v2-migration/#issue-3-backend-method-not-found","title":"Issue 3: Backend Method Not Found","text":"<p>Problem: Direct backend method calls fail</p> <p>Solution: Use the operation dispatch system Python<pre><code># Before - direct backend call\nresult = backend.specific_operation(x)\n\n# After - use dispatched operation\nresult = genesis.ops.specific_operation(x)\n</code></pre></p>"},{"location":"migration/v2-migration/#issue-4-performance-regression","title":"Issue 4: Performance Regression","text":"<p>Problem: Code runs slower after migration</p> <p>Solution: 1. Ensure tensors are on the correct device 2. Use in-place operations where possible 3. Check for unnecessary device transfers</p> Python<pre><code># Check tensor device placement\nprint(f\"Tensor device: {x.device}\")\n\n# Use in-place operations\nx.add_(y)  # Instead of x = x + y\n\n# Minimize device transfers\n# Keep related tensors on the same device\n</code></pre>"},{"location":"migration/v2-migration/#migration-checklist","title":"\u2705 Migration Checklist","text":"<p>Use this checklist to ensure complete migration:</p> <ul> <li> Remove NDArray imports</li> <li> Remove <code>from genesis.ndarray import NDArray</code></li> <li> Remove <code>from genesis.ndarray.device import Device</code></li> <li> <p> Remove other ndarray-specific imports</p> </li> <li> <p> Update tensor creation</p> </li> <li> Replace <code>NDArray(data)</code> with <code>genesis.tensor(data)</code></li> <li> Replace <code>NDArray.zeros()</code> with <code>genesis.zeros()</code></li> <li> <p> Replace <code>NDArray.ones()</code> with <code>genesis.ones()</code></p> </li> <li> <p> Update device handling</p> </li> <li> Use <code>genesis.device()</code> for device creation</li> <li> Update device attribute access</li> <li> <p> Check device transfer methods</p> </li> <li> <p> Update operations</p> </li> <li> Replace direct backend calls with operation dispatch</li> <li> Update custom operation implementations</li> <li> <p> Verify operation behavior consistency</p> </li> <li> <p> Test functionality</p> </li> <li> Run existing tests</li> <li> Verify performance characteristics</li> <li> Check memory usage patterns</li> </ul>"},{"location":"migration/v2-migration/#taking-advantage-of-new-features","title":"\ud83d\ude80 Taking Advantage of New Features","text":""},{"location":"migration/v2-migration/#enhanced-memory-management","title":"Enhanced Memory Management","text":"Python<pre><code># Take advantage of improved memory pooling\ngenesis.cuda.set_memory_fraction(0.9)  # Use 90% of GPU memory\n\n# Monitor memory usage\nstats = genesis.cuda.memory_stats()\nprint(f\"Memory efficiency: {stats.fragmentation_ratio:.2%}\")\n</code></pre>"},{"location":"migration/v2-migration/#improved-device-management","title":"Improved Device Management","text":"Python<pre><code># Use automatic device selection\ndevice = genesis.device('auto')  # Chooses best available device\n\n# Context-based device management\nwith genesis.device('cuda:1'):\n    x = genesis.randn(1000, 1000)  # Automatically on cuda:1\n</code></pre>"},{"location":"migration/v2-migration/#operation-profiling","title":"Operation Profiling","text":"Python<pre><code># Profile operations\nwith genesis.ops.profile() as prof:\n    result = complex_computation(data)\n\nprof.print_stats()  # See performance breakdown\n</code></pre>"},{"location":"migration/v2-migration/#additional-resources","title":"\ud83d\udd17 Additional Resources","text":"<ul> <li>Breaking Changes - Complete list of breaking changes</li> <li>Backend System - Understanding the new backend architecture</li> <li>Device Guide - Device management in v2.0</li> <li>Performance Guide - Optimizing v2.0 code</li> </ul>"},{"location":"migration/v2-migration/#getting-help","title":"\ud83d\udca1 Getting Help","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check the documentation - Most common patterns are covered</li> <li>Search issues - Look for similar problems in GitHub issues</li> <li>Ask questions - Create a new issue with the \"migration\" label</li> <li>Provide examples - Include before/after code snippets</li> </ol> <p>Remember that v2.0 provides better performance and cleaner architecture, so the migration effort is worthwhile!</p>"},{"location":"models/qwen/","title":"Qwen Model Implementation","text":""},{"location":"models/qwen/#overview","title":"Overview","text":"<p>The Genesis framework includes a built-in implementation of the Qwen (Tongyi Qianwen) large language model, supporting complete training and inference workflows.</p>"},{"location":"models/qwen/#model-architecture","title":"Model Architecture","text":"<p>The Qwen model is based on the Transformer architecture with the following features:</p> <ul> <li>Attention Mechanism: Multi-Head Attention with RoPE (Rotary Position Embedding)</li> <li>Activation Function: SwiGLU activation</li> <li>Layer Normalization: RMSNorm</li> <li>Position Encoding: Rotary Position Embedding (RoPE)</li> </ul>"},{"location":"models/qwen/#quick-start","title":"Quick Start","text":""},{"location":"models/qwen/#basic-inference","title":"Basic Inference","text":"Python<pre><code>import genesis\nfrom genesis.models.qwen import QwenModel, QwenConfig\n\n# Create model configuration\nconfig = QwenConfig(\n    vocab_size=32000,\n    n_layer=24,\n    n_head=16,\n    n_embd=2048,\n    max_seq_len=2048\n)\n\n# Create model\nmodel = QwenModel(config)\n\n# Inference\ninput_ids = genesis.tensor([[1, 2, 3, 4, 5]])  # [batch_size, seq_len]\noutput = model(input_ids)\nprint(f\"Output shape: {output.shape}\")  # [1, 5, 32000]\n</code></pre>"},{"location":"models/qwen/#training-example","title":"Training Example","text":"Python<pre><code>import genesis.optim as optim\nimport genesis.nn as nn\n\n# Create optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# Training loop\nfor batch in dataloader:\n    input_ids, labels = batch\n\n    # Forward pass\n    logits = model(input_ids)\n\n    # Calculate loss\n    loss = nn.functional.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        labels.view(-1)\n    )\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Gradient clipping\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # Parameter update\n    optimizer.step()\n</code></pre>"},{"location":"models/qwen/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/qwen/#qwenconfig","title":"QwenConfig","text":"Parameter Type Default Description <code>vocab_size</code> int 32000 Vocabulary size <code>n_layer</code> int 24 Number of Transformer layers <code>n_head</code> int 16 Number of attention heads <code>n_embd</code> int 2048 Hidden dimension <code>max_seq_len</code> int 2048 Maximum sequence length <code>dropout</code> float 0.1 Dropout probability <code>bias</code> bool False Whether to use bias"},{"location":"models/qwen/#performance-optimization","title":"Performance Optimization","text":""},{"location":"models/qwen/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable mixed precision\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    logits = model(input_ids)\n    loss = criterion(logits, labels)\n\n# Gradient scaling\nscaler = genesis.GradScaler()\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"models/qwen/#gradient-checkpointing","title":"Gradient Checkpointing","text":"Python<pre><code># Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing = True\n</code></pre>"},{"location":"models/qwen/#application-examples","title":"Application Examples","text":""},{"location":"models/qwen/#text-generation","title":"Text Generation","text":"Python<pre><code>def generate_text(model, tokenizer, prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = genesis.tensor([input_ids])\n\n    with genesis.no_grad():\n        for _ in range(max_length):\n            logits = model(input_tensor)\n            next_token = logits[0, -1].argmax()\n            input_tensor = genesis.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(input_tensor[0].tolist())\n\n# Usage example\ngenerated = generate_text(model, tokenizer, \"Today's weather\")\nprint(generated)\n</code></pre>"},{"location":"models/qwen/#fine-tuning-training","title":"Fine-tuning Training","text":"<p>Refer to <code>apps/llm/train_sft_qwen.py</code> for complete SFT (Supervised Fine-tuning) implementation.</p>"},{"location":"models/qwen/#file-structure","title":"File Structure","text":"<ul> <li><code>genesis/models/qwen.py</code> - Model implementation</li> <li><code>apps/llm/qwen_model.py</code> - Training configuration and utilities</li> <li><code>apps/llm/train_sft_qwen.py</code> - SFT training script</li> <li><code>apps/llm/chat_qwen.py</code> - Inference chat script</li> </ul>"},{"location":"models/qwen/#related-resources","title":"Related Resources","text":"<ul> <li>Qwen Official Paper</li> <li>Mixed Precision Training Guide</li> <li>LLM Training Hands-on</li> </ul>"},{"location":"ops/","title":"Operation System Overview","text":"<p>Genesis v2.0 features a unified operation dispatch system that routes tensor operations to the appropriate backend implementations while maintaining a consistent API.</p>"},{"location":"ops/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>The operation system provides a clean abstraction layer between user-facing operations and backend-specific implementations:</p> <pre><code>graph TB\n    subgraph \"User Operations\"\n        A[genesis.add] --&gt; B[Operation Call]\n        C[genesis.matmul] --&gt; B\n        D[genesis.softmax] --&gt; B\n    end\n\n    subgraph \"Dispatch Layer\"\n        B --&gt; E[ops/dispatcher.py]\n        E --&gt; F[Device Detection]\n        E --&gt; G[Backend Selection]\n        E --&gt; H[Operation Routing]\n    end\n\n    subgraph \"Backend Operations\"\n        H --&gt; I[ops/cpu/basic.py]\n        H --&gt; J[ops/cuda/basic.py]\n        H --&gt; K[ops/cpu/reduction.py]\n        H --&gt; L[ops/cuda/reduction.py]\n    end\n\n    subgraph \"Implementation\"\n        I --&gt; M[PyTorch Ops]\n        J --&gt; N[Triton Kernels]\n        J --&gt; O[CUDA Kernels]\n    end\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style H fill:#fff3e0\n    style I fill:#e8f5e9\n    style J fill:#ffeb3b</code></pre>"},{"location":"ops/#key-components","title":"\ud83c\udfaf Key Components","text":""},{"location":"ops/#central-dispatcher","title":"Central Dispatcher","text":"<p>The <code>ops/dispatcher.py</code> module serves as the central routing hub: - Operation Registration: Maps operation names to implementations - Device Detection: Automatically determines target device - Backend Selection: Routes to appropriate backend - Type Validation: Ensures type compatibility</p>"},{"location":"ops/#operation-categories","title":"Operation Categories","text":""},{"location":"ops/#basic-operations","title":"Basic Operations","text":"<ul> <li>Location: <code>ops/{backend}/basic.py</code></li> <li>Operations: add, subtract, multiply, divide, power, abs</li> <li>Features: Broadcast support, in-place variants</li> </ul>"},{"location":"ops/#reduction-operations","title":"Reduction Operations","text":"<ul> <li>Location: <code>ops/{backend}/reduction.py</code></li> <li>Operations: sum, mean, max, min, argmax, argmin</li> <li>Features: Multi-dimensional reduction, keepdim support</li> </ul>"},{"location":"ops/#matrix-operations","title":"Matrix Operations","text":"<ul> <li>Location: <code>ops/{backend}/matrix.py</code></li> <li>Operations: matmul, transpose, reshape, flatten</li> <li>Features: Batch operations, memory-efficient implementations</li> </ul>"},{"location":"ops/#activation-functions","title":"Activation Functions","text":"<ul> <li>Location: <code>ops/{backend}/activation.py</code></li> <li>Operations: relu, sigmoid, tanh, softmax, gelu</li> <li>Features: In-place computation, gradient-friendly implementations</li> </ul>"},{"location":"ops/#dispatch-mechanism","title":"\ud83d\ude80 Dispatch Mechanism","text":""},{"location":"ops/#operation-registration","title":"Operation Registration","text":"Python<pre><code># ops/dispatcher.py\nclass OperationDispatcher:\n    \"\"\"Central operation dispatcher.\"\"\"\n\n    def __init__(self):\n        self._operations = {}\n        self._register_default_operations()\n\n    def register(self, name, cpu_impl, cuda_impl=None):\n        \"\"\"Register operation implementations.\"\"\"\n        self._operations[name] = {\n            'cpu': cpu_impl,\n            'cuda': cuda_impl or cpu_impl\n        }\n\n    def dispatch(self, op_name, *args, **kwargs):\n        \"\"\"Dispatch operation to appropriate backend.\"\"\"\n        # Determine device from arguments\n        device = self._infer_device(*args)\n\n        # Select implementation\n        impl = self._operations[op_name][device.type]\n\n        # Execute operation\n        return impl(*args, **kwargs)\n</code></pre>"},{"location":"ops/#automatic-device-inference","title":"Automatic Device Inference","text":"Python<pre><code>def _infer_device(*tensors):\n    \"\"\"Automatically infer target device from tensor arguments.\"\"\"\n    devices = set()\n\n    for tensor in tensors:\n        if hasattr(tensor, 'device'):\n            devices.add(tensor.device)\n\n    if len(devices) == 0:\n        return genesis.device('cpu')  # Default\n    elif len(devices) == 1:\n        return devices.pop()\n    else:\n        raise RuntimeError(f\"Mixed devices not supported: {devices}\")\n</code></pre>"},{"location":"ops/#backend-implementations","title":"\ud83d\udcbb Backend Implementations","text":""},{"location":"ops/#cpu-operations","title":"CPU Operations","text":"<p>CPU operations leverage PyTorch's optimized implementations:</p> Python<pre><code># ops/cpu/basic.py\ndef cpu_add(a, b, out=None):\n    \"\"\"CPU implementation of addition.\"\"\"\n    result = torch.add(a.data, b.data)\n\n    if out is not None:\n        out.data.copy_(result)\n        return out\n    else:\n        return genesis.tensor(result, device=a.device)\n\ndef cpu_matmul(a, b):\n    \"\"\"CPU implementation of matrix multiplication.\"\"\"\n    result = torch.matmul(a.data, b.data)\n    return genesis.tensor(result, device=a.device)\n</code></pre>"},{"location":"ops/#cuda-operations","title":"CUDA Operations","text":"<p>CUDA operations use custom Triton kernels for optimal performance:</p> Python<pre><code># ops/cuda/basic.py\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Triton kernel for element-wise addition.\"\"\"\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef cuda_add(a, b):\n    \"\"\"CUDA implementation using Triton kernel.\"\"\"\n    output = genesis.empty_like(a)\n    n_elements = a.numel()\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n    add_kernel[grid](a.data_ptr(), b.data_ptr(), output.data_ptr(), n_elements)\n\n    return output\n</code></pre>"},{"location":"ops/#configuration-and-extension","title":"\ud83d\udd27 Configuration and Extension","text":""},{"location":"ops/#registering-custom-operations","title":"Registering Custom Operations","text":"Python<pre><code>import genesis\n\n# Define custom operation\ndef my_custom_op_cpu(x):\n    \"\"\"Custom CPU operation.\"\"\"\n    return x * 2 + 1\n\ndef my_custom_op_cuda(x):\n    \"\"\"Custom CUDA operation.\"\"\"\n    # Custom CUDA implementation\n    pass\n\n# Register with dispatcher\ngenesis.ops.register_operation(\n    'my_custom_op',\n    cpu_impl=my_custom_op_cpu,\n    cuda_impl=my_custom_op_cuda\n)\n\n# Use the operation\nresult = genesis.ops.my_custom_op(tensor)\n</code></pre>"},{"location":"ops/#operation-metadata","title":"Operation Metadata","text":"Python<pre><code># Add metadata to operations\ngenesis.ops.set_operation_metadata('matmul', {\n    'requires_grad': True,\n    'supports_autograd': True,\n    'memory_efficient': True,\n    'fused_variants': ['matmul_add', 'matmul_relu']\n})\n</code></pre>"},{"location":"ops/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"ops/#operation-fusion","title":"Operation Fusion","text":"<p>The dispatcher supports operation fusion for improved performance:</p> Python<pre><code># Automatic fusion detection\ndef detect_fusion_opportunities(operations):\n    \"\"\"Detect operations that can be fused.\"\"\"\n    fusion_patterns = [\n        ('matmul', 'add'),      # Matrix multiplication + bias\n        ('conv2d', 'relu'),     # Convolution + activation\n        ('add', 'relu'),        # Addition + activation\n    ]\n\n    for pattern in fusion_patterns:\n        if matches_pattern(operations, pattern):\n            return create_fused_operation(pattern)\n\n    return None\n\n# Fused operation implementation\n@triton.jit\ndef fused_matmul_add_kernel(a_ptr, b_ptr, bias_ptr, output_ptr, ...):\n    \"\"\"Fused matrix multiplication and addition.\"\"\"\n    # Compute matmul and add bias in single kernel\n    pass\n</code></pre>"},{"location":"ops/#kernel-caching","title":"Kernel Caching","text":"Python<pre><code># Kernel compilation caching\nclass KernelCache:\n    \"\"\"Cache compiled kernels for reuse.\"\"\"\n\n    def __init__(self):\n        self._cache = {}\n\n    def get_kernel(self, op_name, input_shapes, dtypes):\n        \"\"\"Get cached kernel or compile new one.\"\"\"\n        cache_key = (op_name, input_shapes, dtypes)\n\n        if cache_key not in self._cache:\n            kernel = self._compile_kernel(op_name, input_shapes, dtypes)\n            self._cache[cache_key] = kernel\n\n        return self._cache[cache_key]\n</code></pre>"},{"location":"ops/#debugging-and-profiling","title":"\ud83d\udd0d Debugging and Profiling","text":""},{"location":"ops/#operation-tracing","title":"Operation Tracing","text":"Python<pre><code># Enable operation tracing\ngenesis.ops.enable_tracing(True)\n\n# Operations will now be traced\nx = genesis.tensor([1, 2, 3])\ny = genesis.tensor([4, 5, 6])\nz = x + y  # Traced: \"add: cpu, shapes=[(3,), (3,)], time=0.05ms\"\n\n# Get trace summary\ntrace = genesis.ops.get_trace()\nprint(trace.summary())\n</code></pre>"},{"location":"ops/#performance-profiling","title":"Performance Profiling","text":"Python<pre><code># Profile operations\nwith genesis.ops.profile() as prof:\n    # Your operations here\n    result = genesis.matmul(a, b)\n    result = genesis.relu(result)\n\n# Analyze results\nprof.print_stats()  # Shows timing breakdown by operation\nprof.export_chrome_trace(\"ops_profile.json\")  # Chrome profiler format\n</code></pre>"},{"location":"ops/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Operation Dispatcher</li> <li>CPU Operations</li> <li>CUDA Operations</li> <li>Backend System</li> <li>Performance Guide</li> </ul>"},{"location":"ops/cpu-ops/","title":"CPU Operations","text":"<p>CPU operation implementations provide CPU versions of all fundamental tensor operations.</p>"},{"location":"ops/cpu-ops/#overview","title":"\ud83d\udccb Overview","text":"<p>CPU operations leverage PyTorch's optimized implementations to ensure efficient execution on CPU.</p>"},{"location":"ops/cpu-ops/#operation-categories","title":"\ud83c\udfaf Operation Categories","text":""},{"location":"ops/cpu-ops/#basic-arithmetic","title":"Basic Arithmetic","text":"Python<pre><code># ops/cpu/basic.py\ndef cpu_add(a, b):\n    return torch.add(a.data, b.data)\n\ndef cpu_multiply(a, b):\n    return torch.mul(a.data, b.data)\n</code></pre>"},{"location":"ops/cpu-ops/#reduction-operations","title":"Reduction Operations","text":"Python<pre><code># ops/cpu/reduction.py\ndef cpu_sum(tensor, dim=None, keepdim=False):\n    return torch.sum(tensor.data, dim=dim, keepdim=keepdim)\n\ndef cpu_mean(tensor, dim=None, keepdim=False):\n    return torch.mean(tensor.data, dim=dim, keepdim=keepdim)\n</code></pre>"},{"location":"ops/cpu-ops/#matrix-operations","title":"Matrix Operations","text":"Python<pre><code># ops/cpu/matrix.py\ndef cpu_matmul(a, b):\n    return torch.matmul(a.data, b.data)\n\ndef cpu_transpose(tensor, dim0, dim1):\n    return torch.transpose(tensor.data, dim0, dim1)\n</code></pre>"},{"location":"ops/cpu-ops/#optimization-strategies","title":"\ud83d\ude80 Optimization Strategies","text":"<ul> <li>Vectorized operations</li> <li>Multi-threaded parallelization</li> <li>Cache-friendly memory access</li> </ul>"},{"location":"ops/cpu-ops/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Operation System Overview</li> <li>CUDA Operations</li> </ul>"},{"location":"ops/cuda-ops/","title":"CUDA Operations","text":"<p>CUDA operation implementations provide high-performance GPU operations using Triton and custom CUDA kernels.</p>"},{"location":"ops/cuda-ops/#overview","title":"\ud83d\udccb Overview","text":"<p>CUDA operations are optimized through custom kernels to achieve optimal GPU performance.</p>"},{"location":"ops/cuda-ops/#triton-kernels","title":"\ud83c\udfaf Triton Kernels","text":""},{"location":"ops/cuda-ops/#elementwise-operations","title":"Elementwise Operations","text":"Python<pre><code>@triton.jit\ndef elementwise_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"ops/cuda-ops/#reduction-kernels","title":"Reduction Kernels","text":"Python<pre><code>@triton.jit\ndef reduction_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    data = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    result = tl.sum(data)\n    tl.store(output_ptr + pid, result)\n</code></pre>"},{"location":"ops/cuda-ops/#optimization-features","title":"\ud83d\ude80 Optimization Features","text":"<ul> <li>Auto-tuning</li> <li>Kernel fusion</li> <li>Shared memory utilization</li> <li>Warp optimization</li> </ul>"},{"location":"ops/cuda-ops/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Operation System Overview</li> <li>CPU Operations</li> </ul>"},{"location":"ops/dispatcher/","title":"Operation Dispatcher","text":"<p>The operation dispatcher is a core component of Genesis v2.0, responsible for routing tensor operations to appropriate backend implementations.</p>"},{"location":"ops/dispatcher/#overview","title":"\ud83d\udccb Overview","text":"<p>The dispatcher provides: - Centralized operation routing - Automatic backend selection - Operation registration and management - Performance optimization opportunities</p>"},{"location":"ops/dispatcher/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<pre><code>graph TB\n    subgraph \"Dispatcher Components\"\n        A[OperationDispatcher] --&gt; B[Operation Registry]\n        A --&gt; C[Device Inference]\n        A --&gt; D[Backend Selector]\n        A --&gt; E[Execution Engine]\n    end\n\n    subgraph \"Operation Flow\"\n        F[User Call] --&gt; G[Dispatcher]\n        G --&gt; H[Device Detection]\n        H --&gt; I[Select Implementation]\n        I --&gt; J[Execute Operation]\n        J --&gt; K[Return Result]\n    end\n\n    style A fill:#e1f5fe\n    style G fill:#f3e5f5</code></pre>"},{"location":"ops/dispatcher/#core-features","title":"\ud83c\udfaf Core Features","text":""},{"location":"ops/dispatcher/#operation-dispatcher-class","title":"Operation Dispatcher Class","text":"Python<pre><code>class OperationDispatcher:\n    \"\"\"Central operation dispatch system.\"\"\"\n\n    def __init__(self):\n        self._operations = {}\n        self._metadata = {}\n        self._cache = {}\n\n    def register(self, name, implementations):\n        \"\"\"Register a new operation.\"\"\"\n        self._operations[name] = implementations\n\n    def dispatch(self, op_name, *args, **kwargs):\n        \"\"\"Dispatch operation to backend.\"\"\"\n        # 1. Validate operation exists\n        if op_name not in self._operations:\n            raise ValueError(f\"Unknown operation: {op_name}\")\n\n        # 2. Infer device\n        device = self._infer_device(args)\n\n        # 3. Select implementation\n        impl = self._select_implementation(op_name, device)\n\n        # 4. Execute operation\n        return impl(*args, **kwargs)\n</code></pre>"},{"location":"ops/dispatcher/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>Operation System Overview</li> <li>CPU Operations</li> <li>CUDA Operations</li> <li>Performance Optimization</li> </ul>"},{"location":"performance/gpu-operations/","title":"GPU Operations Performance Guide","text":"<p>This guide covers GPU operations optimization in Genesis, focusing on the modular GPU operations structure, Triton kernel implementation, and performance tuning strategies.</p>"},{"location":"performance/gpu-operations/#overview","title":"Overview","text":"<p>Genesis implements a sophisticated GPU backend with: - Modular GPU operations using Triton - Custom CUDA memory management - Adaptive block size optimization - Performance monitoring and profiling tools</p>"},{"location":"performance/gpu-operations/#architecture-overview","title":"Architecture Overview","text":""},{"location":"performance/gpu-operations/#modular-gpu-operations-structure","title":"Modular GPU Operations Structure","text":"<p>Genesis separates GPU operations into specialized modules:</p> Text Only<pre><code>genesis/ndarray/gpu_ops/\n\u251c\u2500\u2500 __init__.py          # Operation registry and dispatch\n\u251c\u2500\u2500 basic_ops.py         # Element-wise operations (add, mul, etc.)\n\u251c\u2500\u2500 tensor_ops.py        # Tensor operations (matmul, conv, etc.)  \n\u251c\u2500\u2500 random_ops.py        # Random number generation\n\u2514\u2500\u2500 reduction_ops.py     # Reduction operations (sum, mean, etc.)\n</code></pre>"},{"location":"performance/gpu-operations/#operation-dispatch-system","title":"Operation Dispatch System","text":"Python<pre><code># genesis/ndarray/gpu_ops/__init__.py\nfrom .basic_ops import add_triton, mul_triton, div_triton\nfrom .tensor_ops import matmul_triton, conv2d_triton  \nfrom .reduction_ops import sum_triton, mean_triton\n\n# Operation registry for dynamic dispatch\nGPU_OPS_REGISTRY = {\n    'add': add_triton,\n    'mul': mul_triton,\n    'div': div_triton,\n    'matmul': matmul_triton,\n    'sum': sum_triton,\n    'mean': mean_triton,\n}\n\ndef dispatch_gpu_op(op_name, *args, **kwargs):\n    \"\"\"Dispatch operation to appropriate GPU kernel.\"\"\"\n    if op_name not in GPU_OPS_REGISTRY:\n        raise NotImplementedError(f\"GPU operation {op_name} not implemented\")\n\n    return GPU_OPS_REGISTRY[op_name](*args, **kwargs)\n</code></pre>"},{"location":"performance/gpu-operations/#triton-kernel-implementation","title":"Triton Kernel Implementation","text":""},{"location":"performance/gpu-operations/#basic-element-wise-operations","title":"Basic Element-wise Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/basic_ops.py\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized element-wise addition kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load data with vectorization\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    # Compute\n    output = x + y\n\n    # Store result\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add_triton(x, y):\n    \"\"\"Triton-based tensor addition.\"\"\"\n    output = genesis.empty_like(x)\n    n_elements = x.numel()\n\n    # Adaptive block size based on tensor size\n    if n_elements &lt; 262144:  # &lt; 256K elements\n        BLOCK_SIZE = 256\n    elif n_elements &lt; 4194304:  # &lt; 4M elements  \n        BLOCK_SIZE = 512\n    else:\n        BLOCK_SIZE = 1024\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](\n        x.data_ptr(), y.data_ptr(), output.data_ptr(),\n        n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations/#advanced-tensor-operations","title":"Advanced Tensor Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/tensor_ops.py\n@triton.jit  \ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"High-performance matrix multiplication kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] &lt; K - k * BLOCK_SIZE_K))\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] &lt; K - k * BLOCK_SIZE_K))\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul_triton(a, b):\n    \"\"\"Optimized matrix multiplication using Triton.\"\"\"\n    assert a.shape[-1] == b.shape[-2], f\"Shape mismatch: {a.shape} @ {b.shape}\"\n\n    M, K = a.shape[-2:]\n    K2, N = b.shape[-2:]\n    assert K == K2\n\n    c = genesis.empty((*a.shape[:-2], M, N), dtype=a.dtype, device=a.device)\n\n    # Optimize block sizes based on problem size\n    if M &gt;= 2048 and N &gt;= 2048:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 128, 32\n    elif M &gt;= 512 and N &gt;= 512:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 64, 64, 32\n    else:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 32, 32, 32\n\n    grid = lambda meta: (\n        triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),\n    )\n\n    matmul_kernel[grid](\n        a.data_ptr(), b.data_ptr(), c.data_ptr(),\n        M, N, K,\n        a.stride(-2), a.stride(-1),\n        b.stride(-2), b.stride(-1),\n        c.stride(-2), c.stride(-1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N, \n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n</code></pre>"},{"location":"performance/gpu-operations/#memory-optimized-reduction-operations","title":"Memory-Optimized Reduction Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/reduction_ops.py\n@triton.jit\ndef sum_kernel(\n    input_ptr, output_ptr, \n    n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Memory-efficient summation kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load and sum in blocks\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    block_sum = tl.sum(x)\n\n    # Use atomic add for final reduction\n    tl.atomic_add(output_ptr, block_sum)\n\n@triton.jit\ndef reduce_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE_X: tl.constexpr,\n    BLOCK_SIZE_Y: tl.constexpr  \n):\n    \"\"\"2D reduction kernel with optimal memory access.\"\"\"\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n\n    offs_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offs_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n\n    mask_x = offs_x &lt; n_rows\n    mask_y = offs_y &lt; n_cols\n\n    # Load block\n    ptrs = input_ptr + offs_x[:, None] * n_cols + offs_y[None, :]\n    mask = mask_x[:, None] &amp; mask_y[None, :]\n    x = tl.load(ptrs, mask=mask, other=0.0)\n\n    # Reduce within block\n    result = tl.sum(x, axis=1)  # Sum across columns\n\n    # Store result\n    out_ptrs = output_ptr + offs_x\n    tl.store(out_ptrs, result, mask=mask_x)\n\ndef sum_triton(x, dim=None, keepdim=False):\n    \"\"\"Optimized tensor summation.\"\"\"\n    if dim is None:\n        # Global sum\n        result = genesis.zeros((), dtype=x.dtype, device=x.device)\n        n_elements = x.numel()\n\n        BLOCK_SIZE = min(1024, triton.next_power_of_2(n_elements))\n        grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n\n        sum_kernel[grid](\n            x.data_ptr(), result.data_ptr(),\n            n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n\n        return result\n\n    else:\n        # Dimension-specific reduction\n        # Implementation for specific dimension reduction\n        return reduce_along_dim(x, dim, keepdim)\n</code></pre>"},{"location":"performance/gpu-operations/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"performance/gpu-operations/#1-adaptive-block-size-optimization","title":"1. Adaptive Block Size Optimization","text":"Python<pre><code>class AdaptiveBlockSize:\n    \"\"\"Dynamically optimize block sizes based on tensor characteristics.\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n        self.performance_history = {}\n\n    def get_optimal_block_size(self, operation, tensor_size, dtype):\n        \"\"\"Get optimal block size for given operation and tensor.\"\"\"\n        cache_key = (operation, tensor_size, dtype.name)\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # Determine block size based on tensor size and operation\n        if operation == 'elementwise':\n            if tensor_size &lt; 262144:  # &lt; 256K elements\n                block_size = 256\n            elif tensor_size &lt; 4194304:  # &lt; 4M elements\n                block_size = 512  \n            else:\n                block_size = 1024\n\n        elif operation == 'matmul':\n            # Matrix multiplication specific optimization\n            sqrt_size = int(tensor_size ** 0.5)\n            if sqrt_size &lt; 512:\n                block_size = (32, 32, 32)\n            elif sqrt_size &lt; 2048:\n                block_size = (64, 64, 32)\n            else:\n                block_size = (128, 128, 32)\n\n        elif operation == 'reduction':\n            # Reduction operations optimization\n            block_size = min(1024, triton.next_power_of_2(tensor_size))\n\n        else:\n            # Default fallback\n            block_size = 512\n\n        self.cache[cache_key] = block_size\n        return block_size\n\n    def update_performance(self, operation, tensor_size, dtype, block_size, elapsed_time):\n        \"\"\"Update performance history for future optimization.\"\"\"\n        key = (operation, tensor_size, dtype.name, block_size)\n        if key not in self.performance_history:\n            self.performance_history[key] = []\n\n        self.performance_history[key].append(elapsed_time)\n\n        # Keep only recent measurements\n        if len(self.performance_history[key]) &gt; 10:\n            self.performance_history[key] = self.performance_history[key][-10:]\n\n# Global optimizer instance\nblock_optimizer = AdaptiveBlockSize()\n</code></pre>"},{"location":"performance/gpu-operations/#2-memory-access-pattern-optimization","title":"2. Memory Access Pattern Optimization","text":"Python<pre><code>@triton.jit\ndef coalesced_copy_kernel(\n    src_ptr, dst_ptr,\n    n_elements, stride_src, stride_dst,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Memory-coalesced tensor copy kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Ensure coalesced memory access\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load with proper stride\n    src_offsets = offsets * stride_src\n    dst_offsets = offsets * stride_dst\n\n    data = tl.load(src_ptr + src_offsets, mask=mask)\n    tl.store(dst_ptr + dst_offsets, data, mask=mask)\n\n@triton.jit  \ndef transpose_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Cache-friendly matrix transpose.\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # Tile-based transpose for better cache usage\n    row_start = (pid // (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n    col_start = (pid % (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n\n    rows = row_start + tl.arange(0, BLOCK_SIZE)\n    cols = col_start + tl.arange(0, BLOCK_SIZE)\n\n    row_mask = rows &lt; n_rows\n    col_mask = cols &lt; n_cols\n\n    # Load tile\n    input_offsets = rows[:, None] * n_cols + cols[None, :]\n    mask = row_mask[:, None] &amp; col_mask[None, :]\n\n    tile = tl.load(input_ptr + input_offsets, mask=mask)\n\n    # Store transposed tile\n    output_offsets = cols[:, None] * n_rows + rows[None, :]\n    tl.store(output_ptr + output_offsets, tl.trans(tile), mask=tl.trans(mask))\n</code></pre>"},{"location":"performance/gpu-operations/#3-kernel-fusion-optimization","title":"3. Kernel Fusion Optimization","text":"Python<pre><code>@triton.jit\ndef fused_linear_relu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    n_batch, n_input, n_output,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    \"\"\"Fused linear + ReLU kernel to reduce memory bandwidth.\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # Matrix multiplication logic (simplified)\n    # ... matmul computation ...\n\n    # Fused ReLU activation\n    result = tl.maximum(matmul_result + bias, 0.0)\n\n    # Single memory write\n    tl.store(output_ptr + offsets, result, mask=mask)\n\n@triton.jit\ndef fused_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    seq_len, head_dim,\n    scale: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Fused attention computation kernel.\"\"\"\n    # Compute attention scores\n    scores = compute_qk_scores(q_ptr, k_ptr, seq_len, head_dim)\n\n    # Apply scaling and softmax\n    scores = scores * scale\n    attention_weights = tl_softmax(scores, axis=-1)\n\n    # Apply attention to values\n    output = compute_attention_output(attention_weights, v_ptr, seq_len, head_dim)\n\n    # Single output write\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef fused_linear_relu(x, weight, bias):\n    \"\"\"Fused linear layer with ReLU activation.\"\"\"\n    batch_size, input_size = x.shape\n    output_size = weight.shape[0]\n\n    output = genesis.empty(batch_size, output_size, dtype=x.dtype, device=x.device)\n\n    # Optimal block sizes for fusion\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64  \n    BLOCK_SIZE_K = 32\n\n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']) * \n        triton.cdiv(output_size, meta['BLOCK_SIZE_N']),\n    )\n\n    fused_linear_relu_kernel[grid](\n        x.data_ptr(), weight.data_ptr(), bias.data_ptr(), output.data_ptr(),\n        batch_size, input_size, output_size,\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations/#performance-monitoring-and-profiling","title":"Performance Monitoring and Profiling","text":""},{"location":"performance/gpu-operations/#1-built-in-performance-metrics","title":"1. Built-in Performance Metrics","text":"Python<pre><code>import time\nimport contextlib\n\nclass GPUProfiler:\n    \"\"\"Profile GPU operations performance.\"\"\"\n\n    def __init__(self):\n        self.metrics = {}\n        self.current_operation = None\n\n    @contextlib.contextmanager\n    def profile_operation(self, operation_name):\n        \"\"\"Context manager for profiling operations.\"\"\"\n        self.current_operation = operation_name\n\n        # Synchronize before timing\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        try:\n            yield\n        finally:\n            genesis.cuda.synchronize()\n            end_time = time.perf_counter()\n\n            elapsed = end_time - start_time\n            if operation_name not in self.metrics:\n                self.metrics[operation_name] = []\n\n            self.metrics[operation_name].append(elapsed)\n\n    def get_stats(self, operation_name=None):\n        \"\"\"Get performance statistics.\"\"\"\n        if operation_name:\n            times = self.metrics.get(operation_name, [])\n            if not times:\n                return None\n\n            return {\n                'mean': sum(times) / len(times),\n                'min': min(times),\n                'max': max(times),\n                'count': len(times)\n            }\n        else:\n            stats = {}\n            for op_name in self.metrics:\n                stats[op_name] = self.get_stats(op_name)\n            return stats\n\n    def print_summary(self):\n        \"\"\"Print performance summary.\"\"\"\n        print(\"GPU Operation Performance Summary:\")\n        print(\"-\" * 50)\n\n        for op_name, stats in self.get_stats().items():\n            print(f\"{op_name}:\")\n            print(f\"  Mean: {stats['mean']*1000:.3f}ms\")\n            print(f\"  Min:  {stats['min']*1000:.3f}ms\")\n            print(f\"  Max:  {stats['max']*1000:.3f}ms\")\n            print(f\"  Count: {stats['count']}\")\n            print()\n\n# Global profiler instance\ngpu_profiler = GPUProfiler()\n\n# Example usage in operations\ndef add_with_profiling(x, y):\n    with gpu_profiler.profile_operation('add'):\n        return add_triton(x, y)\n</code></pre>"},{"location":"performance/gpu-operations/#2-memory-bandwidth-analysis","title":"2. Memory Bandwidth Analysis","text":"Python<pre><code>def analyze_memory_bandwidth(operation_func, tensor_sizes, dtype=genesis.float32):\n    \"\"\"Analyze memory bandwidth utilization for operations.\"\"\"\n\n    results = []\n    theoretical_bandwidth = 1555e9  # A800 HBM2e bandwidth in bytes/s\n\n    for size in tensor_sizes:\n        # Create test tensors\n        if isinstance(size, tuple):\n            x = genesis.randn(*size, dtype=dtype, device='cuda')\n            y = genesis.randn(*size, dtype=dtype, device='cuda')\n        else:\n            x = genesis.randn(size, dtype=dtype, device='cuda')\n            y = genesis.randn(size, dtype=dtype, device='cuda')\n\n        # Calculate theoretical bytes transferred\n        bytes_per_element = dtype.itemsize\n        total_elements = x.numel()\n\n        # For binary operations: read 2 tensors + write 1 tensor\n        total_bytes = total_elements * bytes_per_element * 3\n\n        # Warm up\n        for _ in range(5):\n            _ = operation_func(x, y)\n\n        # Time operation\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        num_iterations = 10\n        for _ in range(num_iterations):\n            result = operation_func(x, y)\n\n        genesis.cuda.synchronize()\n        end_time = time.perf_counter()\n\n        # Calculate metrics\n        elapsed_time = (end_time - start_time) / num_iterations\n        achieved_bandwidth = total_bytes / elapsed_time\n        bandwidth_efficiency = achieved_bandwidth / theoretical_bandwidth\n\n        results.append({\n            'size': size,\n            'elements': total_elements,\n            'elapsed_ms': elapsed_time * 1000,\n            'bandwidth_gb_s': achieved_bandwidth / 1e9,\n            'efficiency_percent': bandwidth_efficiency * 100,\n            'theoretical_gb_s': theoretical_bandwidth / 1e9\n        })\n\n        print(f\"Size {size}: {achieved_bandwidth/1e9:.1f} GB/s ({bandwidth_efficiency:.1%})\")\n\n    return results\n\n# Analyze add operation performance\nsizes = [(256, 256), (1024, 1024), (2048, 2048), (4096, 4096)]\nbandwidth_results = analyze_memory_bandwidth(add_triton, sizes)\n</code></pre>"},{"location":"performance/gpu-operations/#3-automated-performance-tuning","title":"3. Automated Performance Tuning","text":"Python<pre><code>class AutoTuner:\n    \"\"\"Automatically tune kernel parameters for optimal performance.\"\"\"\n\n    def __init__(self):\n        self.best_configs = {}\n\n    def tune_kernel(self, kernel_func, test_inputs, param_space):\n        \"\"\"Auto-tune kernel parameters.\"\"\"\n        best_time = float('inf')\n        best_config = None\n\n        print(f\"Tuning kernel with {len(param_space)} configurations...\")\n\n        for i, config in enumerate(param_space):\n            try:\n                # Warm up\n                for _ in range(3):\n                    _ = kernel_func(*test_inputs, **config)\n\n                # Time execution\n                genesis.cuda.synchronize()\n                start_time = time.perf_counter()\n\n                num_runs = 10\n                for _ in range(num_runs):\n                    result = kernel_func(*test_inputs, **config)\n\n                genesis.cuda.synchronize()\n                end_time = time.perf_counter()\n\n                elapsed = (end_time - start_time) / num_runs\n\n                if elapsed &lt; best_time:\n                    best_time = elapsed\n                    best_config = config\n\n                print(f\"Config {i+1}: {elapsed*1000:.3f}ms - {config}\")\n\n            except Exception as e:\n                print(f\"Config {i+1} failed: {e}\")\n\n        print(f\"Best configuration: {best_config} ({best_time*1000:.3f}ms)\")\n        return best_config, best_time\n\n# Example auto-tuning for matrix multiplication\ndef tune_matmul():\n    # Define parameter space\n    block_sizes = [\n        {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64},\n    ]\n\n    # Test inputs\n    a = genesis.randn(1024, 1024, device='cuda')\n    b = genesis.randn(1024, 1024, device='cuda')\n\n    # Run auto-tuner\n    tuner = AutoTuner()\n    best_config, best_time = tuner.tune_kernel(\n        matmul_triton, [a, b], block_sizes\n    )\n\n    return best_config\n</code></pre>"},{"location":"performance/gpu-operations/#best-practices","title":"Best Practices","text":""},{"location":"performance/gpu-operations/#1-kernel-development-guidelines","title":"1. Kernel Development Guidelines","text":"<ul> <li>Memory Coalescing: Ensure contiguous memory access patterns</li> <li>Block Size Optimization: Use powers of 2, consider occupancy</li> <li>Register Usage: Monitor register spilling with large kernels</li> <li>Shared Memory: Use shared memory for data reuse</li> <li>Divergence Minimization: Avoid conditional branches when possible</li> </ul>"},{"location":"performance/gpu-operations/#2-performance-optimization-checklist","title":"2. Performance Optimization Checklist","text":"<ul> <li> Profile memory bandwidth utilization</li> <li> Optimize block sizes for target GPU</li> <li> Minimize kernel launch overhead</li> <li> Use kernel fusion for related operations</li> <li> Monitor GPU occupancy and resource usage</li> <li> Validate numerical accuracy after optimization</li> </ul>"},{"location":"performance/gpu-operations/#3-debugging-gpu-operations","title":"3. Debugging GPU Operations","text":"Python<pre><code>def debug_gpu_operation(operation_func, *inputs):\n    \"\"\"Debug GPU operation with detailed analysis.\"\"\"\n\n    print(\"GPU Operation Debug Information:\")\n    print(\"=\" * 40)\n\n    # Input analysis\n    for i, inp in enumerate(inputs):\n        print(f\"Input {i}:\")\n        print(f\"  Shape: {inp.shape}\")\n        print(f\"  Dtype: {inp.dtype}\")\n        print(f\"  Device: {inp.device}\")\n        print(f\"  Memory usage: {inp.numel() * inp.dtype.itemsize / 1e6:.1f} MB\")\n        print(f\"  Contiguous: {inp.is_contiguous()}\")\n        print()\n\n    # GPU memory status\n    print(\"GPU Memory Status:\")\n    print(f\"  Allocated: {genesis.cuda.memory_allocated() / 1e6:.1f} MB\")\n    print(f\"  Cached: {genesis.cuda.memory_cached() / 1e6:.1f} MB\")\n    print()\n\n    # Execute operation with profiling\n    genesis.cuda.synchronize()\n    start_time = time.perf_counter()\n\n    result = operation_func(*inputs)\n\n    genesis.cuda.synchronize()\n    end_time = time.perf_counter()\n\n    # Results analysis\n    print(\"Operation Results:\")\n    print(f\"  Execution time: {(end_time - start_time) * 1000:.3f}ms\")\n    print(f\"  Output shape: {result.shape}\")\n    print(f\"  Output dtype: {result.dtype}\")\n    print(f\"  Output device: {result.device}\")\n    print()\n\n    # Numerical validation\n    print(\"Numerical Validation:\")\n    print(f\"  Min value: {result.min().item():.6f}\")\n    print(f\"  Max value: {result.max().item():.6f}\")\n    print(f\"  Mean value: {result.mean().item():.6f}\")\n    print(f\"  Has NaN: {genesis.isnan(result).any().item()}\")\n    print(f\"  Has Inf: {genesis.isinf(result).any().item()}\")\n\n    return result\n\n# Example usage\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.randn(1000, 1000, device='cuda')\nresult = debug_gpu_operation(add_triton, x, y)\n</code></pre> <p>This comprehensive guide covers the modular GPU operations architecture in Genesis, providing detailed implementation examples and optimization strategies for maximum performance.</p>"},{"location":"performance/optimization-guide/","title":"Genesis Performance Optimization Guide","text":""},{"location":"performance/optimization-guide/#overview","title":"Overview","text":"<p>This document provides a comprehensive guide to the performance characteristics, current implementation status, and optimization strategies of the Genesis framework. Genesis is designed as a lightweight deep learning framework that pursues competitive performance while maintaining educational value.</p>"},{"location":"performance/optimization-guide/#current-performance-status","title":"Current Performance Status","text":""},{"location":"performance/optimization-guide/#element-wise-operation-add-benchmark-results","title":"Element-wise Operation (ADD) Benchmark Results","text":"<p>Test Environment: - GPU: NVIDIA A800-SXM4-80GB - Memory: 79.3 GB - Theoretical Bandwidth: 1555 GB/s - Test Date: August 2025</p> <p>Performance Summary: - Average Efficiency: 18.0% theoretical bandwidth utilization - Best Performance: 33.1% (batch tensors) - Worst Performance: 3.1% (large tensors) - Overall Status: Development phase</p>"},{"location":"performance/optimization-guide/#performance-by-tensor-size-category","title":"Performance by Tensor Size Category","text":"Category Average Efficiency Status vs PyTorch Small Tensors (64K-262K) 18.9% \u274c Critical 0.19x Medium Tensors (4.2M) 29.6% \ud83d\udd34 Poor 0.27-0.32x Large Tensors (16.8M) 4.7% \u274c Critical 0.03-0.06x XLarge Tensors (67M) 5.4% \u274c Critical 0.05-0.06x Batch Processing 31.2% \ud83d\udd34 Poor 0.29-0.33x"},{"location":"performance/optimization-guide/#detailed-performance-data","title":"Detailed Performance Data","text":"Shape Size PyTorch Genesis Speed Ratio Efficiency Status 256\u00d7256 65.5K 0.019ms 0.104ms 0.19x 18.7% \u274c Critical 2048\u00d72048 4.2M 0.053ms 0.166ms 0.32x 32.0% \ud83d\udd34 Poor 4096\u00d74096 16.8M 0.147ms 2.334ms 0.06x 6.3% \u274c Critical 8192\u00d78192 67M 0.478ms 8.208ms 0.06x 5.8% \u274c Critical"},{"location":"performance/optimization-guide/#architecture-implementation","title":"Architecture Implementation","text":""},{"location":"performance/optimization-guide/#current-add-operation-implementation","title":"Current ADD Operation Implementation","text":"<p>Genesis uses a dual backend architecture: - CPU Backend: PyTorch tensor operations - GPU Backend: Custom CUDA + Triton kernels</p>"},{"location":"performance/optimization-guide/#gpu-kernel-implementation","title":"GPU Kernel Implementation","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized addition kernel for same-shape tensors with better memory access\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"performance/optimization-guide/#adaptive-block-size-configuration","title":"Adaptive Block Size Configuration","text":"<p>Current optimization configuration:</p> Python<pre><code>BLOCK_SIZE_CONFIGS = {\n    (0, 262144): 256,         # Small tensors: smaller blocks improve cache utilization\n    (262144, 4194304): 512,   # Medium tensors: balance occupancy and cache\n    (4194304, float('inf')): 1024,  # Large tensors: larger blocks improve bandwidth\n}\n</code></pre>"},{"location":"performance/optimization-guide/#performance-bottleneck-analysis","title":"Performance Bottleneck Analysis","text":""},{"location":"performance/optimization-guide/#1-primary-bottleneck-triton-kernel-performance","title":"1. Primary Bottleneck: Triton Kernel Performance","text":"<ul> <li>Kernel Overhead: 23.6x slower than PyTorch</li> <li>Root Cause: Triton kernel efficiency far below PyTorch's optimized CUDA kernels</li> <li>Impact: Most severe for large tensors (&gt;16M elements)</li> </ul>"},{"location":"performance/optimization-guide/#2-memory-bandwidth-utilization","title":"2. Memory Bandwidth Utilization","text":"<ul> <li>PyTorch: 71.4% bandwidth efficiency</li> <li>Genesis: 18.0% average efficiency  </li> <li>Theoretical Maximum: 1555 GB/s (A800 HBM2e)</li> </ul> <p>Issues: - Memory access patterns not sufficiently optimized - Large kernels may have register spillage - Memory coalesced access not well optimized</p>"},{"location":"performance/optimization-guide/#3-gpu-occupancy-issues","title":"3. GPU Occupancy Issues","text":"<ul> <li>Block size configuration not achieving optimal occupancy</li> <li>XLarge tensors show significant GPU utilization drop</li> <li>Resource limitations prevent full SM utilization</li> </ul>"},{"location":"performance/optimization-guide/#optimization-roadmap","title":"Optimization Roadmap","text":""},{"location":"performance/optimization-guide/#phase-1-immediate-improvements-completed","title":"Phase 1: Immediate Improvements (Completed)","text":"<p>\u2705 Completed: - Simplified adaptive block size configuration - Professional benchmarking infrastructure - Performance analysis tools</p> <p>\ud83d\udcca Results: - Average efficiency improved from 5.7% to 18.0% - Medium/batch tensors achieving 29-33% efficiency</p>"},{"location":"performance/optimization-guide/#phase-2-kernel-optimization-in-progress","title":"Phase 2: Kernel Optimization (In Progress)","text":"<p>\ud83c\udfaf Target Areas: - Memory access pattern optimization (vectorization, cache-friendly tiling) - Automatic block size tuning - Kernel fusion to reduce memory bandwidth pressure</p>"},{"location":"performance/optimization-guide/#phase-3-advanced-optimization-future","title":"Phase 3: Advanced Optimization (Future)","text":"<ul> <li>Custom CUDA kernel hand optimization</li> <li>Memory layout optimization</li> <li>Multi-GPU support</li> </ul>"},{"location":"performance/optimization-guide/#usage-recommendations","title":"Usage Recommendations","text":""},{"location":"performance/optimization-guide/#genesis-vs-pytorch-choice","title":"Genesis vs PyTorch Choice","text":"<p>Recommend Using Genesis: - Educational learning and framework understanding - Medium batch processing operations (best performance 31% efficiency) - Research requiring custom kernel development</p> <p>Recommend Using PyTorch: - Production environments with maximum performance requirements - Large tensor operations (&gt;16M elements) - Applications sensitive to 5-25x performance differences</p>"},{"location":"performance/optimization-guide/#performance-tips","title":"Performance Tips","text":"<ol> <li>Tensor Size Awareness</li> <li>Optimal performance range: 1M-4M elements</li> <li>Avoid xlarge tensors (&gt;67M)</li> <li> <p>Consider tensor splitting for large operations</p> </li> <li> <p>Memory Management Python<pre><code># Use in-place operations\nresult = genesis.add(a, b, out=existing_tensor)\n</code></pre></p> </li> </ol>"},{"location":"performance/optimization-guide/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"performance/optimization-guide/#built-in-benchmarking","title":"Built-in Benchmarking","text":"Bash<pre><code># Quick performance check\npython benchmark/bench_ops.py --op add --fast\n\n# Comprehensive analysis\npython benchmark/bench_ops.py --op add --size large\n</code></pre>"},{"location":"performance/optimization-guide/#key-metrics","title":"Key Metrics","text":"<ul> <li>Memory Bandwidth Efficiency: Target &gt;50%</li> <li>GPU Utilization: Monitor with <code>nvidia-smi</code></li> <li>Kernel Launch Overhead: Analyze with Nsight Compute</li> </ul>"},{"location":"performance/optimization-guide/#performance-targets","title":"Performance Targets","text":"Tensor Category Minimum Efficiency Target Efficiency Small Tensors 15% 25% Medium Tensors 25% 40% Large Tensors 10% 30% XLarge Tensors 10% 25% Batch Processing 25% 45% <p>Last Updated: August 2025 Framework Version: Genesis 0.3.0-dev Benchmark Environment: A800-SXM4-80GB</p>"},{"location":"training/advanced-features/","title":"Advanced Training Features","text":"<p>Genesis provides several advanced features to enhance training efficiency and model performance.</p>"},{"location":"training/advanced-features/#mixed-precision-training-amp","title":"\ud83d\ude80 Mixed Precision Training (AMP)","text":"<p>Automatic Mixed Precision (AMP) allows you to train models faster with lower memory usage by utilizing FP16/BF16 computations where appropriate while maintaining FP32 master weights for numerical stability.</p>"},{"location":"training/advanced-features/#basic-usage","title":"Basic Usage","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\n\n# Create model and optimizer\nmodel = nn.Linear(1024, 512)\noptimizer = optim.Adam(model.parameters())\n\n# Initialize gradient scaler for mixed precision\nscaler = GradScaler()\n\n# Training loop with AMP\nfor data, target in dataloader:\n    optimizer.zero_grad()\n\n    # Use autocast for automatic mixed precision\n    with autocast():\n        output = model(data)\n        loss = criterion(output, target)\n\n    # Scale the loss and backward pass\n    scaler.scale(loss).backward()\n\n    # Unscale and step optimizer\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"training/advanced-features/#supported-data-types","title":"Supported Data Types","text":"<p>Genesis supports multiple precision formats:</p> <ul> <li>float16 (FP16): Half precision, fastest on most GPUs</li> <li>bfloat16 (BF16): Brain floating point, better range than FP16</li> <li>float32 (FP32): Single precision, default for master weights</li> </ul>"},{"location":"training/advanced-features/#benefits","title":"Benefits","text":"<ul> <li>Speed: Up to 2x faster training on modern GPUs</li> <li>Memory: Reduced memory usage allows larger batch sizes</li> <li>Accuracy: Maintains model accuracy with loss scaling</li> </ul>"},{"location":"training/advanced-features/#gradient-clipping","title":"\u2702\ufe0f Gradient Clipping","text":"<p>Gradient clipping helps prevent gradient explosion in deep networks and improves training stability, especially for RNNs and Transformers.</p>"},{"location":"training/advanced-features/#gradient-norm-clipping","title":"Gradient Norm Clipping","text":"<p>Clips gradients when their L2 norm exceeds a threshold:</p> Python<pre><code>import genesis.nn.utils as nn_utils\n\n# During training\nloss.backward()\n\n# Clip gradients by norm (recommended for most cases)\nnn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\noptimizer.step()\n</code></pre>"},{"location":"training/advanced-features/#gradient-value-clipping","title":"Gradient Value Clipping","text":"<p>Clips gradient values to a specific range:</p> Python<pre><code># Clip gradients by value\nnn_utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n</code></pre>"},{"location":"training/advanced-features/#when-to-use","title":"When to Use","text":"<ul> <li>Gradient Norm Clipping: Recommended for RNNs, LSTMs, and Transformers</li> <li>Gradient Value Clipping: Useful when you need hard limits on gradient values</li> <li>Typical Values: max_norm between 0.5 and 5.0 for most models</li> </ul>"},{"location":"training/advanced-features/#learning-rate-schedulers","title":"\ud83d\udcc8 Learning Rate Schedulers","text":"<p>Learning rate schedulers adjust the learning rate during training to improve convergence and final model performance.</p>"},{"location":"training/advanced-features/#steplr","title":"StepLR","text":"<p>Decays learning rate by gamma every step_size epochs:</p> Python<pre><code>from genesis.optim.lr_scheduler import StepLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # Decay LR every 30 epochs\n</code></pre>"},{"location":"training/advanced-features/#exponentiallr","title":"ExponentialLR","text":"<p>Decays learning rate exponentially:</p> Python<pre><code>from genesis.optim.lr_scheduler import ExponentialLR\n\nscheduler = ExponentialLR(optimizer, gamma=0.95)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # LR = LR * 0.95 each epoch\n</code></pre>"},{"location":"training/advanced-features/#cosineannealinglr","title":"CosineAnnealingLR","text":"<p>Uses cosine annealing schedule:</p> Python<pre><code>from genesis.optim.lr_scheduler import CosineAnnealingLR\n\n# T_max: Maximum number of iterations\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()\n</code></pre>"},{"location":"training/advanced-features/#custom-learning-rate-schedule","title":"Custom Learning Rate Schedule","text":"<p>You can also implement custom schedules:</p> Python<pre><code>def custom_lr_lambda(epoch):\n    # Warmup for first 10 epochs, then decay\n    if epoch &lt; 10:\n        return epoch / 10\n    else:\n        return 0.95 ** (epoch - 10)\n\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n</code></pre>"},{"location":"training/advanced-features/#checkpointing","title":"\ud83d\udcbe Checkpointing","text":"<p>Save and restore model states during training for fault tolerance and model deployment.</p>"},{"location":"training/advanced-features/#saving-checkpoints","title":"Saving Checkpoints","text":"Python<pre><code>import genesis\n\n# Save model state\ngenesis.save_checkpoint({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    'best_accuracy': best_acc\n}, 'checkpoint_epoch_10.pth')\n</code></pre>"},{"location":"training/advanced-features/#loading-checkpoints","title":"Loading Checkpoints","text":"Python<pre><code># Load checkpoint\ncheckpoint = genesis.load_checkpoint('checkpoint_epoch_10.pth')\n\n# Restore model and optimizer states\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n</code></pre>"},{"location":"training/advanced-features/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Saving: Save checkpoints every N epochs</li> <li>Best Model Tracking: Keep the best performing model</li> <li>Metadata Storage: Include training configuration and metrics</li> </ol> Python<pre><code># Example: Save best model during training\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    val_loss = validate(model, val_loader)\n\n    if val_loss &lt; best_loss:\n        best_loss = val_loss\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'best_loss': best_loss\n        }, 'best_model.pth')\n</code></pre>"},{"location":"training/advanced-features/#complete-training-example","title":"\ud83d\udd27 Complete Training Example","text":"<p>Here's a complete example combining all advanced features:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\nfrom genesis.optim.lr_scheduler import CosineAnnealingLR\nimport genesis.nn.utils as nn_utils\n\n# Model setup\nmodel = YourModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\nscaler = GradScaler()\n\n# Training configuration\nmax_grad_norm = 1.0\ncheckpoint_interval = 10\n\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        # Mixed precision forward pass\n        with autocast():\n            output = model(data)\n            loss = criterion(output, target)\n\n        # Scaled backward pass\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        scaler.unscale_(optimizer)\n        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        # Optimizer step with scaling\n        scaler.step(optimizer)\n        scaler.update()\n\n    # Update learning rate\n    scheduler.step()\n\n    # Save checkpoint\n    if epoch % checkpoint_interval == 0:\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n        }, f'checkpoint_epoch_{epoch}.pth')\n</code></pre>"},{"location":"training/advanced-features/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"training/advanced-features/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Use gradient accumulation for larger effective batch sizes</li> <li>Enable gradient checkpointing for very deep models</li> <li>Use mixed precision training to reduce memory usage</li> </ul>"},{"location":"training/advanced-features/#speed-optimization","title":"Speed Optimization","text":"<ul> <li>Use the appropriate data type (FP16 for speed, BF16 for stability)</li> <li>Tune gradient accumulation steps</li> <li>Profile your training loop to identify bottlenecks</li> </ul>"},{"location":"training/advanced-features/#convergence-tips","title":"Convergence Tips","text":"<ul> <li>Start with a learning rate finder to identify optimal LR</li> <li>Use warmup for large batch training</li> <li>Monitor gradient norms to detect instability early</li> </ul>"},{"location":"training/advanced-features/#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>Basic Training Tutorial</li> <li>Performance Tuning Guide</li> <li>Model Architecture Guide</li> <li>Optimizer Documentation</li> </ul>"},{"location":"tutorials/","title":"Tutorial Overview","text":"<p>Welcome to the Genesis deep learning framework tutorial series! These tutorials will help you master all aspects of the Genesis framework from beginner to expert.</p>"},{"location":"tutorials/#tutorial-categories","title":"\ud83d\udcda Tutorial Categories","text":""},{"location":"tutorials/#basic-tutorials","title":"\ud83c\udfaf Basic Tutorials","text":"<p>Suitable for beginners, covering Genesis basic concepts and usage.</p> <ul> <li>Basic Training Tutorial - Learn how to train your first neural network with Genesis</li> <li>Data Processing Tutorial - Data loading, preprocessing, and pipeline construction</li> <li>Model Definition Tutorial - How to define and organize neural network models</li> </ul>"},{"location":"tutorials/#advanced-tutorials","title":"\ud83d\ude80 Advanced Tutorials","text":"<p>Deep dive into Genesis advanced features and optimization techniques.</p> <ul> <li>Mixed Precision Training - Use AMP to accelerate training and save memory</li> <li>Custom Operators - Implement custom neural network operations</li> <li>Performance Tuning - Optimize training performance and memory usage</li> <li>Distributed Training - Multi-GPU parallel training implementation</li> </ul>"},{"location":"tutorials/#practical-projects","title":"\ud83d\udee0\ufe0f Practical Projects","text":"<p>Learn Genesis practical applications through complete projects.</p> <ul> <li>LLM Training Hands-on - Use Genesis to train large language models</li> <li>Mixed Precision Training - AMP techniques in deep learning</li> </ul>"},{"location":"tutorials/#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"tutorials/#beginner-path-1-2-weeks","title":"Beginner Path (1-2 weeks)","text":"<ol> <li>Installation and Environment Setup</li> <li>First Program </li> <li>Basic Training Tutorial</li> <li>Data Processing Tutorial</li> </ol>"},{"location":"tutorials/#advanced-user-path-2-4-weeks","title":"Advanced User Path (2-4 weeks)","text":"<ol> <li>Complete beginner path</li> <li>Custom Operators</li> <li>Mixed Precision Training </li> <li>Performance Tuning</li> <li>Distributed Training</li> </ol>"},{"location":"tutorials/#researcher-path-4-8-weeks","title":"Researcher Path (4-8 weeks)","text":"<ol> <li>Complete advanced user path</li> <li>Deep Architecture Understanding</li> <li>Core Component Source Analysis</li> <li>LLM Training Hands-on</li> <li>Contributing Code</li> </ol>"},{"location":"tutorials/#tutorial-features","title":"\ud83d\udca1 Tutorial Features","text":"<ul> <li>\ud83c\udfaf Practical-oriented - Every tutorial includes complete runnable code</li> <li>\ud83d\udcca Performance comparison - Performance comparison and analysis with other frameworks</li> <li>\ud83d\udd0d Source code analysis - Deep understanding of Genesis internal implementation principles</li> <li>\u26a1 Best practices - Summary of experience and techniques from real projects</li> </ul>"},{"location":"tutorials/#contributing-tutorials","title":"\ud83e\udd1d Contributing Tutorials","text":"<p>We welcome the community to contribute more high-quality tutorials!</p>"},{"location":"tutorials/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the project to your GitHub account</li> <li>Create new Markdown files in the <code>docs/tutorials/</code> directory</li> <li>Write content following the format of existing tutorials</li> <li>Submit a Pull Request</li> </ol>"},{"location":"tutorials/#tutorial-standards","title":"Tutorial Standards","text":"<ul> <li>Clear titles and structure - Use appropriate heading levels</li> <li>Complete code examples - Ensure code can run directly</li> <li>Detailed explanations - Explain the principles and purpose of each step</li> <li>Practical application scenarios - Combine real use cases</li> </ul>"},{"location":"tutorials/#getting-help","title":"\ud83d\udcde Getting Help","text":"<p>Encountered problems during learning?</p> <ul> <li>GitHub Issues - Report issues or suggest improvements</li> <li>Discussions - Discuss technical issues with the community</li> <li>API Documentation - View detailed API reference documentation</li> </ul> <p>Learning Suggestions</p> <p>It's recommended to follow the recommended learning path progressively, and practice hands-on after completing each tutorial to deepen understanding.</p> <p>Ready to start learning?</p> <p>Start Basic Tutorial View API Documentation</p>"},{"location":"tutorials/amp-training/","title":"Automatic Mixed Precision Training","text":"<p>Learn how to use AMP (Automatic Mixed Precision) training in Genesis for better performance and memory efficiency.</p>"},{"location":"tutorials/amp-training/#overview","title":"Overview","text":"<p>Genesis supports mixed precision training with FP16 and BF16 data types to accelerate training and reduce memory usage.</p>"},{"location":"tutorials/amp-training/#basic-amp-training","title":"Basic AMP Training","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nfrom genesis.amp import GradScaler, autocast\n\nmodel = MyModel()\noptimizer = genesis.optim.Adam(model.parameters())\nscaler = GradScaler()\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n\n    with autocast():\n        output = model(batch.data)\n        loss = criterion(output, batch.targets)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"tutorials/amp-training/#advanced-configuration","title":"Advanced Configuration","text":"<p>This tutorial is under construction. More detailed examples and best practices will be added.</p>"},{"location":"tutorials/amp-training/#see-also","title":"See Also","text":"<ul> <li>Mixed Precision Training - Detailed mixed precision guide</li> <li>Performance Tuning - General performance optimization</li> </ul>"},{"location":"tutorials/basic-training/","title":"Basic Training Tutorial","text":"<p>This tutorial will take you from zero to building and training your first neural network using the Genesis deep learning framework. We will learn Genesis core concepts and usage through a complete image classification project.</p>"},{"location":"tutorials/basic-training/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>Through this tutorial, you will learn: - Genesis basic APIs and data structures - How to define and train neural network models - Data loading and preprocessing - Building and optimizing training loops - Model evaluation and saving</p>"},{"location":"tutorials/basic-training/#environment-setup","title":"\ud83d\udee0\ufe0f Environment Setup","text":""},{"location":"tutorials/basic-training/#install-dependencies","title":"Install Dependencies","text":"Bash<pre><code># Ensure Genesis is installed\npip install torch triton numpy matplotlib tqdm\ngit clone https://github.com/phonism/genesis.git\ncd genesis\npip install -e .\n</code></pre>"},{"location":"tutorials/basic-training/#verify-installation","title":"Verify Installation","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Test basic functionality\nx = genesis.randn(2, 3)\nprint(f\"Genesis tensor created: {x.shape}\")\nprint(f\"Genesis modules available: {dir(nn)}\")\n</code></pre>"},{"location":"tutorials/basic-training/#project-handwritten-digit-recognition","title":"\ud83d\udcca Project: Handwritten Digit Recognition","text":"<p>We will build a handwritten digit recognition system using a simple fully connected neural network on synthetic data to demonstrate Genesis capabilities.</p>"},{"location":"tutorials/basic-training/#1-data-preparation","title":"1. Data Preparation","text":"<p>Since Genesis doesn't have built-in data loading utilities yet, we'll create synthetic data that mimics the MNIST structure:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleDataset:\n    \"\"\"Simple dataset class for demonstration\"\"\"\n\n    def __init__(self, num_samples=1000, input_dim=784, num_classes=10):\n        # Generate synthetic data similar to flattened MNIST\n        self.data = genesis.randn(num_samples, input_dim)\n\n        # Create labels based on data patterns (synthetic)\n        labels = genesis.randn(num_samples, num_classes)\n        self.labels = genesis.functional.max(labels, axis=1, keepdims=False)\n\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def get_batch(self, batch_size=32, start_idx=0):\n        \"\"\"Get a batch of data\"\"\"\n        end_idx = min(start_idx + batch_size, self.num_samples)\n        return (self.data[start_idx:end_idx], \n                self.labels[start_idx:end_idx])\n\n# Create datasets\ntrain_dataset = SimpleDataset(num_samples=800, input_dim=784, num_classes=10)\ntest_dataset = SimpleDataset(num_samples=200, input_dim=784, num_classes=10)\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\nprint(f\"Input dimension: 784 (28x28 flattened)\")\nprint(f\"Number of classes: 10\")\n</code></pre>"},{"location":"tutorials/basic-training/#2-model-definition","title":"2. Model Definition","text":"<p>We'll build a simple but effective fully connected neural network using Genesis modules:</p> Python<pre><code>class MNISTNet(nn.Module):\n    \"\"\"Simple fully connected network for digit recognition\"\"\"\n\n    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):\n        super(MNISTNet, self).__init__()\n\n        # Define layers using actual Genesis modules\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, num_classes)\n\n        # Activation and regularization\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # Flatten input if needed\n        if len(x.shape) &gt; 2:\n            x = x.view(x.shape[0], -1)\n\n        # First hidden layer\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        # Second hidden layer\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        # Output layer\n        x = self.fc3(x)\n\n        return x\n\n# Create model instance\nmodel = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)\n\nprint(\"Model structure:\")\nprint(f\"Layer 1: {model.fc1}\")\nprint(f\"Layer 2: {model.fc2}\")\nprint(f\"Layer 3: {model.fc3}\")\nprint(f\"Total parameters: {sum(p.data.size for p in model.parameters())}\")\n</code></pre>"},{"location":"tutorials/basic-training/#3-loss-function-and-optimizer","title":"3. Loss Function and Optimizer","text":"Python<pre><code># Define loss function and optimizer using Genesis\ncriterion = nn.SoftmaxLoss()  # Use Genesis SoftmaxLoss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(f\"Loss function: {criterion}\")\nprint(f\"Optimizer: {optimizer}\")\nprint(f\"Learning rate: 0.001\")\n</code></pre>"},{"location":"tutorials/basic-training/#4-training-loop","title":"4. Training Loop","text":"Python<pre><code>def train_epoch(model, dataset, criterion, optimizer, batch_size=32):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()  # Set to training mode\n\n    total_loss = 0.0\n    num_batches = len(dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch data\n        start_idx = i * batch_size\n        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)\n\n        # Forward pass\n        outputs = model(batch_data)\n        loss = criterion(outputs, batch_labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Apply gradient clipping (optional)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update weights\n        optimizer.step()\n\n        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n    return total_loss / num_batches\n\ndef evaluate(model, dataset, criterion, batch_size=32):\n    \"\"\"Evaluate model performance\"\"\"\n    model.eval()  # Set to evaluation mode\n\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = len(dataset) // batch_size\n\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)\n\n        # Forward pass (no gradients needed)\n        outputs = model(batch_data)\n        loss = criterion(outputs, batch_labels)\n\n        # Calculate accuracy\n        predicted = genesis.functional.max(outputs, axis=1, keepdims=False)\n        total += batch_labels.shape[0]\n        correct += (predicted == batch_labels).sum().data\n\n        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n    accuracy = correct / total\n    avg_loss = total_loss / num_batches\n\n    return avg_loss, accuracy\n\n# Training configuration\nnum_epochs = 10\nbatch_size = 32\n\nprint(\"Starting training...\")\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Batch size: {batch_size}\")\nprint(\"-\" * 50)\n\n# Training loop\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Train for one epoch\n    train_loss = train_epoch(model, train_dataset, criterion, optimizer, batch_size)\n\n    # Evaluate on test set\n    test_loss, test_accuracy = evaluate(model, test_dataset, criterion, batch_size)\n\n    # Record metrics\n    train_losses.append(train_loss)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_accuracy)\n\n    # Print progress\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Test Loss: {test_loss:.4f}\")\n    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n    print(\"-\" * 30)\n\nprint(\"Training completed!\")\n</code></pre>"},{"location":"tutorials/basic-training/#5-model-evaluation-and-visualization","title":"5. Model Evaluation and Visualization","text":"Python<pre><code># Plot training progress\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\n# Plot losses\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot accuracy\nplt.subplot(1, 2, 2)\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Test Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Final evaluation\nfinal_test_loss, final_test_accuracy = evaluate(model, test_dataset, criterion, batch_size)\nprint(f\"\\nFinal Results:\")\nprint(f\"Test Loss: {final_test_loss:.4f}\")\nprint(f\"Test Accuracy: {final_test_accuracy:.4f}\")\n</code></pre>"},{"location":"tutorials/basic-training/#6-model-saving-and-loading","title":"6. Model Saving and Loading","text":"Python<pre><code># Save model using Genesis serialization\nmodel_path = \"mnist_model.pkl\"\ngenesis.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")\n\n# Load model\nmodel_new = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)\nmodel_new.load_state_dict(genesis.load(model_path))\nprint(\"Model loaded successfully!\")\n\n# Verify loaded model works\ntest_loss, test_accuracy = evaluate(model_new, test_dataset, criterion, batch_size)\nprint(f\"Loaded model accuracy: {test_accuracy:.4f}\")\n</code></pre>"},{"location":"tutorials/basic-training/#key-concepts-learned","title":"\ud83c\udf93 Key Concepts Learned","text":""},{"location":"tutorials/basic-training/#1-genesis-tensor-operations","title":"1. Genesis Tensor Operations","text":"<ul> <li>Creating tensors with <code>genesis.randn()</code>, <code>genesis.tensor()</code></li> <li>Basic operations like matrix multiplication and element-wise operations</li> <li>Automatic differentiation with <code>requires_grad</code></li> </ul>"},{"location":"tutorials/basic-training/#2-neural-network-modules","title":"2. Neural Network Modules","text":"<ul> <li>Defining models by inheriting from <code>nn.Module</code></li> <li>Using built-in layers: <code>nn.Linear</code>, <code>nn.ReLU</code>, <code>nn.Dropout</code></li> <li>Understanding forward pass implementation</li> </ul>"},{"location":"tutorials/basic-training/#3-training-process","title":"3. Training Process","text":"<ul> <li>Setting up loss functions and optimizers</li> <li>Implementing training and evaluation loops</li> <li>Using gradient clipping and regularization</li> </ul>"},{"location":"tutorials/basic-training/#4-model-management","title":"4. Model Management","text":"<ul> <li>Saving and loading model state with Genesis serialization</li> <li>Managing model parameters and optimization state</li> </ul>"},{"location":"tutorials/basic-training/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>After completing this tutorial, you can:</p> <ol> <li>Explore more complex models - Try different architectures with more layers</li> <li>Learn advanced features - Explore mixed precision training and learning rate scheduling</li> <li>Work with real data - Integrate with actual datasets when data loading utilities are available</li> <li>Performance optimization - Learn about GPU acceleration and Triton kernel usage</li> </ol>"},{"location":"tutorials/basic-training/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Genesis API Reference - Complete API documentation</li> <li>Advanced Training Features - Mixed precision, schedulers, etc.</li> <li>Performance Optimization - Tips for faster training</li> </ul>"},{"location":"tutorials/basic-training/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/basic-training/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Ensure Genesis is properly installed with <code>pip install -e .</code></li> <li>Shape mismatches: Check tensor dimensions in forward pass</li> <li>Memory issues: Reduce batch size if encountering out-of-memory errors</li> <li>Slow training: Enable GPU support when available</li> </ol>"},{"location":"tutorials/basic-training/#getting-help","title":"Getting Help","text":"<ul> <li>Check the Genesis Documentation</li> <li>Report issues on GitHub Issues</li> <li>Join discussions in the community forums</li> </ul>"},{"location":"tutorials/custom-ops/","title":"Custom Operator Development","text":"<p>Under Development</p> <p>This document is being written and content will be continuously updated.</p> <p>The Genesis framework supports custom operator development, allowing you to implement specialized neural network operations. This tutorial will teach you how to create high-performance custom operators from scratch.</p>"},{"location":"tutorials/custom-ops/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Genesis operator system architecture</li> <li>Learn to implement CPU and GPU versions of custom operators</li> <li>Master Triton kernel programming techniques</li> <li>Learn operator optimization and performance debugging methods</li> </ul>"},{"location":"tutorials/custom-ops/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before starting, please ensure you have: - Completed the Basic Training Tutorial - Understanding of CUDA programming basics - Familiarity with Python C extension development</p>"},{"location":"tutorials/custom-ops/#development-environment","title":"\ud83d\udee0\ufe0f Development Environment","text":"Bash<pre><code># Install development dependencies\npip install triton pybind11 cmake ninja\n</code></pre>"},{"location":"tutorials/custom-ops/#example-rmsnorm-operator","title":"\ud83d\udcdd Example: RMSNorm Operator","text":"<p>We will implement RMSNorm (Root Mean Square Normalization) as an example.</p>"},{"location":"tutorials/custom-ops/#cpu-implementation","title":"CPU Implementation","text":"Python<pre><code># WIP: CPU implementation code will be added in future versions\n</code></pre>"},{"location":"tutorials/custom-ops/#gpu-implementation-triton","title":"GPU Implementation (Triton)","text":"Python<pre><code># WIP: Triton implementation code will be added in future versions\n</code></pre>"},{"location":"tutorials/custom-ops/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":"<ul> <li>Automatic differentiation support</li> <li>Memory optimization techniques</li> <li>Operator fusion strategies</li> </ul> <p>\ud83d\udcd8 Documentation Status: Under development, expected to be completed in v0.2.0.</p>"},{"location":"tutorials/data-loading/","title":"Data Loading in Genesis","text":"<p>Learn how to efficiently load and preprocess data for training with Genesis.</p>"},{"location":"tutorials/data-loading/#overview","title":"Overview","text":"<p>Genesis provides flexible data loading utilities compatible with various data sources and formats.</p>"},{"location":"tutorials/data-loading/#basic-data-loading","title":"Basic Data Loading","text":"Python<pre><code>import genesis\nfrom genesis.data import DataLoader\n\n# Create a simple dataset\ndataset = MyDataset()  # Your custom dataset\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nfor batch in loader:\n    data, targets = batch\n    # Process batch...\n</code></pre>"},{"location":"tutorials/data-loading/#custom-datasets","title":"Custom Datasets","text":"<p>This tutorial is under construction. More examples and detailed documentation will be added soon.</p>"},{"location":"tutorials/data-loading/#see-also","title":"See Also","text":"<ul> <li>Basic Training - Training loops with data loaders</li> <li>Performance Tuning - Optimizing data loading performance</li> </ul>"},{"location":"tutorials/distributed-training/","title":"Distributed Training with Genesis","text":"<p>Learn how to scale your training across multiple GPUs and nodes using Genesis.</p>"},{"location":"tutorials/distributed-training/#overview","title":"Overview","text":"<p>Genesis provides complete distributed training support, including:</p> <ul> <li>NCCL Backend - High-performance GPU communication</li> <li>DistributedDataParallel (DDP) - Data parallel training wrapper</li> <li>Collective Communication Operations - all_reduce, broadcast, all_gather, etc.</li> <li>Single-Process Testing - Convenient for development and debugging</li> </ul>"},{"location":"tutorials/distributed-training/#quick-start","title":"Quick Start","text":""},{"location":"tutorials/distributed-training/#1-basic-distributed-training-setup","title":"1. Basic Distributed Training Setup","text":"Python<pre><code>import genesis\nimport genesis.distributed as dist\nimport genesis.nn as nn\n\n# Initialize distributed process group\ndist.init_process_group(backend='nccl', world_size=2, rank=0)  # Adjust rank per process\n\n# Create model\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(512, 256)\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = self.relu(self.linear(x))\n        return self.output(x)\n\nmodel = MyModel()\n\n# Wrap as distributed data parallel model\ndevice = genesis.device('cuda')\nddp_model = dist.DistributedDataParallel(model, device_ids=[device.index])\n</code></pre>"},{"location":"tutorials/distributed-training/#2-distributed-training-loop","title":"2. Distributed Training Loop","text":"Python<pre><code># Optimizer and loss function\noptimizer = genesis.optim.Adam(ddp_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\nddp_model.train()\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        # Move data to GPU\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = ddp_model(data)\n        loss = criterion(outputs, targets)\n\n        # Backward pass (gradients automatically synchronized)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"tutorials/distributed-training/#3-process-group-management","title":"3. Process Group Management","text":"Python<pre><code># Check distributed status\nif dist.is_initialized():\n    print(f\"Process group initialized\")\n    print(f\"World size: {dist.get_world_size()}\")\n    print(f\"Current rank: {dist.get_rank()}\")\n\n# Synchronize all processes\ndist.barrier()\n\n# Cleanup\ndist.destroy_process_group()\n</code></pre>"},{"location":"tutorials/distributed-training/#advanced-features","title":"Advanced Features","text":""},{"location":"tutorials/distributed-training/#collective-communication-operations","title":"Collective Communication Operations","text":"Python<pre><code>import genesis\n\n# Create test tensor\ndevice = genesis.device('cuda')\ntensor = genesis.ones([4], dtype=genesis.float32, device=device)\n\n# all_reduce - Aggregate across all processes\ndist.all_reduce(tensor, dist.ReduceOp.SUM)  # Sum\ndist.all_reduce(tensor, dist.ReduceOp.MAX)  # Maximum\ndist.all_reduce(tensor, dist.ReduceOp.MIN)  # Minimum\n\n# broadcast - Broadcast operation\nbroadcast_tensor = genesis.randn([8], device=device)\ndist.broadcast(broadcast_tensor, src=0)  # Broadcast from rank 0\n\n# all_gather - Gather data from all processes\ninput_tensor = genesis.randn([4, 8], device=device)\noutput_list = [genesis.zeros_like(input_tensor) for _ in range(dist.get_world_size())]\ndist.all_gather(output_list, input_tensor)\n</code></pre>"},{"location":"tutorials/distributed-training/#single-process-testing-mode","title":"Single-Process Testing Mode","text":"Python<pre><code># Single-process mode for development and debugging\ndef test_single_process():\n    # Initialize single-process distributed environment\n    dist.init_process_group(backend=\"nccl\", world_size=1, rank=0)\n\n    # Create and test model\n    model = MyModel()\n    ddp_model = dist.DistributedDataParallel(model, device_ids=[0])\n\n    # Test forward pass\n    input_data = genesis.randn([8, 512], device='cuda')\n    output = ddp_model(input_data)\n\n    # Test backward pass\n    loss = output.sum()\n    loss.backward()\n\n    print(\"Single-process distributed test successful!\")\n    dist.destroy_process_group()\n\n# Run test\nif __name__ == \"__main__\":\n    test_single_process()\n</code></pre>"},{"location":"tutorials/distributed-training/#multi-gpu-training-scripts","title":"Multi-GPU Training Scripts","text":""},{"location":"tutorials/distributed-training/#launcherpy","title":"launcher.py","text":"Python<pre><code>#!/usr/bin/env python3\n\"\"\"\nMulti-GPU training launcher script\nUsage: python launcher.py --gpus 2\n\"\"\"\n\nimport argparse\nimport subprocess\nimport sys\nimport os\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--gpus', type=int, default=1, help='Number of GPUs')\n    parser.add_argument('--script', type=str, default='train.py', help='Training script')\n    args = parser.parse_args()\n\n    processes = []\n\n    try:\n        for rank in range(args.gpus):\n            env = os.environ.copy()\n            env['CUDA_VISIBLE_DEVICES'] = str(rank)\n            env['RANK'] = str(rank)\n            env['WORLD_SIZE'] = str(args.gpus)\n\n            cmd = [sys.executable, args.script]\n            proc = subprocess.Popen(cmd, env=env)\n            processes.append(proc)\n\n        # Wait for all processes to complete\n        for proc in processes:\n            proc.wait()\n\n    except KeyboardInterrupt:\n        print(\"Stopping training...\")\n        for proc in processes:\n            proc.terminate()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/distributed-training/#trainpy","title":"train.py","text":"Python<pre><code>#!/usr/bin/env python3\n\"\"\"\nDistributed training main script\n\"\"\"\n\nimport os\nimport genesis\nimport genesis.distributed as dist\nimport genesis.nn as nn\n\ndef main():\n    # Get distributed parameters from environment variables\n    rank = int(os.environ.get('RANK', 0))\n    world_size = int(os.environ.get('WORLD_SIZE', 1))\n\n    # Initialize distributed training\n    dist.init_process_group(\n        backend='nccl',\n        world_size=world_size,\n        rank=rank\n    )\n\n    print(f\"Process {rank}/{world_size} started\")\n\n    try:\n        # Create model\n        model = create_model()\n        ddp_model = dist.DistributedDataParallel(\n            model, \n            device_ids=[genesis.cuda.current_device()]\n        )\n\n        # Create optimizer\n        optimizer = genesis.optim.Adam(ddp_model.parameters(), lr=0.001)\n\n        # Training loop\n        train_loop(ddp_model, optimizer, rank)\n\n    finally:\n        # Cleanup distributed environment\n        dist.destroy_process_group()\n\ndef create_model():\n    \"\"\"Create model\"\"\"\n    return nn.Sequential([\n        nn.Linear(784, 512),\n        nn.ReLU(),\n        nn.Linear(512, 256), \n        nn.ReLU(),\n        nn.Linear(256, 10)\n    ])\n\ndef train_loop(model, optimizer, rank):\n    \"\"\"Training loop\"\"\"\n    model.train()\n\n    for epoch in range(10):\n        # Simulate training data\n        data = genesis.randn([32, 784], device='cuda')\n        targets = genesis.randint(0, 10, [32], device='cuda')\n\n        # Forward pass\n        outputs = model(data)\n        loss = nn.functional.cross_entropy(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if rank == 0:  # Only print on main process\n            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/distributed-training/#error-handling-and-debugging","title":"Error Handling and Debugging","text":""},{"location":"tutorials/distributed-training/#common-issues","title":"Common Issues","text":"<ol> <li> <p>NCCL Not Available Python<pre><code>try:\n    dist.init_process_group(backend=\"nccl\", world_size=1, rank=0)\nexcept RuntimeError as e:\n    if \"NCCL library not available\" in str(e):\n        print(\"NCCL library not available, please check CUDA and NCCL installation\")\n    else:\n        raise\n</code></pre></p> </li> <li> <p>Process Group Not Initialized Python<pre><code>if not dist.is_initialized():\n    print(\"Error: Distributed process group not initialized\")\n    print(\"Please call dist.init_process_group() first\")\n</code></pre></p> </li> <li> <p>Device Mismatch Python<pre><code># Ensure model and data are on the same device\ndevice = genesis.device(f'cuda:{genesis.cuda.current_device()}')\nmodel = model.to(device)\ndata = data.to(device)\n</code></pre></p> </li> </ol>"},{"location":"tutorials/distributed-training/#performance-optimization-tips","title":"Performance Optimization Tips","text":""},{"location":"tutorials/distributed-training/#1-gradient-accumulation","title":"1. Gradient Accumulation","text":"Python<pre><code>accumulation_steps = 4\n\nfor i, batch in enumerate(dataloader):\n    outputs = ddp_model(batch['input'])\n    loss = criterion(outputs, batch['target']) / accumulation_steps\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"tutorials/distributed-training/#2-mixed-precision-training","title":"2. Mixed Precision Training","text":"Python<pre><code># Combine automatic mixed precision with distributed training\nscaler = genesis.amp.GradScaler()\n\nwith genesis.amp.autocast():\n    outputs = ddp_model(data)\n    loss = criterion(outputs, targets)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"tutorials/distributed-training/#3-communication-optimization","title":"3. Communication Optimization","text":"Python<pre><code># Enable gradient compression when creating DDP\nddp_model = dist.DistributedDataParallel(\n    model,\n    device_ids=[device.index],\n    find_unused_parameters=False,  # Improve performance\n    gradient_as_bucket_view=True   # Reduce memory usage\n)\n</code></pre>"},{"location":"tutorials/distributed-training/#see-also","title":"See Also","text":"<ul> <li>Advanced Features - Advanced training techniques</li> <li>Performance Tuning - Optimizing distributed performance</li> </ul>"},{"location":"tutorials/llm-training/","title":"Large Language Model Training Guide","text":"<p>This comprehensive guide covers training large language models (LLMs) using Genesis's Qwen implementation, from basic setup to advanced optimization techniques.</p>"},{"location":"tutorials/llm-training/#overview","title":"Overview","text":"<p>Genesis provides LLM training capabilities through two approaches: 1. Pure Genesis Implementation: Native Qwen model in <code>genesis.models.qwen</code> 2. Hybrid Approach: Integration with PyTorch/Transformers for production training</p> <p>This guide covers both approaches, starting with the pure Genesis implementation and then showing the production-ready hybrid approach used in <code>apps/llm/</code>.</p>"},{"location":"tutorials/llm-training/#prerequisites","title":"Prerequisites","text":"<ul> <li>GPU with at least 16GB VRAM (A100/A800 recommended for large models)</li> <li>CUDA 11.8+ and appropriate drivers</li> <li>Python 3.8+ with Genesis installed</li> <li>For hybrid approach: PyTorch and Transformers library</li> </ul> Bash<pre><code>pip install torch transformers datasets accelerate\n</code></pre>"},{"location":"tutorials/llm-training/#approach-1-pure-genesis-implementation","title":"Approach 1: Pure Genesis Implementation","text":""},{"location":"tutorials/llm-training/#1-model-configuration-and-setup","title":"1. Model Configuration and Setup","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nfrom genesis.models.qwen import ModelArgs, Transformer\nimport numpy as np\n\n# Configure Qwen model for educational purposes\nconfig = ModelArgs(\n    block_size=2048,          # Context length\n    vocab_size=32000,         # Vocabulary size\n    n_layer=12,               # Number of transformer layers\n    num_attention_heads=12,   # Number of attention heads\n    hidden_size=768,          # Hidden dimension\n    intermediate_size=3072,   # Feed-forward dimension\n    num_key_value_heads=12,   # Key-value heads (for GQA)\n    head_dim=64,              # Attention head dimension\n    rope_base=10000,          # RoPE base frequency\n    max_position_embeddings=2048\n)\n\nprint(f\"Model configuration:\")\nprint(f\"  Layers: {config.n_layer}\")\nprint(f\"  Hidden size: {config.hidden_size}\")\nprint(f\"  Attention heads: {config.num_attention_heads}\")\nprint(f\"  Vocabulary size: {config.vocab_size}\")\n</code></pre>"},{"location":"tutorials/llm-training/#2-model-instantiation","title":"2. Model Instantiation","text":"Python<pre><code># Create Qwen model using Genesis\nmodel = Transformer(config)\n\n# Calculate model parameters\ntotal_params = sum(p.data.size for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.1f}M\")\n\n# Initialize weights\ndef init_weights(module):\n    \"\"\"Initialize model weights\"\"\"\n    if isinstance(module, nn.Linear):\n        # Initialize weights with small random values\n        module.weight.data = genesis.randn(*module.weight.shape) * 0.02\n        if module.bias is not None:\n            module.bias.data = genesis.zeros(*module.bias.shape)\n\n# Apply weight initialization\nmodel.apply(init_weights)\nprint(\"Model weights initialized\")\n</code></pre>"},{"location":"tutorials/llm-training/#3-simple-data-preparation","title":"3. Simple Data Preparation","text":"Python<pre><code># Simple synthetic data for demonstration\nclass SimpleTextDataset:\n    \"\"\"Simple dataset for language modeling\"\"\"\n\n    def __init__(self, vocab_size=32000, seq_length=512, num_samples=1000):\n        self.vocab_size = vocab_size\n        self.seq_length = seq_length\n        self.num_samples = num_samples\n\n        # Generate random token sequences\n        self.data = genesis.tensor(\n            np.random.randint(0, vocab_size, (num_samples, seq_length))\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def get_batch(self, batch_size=4, start_idx=0):\n        \"\"\"Get a batch of sequences\"\"\"\n        end_idx = min(start_idx + batch_size, self.num_samples)\n        batch_data = self.data[start_idx:end_idx]\n\n        # For language modeling: input = tokens[:-1], target = tokens[1:]\n        input_ids = batch_data[:, :-1]\n        labels = batch_data[:, 1:]\n\n        return input_ids, labels\n\n# Create dataset\ndataset = SimpleTextDataset(vocab_size=config.vocab_size, seq_length=512, num_samples=100)\nprint(f\"Dataset created with {len(dataset)} samples\")\n</code></pre>"},{"location":"tutorials/llm-training/#4-training-setup","title":"4. Training Setup","text":"Python<pre><code>import genesis.optim as optim\n\n# Set up optimizer and loss\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.SoftmaxLoss()\n\nprint(f\"Optimizer: {type(optimizer).__name__}\")\nprint(f\"Learning rate: 1e-4\")\nprint(f\"Loss function: {type(criterion).__name__}\")\n</code></pre>"},{"location":"tutorials/llm-training/#5-training-loop","title":"5. Training Loop","text":"Python<pre><code>def train_step(model, input_ids, labels, criterion, optimizer):\n    \"\"\"Single training step\"\"\"\n    # Forward pass\n    logits = model(input_ids)\n\n    # Reshape for loss calculation\n    # logits: [batch_size, seq_len, vocab_size]\n    # labels: [batch_size, seq_len]\n    batch_size, seq_len, vocab_size = logits.shape\n    logits_flat = logits.view(batch_size * seq_len, vocab_size)\n    labels_flat = labels.view(batch_size * seq_len)\n\n    # Calculate loss\n    loss = criterion(logits_flat, labels_flat)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Gradient clipping\n    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.data ** 2\n    total_norm = total_norm ** 0.5\n\n    # Clip gradients\n    clip_coef = min(1.0, 1.0 / max(total_norm, 1e-6))\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.data = p.grad.data * clip_coef\n\n    # Update weights\n    optimizer.step()\n\n    return loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n# Training configuration\nnum_epochs = 5\nbatch_size = 2  # Small batch size for demo\nlog_interval = 10\n\nprint(\"Starting Genesis Qwen training...\")\nprint(f\"Epochs: {num_epochs}, Batch size: {batch_size}\")\nprint(\"-\" * 50)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    num_batches = len(dataset) // batch_size\n\n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * batch_size\n        input_ids, labels = dataset.get_batch(batch_size, start_idx)\n\n        # Training step\n        loss = train_step(model, input_ids, labels, criterion, optimizer)\n        total_loss += loss\n\n        if batch_idx % log_interval == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{num_batches}, Loss: {loss:.4f}\")\n\n    avg_loss = total_loss / num_batches\n    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n    print(\"-\" * 30)\n\nprint(\"Pure Genesis training completed!\")\n</code></pre>"},{"location":"tutorials/llm-training/#approach-2-production-hybrid-training","title":"Approach 2: Production Hybrid Training","text":"<p>For production use, Genesis integrates with PyTorch and Transformers library. This is the approach used in <code>apps/llm/train_sft_qwen.py</code>.</p>"},{"location":"tutorials/llm-training/#1-setup-and-dependencies","title":"1. Setup and Dependencies","text":"Python<pre><code>import torch\nimport torch.nn as nn\nfrom transformers import (\n    AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n    Trainer, TrainingArguments\n)\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport pandas as pd\nimport os\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\nprint(f\"Vocabulary size: {len(tokenizer)}\")\n</code></pre>"},{"location":"tutorials/llm-training/#2-data-preparation-for-sft","title":"2. Data Preparation for SFT","text":"Python<pre><code>class ConversationDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning with conversation format\"\"\"\n\n    def __init__(self, data_path, tokenizer, max_length=2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.conversations = self.load_conversations(data_path)\n\n    def load_conversations(self, data_path):\n        \"\"\"Load conversation data from JSON lines\"\"\"\n        conversations = []\n\n        # Example conversation format\n        sample_conversations = [\n            {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n                    {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}\n                ]\n            },\n            {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n                    {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks...\"}\n                ]\n            }\n        ]\n\n        return sample_conversations\n\n    def format_conversation(self, messages):\n        \"\"\"Format conversation into training text\"\"\"\n        text = \"\"\n        for i, message in enumerate(messages):\n            role = message[\"role\"]\n            content = message[\"content\"].strip()\n\n            if i != len(messages) - 1:\n                text += f\"&lt;|im_start|&gt;{role}\\n{content}&lt;|im_end|&gt;\\n\"\n            else:\n                # Last message (assistant response)\n                text += f\"&lt;|im_start|&gt;{role}\\n{content}&lt;|im_end|&gt;\"\n\n        return text\n\n    def __len__(self):\n        return len(self.conversations)\n\n    def __getitem__(self, idx):\n        conversation = self.conversations[idx]\n        text = self.format_conversation(conversation[\"messages\"])\n\n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n\n        # For language modeling, labels are the same as input_ids\n        labels = input_ids.clone()\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n\n# Create dataset\ndataset = ConversationDataset(\"./data\", tokenizer)\nprint(f\"Dataset created with {len(dataset)} conversations\")\n</code></pre>"},{"location":"tutorials/llm-training/#3-model-setup-with-transformers","title":"3. Model Setup with Transformers","text":"Python<pre><code># Load pre-trained Qwen model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-0.5B\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(f\"Model loaded: {model.config.model_type}\")\nprint(f\"Parameters: {model.num_parameters() / 1e6:.1f}M\")\n</code></pre>"},{"location":"tutorials/llm-training/#4-training-configuration","title":"4. Training Configuration","text":"Python<pre><code># Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen_sft_output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=100,\n    max_steps=1000,\n    learning_rate=5e-5,\n    fp16=True,  # Mixed precision training\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    evaluation_strategy=\"steps\",\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=None,  # Disable wandb logging\n    gradient_checkpointing=True,\n    dataloader_drop_last=True,\n    remove_unused_columns=False,\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Mixed precision: {training_args.fp16}\")\n</code></pre>"},{"location":"tutorials/llm-training/#5-trainer-setup-and-training","title":"5. Trainer Setup and Training","text":"Python<pre><code># Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    eval_dataset=dataset,  # Using same dataset for demo\n    tokenizer=tokenizer,\n)\n\nprint(\"Trainer created successfully\")\n\n# Start training\nprint(\"Starting supervised fine-tuning...\")\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./qwen_sft_final\")\ntokenizer.save_pretrained(\"./qwen_sft_final\")\n\nprint(\"Training completed and model saved!\")\n</code></pre>"},{"location":"tutorials/llm-training/#6-inference-with-trained-model","title":"6. Inference with Trained Model","text":"Python<pre><code># Load trained model for inference\nfrom transformers import pipeline\n\n# Create text generation pipeline\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"./qwen_sft_final\",\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Test the model\ntest_prompt = \"&lt;|im_start|&gt;user\\nWhat is artificial intelligence?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n\nresponse = generator(\n    test_prompt,\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(\"Generated response:\")\nprint(response[0][\"generated_text\"])\n</code></pre>"},{"location":"tutorials/llm-training/#advanced-features","title":"Advanced Features","text":""},{"location":"tutorials/llm-training/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Genesis supports mixed precision training for both approaches:</p> Python<pre><code># For pure Genesis (in development)\ngenesis.enable_autocast = True\n\n# For hybrid approach (using PyTorch)\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"tutorials/llm-training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"Python<pre><code># Genesis optimizer with learning rate scheduling\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000\n)\n\n# Update learning rate after each step\nscheduler.step()\n</code></pre>"},{"location":"tutorials/llm-training/#model-checkpointing","title":"Model Checkpointing","text":"Python<pre><code># Save Genesis model\ngenesis.save_checkpoint({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'epoch': epoch,\n    'loss': loss,\n}, 'checkpoint.pkl')\n\n# Load Genesis model\ncheckpoint = genesis.load_checkpoint('checkpoint.pkl')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n</code></pre>"},{"location":"tutorials/llm-training/#performance-tips","title":"Performance Tips","text":"<ol> <li>Batch Size: Start with smaller batch sizes (2-4) for Genesis implementation</li> <li>Gradient Accumulation: Use gradient accumulation for effective larger batch sizes</li> <li>Mixed Precision: Enable FP16 to reduce memory usage and increase speed</li> <li>Gradient Clipping: Prevent gradient explosion in transformer training</li> <li>Learning Rate: Use warmup and cosine decay scheduling</li> </ol>"},{"location":"tutorials/llm-training/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/llm-training/#common-issues","title":"Common Issues","text":"<ol> <li>Out of Memory: Reduce batch size or sequence length</li> <li>Gradient Explosion: Enable gradient clipping</li> <li>Slow Convergence: Check learning rate and warmup schedule</li> <li>NaN Loss: Reduce learning rate or check data quality</li> </ol>"},{"location":"tutorials/llm-training/#memory-optimization","title":"Memory Optimization","text":"Python<pre><code># Reduce memory usage\n- Use smaller batch sizes\n- Enable gradient checkpointing\n- Use mixed precision (FP16)\n- Reduce sequence length\n- Use gradient accumulation instead of large batches\n</code></pre>"},{"location":"tutorials/llm-training/#next-steps","title":"Next Steps","text":"<ol> <li>Scale up: Try larger models and datasets</li> <li>Fine-tuning: Experiment with different fine-tuning strategies</li> <li>Evaluation: Implement proper evaluation metrics</li> <li>Deployment: Set up inference pipelines</li> <li>Optimization: Profile and optimize training performance</li> </ol> <p>This guide demonstrates both the educational pure Genesis approach and the production-ready hybrid approach used in real applications.</p>"},{"location":"tutorials/mixed-precision/","title":"Mixed Precision Training Guide","text":"<p>Mixed precision training is a technique that uses both 16-bit (half precision) and 32-bit (single precision) floating-point numbers during training to reduce memory usage and accelerate training while maintaining model accuracy. Genesis provides comprehensive support for mixed precision training with automatic mixed precision (AMP) capabilities.</p>"},{"location":"tutorials/mixed-precision/#overview","title":"Overview","text":""},{"location":"tutorials/mixed-precision/#benefits-of-mixed-precision-training","title":"Benefits of Mixed Precision Training","text":"<ul> <li>Memory Efficiency: Reduces memory usage by ~50%</li> <li>Speed Improvement: Faster training on modern GPUs with Tensor Cores</li> <li>Model Accuracy: Maintains training stability with automatic loss scaling</li> <li>Larger Models: Enables training of larger models on the same hardware</li> </ul>"},{"location":"tutorials/mixed-precision/#supported-precision-types","title":"Supported Precision Types","text":"<p>Genesis supports multiple precision formats:</p> <ul> <li>float32 (FP32): Standard single precision (default)</li> <li>float16 (FP16): IEEE half precision </li> <li>bfloat16 (BF16): Brain float format with larger dynamic range</li> </ul>"},{"location":"tutorials/mixed-precision/#data-type-system","title":"Data Type System","text":""},{"location":"tutorials/mixed-precision/#understanding-genesis-dtypes","title":"Understanding Genesis DTypes","text":"Python<pre><code>import genesis\n\n# Available precision types\nprint(\"Available dtypes:\")\nprint(f\"FP32: {genesis.float32}\")  # Standard precision\nprint(f\"FP16: {genesis.float16}\")  # Half precision\nprint(f\"BF16: {genesis.bfloat16}\") # Brain float\n\n# Check dtype properties\ndtype = genesis.float16\nprint(f\"Name: {dtype.name}\")\nprint(f\"Size: {dtype.itemsize} bytes\")\nprint(f\"Is floating: {dtype.is_floating_point}\")\nprint(f\"NumPy type: {dtype.numpy_dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#creating-mixed-precision-tensors","title":"Creating Mixed Precision Tensors","text":"Python<pre><code>import genesis\n\n# Create tensors with different precisions\nfp32_tensor = genesis.randn(1000, 1000, dtype=genesis.float32)\nfp16_tensor = genesis.randn(1000, 1000, dtype=genesis.float16) \nbf16_tensor = genesis.randn(1000, 1000, dtype=genesis.bfloat16)\n\nprint(f\"FP32 memory: {fp32_tensor.numel() * 4} bytes\")\nprint(f\"FP16 memory: {fp16_tensor.numel() * 2} bytes\") \nprint(f\"BF16 memory: {bf16_tensor.numel() * 2} bytes\")\n\n# Type conversion\nfp16_from_fp32 = fp32_tensor.half()    # Convert to FP16\nfp32_from_fp16 = fp16_tensor.float()   # Convert to FP32\n</code></pre>"},{"location":"tutorials/mixed-precision/#automatic-mixed-precision-amp","title":"Automatic Mixed Precision (AMP)","text":""},{"location":"tutorials/mixed-precision/#basic-amp-usage","title":"Basic AMP Usage","text":"<p>Genesis provides automatic mixed precision through the <code>autocast</code> context and enable flag:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Enable automatic mixed precision globally\ngenesis.enable_autocast = True\n\n# Create model and data\nmodel = nn.Linear(784, 10).cuda()\nx = genesis.randn(32, 784, device='cuda')\nlabels = genesis.randint(0, 10, (32,), device='cuda')\n\n# Forward pass with automatic casting\noutputs = model(x)  # Automatically uses mixed precision\n\n# Loss computation (typically done in FP32)\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(outputs, labels)\n\nprint(f\"Input dtype: {x.dtype}\")\nprint(f\"Output dtype: {outputs.dtype}\")\nprint(f\"Loss dtype: {loss.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#manual-amp-control","title":"Manual AMP Control","text":"<p>For fine-grained control, use the <code>autocast</code> context manager:</p> Python<pre><code>import genesis\n\n# Disable global autocast\ngenesis.enable_autocast = False\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n).cuda()\n\nx = genesis.randn(32, 784, device='cuda')\n\n# Manual mixed precision control\nwith genesis.autocast():\n    # Operations inside this block use FP16/BF16\n    hidden = model[0](x)  # Linear layer in FP16\n    activated = model[1](hidden)  # ReLU in FP16\n\n# Operations outside use default precision\noutputs = model[2](activated)  # This will be FP32\n\nprint(f\"Hidden dtype: {hidden.dtype}\")\nprint(f\"Activated dtype: {activated.dtype}\")\nprint(f\"Output dtype: {outputs.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#training-with-mixed-precision","title":"Training with Mixed Precision","text":""},{"location":"tutorials/mixed-precision/#simple-mixed-precision-training-loop","title":"Simple Mixed Precision Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 512),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, 10)\n).cuda()\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Enable mixed precision\ngenesis.enable_autocast = True\n\ndef train_epoch_amp(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0.0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # Backward pass\n        loss.backward()\n\n        # Gradient clipping (important for stability)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Optimizer step\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}: loss={loss.item():.4f}')\n\n    return total_loss / len(dataloader)\n\n# Training\nfor epoch in range(10):\n    avg_loss = train_epoch_amp(model, train_loader, optimizer, criterion)\n    print(f'Epoch {epoch}: avg loss = {avg_loss:.4f}')\n</code></pre>"},{"location":"tutorials/mixed-precision/#advanced-mixed-precision-with-loss-scaling","title":"Advanced Mixed Precision with Loss Scaling","text":"<p>For training stability, especially with FP16, loss scaling is recommended:</p> Python<pre><code>class GradScaler:\n    \"\"\"Gradient scaler for mixed precision training.\"\"\"\n\n    def __init__(self, init_scale=2**16, growth_factor=2.0, backoff_factor=0.5, \n                 growth_interval=2000):\n        self.scale = init_scale\n        self.growth_factor = growth_factor\n        self.backoff_factor = backoff_factor\n        self.growth_interval = growth_interval\n        self._growth_tracker = 0\n\n    def scale_loss(self, loss):\n        \"\"\"Scale loss to prevent gradient underflow.\"\"\"\n        return loss * self.scale\n\n    def unscale_gradients(self, optimizer):\n        \"\"\"Unscale gradients before optimizer step.\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    param.grad.data.div_(self.scale)\n\n    def step(self, optimizer):\n        \"\"\"Step optimizer with gradient overflow detection.\"\"\"\n        # Check for gradient overflow\n        has_overflow = self._check_overflow(optimizer)\n\n        if has_overflow:\n            # Skip optimizer step and reduce scale\n            self.scale *= self.backoff_factor\n            self.scale = max(self.scale, 1.0)\n            self._growth_tracker = 0\n            return False\n        else:\n            # Normal optimizer step\n            optimizer.step()\n\n            # Increase scale periodically\n            self._growth_tracker += 1\n            if self._growth_tracker &gt;= self.growth_interval:\n                self.scale *= self.growth_factor\n                self._growth_tracker = 0\n\n            return True\n\n    def _check_overflow(self, optimizer):\n        \"\"\"Check if any gradients have overflowed.\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    if genesis.isnan(param.grad).any() or genesis.isinf(param.grad).any():\n                        return True\n        return False\n\n# Training with gradient scaling\nscaler = GradScaler()\n\ndef train_with_scaling(model, dataloader, optimizer, criterion, scaler):\n    model.train()\n    total_loss = 0.0\n    successful_steps = 0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        with genesis.autocast():\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n\n        # Scale loss to prevent gradient underflow\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # Unscale gradients and check for overflow\n        scaler.unscale_gradients(optimizer)\n\n        # Gradient clipping on unscaled gradients\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Step optimizer with overflow detection\n        if scaler.step(optimizer):\n            successful_steps += 1\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}: loss={loss.item():.4f}, scale={scaler.scale:.0f}')\n\n    success_rate = successful_steps / len(dataloader)\n    print(f'Training success rate: {success_rate:.1%}')\n\n    return total_loss / len(dataloader)\n</code></pre>"},{"location":"tutorials/mixed-precision/#precision-specific-considerations","title":"Precision-Specific Considerations","text":""},{"location":"tutorials/mixed-precision/#fp16-half-precision","title":"FP16 (Half Precision)","text":"Python<pre><code>import genesis\n\n# FP16 characteristics\nfp16_info = {\n    'range': '\u00b165,504',\n    'precision': '~3-4 decimal digits',\n    'special_values': ['inf', '-inf', 'nan'],\n    'benefits': ['Faster on Tensor Cores', '50% memory reduction'],\n    'challenges': ['Limited range', 'Gradient underflow']\n}\n\n# Best practices for FP16\ndef create_fp16_model():\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.LayerNorm(256),  # LayerNorm works well with FP16\n        nn.ReLU(),\n        nn.Linear(256, 10)\n    )\n\n    # Initialize with appropriate scale for FP16\n    for module in model.modules():\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=1.0)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    return model\n\n# Monitor FP16 training\ndef check_fp16_health(model):\n    \"\"\"Check model health during FP16 training.\"\"\"\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            param_norm = param.norm().item()\n\n            print(f\"{name}:\")\n            print(f\"  Param norm: {param_norm:.2e}\")\n            print(f\"  Grad norm: {grad_norm:.2e}\")\n\n            # Check for problematic values\n            if grad_norm &lt; 1e-7:\n                print(f\"  WARNING: Very small gradients detected!\")\n            if grad_norm &gt; 1e4:\n                print(f\"  WARNING: Very large gradients detected!\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#bf16-brain-float","title":"BF16 (Brain Float)","text":"Python<pre><code>import genesis\n\n# BF16 advantages\nbf16_info = {\n    'range': 'Same as FP32 (\u00b13.4\u00d710^38)',\n    'precision': '~2-3 decimal digits', \n    'benefits': ['Larger range than FP16', 'More stable training'],\n    'hardware': ['A100', 'H100', 'TPUs']\n}\n\n# BF16 is often more stable than FP16\ndef train_with_bf16():\n    # Create model with BF16\n    model = nn.Linear(1000, 100).cuda()\n    x = genesis.randn(32, 1000, dtype=genesis.bfloat16, device='cuda')\n\n    # BF16 forward pass\n    output = model(x)\n    print(f\"Input: {x.dtype}, Output: {output.dtype}\")\n\n    # BF16 typically doesn't need loss scaling\n    loss = output.sum()\n    loss.backward()\n\n    return model\n\n# Compare precisions\ndef compare_precisions():\n    sizes = [100, 1000, 10000]\n\n    for size in sizes:\n        # Create test data\n        data_fp32 = genesis.randn(size, size)\n        data_fp16 = data_fp32.half()\n        data_bf16 = data_fp32.to(genesis.bfloat16)\n\n        # Simple computation\n        result_fp32 = genesis.matmul(data_fp32, data_fp32)\n        result_fp16 = genesis.matmul(data_fp16, data_fp16)\n        result_bf16 = genesis.matmul(data_bf16, data_bf16)\n\n        # Compare accuracy\n        error_fp16 = (result_fp32 - result_fp16.float()).abs().mean()\n        error_bf16 = (result_fp32 - result_bf16.float()).abs().mean()\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  FP16 error: {error_fp16:.2e}\")\n        print(f\"  BF16 error: {error_bf16:.2e}\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#memory-optimization","title":"Memory Optimization","text":""},{"location":"tutorials/mixed-precision/#memory-usage-analysis","title":"Memory Usage Analysis","text":"Python<pre><code>import genesis\n\ndef analyze_memory_usage():\n    \"\"\"Analyze memory usage of different precision types.\"\"\"\n\n    # Model sizes\n    sizes = [(1000, 1000), (2000, 2000), (5000, 5000)]\n\n    for h, w in sizes:\n        print(f\"\\nTensor size: {h}x{w}\")\n\n        # Create tensors\n        fp32_tensor = genesis.randn(h, w, dtype=genesis.float32, device='cuda')\n        fp16_tensor = genesis.randn(h, w, dtype=genesis.float16, device='cuda')\n        bf16_tensor = genesis.randn(h, w, dtype=genesis.bfloat16, device='cuda')\n\n        # Memory usage\n        fp32_memory = fp32_tensor.numel() * 4  # 4 bytes per float32\n        fp16_memory = fp16_tensor.numel() * 2  # 2 bytes per float16\n        bf16_memory = bf16_tensor.numel() * 2  # 2 bytes per bfloat16\n\n        print(f\"  FP32: {fp32_memory / 1e6:.1f} MB\")\n        print(f\"  FP16: {fp16_memory / 1e6:.1f} MB ({fp16_memory/fp32_memory:.1%})\")\n        print(f\"  BF16: {bf16_memory / 1e6:.1f} MB ({bf16_memory/fp32_memory:.1%})\")\n\n        # Cleanup\n        del fp32_tensor, fp16_tensor, bf16_tensor\n        genesis.cuda.empty_cache()\n\nanalyze_memory_usage()\n</code></pre>"},{"location":"tutorials/mixed-precision/#gradient-checkpointing-with-mixed-precision","title":"Gradient Checkpointing with Mixed Precision","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\nclass CheckpointedModule(nn.Module):\n    \"\"\"Module with gradient checkpointing support.\"\"\"\n\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.checkpoint = True\n\n    def forward(self, x):\n        def run_layers(x, layers):\n            for layer in layers:\n                x = layer(x)\n            return x\n\n        if self.training and self.checkpoint:\n            # Use gradient checkpointing to save memory\n            return genesis.utils.checkpoint(run_layers, x, self.layers)\n        else:\n            return run_layers(x, self.layers)\n\n# Create memory-efficient model\ndef create_checkpointed_model():\n    layers = [\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Linear(512, 10)\n    ]\n\n    return CheckpointedModule(layers)\n\n# Training with checkpointing and mixed precision\ndef train_memory_efficient():\n    model = create_checkpointed_model().cuda()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Enable mixed precision\n    genesis.enable_autocast = True\n\n    for epoch in range(10):\n        for batch in dataloader:\n            data, targets = batch\n            data = data.cuda()\n            targets = targets.cuda()\n\n            optimizer.zero_grad()\n\n            # Forward pass with checkpointing and mixed precision\n            outputs = model(data)\n            loss = nn.CrossEntropyLoss()(outputs, targets)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch} completed\")\n</code></pre>"},{"location":"tutorials/mixed-precision/#performance-benchmarking","title":"Performance Benchmarking","text":""},{"location":"tutorials/mixed-precision/#mixed-precision-performance-comparison","title":"Mixed Precision Performance Comparison","text":"Python<pre><code>import genesis\nimport time\n\ndef benchmark_precision_performance():\n    \"\"\"Benchmark different precision formats.\"\"\"\n\n    # Model setup\n    sizes = [512, 1024, 2048]\n    batch_sizes = [16, 32, 64]\n\n    results = {}\n\n    for size in sizes:\n        for batch_size in batch_sizes:\n            print(f\"\\nBenchmarking: size={size}, batch_size={batch_size}\")\n\n            # Create models\n            model_fp32 = nn.Linear(size, size).cuda()\n            model_fp16 = nn.Linear(size, size).cuda().half()\n\n            # Create data\n            data_fp32 = genesis.randn(batch_size, size, device='cuda')\n            data_fp16 = data_fp32.half()\n\n            # Benchmark FP32\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp32 = model_fp32(data_fp32)\n\n            torch.cuda.synchronize()\n            fp32_time = time.time() - start_time\n\n            # Benchmark FP16\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp16 = model_fp16(data_fp16)\n\n            torch.cuda.synchronize()\n            fp16_time = time.time() - start_time\n\n            # Results\n            speedup = fp32_time / fp16_time\n            print(f\"  FP32 time: {fp32_time:.3f}s\")\n            print(f\"  FP16 time: {fp16_time:.3f}s\") \n            print(f\"  Speedup: {speedup:.2f}x\")\n\n            results[(size, batch_size)] = {\n                'fp32_time': fp32_time,\n                'fp16_time': fp16_time,\n                'speedup': speedup\n            }\n\n    return results\n\n# Run benchmark\nbenchmark_results = benchmark_precision_performance()\n</code></pre>"},{"location":"tutorials/mixed-precision/#best-practices-and-troubleshooting","title":"Best Practices and Troubleshooting","text":""},{"location":"tutorials/mixed-precision/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with automatic mixed precision before manual control</li> <li>Monitor Training: Watch for gradient underflow/overflow</li> <li>Use Loss Scaling: Essential for FP16 stability</li> <li>Gradient Clipping: Helps prevent gradient explosion</li> <li>Layer-wise Precision: Some layers may need FP32 (e.g., batch norm)</li> </ol>"},{"location":"tutorials/mixed-precision/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Python<pre><code># Issue 1: Gradient Underflow\ndef handle_gradient_underflow():\n    \"\"\"Handle gradient underflow in FP16 training.\"\"\"\n\n    # Solution 1: Use loss scaling\n    scaler = GradScaler(init_scale=2**16)\n\n    # Solution 2: Skip problematic batches\n    def safe_backward(loss, scaler):\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # Check for problems before optimizer step\n        has_inf_or_nan = any(\n            genesis.isinf(p.grad).any() or genesis.isnan(p.grad).any()\n            for p in model.parameters() \n            if p.grad is not None\n        )\n\n        if has_inf_or_nan:\n            print(\"Skipping step due to inf/nan gradients\")\n            optimizer.zero_grad()\n            return False\n\n        return True\n\n# Issue 2: Model Divergence\ndef prevent_model_divergence():\n    \"\"\"Prevent model divergence in mixed precision.\"\"\"\n\n    # Solution 1: Lower learning rate\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower LR\n\n    # Solution 2: Warmup schedule\n    scheduler = optim.get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=1000, num_training_steps=10000\n    )\n\n    # Solution 3: Monitor loss closely\n    def check_loss_stability(loss, loss_history):\n        loss_history.append(loss.item())\n\n        if len(loss_history) &gt; 100:\n            recent_losses = loss_history[-50:]\n            if any(l &gt; 10 * min(recent_losses) for l in recent_losses):\n                print(\"WARNING: Loss instability detected!\")\n                return False\n\n        return True\n\n# Issue 3: Accuracy Degradation\ndef maintain_accuracy():\n    \"\"\"Maintain model accuracy with mixed precision.\"\"\"\n\n    # Solution 1: Use BF16 instead of FP16\n    genesis.enable_autocast = True\n    default_dtype = genesis.bfloat16\n\n    # Solution 2: Keep critical layers in FP32\n    class MixedPrecisionModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Linear(784, 256),  # FP16/BF16\n                nn.ReLU(),\n                nn.Linear(256, 128),  # FP16/BF16\n                nn.ReLU()\n            )\n\n            # Keep output layer in FP32 for stability\n            self.classifier = nn.Linear(128, 10).float()\n\n        def forward(self, x):\n            with genesis.autocast():\n                features = self.features(x)\n\n            # Output layer in FP32\n            output = self.classifier(features.float())\n            return output\n</code></pre>"},{"location":"tutorials/mixed-precision/#debugging-mixed-precision-training","title":"Debugging Mixed Precision Training","text":"Python<pre><code>def debug_mixed_precision():\n    \"\"\"Debug mixed precision training issues.\"\"\"\n\n    # 1. Check tensor dtypes throughout the model\n    def print_tensor_info(tensor, name):\n        print(f\"{name}:\")\n        print(f\"  Shape: {tensor.shape}\")\n        print(f\"  Dtype: {tensor.dtype}\")\n        print(f\"  Device: {tensor.device}\")\n        print(f\"  Requires grad: {tensor.requires_grad}\")\n        print(f\"  Min/Max: {tensor.min():.2e} / {tensor.max():.2e}\")\n        print()\n\n    # 2. Monitor gradient norms\n    def check_gradient_norms(model):\n        total_norm = 0.0\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.norm().item()\n                total_norm += grad_norm ** 2\n                print(f\"{name}: grad_norm = {grad_norm:.2e}\")\n\n        total_norm = total_norm ** 0.5\n        print(f\"Total gradient norm: {total_norm:.2e}\")\n        return total_norm\n\n    # 3. Validate numerical stability\n    def check_numerical_stability(tensor):\n        \"\"\"Check for numerical issues.\"\"\"\n        has_nan = genesis.isnan(tensor).any()\n        has_inf = genesis.isinf(tensor).any()\n\n        if has_nan:\n            print(\"WARNING: NaN values detected!\")\n        if has_inf:\n            print(\"WARNING: Inf values detected!\")\n\n        return not (has_nan or has_inf)\n\n# Usage in training loop\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        # Forward pass\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # Debug information\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}:\")\n            print_tensor_info(data, \"Input\")\n            print_tensor_info(outputs, \"Output\") \n            print_tensor_info(loss, \"Loss\")\n\n            # Check gradients after backward pass\n            loss.backward()\n            grad_norm = check_gradient_norms(model)\n\n            if grad_norm &gt; 10.0:\n                print(\"WARNING: Large gradient norm detected!\")\n</code></pre> <p>This comprehensive guide covers all aspects of mixed precision training in Genesis, from basic usage to advanced optimization techniques and troubleshooting strategies.</p>"},{"location":"tutorials/model-definition/","title":"Model Definition in Genesis","text":"<p>Learn how to define neural network models using Genesis.</p>"},{"location":"tutorials/model-definition/#overview","title":"Overview","text":"<p>Genesis provides PyTorch-like APIs for defining neural network architectures.</p>"},{"location":"tutorials/model-definition/#basic-model-definition","title":"Basic Model Definition","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, 128)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x\n\n# Create model instance\nmodel = SimpleNet(784, 10)\n</code></pre>"},{"location":"tutorials/model-definition/#advanced-architectures","title":"Advanced Architectures","text":"<p>This tutorial is under construction. More examples and patterns will be added.</p>"},{"location":"tutorials/model-definition/#see-also","title":"See Also","text":"<ul> <li>Basic Training - Training your defined models</li> <li>Custom Operations - Creating custom layers</li> </ul>"},{"location":"tutorials/performance-tuning/","title":"Performance Tuning Guide","text":"<p>Under Development</p> <p>This document is being written and content will be continuously updated.</p> <p>This guide will teach you how to optimize Genesis model training performance, including memory usage, computational efficiency, and I/O optimization.</p>"},{"location":"tutorials/performance-tuning/#optimization-goals","title":"\ud83c\udfaf Optimization Goals","text":"<ul> <li>Training Speed: Increase samples processed per second</li> <li>Memory Efficiency: Reduce GPU memory usage</li> <li>Throughput: Maximize hardware utilization</li> </ul>"},{"location":"tutorials/performance-tuning/#performance-analysis-tools","title":"\ud83d\udcca Performance Analysis Tools","text":""},{"location":"tutorials/performance-tuning/#built-in-profiler","title":"Built-in Profiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# WIP: Performance analysis code example\nwith profiler.profile() as prof:\n    # Training code\n    pass\n\nprof.print_stats()\n</code></pre>"},{"location":"tutorials/performance-tuning/#optimization-strategies","title":"\u26a1 Optimization Strategies","text":""},{"location":"tutorials/performance-tuning/#1-memory-optimization","title":"1. Memory Optimization","text":"<ul> <li>Gradient accumulation</li> <li>Checkpoint techniques</li> <li>Mixed precision training</li> </ul>"},{"location":"tutorials/performance-tuning/#2-compute-optimization","title":"2. Compute Optimization","text":"<ul> <li>Operator fusion</li> <li>Triton kernel optimization</li> <li>CUDA stream overlap</li> </ul>"},{"location":"tutorials/performance-tuning/#3-io-optimization","title":"3. I/O Optimization","text":"<ul> <li>Data prefetching</li> <li>Multi-process data loading</li> <li>Memory mapping</li> </ul>"},{"location":"tutorials/performance-tuning/#benchmarking","title":"\ud83d\udcc8 Benchmarking","text":"<ul> <li>Performance comparison with PyTorch</li> <li>Performance testing with different configurations</li> <li>Bottleneck identification methods</li> </ul> <p>\ud83d\udcd8 Documentation Status: Under development, expected to be completed in v0.2.0.</p>"}]}