# CUDAå­˜å‚¨ç³»ç»Ÿ

Genesisçš„CUDAå­˜å‚¨ï¼ˆCUDAStorageï¼‰æ˜¯æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›çº¯CUDAå®ç°çš„GPUå†…å­˜ç®¡ç†å’Œæ“ä½œï¼Œå®Œå…¨ç‹¬ç«‹äºPyTorchï¼Œç›´æ¥ä½¿ç”¨CUDA Python APIã€‚

## ğŸ¯ è®¾è®¡ç›®æ ‡

### ç‹¬ç«‹æ€§
- **çº¯CUDAå®ç°**ï¼šä¸ä¾èµ–PyTorchçš„GPUåç«¯
- **ç›´æ¥å†…å­˜ç®¡ç†**ï¼šä½¿ç”¨CUDA Python APIç›´æ¥ç®¡ç†GPUå†…å­˜
- **é«˜æ€§èƒ½**ï¼šé’ˆå¯¹GPUä¼˜åŒ–çš„å†…å­˜è®¿é—®æ¨¡å¼

### å…¼å®¹æ€§  
- **PyTorché£æ ¼API**ï¼šä¿æŒä¸PyTorchå¼ é‡çš„æ¥å£å…¼å®¹æ€§
- **è‡ªåŠ¨å¾®åˆ†æ”¯æŒ**ï¼šä¸Genesisè‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿæ— ç¼é›†æˆ
- **ç±»å‹å®‰å…¨**ï¼šå®Œæ•´çš„ç±»å‹æ³¨è§£å’Œè¿è¡Œæ—¶æ£€æŸ¥

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### IndexPlanæ¶æ„

CUDATensorä½¿ç”¨å…ˆè¿›çš„IndexPlanæ¶æ„æ¥å¤„ç†å¤æ‚çš„å¼ é‡ç´¢å¼•æ“ä½œï¼š

```python
class IndexKind(Enum):
    VIEW = "view"           # çº¯è§†å›¾æ“ä½œï¼Œé›¶æ‹·è´
    GATHER = "gather"       # æ”¶é›†æ“ä½œï¼Œç”¨äºé«˜çº§ç´¢å¼•  
    SCATTER = "scatter"     # æ•£å¸ƒæ“ä½œï¼Œç”¨äºèµ‹å€¼
    COPY = "copy"          # æ­¥é•¿æ‹·è´
    FILL = "fill"          # å¡«å……æ“ä½œ

@dataclass
class IndexPlan:
    """ç»Ÿä¸€çš„ç´¢å¼•è®¡åˆ’"""
    kind: IndexKind
    result_shape: Optional[Tuple[int, ...]] = None
    result_strides: Optional[Tuple[int, ...]] = None
    ptr_offset_bytes: int = 0
    index_tensor: Optional['CUDATensor'] = None
    needs_mask_compaction: bool = False
    temp_memory_bytes: int = 0
```

### å†…å­˜ç®¡ç†

```python
class AsyncMemoryPool:
    """å¼‚æ­¥å†…å­˜æ± ï¼Œä¼˜åŒ–GPUå†…å­˜åˆ†é…æ€§èƒ½"""
    
    def __init__(self):
        self.free_blocks = {}  # æŒ‰å¤§å°ç»„ç»‡çš„ç©ºé—²å—
        self.allocated_blocks = {}  # å·²åˆ†é…çš„å—
        self.alignment = 512  # å†…å­˜å¯¹é½ï¼Œä¸PyTorchä¸€è‡´
        
    def allocate(self, size_bytes: int) -> int:
        """åˆ†é…å¯¹é½çš„GPUå†…å­˜"""
        
    def deallocate(self, ptr: int):
        """é‡Šæ”¾GPUå†…å­˜åˆ°æ± ä¸­é‡ç”¨"""
```

## ğŸ’¡ æ ¸å¿ƒç‰¹æ€§

### 1. é«˜æ•ˆçš„ç´¢å¼•æ“ä½œ

```python
import genesis

# åˆ›å»ºCUDAå¼ é‡
x = genesis.randn(1000, 1000, device='cuda')

# åŸºç¡€ç´¢å¼• - ä½¿ç”¨VIEWæ“ä½œï¼Œé›¶æ‹·è´
y = x[10:20, 50:100]  # IndexPlan.kind = VIEW

# é«˜çº§ç´¢å¼• - ä½¿ç”¨GATHERæ“ä½œ  
indices = genesis.tensor([1, 3, 5, 7], device='cuda')
z = x[indices]  # IndexPlan.kind = GATHER

# å¸ƒå°”ç´¢å¼• - è‡ªåŠ¨ä¼˜åŒ–
mask = x > 0.5
w = x[mask]  # æ ¹æ®ç¨ å¯†åº¦é€‰æ‹©æœ€ä¼˜ç­–ç•¥
```

### 2. å†…å­˜é«˜æ•ˆçš„æ“ä½œ

```python
# å°±åœ°æ“ä½œï¼Œé¿å…å†…å­˜åˆ†é…
x = genesis.randn(1000, 1000, device='cuda')
x += 1.0  # å°±åœ°åŠ æ³•

# è§†å›¾æ“ä½œï¼Œé›¶æ‹·è´
y = x.view(100, 10000)  # æ”¹å˜å½¢çŠ¶ä½†ä¸å¤åˆ¶æ•°æ®
z = x.transpose(0, 1)   # è½¬ç½®è§†å›¾

# æ­¥é•¿æ“ä½œï¼Œé«˜æ•ˆå®ç°
w = x[::2, ::3]  # æ­¥é•¿ç´¢å¼•ï¼Œä½¿ç”¨ä¼˜åŒ–çš„COPYæ“ä½œ
```

### 3. Tritonå†…æ ¸é›†æˆ

```python
@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    """ä¼˜åŒ–çš„TritonåŠ æ³•å†…æ ¸"""
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    
    tl.store(output_ptr + offsets, output, mask=mask)

# CUDATensorè‡ªåŠ¨è°ƒç”¨ä¼˜åŒ–çš„Tritonå†…æ ¸
def add_cuda_tensor(x: CUDATensor, y: CUDATensor) -> CUDATensor:
    """CUDAå¼ é‡åŠ æ³•ï¼Œä½¿ç”¨Tritonä¼˜åŒ–"""
    output = CUDATensor(x.shape, x.dtype)
    
    n_elements = x.numel()
    BLOCK_SIZE = 1024
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    
    add_kernel[grid](x.data_ptr(), y.data_ptr(), output.data_ptr(), 
                     n_elements, BLOCK_SIZE=BLOCK_SIZE)
    
    return output
```

## ğŸš€ åŸºç¡€ä½¿ç”¨

### åˆ›å»ºå¼ é‡

```python
import genesis

# ä»æ•°æ®åˆ›å»º
data = [[1.0, 2.0], [3.0, 4.0]]
tensor = genesis.tensor(data, device='cuda')

# ç›´æ¥åˆ›å»ºç‰¹å®šå½¢çŠ¶
zeros = genesis.zeros(100, 100, device='cuda')
ones = genesis.ones(50, 50, device='cuda')  
random = genesis.randn(200, 200, device='cuda')

# æŒ‡å®šæ•°æ®ç±»å‹
float16_tensor = genesis.randn(100, 100, dtype=genesis.float16, device='cuda')
int_tensor = genesis.randint(0, 10, (50, 50), device='cuda')

print(f"å¼ é‡å½¢çŠ¶: {tensor.shape}")
print(f"æ•°æ®ç±»å‹: {tensor.dtype}")
print(f"è®¾å¤‡: {tensor.device}")
print(f"æ­¥é•¿: {tensor.strides}")
```

### åŸºç¡€æ“ä½œ

```python
# æ•°å­¦è¿ç®—
x = genesis.randn(100, 100, device='cuda')
y = genesis.randn(100, 100, device='cuda')

# é€å…ƒç´ è¿ç®—
z = x + y      # åŠ æ³•
w = x * y      # ä¹˜æ³•  
u = x.pow(2)   # å¹‚è¿ç®—
v = x.exp()    # æŒ‡æ•°å‡½æ•°

# å½’çº¦æ“ä½œ
sum_all = x.sum()           # å…¨å±€æ±‚å’Œ
sum_dim = x.sum(dim=0)      # æŒ‰ç»´åº¦æ±‚å’Œ
mean_val = x.mean()         # å¹³å‡å€¼
max_val, indices = x.max(dim=1)  # æœ€å¤§å€¼å’Œç´¢å¼•

# çº¿æ€§ä»£æ•°
a = genesis.randn(100, 50, device='cuda')
b = genesis.randn(50, 200, device='cuda') 
c = genesis.matmul(a, b)    # çŸ©é˜µä¹˜æ³•

# å½¢çŠ¶æ“ä½œ
reshaped = x.view(10, 1000)        # æ”¹å˜å½¢çŠ¶
transposed = x.transpose(0, 1)     # è½¬ç½®  
flattened = x.flatten()            # å±•å¹³
```

### é«˜çº§ç´¢å¼•

```python
# åˆ›å»ºæµ‹è¯•å¼ é‡
data = genesis.arange(0, 100, device='cuda').view(10, 10)
print("åŸå§‹æ•°æ®:")
print(data)

# åŸºç¡€åˆ‡ç‰‡
slice_basic = data[2:5, 3:7]  # è¡Œ2-4ï¼Œåˆ—3-6
print("åŸºç¡€åˆ‡ç‰‡:", slice_basic.shape)

# æ­¥é•¿ç´¢å¼•
slice_stride = data[::2, 1::2]  # æ¯éš”ä¸€è¡Œï¼Œä»ç¬¬1åˆ—å¼€å§‹æ¯éš”ä¸€åˆ—
print("æ­¥é•¿ç´¢å¼•:", slice_stride.shape)

# é«˜çº§ç´¢å¼• - æ•´æ•°æ•°ç»„
row_indices = genesis.tensor([0, 2, 4, 6], device='cuda')
col_indices = genesis.tensor([1, 3, 5, 7], device='cuda')
advanced = data[row_indices, col_indices]  # é€‰æ‹©ç‰¹å®šä½ç½®
print("é«˜çº§ç´¢å¼•ç»“æœ:", advanced)

# å¸ƒå°”ç´¢å¼•
mask = data > 50
masked_data = data[mask]  # é€‰æ‹©å¤§äº50çš„å…ƒç´ 
print("å¸ƒå°”ç´¢å¼•ç»“æœ:", masked_data)

# æ··åˆç´¢å¼•
mixed = data[row_indices, 2:8]  # ç‰¹å®šè¡Œçš„åˆ—èŒƒå›´
print("æ··åˆç´¢å¼•:", mixed.shape)
```

## ğŸ”§ å†…å­˜ç®¡ç†

### å†…å­˜æ± ä¼˜åŒ–

```python
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
print(f"å·²åˆ†é…å†…å­˜: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB")
print(f"ç¼“å­˜å†…å­˜: {genesis.cuda.memory_cached() / 1024**2:.1f} MB")

# æ‰‹åŠ¨å†…å­˜ç®¡ç†
x = genesis.randn(1000, 1000, device='cuda')
print(f"åˆ›å»ºå¼ é‡å: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB")

del x  # åˆ é™¤å¼•ç”¨
genesis.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜
print(f"æ¸…ç†å: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB")

# å†…å­˜å¿«ç…§ï¼ˆè°ƒè¯•ç”¨ï¼‰
snapshot = genesis.cuda.memory_snapshot()
for entry in snapshot[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ¡ç›®
    print(f"åœ°å€: {entry['address']}, å¤§å°: {entry['size']} bytes")
```

### å¼‚æ­¥æ“ä½œ

```python
# å¼‚æ­¥å†…å­˜æ“ä½œ
with genesis.cuda.stream():
    x = genesis.randn(1000, 1000, device='cuda')
    y = genesis.randn(1000, 1000, device='cuda')
    z = genesis.matmul(x, y)  # å¼‚æ­¥æ‰§è¡Œ
    
    # å…¶ä»–CPUå·¥ä½œå¯ä»¥å¹¶è¡Œè¿›è¡Œ
    print("çŸ©é˜µä¹˜æ³•æ­£åœ¨GPUä¸Šå¼‚æ­¥æ‰§è¡Œ...")
    
    # åŒæ­¥ç­‰å¾…ç»“æœ  
    genesis.cuda.synchronize()
    print("è®¡ç®—å®Œæˆ:", z.shape)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–

```python
def inefficient_access():
    """ä½æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼"""
    x = genesis.randn(1000, 1000, device='cuda')
    result = genesis.zeros(1000, device='cuda')
    
    # éè¿ç»­è®¿é—®ï¼Œç¼“å­˜æœªå‘½ä¸­
    for i in range(1000):
        result[i] = x[i, ::10].sum()  # æ­¥é•¿è®¿é—®
    
    return result

def efficient_access():  
    """é«˜æ•ˆçš„å†…å­˜è®¿é—®æ¨¡å¼"""
    x = genesis.randn(1000, 1000, device='cuda')
    
    # è¿ç»­è®¿é—®ï¼Œå……åˆ†åˆ©ç”¨ç¼“å­˜
    indices = genesis.arange(0, 1000, 10, device='cuda')
    selected = x[:, indices]  # æ‰¹é‡é€‰æ‹©
    result = selected.sum(dim=1)  # å‘é‡åŒ–æ±‚å’Œ
    
    return result

# æ€§èƒ½å¯¹æ¯”
import time

start = time.time()
result1 = inefficient_access()
time1 = time.time() - start

start = time.time()  
result2 = efficient_access()
time2 = time.time() - start

print(f"ä½æ•ˆæ–¹æ³•: {time1:.4f}s")
print(f"é«˜æ•ˆæ–¹æ³•: {time2:.4f}s")  
print(f"åŠ é€Ÿæ¯”: {time1/time2:.2f}x")
```

### 2. æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
def batch_operations_demo():
    """å±•ç¤ºæ‰¹é‡æ“ä½œçš„æ€§èƒ½ä¼˜åŠ¿"""
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    matrices = [genesis.randn(100, 100, device='cuda') for _ in range(10)]
    
    # æ–¹æ³•1: é€ä¸ªå¤„ç†ï¼ˆä½æ•ˆï¼‰
    start = time.time()
    results1 = []
    for matrix in matrices:
        result = matrix.exp().sum()
        results1.append(result)
    time1 = time.time() - start
    
    # æ–¹æ³•2: æ‰¹é‡å¤„ç†ï¼ˆé«˜æ•ˆï¼‰
    start = time.time()
    batched = genesis.stack(matrices, dim=0)  # [10, 100, 100]
    results2 = batched.exp().sum(dim=(1, 2))  # [10]
    time2 = time.time() - start
    
    print(f"é€ä¸ªå¤„ç†: {time1:.4f}s")
    print(f"æ‰¹é‡å¤„ç†: {time2:.4f}s")
    print(f"åŠ é€Ÿæ¯”: {time1/time2:.2f}x")

batch_operations_demo()
```

### 3. å°±åœ°æ“ä½œ

```python
def inplace_operations_demo():
    """å±•ç¤ºå°±åœ°æ“ä½œçš„å†…å­˜æ•ˆç‡"""
    
    # éå°±åœ°æ“ä½œï¼ˆåˆ›å»ºæ–°å¼ é‡ï¼‰
    x = genesis.randn(1000, 1000, device='cuda')
    start_memory = genesis.cuda.memory_allocated()
    
    y = x + 1.0      # åˆ›å»ºæ–°å¼ é‡
    z = y * 2.0      # å†åˆ›å»ºæ–°å¼ é‡
    w = z.exp()      # åˆåˆ›å»ºæ–°å¼ é‡
    
    memory_after = genesis.cuda.memory_allocated()
    print(f"éå°±åœ°æ“ä½œå†…å­˜å¢é•¿: {(memory_after - start_memory) / 1024**2:.1f} MB")
    
    # å°±åœ°æ“ä½œï¼ˆä¿®æ”¹åŸå¼ é‡ï¼‰
    x = genesis.randn(1000, 1000, device='cuda')
    start_memory = genesis.cuda.memory_allocated()
    
    x += 1.0         # å°±åœ°åŠ æ³•
    x *= 2.0         # å°±åœ°ä¹˜æ³•  
    x.exp_()         # å°±åœ°æŒ‡æ•°å‡½æ•°
    
    memory_after = genesis.cuda.memory_allocated()
    print(f"å°±åœ°æ“ä½œå†…å­˜å¢é•¿: {(memory_after - start_memory) / 1024**2:.1f} MB")

inplace_operations_demo()
```

## ğŸ› è°ƒè¯•å’Œè¯Šæ–­

### å†…å­˜æ³„æ¼æ£€æµ‹

```python
def detect_memory_leaks():
    """æ£€æµ‹å†…å­˜æ³„æ¼"""
    genesis.cuda.empty_cache()
    initial_memory = genesis.cuda.memory_allocated()
    
    # æ‰§è¡Œä¸€äº›æ“ä½œ
    for i in range(100):
        x = genesis.randn(100, 100, device='cuda')
        y = x.matmul(x)
        del x, y
    
    genesis.cuda.empty_cache()
    final_memory = genesis.cuda.memory_allocated()
    
    if final_memory > initial_memory:
        print(f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼: {(final_memory - initial_memory) / 1024**2:.1f} MB")
    else:
        print("æœªæ£€æµ‹åˆ°å†…å­˜æ³„æ¼")

detect_memory_leaks()
```

### é”™è¯¯è¯Šæ–­

```python
def diagnose_cuda_errors():
    """CUDAé”™è¯¯è¯Šæ–­"""
    try:
        # å¯èƒ½å‡ºé”™çš„æ“ä½œ
        x = genesis.randn(1000, 1000, device='cuda')
        y = genesis.randn(500, 500, device='cuda')  # å½¢çŠ¶ä¸åŒ¹é…
        z = genesis.matmul(x, y)
        
    except RuntimeError as e:
        print(f"CUDAé”™è¯¯: {e}")
        
        # æ£€æŸ¥CUDAçŠ¶æ€
        if genesis.cuda.is_available():
            print(f"CUDAè®¾å¤‡: {genesis.cuda.get_device_name()}")
            print(f"CUDAèƒ½åŠ›: {genesis.cuda.get_device_capability()}")
            print(f"å¯ç”¨å†…å­˜: {genesis.cuda.get_device_properties().total_memory / 1024**3:.1f} GB")
        else:
            print("CUDAä¸å¯ç”¨")

diagnose_cuda_errors()
```

## ğŸ”„ ä¸PyTorchäº’æ“ä½œ

```python
import torch

def pytorch_interop_demo():
    """å±•ç¤ºä¸PyTorchçš„äº’æ“ä½œæ€§"""
    
    # Genesiså¼ é‡è½¬PyTorch
    genesis_tensor = genesis.randn(100, 100, device='cuda')
    
    # è½¬æ¢ä¸ºPyTorchï¼ˆå…±äº«å†…å­˜ï¼‰
    pytorch_tensor = torch.as_tensor(genesis_tensor.detach().cpu().numpy()).cuda()
    
    print(f"Genesiså½¢çŠ¶: {genesis_tensor.shape}")
    print(f"PyTorchå½¢çŠ¶: {pytorch_tensor.shape}")
    
    # PyTorchå¼ é‡è½¬Genesis  
    torch_data = torch.randn(50, 50, device='cuda')
    genesis_from_torch = genesis.tensor(torch_data.cpu().numpy(), device='cuda')
    
    print(f"è½¬æ¢æˆåŠŸï¼ŒGenesiså¼ é‡: {genesis_from_torch.shape}")

pytorch_interop_demo()
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

```python
def benchmark_cuda_tensor():
    """CUDAå¼ é‡æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    sizes = [100, 500, 1000, 2000]
    
    print("çŸ©é˜µä¹˜æ³•æ€§èƒ½å¯¹æ¯” (Genesis vs PyTorch):")
    print("-" * 50)
    
    for size in sizes:
        # Genesisæµ‹è¯•
        x_gen = genesis.randn(size, size, device='cuda')
        y_gen = genesis.randn(size, size, device='cuda')
        
        genesis.cuda.synchronize()
        start = time.time()
        for _ in range(10):
            z_gen = genesis.matmul(x_gen, y_gen)
        genesis.cuda.synchronize()
        genesis_time = (time.time() - start) / 10
        
        # PyTorchæµ‹è¯•
        x_torch = torch.randn(size, size, device='cuda')
        y_torch = torch.randn(size, size, device='cuda')
        
        torch.cuda.synchronize()
        start = time.time()
        for _ in range(10):
            z_torch = torch.matmul(x_torch, y_torch)
        torch.cuda.synchronize() 
        pytorch_time = (time.time() - start) / 10
        
        ratio = genesis_time / pytorch_time
        print(f"{size}x{size}: Genesis {genesis_time:.4f}s, PyTorch {pytorch_time:.4f}s, æ¯”ç‡ {ratio:.2f}")

benchmark_cuda_tensor()
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å†…å­˜ç®¡ç†æœ€ä½³å®è·µ

```python
# âœ… å¥½çš„åšæ³•
def good_memory_practice():
    with genesis.cuda.device(0):  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
        x = genesis.randn(1000, 1000, device='cuda')
        
        # ä½¿ç”¨å°±åœ°æ“ä½œ
        x += 1.0
        x *= 0.5
        
        # åŠæ—¶é‡Šæ”¾å¤§å¼ é‡
        del x
        genesis.cuda.empty_cache()

# âŒ é¿å…çš„åšæ³•  
def bad_memory_practice():
    tensors = []
    for i in range(100):
        x = genesis.randn(1000, 1000, device='cuda')
        y = x + 1.0  # åˆ›å»ºé¢å¤–å‰¯æœ¬
        tensors.append(y)  # ä¿æŒæ‰€æœ‰å¼•ç”¨ï¼Œå†…å­˜æ— æ³•é‡Šæ”¾
    # å†…å­˜ä¼šå¿«é€Ÿè€—å°½
```

### 2. æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

```python
# âœ… å‘é‡åŒ–æ“ä½œ
def vectorized_operations():
    x = genesis.randn(1000, 1000, device='cuda')
    
    # ä½¿ç”¨å‘é‡åŒ–å‡½æ•°
    result = genesis.relu(x).sum(dim=1).mean()
    
# âŒ é¿å…å¾ªç¯
def avoid_loops():
    x = genesis.randn(1000, 1000, device='cuda')
    
    # é¿å…Pythonå¾ªç¯
    result = 0
    for i in range(1000):
        result += x[i].sum()  # æ¯æ¬¡éƒ½å¯åŠ¨CUDA kernel
```

### 3. è°ƒè¯•æœ€ä½³å®è·µ

```python
# å¯ç”¨CUDAé”™è¯¯æ£€æŸ¥
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# ä½¿ç”¨æ–­è¨€æ£€æŸ¥å¼ é‡å±æ€§
def safe_tensor_operation(x, y):
    assert x.device == y.device, "å¼ é‡å¿…é¡»åœ¨åŒä¸€è®¾å¤‡ä¸Š"
    assert x.shape == y.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {x.shape} vs {y.shape}"
    
    return x + y
```

## â“ å¸¸è§é—®é¢˜

### Q: CUDAå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: 
```python
# å‡å°æ‰¹é‡å¤§å°
batch_size = 32  # æ”¹ä¸º16æˆ–8

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps

# å¯ç”¨æ··åˆç²¾åº¦
x = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')

# å®šæœŸæ¸…ç†å†…å­˜
genesis.cuda.empty_cache()
```

### Q: ä¸ºä»€ä¹ˆCUDAæ“ä½œå¾ˆæ…¢ï¼Ÿ  
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
```python
# 1. ç¡®ä¿å¼ é‡åœ¨GPUä¸Š
assert x.device.type == 'cuda'

# 2. é¿å…é¢‘ç¹çš„CPU-GPUä¼ è¾“
# é”™è¯¯åšæ³•
for i in range(1000):
    cpu_data = x.cpu().numpy()  # æ¯æ¬¡éƒ½ä¼ è¾“

# æ­£ç¡®åšæ³•
cpu_data = x.cpu().numpy()  # åªä¼ è¾“ä¸€æ¬¡

# 3. ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹
x = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')  # æ›´å¿«
```

### Q: å¦‚ä½•è°ƒè¯•CUDA kernelé”™è¯¯ï¼Ÿ
A:
```python
# 1. å¯ç”¨åŒæ­¥é”™è¯¯æ£€æŸ¥
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# 2. æ£€æŸ¥tensoræœ‰æ•ˆæ€§
def check_tensor(tensor, name):
    assert not torch.isnan(tensor).any(), f"{name}åŒ…å«NaN"
    assert not torch.isinf(tensor).any(), f"{name}åŒ…å«Inf"
    print(f"{name}: shape={tensor.shape}, dtype={tensor.dtype}")

# 3. ä½¿ç”¨CUDAè°ƒè¯•å·¥å…·
# cuda-memcheck python your_script.py
# compute-sanitizer python your_script.py
```

---

!!! tip "æ€§èƒ½æç¤º"
    CUDAå¼ é‡çš„æ€§èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå†…å­˜è®¿é—®æ¨¡å¼å’Œæ‰¹é‡æ“ä½œçš„ä½¿ç”¨ã€‚ä¼˜å…ˆè€ƒè™‘å‘é‡åŒ–æ“ä½œå’Œåˆç†çš„å†…å­˜å¸ƒå±€ã€‚

**å‡†å¤‡æ·±å…¥äº†è§£æ›´å¤šå—ï¼Ÿ**

[ä¸‹ä¸€æ­¥ï¼šå¼ é‡æ“ä½œä¼˜åŒ–](tensor-operations.zh.md){ .md-button .md-button--primary }
[è¿”å›æ ¸å¿ƒç»„ä»¶](index.zh.md){ .md-button }