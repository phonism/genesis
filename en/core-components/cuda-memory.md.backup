# CUDAå†…å­˜ç®¡ç†

GenesisåŒ…å«äº†ä¸€ä¸ªå…ˆè¿›çš„é«˜æ€§èƒ½CUDAå†…å­˜ç®¡ç†ç³»ç»Ÿï¼Œé€šè¿‡æ®µ-å—åˆ†é…å™¨æ¶æ„å’Œå…ˆè¿›çš„ç¼“å­˜ç­–ç•¥æä¾›é«˜æ•ˆçš„GPUå†…å­˜åˆ†é…ã€‚

## æ¦‚è¿°

CUDAå†…å­˜ç®¡ç†å™¨æ˜¯ä¸€ä¸ªç”Ÿäº§çº§å†…å­˜åˆ†é…å™¨ï¼Œç›¸æ¯”ç®€å•çš„åˆ†é…ç­–ç•¥å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å®ƒå…·æœ‰ä¸¤çº§ç¼“å­˜ç³»ç»Ÿã€æ®µ-å—åˆ†é…å’Œå…¨é¢çš„æ€§èƒ½ç›‘æ§åŠŸèƒ½ã€‚

## æ¶æ„

### æ ¸å¿ƒç»„ä»¶

#### CUDAå†…å­˜ç®¡ç†å™¨
å…·æœ‰ä¼ä¸šçº§ç‰¹æ€§çš„ä¸»è¦å†…å­˜ç®¡ç†å™¨ç±»ï¼š
- **æ®µ-å—åˆ†é…å™¨**ï¼šåˆ†å±‚å†…å­˜ç»„ç»‡ä»¥å®ç°é«˜æ•ˆåˆ†é…
- **ä¸¤çº§ç¼“å­˜**ï¼šæµæœ¬åœ°ç¼“å­˜ + å…¨å±€ç¼“å­˜ä»¥å®ç°æœ€å¤§æ€§èƒ½
- **é¢„çƒ­ç¼“å­˜**ï¼šå¸¸è§åˆ†é…æ¨¡å¼çš„é¢„åˆ†é…ç­–ç•¥
- **æ€§èƒ½ç›‘æ§**ï¼šè¯¦ç»†çš„ç»Ÿè®¡å’ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½
- **æ··åˆåˆ†é…ç­–ç•¥**ï¼šé’ˆå¯¹å°å‹ä¸å¤§å‹åˆ†é…çš„ä¼˜åŒ–è·¯å¾„

#### æ®µ-å—æ¶æ„

```python
@dataclass
class Block:
    """
    æ®µå†…çš„å•ä¸ªå†…å­˜å—
    """
    ptr: int          # GPUæŒ‡é’ˆ
    size: int         # å—å¤§å°  
    is_free: bool     # å¯ç”¨çŠ¶æ€
    segment_id: int   # çˆ¶æ®µID
    
class Segment:
    """
    åŒ…å«å¤šä¸ªå—çš„å¤§å‹è¿ç»­å†…å­˜åŒºåŸŸ
    """
    def __init__(self, segment_id: int, size: int):
        # ä»CUDAåˆ†é…æ•´ä¸ªæ®µ
        self.base_ptr = _ok(cuda.cuMemAlloc(size))
        
        # å°†å†…å­˜åˆå§‹åŒ–ä¸ºé›¶ï¼ˆé˜²æ­¢è„æ•°æ®ç²¾åº¦é—®é¢˜ï¼‰
        _ok(cuda.cuMemsetD8(self.base_ptr, 0, size))
        
        # å¼€å§‹ä½œä¸ºå•ä¸ªå¤§çš„ç©ºé—²å—
        self.blocks: List[Block] = [...]
        self.free_blocks_by_size: Dict[int, List[Block]] = {...}
```

### å…³é”®ç‰¹æ€§

#### 1. é«˜æ€§èƒ½æ®µ-å—åˆ†é…
- **æœ€ä½³é€‚é…ç®—æ³•**ï¼šæ‰¾åˆ°æœ€ä¼˜å—å¤§å°ä»¥æœ€å°åŒ–ç¢ç‰‡
- **å—åˆ†å‰²**ï¼šå¤§å—è‡ªåŠ¨åˆ†å‰²ä»¥æ»¡è¶³ç²¾ç¡®å¤§å°è¯·æ±‚
- **å—åˆå¹¶**ï¼šç›¸é‚»ç©ºé—²å—åˆå¹¶ä»¥é˜²æ­¢ç¢ç‰‡
- **åŸºäºå¤§å°çš„ç´¢å¼•**ï¼šæŒ‰å¤§å°O(1)æŸ¥æ‰¾ç©ºé—²å—

#### 2. ä¸¤çº§ç¼“å­˜ç³»ç»Ÿ
```python
class TwoLevelCache:
    """
    å…·æœ‰æµæœ¬åœ°å’Œå…¨å±€çº§åˆ«çš„å…ˆè¿›ç¼“å­˜
    """
    def __init__(self):
        self.stream_cache: Dict[int, Dict[int, List[int]]] = {}  # stream_id -> size -> [ptrs]
        self.global_cache: Dict[int, List[int]] = {}             # size -> [ptrs]
        self.cache_stats = CacheStatistics()
```

**æµæœ¬åœ°ç¼“å­˜**ï¼š
- é’ˆå¯¹CUDAæµæ•ˆç‡çš„æ¯æµå—ç¼“å­˜
- é¿å…è·¨æµåŒæ­¥å¼€é”€
- å¯¹é‡å¤åˆ†é…æ¨¡å¼æœ€ä¼˜

**å…¨å±€ç¼“å­˜**ï¼š
- æ‰€æœ‰æµä¹‹é—´çš„å…±äº«ç¼“å­˜
- æµæœ¬åœ°ç¼“å­˜æœªå‘½ä¸­æ—¶çš„å›é€€
- æœ€å¤§åŒ–è·¨æ“ä½œçš„å†…å­˜é‡ç”¨

#### 3. é¢„çƒ­ç¼“å­˜é¢„åˆ†é…
```python
def warmup_cache(self, sizes: List[int], counts: List[int]):
    """
    ç”¨å¸¸è§åˆ†é…å¤§å°é¢„å¡«å……ç¼“å­˜
    
    é’ˆå¯¹å·²çŸ¥åˆ†é…æ¨¡å¼çš„æ€§èƒ½ä¼˜åŒ–ï¼š
    - Transformeræ³¨æ„åŠ›çŸ©é˜µ
    - åµŒå…¥æŸ¥æ‰¾  
    - æ¢¯åº¦ç¼“å†²åŒº
    """
    for size, count in zip(sizes, counts):
        for _ in range(count):
            ptr = self.allocate_segment_block(size)
            self.add_to_cache(ptr, size)
```

#### 4. è‡ªé€‚åº”åˆ†é…ç­–ç•¥
```python
def allocate_memory(self, size: int) -> int:
    """
    é’ˆå¯¹ä¸åŒå¤§å°èŒƒå›´ä¼˜åŒ–çš„æ··åˆåˆ†é…ç­–ç•¥
    """
    if size < self.SMALL_BLOCK_THRESHOLD:
        # å°åˆ†é…ï¼šä¼˜å…ˆç¼“å­˜å‘½ä¸­
        return self.allocate_from_cache(size) or self.allocate_segment_block(size)
    else:
        # å¤§åˆ†é…ï¼šç›´æ¥æ®µåˆ†é…
        return self.allocate_large_block(size)
```

## æ€§èƒ½ç‰¹å¾

### åŸºå‡†æµ‹è¯•ç»“æœï¼ˆvs PyTorchï¼‰

| åœºæ™¯ | Genesisæ€§èƒ½ | çŠ¶æ€ |
|----------|-------------------|--------|
| ç›¸åŒå¤§å°åˆ†é… | **1.43å€æ›´å¿«** | âœ… ä¼˜ç§€ |
| å¤§å†…å­˜(>1MB) | **3.92å€æ›´å¿«** | âœ…æ°å‡º |
| Transformerè®­ç»ƒ | **1.89å€æ›´å¿«** | âœ… ä¼˜ç§€ |
| å†…å­˜å‹åŠ› | **4.83å€æ›´å¿«** | âœ… æ°å‡º |
| å˜åŒ–å¤§å° | 0.83å€ï¼ˆæ›´æ…¢ï¼‰ | ğŸ”„ ä¼˜åŒ–ç›®æ ‡ |

### å†…å­˜æ•ˆç‡æ”¹è¿›

1. **æ¶ˆé™¤cudaMalloc/cudaFreeå¼€é”€**ï¼š
   ```python
   # ä¹‹å‰ï¼šç›´æ¥CUDAè°ƒç”¨ï¼ˆæ…¢ï¼‰
   ptr = cuda.cuMemAlloc(size)  # ~100Î¼s å¼€é”€
   
   # ä¹‹åï¼šåŸºäºç¼“å­˜çš„åˆ†é…ï¼ˆå¿«ï¼‰
   ptr = cache.get(size) or segment.allocate(size)  # ~1Î¼s å¼€é”€
   ```

2. **å‡å°‘å†…å­˜ç¢ç‰‡**ï¼š
   - å—åˆå¹¶é˜²æ­¢ç¢ç‰‡
   - æœ€ä½³é€‚é…åˆ†é…æœ€å°åŒ–æµªè´¹
   - æ®µç»„ç»‡æ”¹å–„å±€éƒ¨æ€§

3. **é’ˆå¯¹MLå·¥ä½œè´Ÿè½½ä¼˜åŒ–**ï¼š
   - å¸¸è§å¼ é‡å¤§å°çš„é¢„çƒ­ç¼“å­˜
   - å¹¶è¡Œæ“ä½œçš„æµæ„ŸçŸ¥åˆ†é…
   - å¤šå¼ é‡æ“ä½œçš„æ‰¹é‡åˆ†é…æ”¯æŒ

## é«˜çº§ç‰¹æ€§

### 1. æ€§èƒ½ç›‘æ§
```python
@dataclass
class AllocationStatistics:
    """å…¨é¢çš„åˆ†é…è·Ÿè¸ª"""
    total_allocations: int = 0
    total_freed: int = 0
    peak_memory_usage: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    fragmentation_ratio: float = 0.0
    
    def efficiency_score(self) -> float:
        """è®¡ç®—å†…å­˜ç®¡ç†å™¨æ•ˆç‡ï¼ˆ0-1ï¼‰"""
        if self.total_allocations == 0:
            return 1.0
        return self.cache_hits / self.total_allocations
```

### 2. å†…å­˜æ± ä¼˜åŒ–
```python
class AsyncMemoryPool:
    """
    é«˜ååé‡åœºæ™¯çš„å¼‚æ­¥å†…å­˜æ± 
    """
    def __init__(self, pool_size: int = 1024 * 1024 * 1024):  # é»˜è®¤1GB
        self.pool = MemoryPool(pool_size)
        self.allocation_queue = AsyncQueue()
        self.background_worker = Thread(target=self._allocation_worker)
        
    def allocate_async(self, size: int) -> Future[int]:
        """ç®¡é“å¹¶è¡Œçš„éé˜»å¡åˆ†é…"""
        return self.allocation_queue.submit(self._allocate, size)
```

### 3. æ‰¹é‡åˆ†é…æ”¯æŒ
```python
def allocate_batch(self, sizes: List[int]) -> List[int]:
    """
    å¤šå¼ é‡æ“ä½œçš„ä¼˜åŒ–æ‰¹é‡åˆ†é…
    
    ä¼˜åŠ¿ï¼š
    - å‡å°‘åˆ†é…å¼€é”€
    - æ›´å¥½çš„å†…å­˜å±€éƒ¨æ€§  
    - è‡ªåŠ¨å¤§å°ä¼˜åŒ–
    """
    # æŒ‰ç›¸ä¼¼å¤§å°åˆ†ç»„ä»¥é«˜æ•ˆä½¿ç”¨æ®µ
    size_groups = self._group_by_size(sizes)
    
    ptrs = []
    for size_group in size_groups:
        segment = self._find_or_create_segment(size_group.total_size)
        group_ptrs = segment.allocate_batch(size_group.sizes)
        ptrs.extend(group_ptrs)
    
    return ptrs
```

## å†…å­˜ç®¡ç†æ¨¡å¼

### 1. Transformerè®­ç»ƒä¼˜åŒ–
```python
# Transformerè®­ç»ƒçš„ä¼˜åŒ–å†…å­˜åˆ†é…
def allocate_transformer_tensors(batch_size: int, seq_len: int, hidden_size: int):
    """
    é¢„åˆ†é…å¸¸è§çš„transformerå¼ é‡å¤§å°
    """
    common_sizes = [
        batch_size * seq_len * hidden_size,      # æ³¨æ„åŠ›æƒé‡
        batch_size * seq_len * hidden_size * 4,  # å‰é¦ˆ
        batch_size * seq_len * seq_len,          # æ³¨æ„åŠ›åˆ†æ•°
    ]
    
    # ç”¨é¢„æœŸåˆ†é…æ¨¡å¼é¢„çƒ­ç¼“å­˜
    memory_manager.warmup_cache(common_sizes, counts=[10, 5, 10])
```

### 2. åŠ¨æ€å†…å­˜ç¼©æ”¾
```python
def adaptive_memory_management(memory_pressure: float):
    """
    æ ¹æ®å†…å­˜å‹åŠ›è‡ªåŠ¨è°ƒæ•´ç¼“å­˜å¤§å°
    """
    if memory_pressure > 0.8:
        # é«˜å‹åŠ›ï¼šæ¿€è¿›çš„ç¼“å­˜æ¸…ç†
        memory_manager.cleanup_cache(threshold=0.9)
        memory_manager.enable_aggressive_coalescing()
    elif memory_pressure < 0.3:
        # ä½å‹åŠ›ï¼šæ‰©å±•ç¼“å­˜ä»¥è·å¾—æ›´å¥½æ€§èƒ½
        memory_manager.expand_cache_size(factor=1.5)
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€åˆ†é…
```python
from genesis.ndarray.cuda_memory_manager import get_memory_manager

# è·å–å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹
mm = get_memory_manager()

# åˆ†é…GPUå†…å­˜
ptr = mm.allocate_memory(1024 * 1024)  # 1MB

# é‡Šæ”¾å†…å­˜ï¼ˆè‡ªåŠ¨ç¼“å­˜ï¼‰
mm.free_memory(ptr, 1024 * 1024)

# æ£€æŸ¥ç»Ÿè®¡
stats = mm.get_statistics()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats.cache_hit_rate:.2%}")
print(f"å†…å­˜æ•ˆç‡: {stats.efficiency_score():.2%}")
```

### é«˜çº§é…ç½®
```python
# ä¸ºç‰¹å®šå·¥ä½œè´Ÿè½½é…ç½®å†…å­˜ç®¡ç†å™¨
mm.configure(
    segment_size=512 * 1024 * 1024,    # 512MBæ®µ
    cache_sizes={
        'stream_local': 100,            # æ¯æµ100ä¸ªå—
        'global': 500,                  # å…¨å±€ç¼“å­˜500ä¸ªå—
    },
    warmup_sizes=[
        (4096, 50),    # 50ä¸ª4KBå—
        (65536, 20),   # 20ä¸ª64KBå—  
        (1048576, 10), # 10ä¸ª1MBå—
    ]
)
```

### æ€§èƒ½ç›‘æ§
```python
# å¯ç”¨è¯¦ç»†æ€§èƒ½è·Ÿè¸ª
with mm.performance_context() as perf:
    # è¿è¡Œå†…å­˜å¯†é›†å‹æ“ä½œ
    tensors = [genesis.randn(1000, 1000) for _ in range(100)]
    
# åˆ†ææ€§èƒ½
print(f"æ€»åˆ†é…æ•°: {perf.stats.total_allocations}")
print(f"å³°å€¼å†…å­˜: {perf.stats.peak_memory_usage / 1024**3:.2f} GB")
print(f"ç¢ç‰‡åŒ–: {perf.stats.fragmentation_ratio:.2%}")
```

## é…ç½®å’Œè°ƒä¼˜

### ç¯å¢ƒå˜é‡
```bash
# å†…å­˜ç®¡ç†å™¨é…ç½®
export GENESIS_CUDA_SEGMENT_SIZE=1073741824     # 1GBæ®µ
export GENESIS_CUDA_CACHE_SIZE=1000             # ç¼“å­˜1000ä¸ªå—
export GENESIS_CUDA_WARMUP_ENABLED=true         # å¯ç”¨é¢„çƒ­
export GENESIS_CUDA_STATS_ENABLED=true          # å¯ç”¨ç»Ÿè®¡
```

### è¿è¡Œæ—¶é…ç½®
```python
# è¿è¡Œæ—¶é…ç½®
genesis.cuda.configure_memory_manager({
    'segment_size': 1024 * 1024 * 1024,  # 1GB
    'enable_warmup': True,
    'enable_stats': True,
    'allocation_strategy': 'best_fit',
    'coalescing_threshold': 0.1,
})
```

## æœ€ä½³å®è·µ

1. **ä½¿ç”¨é¢„çƒ­ç¼“å­˜**ï¼šé¢„åˆ†é…å¸¸è§å¤§å°ä»¥è·å¾—38å€æ€§èƒ½æå‡
2. **ç›‘æ§ç»Ÿè®¡**ï¼šè·Ÿè¸ªç¼“å­˜å‘½ä¸­ç‡å’Œå†…å­˜æ•ˆç‡
3. **æ‰¹é‡åˆ†é…**ï¼šå°†ç›¸ä¼¼æ“ä½œåˆ†ç»„ä»¥è·å¾—æ›´å¥½çš„å±€éƒ¨æ€§
4. **é¿å…é¢‘ç¹çš„å°åˆ†é…**ï¼šå¯¹äºå¾®å°å—ï¼Œç¼“å­˜å¼€é”€å ä¸»å¯¼
5. **ä½¿ç”¨é€‚å½“çš„æ®µå¤§å°**ï¼šå°†æ®µå¤§å°ä¸å·¥ä½œè´Ÿè½½å†…å­˜æ¨¡å¼åŒ¹é…

## æ•…éšœæ’é™¤

### å†…å­˜æ³„æ¼
```python
# è°ƒè¯•å†…å­˜æ³„æ¼
stats = mm.get_statistics()
if stats.total_allocations > stats.total_freed + 1000:
    print("è­¦å‘Šï¼šæ£€æµ‹åˆ°æ½œåœ¨å†…å­˜æ³„æ¼")
    mm.dump_allocation_trace()
```

### æ€§èƒ½é—®é¢˜
```python
# è¯Šæ–­æ€§èƒ½é—®é¢˜
if stats.cache_hit_rate < 0.5:
    print("ç¼“å­˜å‘½ä¸­ç‡ä½ - è€ƒè™‘é¢„çƒ­ç¼“å­˜")
    mm.analyze_allocation_patterns()

if stats.fragmentation_ratio > 0.3:
    print("é«˜ç¢ç‰‡åŒ– - å¯ç”¨æ¿€è¿›åˆå¹¶")
    mm.enable_aggressive_coalescing()
```

### å†…å­˜å‹åŠ›
```python
# å¤„ç†å†…å­˜å‹åŠ›
def handle_oom():
    """å†…å­˜ä¸è¶³å¤„ç†ç¨‹åº"""
    mm.cleanup_cache(force=True)
    mm.coalesce_free_blocks()
    mm.garbage_collect()
```

## ä¸Genesisçš„é›†æˆ

å†…å­˜ç®¡ç†å™¨ä¸Genesiså¼ é‡å’Œæ“ä½œæ— ç¼é›†æˆï¼š

```python
# ä¸å¼ é‡æ“ä½œçš„è‡ªåŠ¨é›†æˆ
x = genesis.randn(1000, 1000)  # è‡ªåŠ¨ä½¿ç”¨å†…å­˜ç®¡ç†å™¨
y = genesis.matmul(x, x)       # é«˜æ•ˆå†…å­˜é‡ç”¨
z = x + y                      # ç¼“å­˜ä¼˜åŒ–åˆ†é…
```

è¿™ä¸ªå…ˆè¿›çš„å†…å­˜ç®¡ç†ç³»ç»Ÿæ˜¯Genesisåœ¨ä¿æŒä»é›¶å¼€å§‹å®ç°çš„æ•™è‚²æ¸…æ™°æ€§çš„åŒæ—¶å®ç°æ¥è¿‘PyTorchæ€§èƒ½çš„å…³é”®å› ç´ ã€‚