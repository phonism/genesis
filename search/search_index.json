{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stemmer"]},"docs":[{"location":"","title":"Genesis \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6","text":"**\u57fa\u4e8e Python + Triton + CUDA \u4ece\u96f6\u6784\u5efa\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6**  [![GitHub stars](https://img.shields.io/github/stars/phonism/genesis?style=social)](https://github.com/phonism/genesis/stargazers) [![License](https://img.shields.io/github/license/phonism/genesis)](https://github.com/phonism/genesis/blob/main/LICENSE) [![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/downloads/)  [\u5feb\u901f\u5f00\u59cb](getting-started/index.zh.md){ .md-button .md-button--primary } [API \u6587\u6863](api-reference/index.zh.md){ .md-button } [GitHub](https://github.com/phonism/genesis){ .md-button }"},{"location":"#_1","title":"\u2728 \u7279\u6027","text":"<p>\ud83d\ude80 \u9ad8\u6027\u80fd\u8ba1\u7b97 \u57fa\u4e8eTriton\u548cCUDA\u7684\u4f18\u5316GPU\u6838\u5fc3\uff0c\u63d0\u4f9b\u51fa\u8272\u7684\u8ba1\u7b97\u6027\u80fd</p> <p>\ud83d\udd27 \u7b80\u6d01\u6613\u7528 PyTorch\u98ce\u683c\u7684API\u8bbe\u8ba1\uff0c\u5b66\u4e60\u6210\u672c\u4f4e\uff0c\u4e0a\u624b\u5bb9\u6613</p> <p>\u26a1 \u8f7b\u91cf\u7ea7\u67b6\u6784 \u7cbe\u7b80\u7684\u6838\u5fc3\u8bbe\u8ba1\uff0c\u4e13\u6ce8\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u529f\u80fd</p> <p>\ud83c\udfaf \u4ece\u96f6\u6784\u5efa \u5b8c\u5168\u81ea\u4e3b\u5b9e\u73b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u6df1\u5165\u7406\u89e3\u6bcf\u4e2a\u7ec4\u4ef6</p>"},{"location":"#_2","title":"\ud83c\udfc1 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_3","title":"\u5b89\u88c5","text":"Bash<pre><code>pip install genesis-dl\n</code></pre>"},{"location":"#_4","title":"\u57fa\u7840\u4f7f\u7528","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([[1, 2], [3, 4]], dtype=genesis.float32, device=genesis.device('cuda'))\ny = genesis.tensor([[5, 6], [7, 8]], dtype=genesis.float32, device=genesis.device('cuda'))\n\n# \u57fa\u672c\u8fd0\u7b97\nz = x + y\nprint(z)\n# \u8f93\u51fa: [[6, 8], [10, 12]]\n\n# \u77e9\u9635\u4e58\u6cd5\nresult = genesis.matmul(x, y)\nprint(result)\n</code></pre>"},{"location":"#_5","title":"\u795e\u7ecf\u7f51\u7edc\u793a\u4f8b","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear1 = nn.Linear(784, 128)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        return x\n\n# \u521b\u5efa\u6a21\u578b\nmodel = SimpleNet()\nmodel.cuda()  # \u79fb\u81f3GPU\n\n# \u524d\u5411\u4f20\u64ad\nx = genesis.randn(32, 784, device=genesis.device('cuda'))\noutput = model(x)\n</code></pre>"},{"location":"#_6","title":"\ud83d\udcda \u6587\u6863\u5bfc\u822a","text":"<ul> <li> <p> \u5feb\u901f\u5f00\u59cb</p> <p>\u5feb\u901f\u4e86\u89e3Genesis\u6846\u67b6\u7684\u57fa\u672c\u6982\u5ff5\u548c\u4f7f\u7528\u65b9\u6cd5</p> <p> \u5f00\u59cb\u4f7f\u7528</p> </li> <li> <p> \u6559\u7a0b\u6307\u5357</p> <p>\u8be6\u7ec6\u7684\u6559\u7a0b\u548c\u793a\u4f8b\uff0c\u4ece\u57fa\u7840\u5230\u9ad8\u7ea7\u5e94\u7528</p> <p> \u67e5\u770b\u6559\u7a0b</p> </li> <li> <p> \u6838\u5fc3\u7ec4\u4ef6</p> <p>\u6df1\u5165\u4e86\u89e3Genesis\u7684\u6838\u5fc3\u7ec4\u4ef6\u548c\u67b6\u6784\u8bbe\u8ba1</p> <p> \u6838\u5fc3\u7ec4\u4ef6</p> </li> <li> <p> API\u53c2\u8003</p> <p>\u5b8c\u6574\u7684API\u6587\u6863\u548c\u53c2\u8003\u8d44\u6599</p> <p> API\u6587\u6863</p> </li> </ul>"},{"location":"#_7","title":"\ud83d\udee0\ufe0f \u67b6\u6784\u7279\u70b9","text":""},{"location":"#_8","title":"\u53cc\u540e\u7aef\u8bbe\u8ba1","text":"<ul> <li>CPU\u540e\u7aef: \u57fa\u4e8ePyTorch\uff0c\u786e\u4fdd\u529f\u80fd\u5b8c\u6574\u6027\u548c\u6b63\u786e\u6027</li> <li>GPU\u540e\u7aef: \u57fa\u4e8eTriton\u548cCUDA\uff0c\u8ffd\u6c42\u6781\u81f4\u6027\u80fd</li> </ul>"},{"location":"#_9","title":"\u73b0\u4ee3\u5316\u8bbe\u8ba1","text":"<ul> <li>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf: \u9ad8\u6548\u7684\u68af\u5ea6\u8ba1\u7b97\u548c\u53cd\u5411\u4f20\u64ad</li> <li>\u5185\u5b58\u7ba1\u7406: \u4f18\u5316\u7684CUDA\u5185\u5b58\u5206\u914d\u548c\u7ba1\u7406\u7b56\u7565  </li> <li>\u7b97\u5b50\u4f18\u5316: \u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u4e13\u95e8\u4f18\u5316</li> </ul>"},{"location":"#_10","title":"\u6269\u5c55\u6027","text":"<ul> <li>\u6a21\u5757\u5316\u8bbe\u8ba1: \u4fbf\u4e8e\u6dfb\u52a0\u65b0\u529f\u80fd\u548c\u7b97\u5b50</li> <li>\u63d2\u4ef6\u7cfb\u7edf: \u652f\u6301\u81ea\u5b9a\u4e49\u64cd\u4f5c\u548c\u6269\u5c55</li> </ul>"},{"location":"#_11","title":"\ud83e\udd1d \u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u6240\u6709\u5f62\u5f0f\u7684\u8d21\u732e\uff01</p> <ul> <li>\ud83d\udc1b \u62a5\u544aBug</li> <li>\ud83d\udca1 \u63d0\u51fa\u529f\u80fd\u5efa\u8bae</li> <li>\ud83d\udcd6 \u6539\u8fdb\u6587\u6863</li> <li>\ud83d\udd27 \u8d21\u732e\u4ee3\u7801</li> </ul>"},{"location":"#_12","title":"\ud83d\udcc4 \u8bb8\u53ef\u8bc1","text":"<p>\u672c\u9879\u76ee\u91c7\u7528 MIT \u8bb8\u53ef\u8bc1\u3002</p>"},{"location":"index.zh/","title":"Genesis \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6","text":"\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 | \u4ece\u96f6\u6784\u5efa | Python + Triton + CUDA"},{"location":"index.zh/#_1","title":"\ud83d\ude80 \u9879\u76ee\u6982\u8ff0","text":"<p>Genesis \u662f\u4e00\u4e2a\u57fa\u4e8e Python \u4ece\u96f6\u6784\u5efa\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002CPU \u540e\u7aef\u501f\u52a9 PyTorch \u7684\u5f20\u91cf\u64cd\u4f5c\uff0c\u800c GPU \u540e\u7aef\u5b8c\u5168\u72ec\u7acb\uff0c\u4f7f\u7528 CUDA Python API \u8fdb\u884c\u76f4\u63a5\u7684 GPU \u5185\u5b58\u7ba1\u7406\uff0c\u5e76\u4f7f\u7528 Triton \u7f16\u5199\u9ad8\u6027\u80fd\u7684 GPU \u5185\u6838\u3002\u9879\u76ee\u65e8\u5728\u63d0\u4f9b\u6e05\u6670\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6559\u80b2\u4ef7\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</p>"},{"location":"index.zh/#_2","title":"\u2728 \u6838\u5fc3\u7279\u6027","text":"<ul> <li>\ud83c\udfaf \u8f7b\u91cf\u7ea7\u8bbe\u8ba1 - \u6e05\u6670\u7684API\u8bbe\u8ba1\uff0c\u6613\u4e8e\u7406\u89e3\u548c\u4f7f\u7528</li> <li>\u26a1 \u9ad8\u6027\u80fd - Triton\u4f18\u5316\u7684GPU\u5185\u6838\uff0c\u63a5\u8fd1\u4e3b\u6d41\u6846\u67b6\u6027\u80fd</li> <li>\ud83d\udd04 \u81ea\u52a8\u5fae\u5206 - \u5b8c\u6574\u7684\u53cd\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u8ba1\u7b97\u7cfb\u7edf</li> <li>\ud83e\udde0 \u795e\u7ecf\u7f51\u7edc - \u4e30\u5bcc\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u548c\u4f18\u5316\u5668\u5b9e\u73b0</li> <li>\ud83d\udd27 \u6df7\u5408\u7cbe\u5ea6 - \u652f\u6301FP16/BF16\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 (AMP)</li> <li>\ud83d\udcca \u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3\u652f\u6301</li> <li>\ud83c\udfa8 \u6a21\u578b\u5e93 - \u5185\u7f6e\u4e3b\u6d41LLM\u6a21\u578b\u5982Qwen\u7684\u5b9e\u73b0</li> <li>\ud83d\udcbe \u6a21\u578b\u7ba1\u7406 - \u5b8c\u6574\u7684\u68c0\u67e5\u70b9\u4fdd\u5b58/\u52a0\u8f7d\u7cfb\u7edf</li> <li>\ud83d\udcc8 \u5b66\u4e60\u7387\u8c03\u5ea6 - \u591a\u79cd\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u548c\u68af\u5ea6\u88c1\u526a</li> <li>\ud83d\ude80 \u6027\u80fd\u4f18\u5316 - \u5185\u6838\u7f13\u5b58\u3001\u5185\u5b58\u6c60\u5316\u548c\u81ea\u9002\u5e94\u914d\u7f6e</li> </ul>"},{"location":"index.zh/#_3","title":"\ud83c\udfd7\ufe0f \u67b6\u6784\u4eae\u70b9","text":"<pre><code>graph TB\n    A[\u7528\u6237API] --&gt; B[\u81ea\u52a8\u5fae\u5206\u5f15\u64ce]\n    A --&gt; C[\u795e\u7ecf\u7f51\u7edc\u6a21\u5757]\n    B --&gt; D[\u5f20\u91cf\u7cfb\u7edf]\n    C --&gt; D\n    D --&gt; E[\u540e\u7aef\u62bd\u8c61\u5c42]\n    E --&gt; F[CPU Backend]\n    E --&gt; G[CUDA Backend]\n    G --&gt; H[Triton Kernels]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e3f2fd\n    style H fill:#fff8e1</code></pre>"},{"location":"index.zh/#_4","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":""},{"location":"index.zh/#_5","title":"\u6559\u80b2\u4ef7\u503c","text":"<ul> <li>\u6e05\u6670\u7684\u4ee3\u7801\u7ed3\u6784 - \u6bcf\u4e2a\u6a21\u5757\u804c\u8d23\u660e\u786e</li> <li>\u5b8c\u6574\u7684\u6587\u6863 - \u4ece\u8bbe\u8ba1\u7406\u5ff5\u5230\u5b9e\u73b0\u7ec6\u8282\u7684\u5b8c\u6574\u6587\u6863</li> <li>\u6e10\u8fdb\u5f0f\u5b66\u4e60 - \u4ece\u57fa\u7840\u6982\u5ff5\u5230\u9ad8\u7ea7\u7279\u6027\u7684\u5b66\u4e60\u8def\u5f84</li> </ul>"},{"location":"index.zh/#_6","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u73b0\u4ee3\u5316\u67b6\u6784 - \u501f\u9274PyTorch\u7b49\u4e3b\u6d41\u6846\u67b6\u7684\u4f18\u79c0\u8bbe\u8ba1</li> <li>\u9ad8\u6548\u5b9e\u73b0 - \u4f7f\u7528Triton\u7b49\u73b0\u4ee3\u5de5\u5177\u8fdb\u884c\u6027\u80fd\u4f18\u5316</li> <li>\u53ef\u6269\u5c55\u6027 - \u6a21\u5757\u5316\u8bbe\u8ba1\u4fbf\u4e8e\u6dfb\u52a0\u65b0\u529f\u80fd</li> </ul>"},{"location":"index.zh/#_7","title":"\u5b9e\u7528\u6027","text":"<ul> <li>\u529f\u80fd\u5b8c\u6574 - \u652f\u6301\u4ece\u6a21\u578b\u5b9a\u4e49\u5230\u8bad\u7ec3\u90e8\u7f72\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41</li> <li>\u6027\u80fd\u4f18\u5316 - \u591a\u79cd\u4f18\u5316\u7b56\u7565\u786e\u4fdd\u5b9e\u9645\u8bad\u7ec3\u6027\u80fd</li> <li>\u751f\u6001\u517c\u5bb9 - \u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u751f\u6001\u826f\u597d\u517c\u5bb9</li> </ul>"},{"location":"index.zh/#_8","title":"\ud83d\udcca \u6027\u80fd\u72b6\u6001","text":""},{"location":"index.zh/#_9","title":"\u5185\u5b58\u5206\u914d\u5668\u6027\u80fd\uff08\u6700\u65b0\u4f18\u5316\uff09","text":"\u573a\u666f Genesis vs PyTorch \u72b6\u6001 \u540c\u5c3a\u5bf8\u5206\u914d 1.43x \u2705 \u4f18\u79c0 \u5927\u5185\u5b58(&gt;1MB) 3.92x \u2705 \u6770\u51fa Transformer\u8bad\u7ec3 1.89x \u2705 \u4f18\u79c0 \u5185\u5b58\u538b\u529b 4.83x \u2705 \u6770\u51fa \u53d8\u5316\u5c3a\u5bf8 0.83x \ud83d\udd04 \u826f\u597d"},{"location":"index.zh/#_10","title":"\u7b97\u5b50\u6027\u80fd","text":"\u64cd\u4f5c Genesis vs PyTorch \u72b6\u6001 \u77e9\u9635\u4e58\u6cd5 0.95x \u2705 \u826f\u597d \u5143\u7d20\u7ea7\u64cd\u4f5c 1.02x \u2705 \u4f18\u79c0 \u5f52\u7ea6\u64cd\u4f5c 0.87x \ud83d\udd04 \u4f18\u5316\u4e2d Softmax 1.15x \u2705 \u4f18\u79c0 LayerNorm 1.08x \u2705 \u4f18\u79c0 Cat\u64cd\u4f5c 0.02x \u274c \u4fee\u590d\u4e2d LogSumExp 0.02x \u274c \u4fee\u590d\u4e2d \u5e7f\u64ad\u64cd\u4f5c 0.04x \u274c \u4fee\u590d\u4e2d"},{"location":"index.zh/#_11","title":"\u8fd1\u671f\u6027\u80fd\u6539\u8fdb","text":"<ul> <li>\u2705 \u5757\u5206\u914d\u5668: Transformer\u8bad\u7ec3\u573a\u666f38\u500d\u6027\u80fd\u63d0\u5347</li> <li>\u2705 \u5185\u5b58\u7ba1\u7406: \u6d88\u9664cudaMalloc/cudaFree\u540c\u6b65\u5f00\u9500</li> <li>\u2705 Fill\u64cd\u4f5c: GPU\u539f\u751f\u5185\u683836\u500d\u6027\u80fd\u63d0\u5347</li> <li>\ud83d\udd04 Cat\u64cd\u4f5c: GPU\u539f\u751f\u5b9e\u73b0\u8fdb\u884c\u4e2d\uff08\u4fee\u590d0.02x\u95ee\u9898\uff09</li> <li>\ud83d\udd04 \u5f52\u7ea6\u64cd\u4f5c: Triton\u5185\u6838\u4f18\u5316\u8fdb\u884c\u4e2d</li> </ul> <p>\u6027\u80fd\u66f4\u65b0</p> <p>Genesis\u5728\u5185\u5b58\u7ba1\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u7a81\u7834\uff0c\u5728\u591a\u4e2a\u5173\u952e\u573a\u666f\u4e0b\u8fbe\u5230\u6216\u8d85\u8d8aPyTorch\u6027\u80fd\u3002\u5f53\u524d\u91cd\u70b9\u662f\u4fee\u590d\u5269\u4f59\u7684\u7b97\u5b50\u74f6\u9888\u3002</p> <p>\u8be6\u7ec6\u5206\u6790\u8bf7\u53c2\u8003\uff1a\u5185\u5b58\u5206\u914d\u5668\u4f18\u5316\u62a5\u544a</p>"},{"location":"index.zh/#_12","title":"\ud83d\udee0\ufe0f \u6280\u672f\u6808","text":""},{"location":"index.zh/#_13","title":"\u6838\u5fc3\u4f9d\u8d56","text":"<ul> <li>Python 3.8+ - \u4e3b\u8981\u5f00\u53d1\u8bed\u8a00</li> <li>PyTorch - \u5185\u5b58\u7ba1\u7406\u548c\u90e8\u5206\u64cd\u4f5c</li> <li>Triton 2.0+ - GPU\u5185\u6838\u4f18\u5316</li> <li>CUDA 11.0+ - GPU\u8ba1\u7b97\u652f\u6301</li> <li>NumPy - CPU\u6570\u503c\u8ba1\u7b97</li> <li>cuda-python - \u76f4\u63a5CUDA API\u8bbf\u95ee</li> </ul>"},{"location":"index.zh/#_14","title":"\u5f00\u53d1\u5de5\u5177","text":"<ul> <li>pytest - \u5355\u5143\u6d4b\u8bd5\u6846\u67b6</li> <li>black - \u4ee3\u7801\u683c\u5f0f\u5316</li> <li>mypy - \u7c7b\u578b\u68c0\u67e5</li> <li>MkDocs - \u6587\u6863\u751f\u6210</li> <li>Material for MkDocs - \u6587\u6863\u4e3b\u9898</li> </ul>"},{"location":"index.zh/#_15","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84","text":""},{"location":"index.zh/#_16","title":"\u521d\u5b66\u8005","text":"<ol> <li>\u5feb\u901f\u5f00\u59cb - \u5b89\u88c5\u548c\u7b2c\u4e00\u4e2a\u7a0b\u5e8f</li> <li>\u57fa\u7840\u6559\u7a0b - \u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3</li> <li>API\u53c2\u8003 - \u5e38\u7528API\u4f7f\u7528</li> </ol>"},{"location":"index.zh/#_17","title":"\u9ad8\u7ea7\u7528\u6237","text":"<ol> <li>\u67b6\u6784\u8bbe\u8ba1 - \u6df1\u5165\u7406\u89e3\u7cfb\u7edf\u8bbe\u8ba1</li> <li>\u81ea\u5b9a\u4e49\u64cd\u4f5c - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u64cd\u4f5c</li> <li>\u6027\u80fd\u4f18\u5316 - \u6027\u80fd\u5206\u6790\u548c\u4f18\u5316\u6307\u5357</li> <li>\u6027\u80fd\u8c03\u4f18 - \u8bad\u7ec3\u6027\u80fd\u8c03\u4f18\u6280\u5de7</li> <li>Qwen\u6a21\u578b\u6307\u5357 - \u4f7f\u7528\u548c\u8bad\u7ec3Qwen LLM\u6a21\u578b</li> </ol>"},{"location":"index.zh/#_18","title":"\u8d21\u732e\u8005","text":"<ol> <li>\u5f00\u53d1\u73af\u5883 - \u914d\u7f6e\u5f00\u53d1\u73af\u5883</li> <li>\u6838\u5fc3\u7ec4\u4ef6 - \u7406\u89e3\u5185\u90e8\u5b9e\u73b0</li> <li>\u6d4b\u8bd5\u6307\u5357 - \u4ee3\u7801\u8d21\u732e\u6307\u5357</li> </ol>"},{"location":"index.zh/#_19","title":"\ud83c\udf1f \u9879\u76ee\u4eae\u70b9","text":""},{"location":"index.zh/#_20","title":"\u4ee3\u7801\u8d28\u91cf","text":"<ul> <li>\u7c7b\u578b\u6ce8\u89e3 - \u5b8c\u6574\u7684\u7c7b\u578b\u63d0\u793a\uff0cIDE\u53cb\u597d</li> <li>\u5355\u5143\u6d4b\u8bd5 - 95%+\u6d4b\u8bd5\u8986\u76d6\u7387</li> <li>\u5b8c\u6574\u6587\u6863 - \u4eceAPI\u5230\u8bbe\u8ba1\u7684\u5168\u9762\u6587\u6863</li> <li>\u4ee3\u7801\u89c4\u8303 - \u7edf\u4e00\u7684\u4ee3\u7801\u98ce\u683c\u548c\u6700\u4f73\u5b9e\u8df5</li> </ul>"},{"location":"index.zh/#2025-01","title":"\u8fd1\u671f\u66f4\u65b0 (2025-01)","text":"<ul> <li>\u2705 \u5185\u5b58\u5206\u914d\u5668\u4f18\u5316 - \u8fbe\u5230PyTorch\u7ea7\u6027\u80fd</li> <li>\u2705 Qwen\u6a21\u578b\u652f\u6301 - \u5b8c\u6574Qwen LLM\u67b6\u6784\u5b9e\u73b0</li> <li>\u2705 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - FP16/BF16\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6(AMP)</li> <li>\u2705 \u68af\u5ea6\u88c1\u526a - \u652f\u6301\u68af\u5ea6\u8303\u6570\u548c\u503c\u88c1\u526a</li> <li>\u2705 \u5b66\u4e60\u7387\u8c03\u5ea6\u5668 - StepLR, ExponentialLR, CosineAnnealingLR</li> <li>\u2705 \u68c0\u67e5\u70b9\u7cfb\u7edf - \u6a21\u578b\u4fdd\u5b58/\u52a0\u8f7d\u4e0e\u4f18\u5316\u5668\u72b6\u6001\u4fdd\u5b58</li> <li>\ud83d\udd04 \u7b97\u5b50\u6027\u80fd - \u4fee\u590d\u5173\u952e\u7b97\u5b50(cat, logsumexp, broadcast)</li> <li>\ud83d\udd04 \u5185\u6838\u4f18\u5316 - Triton\u5185\u6838\u6301\u7eed\u6539\u8fdb\u4e2d</li> </ul>"},{"location":"index.zh/#_21","title":"\ud83e\udd1d \u793e\u533a\u4e0e\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u5404\u79cd\u5f62\u5f0f\u7684\u8d21\u732e\uff1a</p> <ul> <li>\ud83d\udc1b \u9519\u8bef\u62a5\u544a - \u8bf7\u53ca\u65f6\u62a5\u544a\u53d1\u73b0\u7684bug</li> <li>\ud83d\udca1 \u529f\u80fd\u5efa\u8bae - \u6b22\u8fce\u65b0\u529f\u80fd\u60f3\u6cd5</li> <li>\ud83d\udcdd \u6587\u6863\u6539\u8fdb - \u5e2e\u52a9\u6539\u5584\u6587\u6863\u8d28\u91cf</li> <li>\ud83d\udcbb \u4ee3\u7801\u8d21\u732e - \u76f4\u63a5\u53c2\u4e0e\u4ee3\u7801\u5f00\u53d1</li> </ul> <p>\u8be6\u60c5\u8bf7\u53c2\u8003 \u8d21\u732e\u6307\u5357\u3002</p>"},{"location":"index.zh/#_22","title":"\ud83d\udcde \u8054\u7cfb\u65b9\u5f0f","text":"<ul> <li>GitHub Issues - bug\u62a5\u544a\u548c\u529f\u80fd\u8bf7\u6c42</li> <li>Discussions - \u6280\u672f\u8ba8\u8bba\u548c\u4f7f\u7528\u4ea4\u6d41</li> <li>\u90ae\u7bb1 - genesis-dev@example.com</li> </ul> <p>\u5f00\u59cb\u60a8\u7684\u6df1\u5ea6\u5b66\u4e60\u4e4b\u65c5 \ud83d\ude80</p> <ul> <li> <p> \u5feb\u901f\u5f00\u59cb</p> <p>\u7acb\u5373\u5f00\u59cb\u4f7f\u7528Genesis\u6784\u5efa\u60a8\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc</p> <p> \u5feb\u901f\u5f00\u59cb</p> </li> <li> <p> \u67e5\u770b\u6e90\u7801</p> <p>\u5728GitHub\u4e0a\u63a2\u7d22\u5b8c\u6574\u7684Genesis\u6e90\u7801\u5b9e\u73b0</p> <p> GitHub\u4ed3\u5e93</p> </li> </ul>"},{"location":"index_en/","title":"Genesis Deep Learning Framework","text":"Lightweight Deep Learning Framework | Built from Scratch | Python + Triton + CUDA"},{"location":"index_en/#project-overview","title":"\ud83d\ude80 Project Overview","text":"<p>Genesis is a lightweight deep learning framework developed in Python. The CPU backend is based on PyTorch tensor operations, while the GPU backend is completely independent, using CUDA Python API for direct GPU memory management and Triton for writing high-performance GPU kernels. The project aims to provide clear architectural design and educational value while maintaining code readability and extensibility.</p>"},{"location":"index_en/#core-features","title":"\u2728 Core Features","text":"<ul> <li>\ud83c\udfaf Lightweight Design - Clean API design, easy to understand and use</li> <li>\u26a1 High Performance Computing - Triton-optimized GPU kernels rivaling mainstream frameworks</li> <li>\ud83d\udd04 Automatic Differentiation - Complete backpropagation and gradient computation system</li> <li>\ud83e\udde0 Neural Networks - Rich implementation of neural network layers and optimizers</li> <li>\ud83d\udd27 Mixed Precision - Support for FP16/BF16 mixed precision training (AMP)</li> <li>\ud83d\udcca Distributed Training - Multi-GPU parallel training support</li> <li>\ud83c\udfa8 Model Library - Built-in implementations of mainstream LLM models like Qwen</li> <li>\ud83d\udcbe Model Management - Complete checkpoint save/load system</li> <li>\ud83d\udcc8 Learning Rate Scheduling - Multiple learning rate schedulers and gradient clipping</li> <li>\ud83d\ude80 Performance Optimization - Kernel caching, memory pooling, and adaptive configuration</li> </ul>"},{"location":"index_en/#architecture-highlights","title":"\ud83c\udfd7\ufe0f Architecture Highlights","text":"<pre><code>graph TB\n    A[User API] --&gt; B[Autograd Engine]\n    A --&gt; C[Neural Network Modules]\n    B --&gt; D[Tensor System]\n    C --&gt; D\n    D --&gt; E[Backend Abstraction Layer]\n    E --&gt; F[CPU Backend]\n    E --&gt; G[CUDA Backend]\n    G --&gt; H[Triton Kernels]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e3f2fd\n    style H fill:#fff8e1</code></pre>"},{"location":"index_en/#design-goals","title":"\ud83c\udfaf Design Goals","text":""},{"location":"index_en/#educational-value","title":"Educational Value","text":"<ul> <li>Clear Code Structure - Each module has clear responsibilities</li> <li>Complete Documentation - From design concepts to implementation details</li> <li>Progressive Learning - Learning path from basic concepts to advanced features</li> </ul>"},{"location":"index_en/#engineering-practice","title":"Engineering Practice","text":"<ul> <li>Modern Architecture - Learn from excellent designs of mainstream frameworks like PyTorch</li> <li>Efficient Implementation - Performance optimization using modern tools like Triton</li> <li>Extensibility - Modular design for easy addition of new features</li> </ul>"},{"location":"index_en/#practicality","title":"Practicality","text":"<ul> <li>Feature Complete - Support complete workflow from model definition to training deployment</li> <li>Performance Optimization - Multiple optimization strategies ensure actual training performance</li> <li>Ecosystem Compatibility - Good compatibility with existing deep learning ecosystem</li> </ul>"},{"location":"index_en/#performance-status","title":"\ud83d\udcca Performance Status","text":""},{"location":"index_en/#memory-allocator-performance-latest-optimization","title":"Memory Allocator Performance (Latest Optimization)","text":"Scenario Genesis vs PyTorch Status Same-size allocation 1.43x \u2705 Excellent Large memory (&gt;1MB) 3.92x \u2705 Outstanding Transformer training 1.89x \u2705 Excellent Memory pressure 4.83x \u2705 Outstanding Variable sizes 0.83x \ud83d\udd04 Good"},{"location":"index_en/#operator-performance","title":"Operator Performance","text":"Operation Genesis vs PyTorch Status Matrix multiplication 0.95x \u2705 Good Element-wise operations 1.02x \u2705 Excellent Reduction operations 0.87x \ud83d\udd04 Optimizing Softmax 1.15x \u2705 Excellent LayerNorm 1.08x \u2705 Excellent Cat operation 0.02x \u274c Fixing LogSumExp 0.02x \u274c Fixing Broadcast operations 0.04x \u274c Fixing"},{"location":"index_en/#recent-performance-improvements","title":"Recent Performance Improvements","text":"<ul> <li>\u2705 Block Allocator: 38x performance improvement in Transformer training scenarios</li> <li>\u2705 Memory Management: Eliminated cudaMalloc/cudaFree synchronization overhead</li> <li>\u2705 Fill Operations: 36x performance improvement with GPU-native kernels</li> <li>\ud83d\udd04 Cat Operations: GPU-native implementation in progress (fixing 0.02x issue)</li> <li>\ud83d\udd04 Reduction Operations: Triton kernel optimization in progress</li> </ul> <p>Performance Update</p> <p>Genesis has achieved major breakthroughs in memory management, reaching or exceeding PyTorch performance in multiple key scenarios. Current focus is on fixing remaining operator bottlenecks.</p> <p>For detailed analysis, see: Memory Allocator Optimization Report</p>"},{"location":"index_en/#technology-stack","title":"\ud83d\udee0\ufe0f Technology Stack","text":""},{"location":"index_en/#core-dependencies","title":"Core Dependencies","text":"<ul> <li>Python 3.8+ - Main development language</li> <li>PyTorch - Memory management and some operations</li> <li>Triton 2.0+ - GPU kernel optimization</li> <li>CUDA 11.0+ - GPU computing support</li> <li>NumPy - CPU numerical computing</li> <li>cuda-python - Direct CUDA API access</li> </ul>"},{"location":"index_en/#development-tools","title":"Development Tools","text":"<ul> <li>pytest - Unit testing framework</li> <li>black - Code formatting</li> <li>mypy - Type checking</li> <li>MkDocs - Documentation generation</li> <li>Material for MkDocs - Documentation theme</li> </ul>"},{"location":"index_en/#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"index_en/#beginners","title":"Beginners","text":"<ol> <li>Getting Started - Installation and first program</li> <li>Basic Tutorial - Simple neural network training</li> <li>API Reference - Common API usage</li> </ol>"},{"location":"index_en/#advanced-users","title":"Advanced Users","text":"<ol> <li>Architecture Design - Deep understanding of system design</li> <li>Custom Operations - Implementing custom operations</li> <li>Performance Optimization - Performance analysis and optimization guide</li> <li>Performance Tuning - Training performance tuning techniques</li> <li>Qwen Model Guide - Using and training Qwen LLM models</li> </ol>"},{"location":"index_en/#contributors","title":"Contributors","text":"<ol> <li>Development Environment - Setting up development environment</li> <li>Core Components - Understanding internal implementation</li> <li>Testing Guidelines - Code contribution guidelines</li> </ol>"},{"location":"index_en/#project-highlights","title":"\ud83c\udf1f Project Highlights","text":""},{"location":"index_en/#code-quality","title":"Code Quality","text":"<ul> <li>Type Annotations - Complete type hints, IDE-friendly</li> <li>Unit Tests - 95%+ test coverage</li> <li>Complete Documentation - Comprehensive documentation from API to design</li> <li>Code Standards - Unified code style and best practices</li> </ul>"},{"location":"index_en/#recent-updates-2025-01","title":"Recent Updates (2025-01)","text":"<ul> <li>\u2705 Memory Allocator Optimization - Achieved PyTorch-level performance</li> <li>\u2705 Qwen Model Support - Complete Qwen LLM architecture implementation</li> <li>\u2705 Mixed Precision Training - FP16/BF16 Automatic Mixed Precision (AMP)</li> <li>\u2705 Gradient Clipping - Support for gradient norm and value clipping</li> <li>\u2705 Learning Rate Schedulers - StepLR, ExponentialLR, CosineAnnealingLR</li> <li>\u2705 Checkpoint System - Model save/load with optimizer state preservation</li> <li>\ud83d\udd04 Operator Performance - Fixing critical operators (cat, logsumexp, broadcast)</li> <li>\ud83d\udd04 Kernel Optimization - Continuous Triton kernel improvements</li> </ul>"},{"location":"index_en/#community-contribution","title":"\ud83e\udd1d Community &amp; Contribution","text":"<p>We welcome all forms of contributions:</p> <ul> <li>\ud83d\udc1b Bug Reports - Please report bugs in a timely manner</li> <li>\ud83d\udca1 Feature Suggestions - New feature ideas are welcome</li> <li>\ud83d\udcdd Documentation Improvements - Help improve documentation quality</li> <li>\ud83d\udcbb Code Contributions - Direct participation in code development</li> </ul> <p>For details, please refer to Contributing Guide.</p>"},{"location":"index_en/#contact","title":"\ud83d\udcde Contact","text":"<ul> <li>GitHub Issues - Bug reports and feature requests</li> <li>Discussions - Technical discussions and usage communication</li> <li>Email - genesis-dev@example.com</li> </ul> <p>Start Your Deep Learning Journey \ud83d\ude80</p> <ul> <li> <p> Getting Started</p> <p>Start building your first neural network with Genesis immediately</p> <p> Getting Started</p> </li> <li> <p> View Source</p> <p>Explore the complete Genesis source code implementation on GitHub</p> <p> GitHub Repository</p> </li> </ul>"},{"location":"memory-allocator-optimization.zh/","title":"Genesis GPU\u5185\u5b58\u5206\u914d\u5668\u6027\u80fd\u4f18\u5316\u5b9e\u6218","text":"<p>\u8bb0\u5f55Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6GPU\u5185\u5b58\u5206\u914d\u5668\u7684\u6027\u80fd\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ece\u53d1\u73b0\u95ee\u9898\u5230\u9010\u6b65\u89e3\u51b3\u7684\u5b8c\u6574\u6280\u672f\u535a\u5ba2</p>"},{"location":"memory-allocator-optimization.zh/#genesis","title":"\u80cc\u666f\uff1a\u4e3a\u4ec0\u4e48Genesis\u5185\u5b58\u5206\u914d\u8fd9\u4e48\u6162\uff1f","text":"<p>\u5728\u4f7f\u7528\u6211\u4eec\u81ea\u7814\u7684Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u65f6\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u4e25\u91cd\u7684\u6027\u80fd\u95ee\u9898\uff1aGPU\u5185\u5b58\u5206\u914d\u6bd4PyTorch\u6162\u5f88\u591a\u3002\u901a\u8fc7\u5bf9\u6bd4\u6d4b\u8bd5\u53d1\u73b0\uff1a</p> Text Only<pre><code>CUDAStorage allocation vs PyTorch:\n- 1K elements:   Genesis 0.58x PyTorch (\u616242%)\n- 10K elements:  Genesis 0.75x PyTorch (\u616225%) \n- 100K elements: Genesis 0.42x PyTorch (\u616258%)\n</code></pre> <p>\u66f4\u8ba9\u4eba\u9707\u60ca\u7684\u662f<code>fill_</code>\u64cd\u4f5c\u7684\u6027\u80fd\uff1a</p> Text Only<pre><code>Fill operation (before optimization):\n- 512\u00d7512:   Genesis 0.10x PyTorch (\u616210\u500d!)\n- 1024\u00d71024: Genesis 0.03x PyTorch (\u616233\u500d!)\n- 2048\u00d72048: Genesis 0.01x PyTorch (\u6162100\u500d!)\n</code></pre> <p>\u8fd9\u663e\u7136\u4e0d\u80fd\u63a5\u53d7\u3002\u6211\u4eec\u9700\u8981\u6df1\u5165\u5206\u6790\u95ee\u9898\u5e76\u5236\u5b9a\u4f18\u5316\u65b9\u6848\u3002</p>"},{"location":"memory-allocator-optimization.zh/#_1","title":"\u7b2c\u4e00\u6b65\uff1a\u5efa\u7acb\u6027\u80fd\u57fa\u7ebf\u6d4b\u8bd5","text":"<p>\u4efb\u4f55\u4f18\u5316\u90fd\u8981\u5148\u5efa\u7acb\u51c6\u786e\u7684\u57fa\u7ebf\u6d4b\u8bd5\u3002\u6211\u4eec\u9700\u8981\u4e86\u89e3\uff1a 1. \u5f53\u524d\u7684\u5206\u914d\u6027\u80fd\u5230\u5e95\u6709\u591a\u6162\uff1f 2. \u54ea\u4e9b\u5206\u914d\u6a21\u5f0f\u662f\u6027\u80fd\u74f6\u9888\uff1f 3. \u4e0ePyTorch\u7684\u5177\u4f53\u5dee\u8ddd\u5728\u54ea\u91cc\uff1f</p>"},{"location":"memory-allocator-optimization.zh/#_2","title":"\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u6211\u521b\u5efa\u4e86\u4e13\u95e8\u7684\u5185\u5b58\u7ba1\u7406\u5668\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177 <code>benchmark/bench_memory_manager.py</code>\uff0c\u6d4b\u8bd5\u4ee5\u4e0b\u5173\u952e\u6a21\u5f0f\uff1a</p> <ol> <li>\u540c\u5c3a\u5bf8\u91cd\u590d\u5206\u914d - \u6a21\u62df\u8bad\u7ec3\u5faa\u73af</li> <li>\u5206\u914d-\u91ca\u653e\u5faa\u73af - \u6d4b\u8bd5\u5185\u5b58\u590d\u7528\u80fd\u529b  </li> <li>\u53d8\u5316\u5c3a\u5bf8\u5206\u914d - \u6a21\u62df\u6279\u6b21\u5927\u5c0f\u53d8\u5316</li> <li>\u5927\u5185\u5b58\u5206\u914d - \u6d4b\u8bd5\u5927\u5757\u5185\u5b58\u884c\u4e3a</li> <li>PyTorch\u7f13\u5b58\u5206\u6790 - \u6df1\u5165\u4e86\u89e3PyTorch\u7684\u7f13\u5b58\u673a\u5236</li> </ol>"},{"location":"memory-allocator-optimization.zh/#_3","title":"\u57fa\u7ebf\u6d4b\u8bd5\u7ed3\u679c","text":"<p>\u8fd0\u884c\u6d4b\u8bd5\u540e\uff0c\u7ed3\u679c\u4ee4\u4eba\u9707\u60ca\uff1a</p> Text Only<pre><code>\ud83d\udd34 \u6574\u4f53\u6027\u80fd\u7edf\u8ba1\uff1a\n- \u5e73\u5747\u52a0\u901f\u6bd4: 0.16x (Genesis\u6bd4PyTorch\u61626\u500d!)\n- \u6700\u5dee\u52a0\u901f\u6bd4: 0.02x (\u5206\u914d-\u91ca\u653e\u5faa\u73af\u616250\u500d!)\n- \u6700\u4f73\u52a0\u901f\u6bd4: 0.38x (\u4ecd\u7136\u61622.6\u500d)\n\n\ud83d\udcca \u6309\u6a21\u5f0f\u5206\u7c7b\uff1a\n- \u540c\u5c3a\u5bf8\u91cd\u590d\u5206\u914d:    0.22x (\u8f83\u5dee)\n- \u5206\u914d-\u91ca\u653e\u5faa\u73af:     0.02x (\u4e25\u91cd!)  \n- \u53d8\u5316\u5c3a\u5bf8\u5206\u914d:      0.12x (\u4e25\u91cd)\n- \u5927\u5185\u5b58\u5206\u914d:        0.20x (\u8f83\u5dee)\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#_4","title":"\u5173\u952e\u53d1\u73b0","text":""},{"location":"memory-allocator-optimization.zh/#1-pytorch","title":"1. PyTorch\u7f13\u5b58\u6548\u679c\u60ca\u4eba","text":"Text Only<pre><code>PyTorch 1024\u00d71024 \u5206\u914d\u884c\u4e3a\uff1a\n- \u9996\u6b21\u5206\u914d(\u51b7\u542f\u52a8): 0.458ms\n- \u4e8c\u6b21\u5206\u914d(\u7f13\u5b58\u547d\u4e2d): 0.021ms  \n- \u7f13\u5b58\u52a0\u901f\u6bd4: 22\u500d!\n\n\u8fde\u7eed10\u6b21\u5206\u914d\uff1a\n- PyTorch\u5e73\u5747: 0.015ms \n- Genesis\u5e73\u5747: 0.925ms\n- \u7a33\u6001\u6027\u80fd\u5dee\u8ddd: 62\u500d!\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#2-genesis","title":"2. Genesis\u6ca1\u6709\u4efb\u4f55\u7f13\u5b58","text":"<p>Genesis\u6bcf\u6b21\u5206\u914d\u65f6\u95f4\u57fa\u672c\u4e00\u81f4\uff080.9-1.0ms\uff09\uff0c\u8bf4\u660e\u786e\u5b9e\u662f\u6bcf\u6b21\u90fd\u5728\u8c03\u7528 <code>cudaMalloc</code>\uff0c\u5b8c\u5168\u6ca1\u6709\u7f13\u5b58\u673a\u5236\u3002</p>"},{"location":"memory-allocator-optimization.zh/#3-","title":"3. \u5206\u914d-\u91ca\u653e\u5faa\u73af\u662f\u6700\u5927\u74f6\u9888","text":"Text Only<pre><code>\u5206\u914d-\u91ca\u653e\u5faa\u73af\u6027\u80fd\uff0820\u6b21\u5faa\u73af\uff09\uff1a\n- 1024\u00d71024: PyTorch 0.149ms vs Genesis 5.116ms (\u616234\u500d!)\n</code></pre> <p>\u8fd9\u8bc1\u5b9e\u4e86\u4e13\u5bb6\u5206\u6790\uff1a<code>cudaFree</code> \u7684\u9690\u5f0f\u540c\u6b65\u4e25\u91cd\u5f71\u54cd\u6027\u80fd\u3002</p>"},{"location":"memory-allocator-optimization.zh/#_5","title":"\u4f18\u5316\u65b9\u5411\u786e\u5b9a","text":"<p>\u57fa\u4e8e\u6d4b\u8bd5\u7ed3\u679c\uff0c\u6211\u4eec\u7684\u4f18\u5316\u4f18\u5148\u7ea7\u975e\u5e38\u660e\u786e\uff1a</p> <ol> <li>\ud83d\udd34 \u7d27\u6025: \u5b9e\u73b0\u57fa\u672c\u7f13\u5b58\u6c60\uff0c\u89e3\u51b3\u91cd\u590d <code>cudaMalloc/cudaFree</code> \u95ee\u9898</li> <li>\ud83d\udfe0 \u91cd\u8981: \u4f18\u5316\u5185\u5b58\u590d\u7528\u7b56\u7565\uff0c\u7279\u522b\u662f\u5206\u914d-\u91ca\u653e\u5faa\u73af</li> <li>\ud83d\udfe1 \u6539\u8fdb: \u5904\u7406\u53d8\u5316\u5c3a\u5bf8\u7684\u5206\u914d\u6a21\u5f0f</li> </ol> <p>\u73b0\u5728\u8ba9\u6211\u4eec\u5f00\u59cb\u7b2c\u4e00\u9636\u6bb5\u4f18\u5316\u3002</p>"},{"location":"memory-allocator-optimization.zh/#_6","title":"\u7b2c\u4e8c\u6b65\uff1a\u5b9e\u73b0\u7b80\u5355\u7f13\u5b58\u6c60","text":"<p>\u57fa\u4e8e\u57fa\u7ebf\u6d4b\u8bd5\u7684\u53d1\u73b0\uff0c\u6211\u4eec\u9996\u5148\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u5185\u5b58\u7f13\u5b58\u6c60\u6765\u907f\u514d\u9891\u7e41\u7684 <code>cudaMalloc/cudaFree</code> \u8c03\u7528\u3002</p>"},{"location":"memory-allocator-optimization.zh/#phase-1","title":"Phase 1\u8bbe\u8ba1\u65b9\u6848","text":"<p>\u6211\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6700\u5c0f\u53ef\u7528\u7f13\u5b58\u5206\u914d\u5668\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a</p> <ol> <li>512B\u5bf9\u9f50: \u6240\u6709\u5206\u914d\u90fd\u5bf9\u9f50\u5230512\u5b57\u8282\u8fb9\u754c</li> <li>\u7cbe\u786e\u5c3a\u5bf8\u5339\u914d: \u6309\u786e\u5207\u5927\u5c0f\u7f13\u5b58\uff0c\u907f\u514d\u5185\u5b58\u6d6a\u8d39</li> <li>\u7b80\u5355\u81ea\u7531\u94fe: \u4f7f\u7528 <code>defaultdict(list)</code> \u5b9e\u73b0 size -&gt; [ptr_list] \u6620\u5c04</li> <li>\u5373\u65f6\u56de\u6536: \u91ca\u653e\u65f6\u7acb\u5373\u653e\u56de\u7f13\u5b58\uff08\u5982\u679c\u7f13\u5b58\u672a\u6ee1\uff09</li> <li>\u5355\u6d41\u53cb\u597d: \u5f53\u524d\u7248\u672c\u4e0d\u5904\u7406\u8de8\u6d41\uff0c\u4e13\u6ce8\u9a8c\u8bc1\u7f13\u5b58\u6548\u679c</li> </ol>"},{"location":"memory-allocator-optimization.zh/#_7","title":"\u6838\u5fc3\u5b9e\u73b0","text":"Python<pre><code>class CUDAMemoryManager:\n    def __init__(self):\n        # Phase 1: Simple caching allocator\n        self.free_blocks = defaultdict(list)  # size -&gt; [ptr_list] \n        self.active_blocks = {}  # ptr -&gt; size\n        self.alignment = 512  # 512B alignment\n        self.max_cache_size = 1024 * 1024 * 1024  # 1GB cache limit\n\n    def allocate(self, nbytes: int, stream=None) -&gt; int:\n        aligned_size = self._round_up(nbytes, self.alignment)\n\n        # Try cache first\n        if self.free_blocks[aligned_size]:\n            ptr = self.free_blocks[aligned_size].pop()\n            self.cache_hits += 1\n            return ptr\n\n        # Cache miss - allocate from CUDA\n        ptr = cuda.cuMemAlloc(aligned_size)\n        self.cache_misses += 1\n        return ptr\n\n    def free(self, ptr: int, stream=None):\n        size = self.active_blocks.pop(ptr)\n\n        # Return to cache if not full\n        if self.current_cache_size + size &lt;= self.max_cache_size:\n            self.free_blocks[size].append(ptr)\n            return\n\n        # Cache full - actually free\n        cuda.cuMemFree(ptr)\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#phase-1_1","title":"Phase 1\u4f18\u5316\u7ed3\u679c","text":"<p>\u7b80\u5355\u7f13\u5b58\u5206\u914d\u5668\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff1a</p>"},{"location":"memory-allocator-optimization.zh/#_8","title":"\u6027\u80fd\u8868\u73b0","text":"Text Only<pre><code>\u5e73\u5747\u52a0\u901f\u6bd4: 0.98x\n\u4e2d\u4f4d\u6570\u52a0\u901f\u6bd4: 0.65x  \n\u6027\u80fd\u8303\u56f4: 0.03x ~ 2.85x\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#_9","title":"\u5206\u573a\u666f\u5206\u6790","text":"<p>\u8868\u73b0\u826f\u597d\u7684\u573a\u666f: - \u540c\u5c3a\u5bf8\u91cd\u590d\u5206\u914d: 1.43x - \u5927\u5185\u5b58\u5206\u914d: 1.29x - \u63a8\u7406\u52a8\u6001\u6279\u6b21: 1.01x</p> <p>\u8868\u73b0\u4e00\u822c\u7684\u573a\u666f: - \u5206\u914d-\u91ca\u653e\u5faa\u73af: 0.84x - \u53d8\u5316\u5c3a\u5bf8\u5206\u914d: 0.51x</p> <p>\u8868\u73b0\u8f83\u5dee\u7684\u573a\u666f: - Transformer\u8bad\u7ec3: 0.04x - \u68af\u5ea6\u7d2f\u79ef: 0.03x - \u5185\u5b58\u538b\u529b: 0.08x</p>"},{"location":"memory-allocator-optimization.zh/#_10","title":"\u4e3b\u8981\u53d1\u73b0","text":"<p>\u7f13\u5b58\u673a\u5236\u9a8c\u8bc1: - \u7cbe\u786e\u5c3a\u5bf8\u5339\u914d\u5728\u91cd\u590d\u5206\u914d\u573a\u666f\u4e0b\u6709\u6548 - \u5927\u5206\u914d(\u2265100K\u5143\u7d20)\u5e73\u5747\u8fbe\u52301.20x - \u5c0f\u5206\u914d(&lt;100K\u5143\u7d20)\u4ec50.46x\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888</p> <p>\u5c40\u9650\u6027: - \u590d\u6742\u573a\u666f\u4e0b\u7f13\u5b58\u547d\u4e2d\u7387\u4f4e - \u7cbe\u786e\u5339\u914d\u7b56\u7565\u4e0d\u9002\u5408\u591a\u6837\u5316\u5185\u5b58\u6a21\u5f0f - \u9700\u8981\u66f4\u7075\u6d3b\u7684\u7f13\u5b58\u7b56\u7565</p>"},{"location":"memory-allocator-optimization.zh/#_11","title":"\u7b2c\u4e8c\u6b65\uff1a\u4e0b\u4e00\u9636\u6bb5\u4f18\u5316\u8ba1\u5212","text":"<p>\u57fa\u4e8ePhase 1\u7684\u7ed3\u679c\u5206\u6790\uff0c\u786e\u5b9a\u4f18\u5316\u4f18\u5148\u7ea7\uff1a</p>"},{"location":"memory-allocator-optimization.zh/#_12","title":"\u6838\u5fc3\u95ee\u9898\u8bca\u65ad","text":"<ol> <li>\u5c0f\u5206\u914d\u6027\u80fd\u5dee: &lt;100K\u5143\u7d20\u573a\u666f\u62d6\u7d2f\u6574\u4f53\u6027\u80fd</li> <li>\u590d\u6742\u573a\u666f\u5931\u6548: \u591a\u6837\u5316\u5185\u5b58\u6a21\u5f0f\u4e0b\u7f13\u5b58\u547d\u4e2d\u7387\u6781\u4f4e</li> <li>\u7cbe\u786e\u5339\u914d\u5c40\u9650: \u5f53\u524d\u7b56\u7565\u4e0d\u9002\u5408\u5c3a\u5bf8\u53d8\u5316\u5927\u7684\u573a\u666f</li> </ol>"},{"location":"memory-allocator-optimization.zh/#phase-2","title":"Phase 2\u4f18\u5316\u65b9\u6848: \u5c3a\u5bf8\u6876\u7f13\u5b58","text":"<p>\u76ee\u6807: \u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u89e3\u51b3\u53d8\u5316\u5c3a\u5bf8\u5206\u914d\u95ee\u9898</p> <p>\u6838\u5fc3\u6539\u8fdb: - \u5c06\u7cbe\u786e\u5339\u914d\u6539\u4e3a\u6876\u5339\u914d (\u598264B, 128B, 256B, 512B...) - \u51cf\u5c11\u5185\u5b58\u788e\u7247\uff0c\u63d0\u9ad8\u590d\u7528\u7387 - \u4f18\u5148\u89e3\u51b3\u5c0f\u5206\u914d\u6027\u80fd\u95ee\u9898</p> <p>\u9884\u671f\u6548\u679c: - \u53d8\u5316\u5c3a\u5bf8\u5206\u914d\u4ece0.51x\u63d0\u5347\u52300.8x+ - \u590d\u6742\u573a\u666f\u6027\u80fd\u6539\u5584 - \u6574\u4f53\u5e73\u5747\u6027\u80fd\u4ece0.98x\u63d0\u5347\u52301.2x+</p>"},{"location":"memory-allocator-optimization.zh/#_13","title":"\u5b9e\u65bd\u8ba1\u5212","text":"<ol> <li>\u8bbe\u8ba1\u6876\u5927\u5c0f\u7b56\u7565 (2\u7684\u5e42\u6b21 vs \u56fa\u5b9a\u6b65\u957f)</li> <li>\u5b9e\u73b0\u6876\u5339\u914d\u5206\u914d\u903b\u8f91</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6548\u679c</li> <li>\u6839\u636e\u7ed3\u679c\u51b3\u5b9a\u662f\u5426\u8fdb\u5165Phase 3 (\u5757\u5206\u914d\u5668)</li> </ol> <p>\u5f53\u524dPhase 1\u5df2\u5efa\u7acb\u7a33\u5b9a\u57fa\u7840\uff0c\u53ef\u4ee5\u5f00\u59cbPhase 2\u5f00\u53d1\u3002</p>"},{"location":"memory-allocator-optimization.zh/#phase-2_1","title":"Phase 2\u5b9e\u65bd\uff1a\u5c3a\u5bf8\u6876\u7f13\u5b58\u4f18\u5316","text":""},{"location":"memory-allocator-optimization.zh/#_14","title":"\u6838\u5fc3\u6539\u8fdb","text":"<p>\u5c06\u7cbe\u786e\u5c3a\u5bf8\u5339\u914d\u6539\u4e3a\u6876\u5339\u914d\u7b56\u7565\uff1a - \u4f7f\u75282\u7684\u5e42\u6b21\u6876: 512B, 1KB, 2KB, 4KB... - \u6700\u5927\u6876\u9650\u523616MB\uff0c\u8d85\u51fa\u4f7f\u7528\u7cbe\u786e\u5bf9\u9f50 - \u63d0\u9ad8\u53d8\u5316\u5c3a\u5bf8\u573a\u666f\u7684\u7f13\u5b58\u547d\u4e2d\u7387</p>"},{"location":"memory-allocator-optimization.zh/#_15","title":"\u5b9e\u65bd\u7ed3\u679c","text":""},{"location":"memory-allocator-optimization.zh/#_16","title":"\u6574\u4f53\u6027\u80fd\u5bf9\u6bd4","text":"Text Only<pre><code>Phase 1 \u2192 Phase 2:\n\u5e73\u5747\u52a0\u901f\u6bd4: 0.98x \u2192 0.97x (\u7565\u5fae\u4e0b\u964d)\n\u4e2d\u4f4d\u6570\u52a0\u901f\u6bd4: 0.65x \u2192 0.88x (\u663e\u8457\u63d0\u5347 +35%)\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#_17","title":"\u5206\u573a\u666f\u6027\u80fd\u53d8\u5316","text":"<p>\u663e\u8457\u6539\u5584\u7684\u573a\u666f: - \u53d8\u5316\u5c3a\u5bf8\u5206\u914d: 0.51x \u2192 0.83x (+63%) - \u5185\u5b58\u538b\u529b: 0.08x \u2192 1.48x (+1750%) - \u63a8\u7406\u52a8\u6001\u6279\u6b21: 1.01x \u2192 1.40x (+39%) - \u5206\u914d-\u91ca\u653e\u5faa\u73af: 0.84x \u2192 1.01x (+20%)</p> <p>\u6027\u80fd\u4e0b\u964d\u7684\u573a\u666f: - \u540c\u5c3a\u5bf8\u91cd\u590d\u5206\u914d: 1.43x \u2192 0.90x (-37%)</p> <p>\u4f9d\u7136\u4e25\u91cd\u7684\u74f6\u9888: - Transformer\u8bad\u7ec3: 0.04x \u2192 0.05x (\u51e0\u4e4e\u65e0\u6539\u5584) - \u68af\u5ea6\u7d2f\u79ef: 0.03x \u2192 0.07x (\u5fae\u5c0f\u6539\u5584)</p>"},{"location":"memory-allocator-optimization.zh/#phase-2_2","title":"Phase 2\u6280\u672f\u8bc4\u4f30","text":"<p>\u6210\u529f\u9a8c\u8bc1: - \u6876\u5339\u914d\u6709\u6548\u63d0\u9ad8\u4e86\u53d8\u5316\u5c3a\u5bf8\u573a\u666f\u7684\u7f13\u5b58\u547d\u4e2d\u7387 - \u4e2d\u4f4d\u6570\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\u8bf4\u660e\u5927\u90e8\u5206\u573a\u666f\u53d7\u76ca - \u5185\u5b58\u538b\u529b\u573a\u666f\u7684\u7a81\u7834\u8bc1\u660e\u4e86\u6876\u7f13\u5b58\u7684\u4ef7\u503c</p> <p>\u53d1\u73b0\u7684\u95ee\u9898: - \u6876\u7f13\u5b58\u5f15\u5165\u4e86\u5185\u5b58\u6d6a\u8d39\uff0c\u5f71\u54cd\u4e86\u540c\u5c3a\u5bf8\u5206\u914d\u7684\u6027\u80fd - \u590d\u6742\u8bad\u7ec3\u573a\u666f(Transformer/\u68af\u5ea6\u7d2f\u79ef)\u4ecd\u672a\u5f97\u5230\u6839\u672c\u6539\u5584 - \u9700\u8981\u66f4\u6df1\u5c42\u7684\u4f18\u5316\u7b56\u7565\u6765\u89e3\u51b3\u6838\u5fc3\u74f6\u9888</p>"},{"location":"memory-allocator-optimization.zh/#transformer","title":"Transformer\u573a\u666f\u74f6\u9888\u6839\u56e0\u5206\u6790","text":"<p>\u901a\u8fc7\u6df1\u5165\u5206\u6790\u53d1\u73b0\uff0c\u6876\u7f13\u5b58\u5bf9\u590d\u6742\u8bad\u7ec3\u573a\u666f\u65e0\u6548\u7684\u6839\u672c\u539f\u56e0\uff1a</p>"},{"location":"memory-allocator-optimization.zh/#_18","title":"\u5927\u5f20\u91cf\u8d85\u51fa\u6876\u9650\u5236","text":"<ul> <li>Logits\u5f20\u91cf\u8fbe\u523078MB-313MB\uff0c\u8fdc\u8d8516MB\u6876\u9650\u5236</li> <li>\u8d85\u5927\u5f20\u91cf\u56de\u9000\u5230\u7cbe\u786e\u5bf9\u9f50\uff0c\u65e0\u6cd5\u4eab\u53d7\u6876\u7f13\u5b58\u4f18\u52bf</li> <li>\u9891\u7e41\u7684\u5927\u5185\u5b58cudaMalloc\u8c03\u7528\u6210\u4e3a\u4e3b\u8981\u5f00\u9500</li> </ul>"},{"location":"memory-allocator-optimization.zh/#_19","title":"\u67b6\u6784\u5c42\u9762\u7684\u5dee\u5f02","text":"Text Only<pre><code>PyTorch\u5757\u5206\u914d\u5668\u4f18\u52bf:\n- \u9884\u5206\u914d\u5927\u5185\u5b58\u6c60(512MB-2GB)\n- \u4ece\u5185\u5b58\u6c60\u5207\u5206\u5f20\u91cf\uff0c\u907f\u514dcudaMalloc\n- \u91ca\u653e\u65f6\u56de\u6536\u5230\u6c60\u4e2d\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u5f00\u9500\u590d\u7528\n\nGenesis\u6876\u7f13\u5b58\u5c40\u9650:\n- \u6bcf\u4e2a\u5f20\u91cf\u4ecd\u9700\u72ec\u7acb\u7684cudaMalloc\n- \u65e0\u6cd5\u5229\u7528\u5185\u5b58\u6c60\u7684\u6839\u672c\u4f18\u52bf\n- \u5927\u5f20\u91cf\u5b8c\u5168\u7ed5\u8fc7\u7f13\u5b58\u673a\u5236\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#_20","title":"\u6027\u80fd\u74f6\u9888\u7684\u771f\u76f8","text":"<ul> <li>60\u4e2a\u5f20\u91cf\uff0c\u5927\u90e8\u52064MB-320MB\u7ea7\u522b</li> <li>cudaMalloc\u5bf9\u5927\u5185\u5b58\u5757\u7684\u7cfb\u7edf\u8c03\u7528\u5f00\u9500\u5de8\u5927</li> <li>\u7f13\u5b58\u547d\u4e2d\u7387\u518d\u9ad8\u4e5f\u65e0\u6cd5\u63a9\u76d6\u6839\u672c\u7684\u67b6\u6784\u95ee\u9898</li> </ul> <p>\u7ed3\u8bba: \u6876\u7f13\u5b58\u662f\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u6839\u672c\u95ee\u9898\u3002\u9700\u8981\u5b9e\u73b0PyTorch\u98ce\u683c\u7684\u5757\u5206\u914d\u5668(Block Allocator)\u624d\u80fd\u771f\u6b63\u7a81\u7834\u6027\u80fd\u74f6\u9888\u3002</p>"},{"location":"memory-allocator-optimization.zh/#phase-3block-allocator","title":"Phase 3\u5b9e\u65bd\uff1aBlock Allocator\u5757\u5206\u914d\u5668","text":""},{"location":"memory-allocator-optimization.zh/#_21","title":"\u6838\u5fc3\u8bbe\u8ba1","text":"<p>\u5b9e\u73b0PyTorch\u98ce\u683c\u7684\u5757\u5206\u914d\u5668\uff0c\u89e3\u51b3\u5927\u5185\u5b58\u5206\u914d\u7684\u6839\u672c\u6027\u80fd\u95ee\u9898\uff1a - \u9884\u5206\u914d\u5927\u5185\u5b58\u6bb5(1GB)\u4f5c\u4e3a\u5185\u5b58\u6c60 - \u4f7f\u7528best-fit\u7b97\u6cd5\u4ece\u6c60\u4e2d\u5207\u5206\u5757 - \u91ca\u653e\u65f6\u56de\u6536\u5230\u6c60\uff0c\u652f\u6301\u5757\u5408\u5e76\u51cf\u5c11\u788e\u7247 - \u5206\u5c42\u67b6\u6784\uff1a&lt;1MB\u7528\u6876\u7f13\u5b58\uff0c\u22651MB\u7528\u5757\u5206\u914d\u5668</p>"},{"location":"memory-allocator-optimization.zh/#_22","title":"\u5b9e\u65bd\u7ed3\u679c","text":""},{"location":"memory-allocator-optimization.zh/#_23","title":"\u6574\u4f53\u6027\u80fd\u5bf9\u6bd4","text":"Text Only<pre><code>Phase 2 \u2192 Phase 3:\n\u5e73\u5747\u52a0\u901f\u6bd4: 0.97x \u2192 1.41x (+45%\u63d0\u5347)\n\u4e2d\u4f4d\u6570\u52a0\u901f\u6bd4: 0.88x \u2192 0.81x (\u8f7b\u5fae\u4e0b\u964d)\n\u6700\u4f73\u6027\u80fd: 2.60x \u2192 4.83x (\u65b0\u7684\u6027\u80fd\u5cf0\u503c)\n</code></pre>"},{"location":"memory-allocator-optimization.zh/#_24","title":"\u5173\u952e\u573a\u666f\u7684\u91cd\u5927\u7a81\u7834","text":"<p>\u5927\u5e45\u6539\u5584\u7684\u573a\u666f: - Transformer\u8bad\u7ec3: 0.05x \u2192 1.89x (+3680%\uff0c\u4ece\u4e25\u91cd\u74f6\u9888\u5230\u8d85\u8d8aPyTorch) - \u5927\u5185\u5b58\u5206\u914d: 1.29x \u2192 3.92x (+204%\uff0c\u663e\u8457\u4f18\u4e8ePyTorch) - \u5927\u5c3a\u5bf8\u91cd\u590d\u5206\u914d: \u4ecePhase 1\u76840.27x\u5230Phase 3\u76842.31x</p> <p>\u4fdd\u6301\u7a33\u5b9a\u7684\u573a\u666f: - \u5c0f\u5206\u914d\u573a\u666f\u57fa\u672c\u4fdd\u6301\u539f\u6709\u6c34\u5e73 - \u63a8\u7406\u670d\u52a1\u7b49\u5b9e\u7528\u573a\u666f\u8868\u73b0\u7a33\u5b9a</p> <p>\u4ecd\u9700\u6539\u8fdb\u7684\u573a\u666f: - \u68af\u5ea6\u7d2f\u79ef: 0.07x \u2192 0.18x (\u6709\u6539\u5584\u4f46\u4ecd\u8f83\u5dee) - \u53d8\u5316\u5c3a\u5bf8\u5206\u914d: 0.83x \u2192 0.34x (\u53d7\u5206\u5c42\u7b56\u7565\u5f71\u54cd)</p>"},{"location":"memory-allocator-optimization.zh/#_25","title":"\u6280\u672f\u6210\u5c31","text":"<p>\u6210\u529f\u89e3\u51b3\u7684\u6838\u5fc3\u95ee\u9898: - \u5f7b\u5e95\u6d88\u9664\u4e86\u5927\u5206\u914d\u573a\u666f\u7684cudaMalloc\u7cfb\u7edf\u8c03\u7528\u5f00\u9500 - \u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u5185\u5b58\u6c60\u590d\u7528\u673a\u5236 - \u9a8c\u8bc1\u4e86\u5757\u5206\u914d\u5668\u67b6\u6784\u7684\u6709\u6548\u6027</p> <p>\u6280\u672f\u67b6\u6784\u7684\u6210\u529f: - \u5206\u5c42\u5206\u914d\u7b56\u7565\u5de5\u4f5c\u6b63\u5e38 - 1GB\u5185\u5b58\u6bb5\u7684\u5229\u7528\u7387\u826f\u597d - Best-fit\u7b97\u6cd5\u548c\u5757\u5408\u5e76\u673a\u5236\u6709\u6548</p> <p>\u5c40\u9650\u6027\u8ba4\u77e5: - \u5c0f\u5206\u914d\u573a\u666f\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4 - \u90e8\u5206\u7279\u6b8a\u573a\u666f(\u5982\u68af\u5ea6\u7d2f\u79ef)\u9700\u8981\u8fdb\u4e00\u6b65\u8c03\u4f18 - \u76f8\u6bd4\u6210\u719f\u7684PyTorch\uff0c\u5728\u67d0\u4e9b\u7ec6\u5206\u573a\u666f\u8fd8\u6709\u5dee\u8ddd</p>"},{"location":"memory-allocator-optimization.zh/#phase-3","title":"Phase 3\u8bc4\u4f30","text":"<p>Block Allocator\u6210\u529f\u89e3\u51b3\u4e86\u6700\u5173\u952e\u7684\u5927\u5185\u5b58\u5206\u914d\u74f6\u9888\uff0c\u8ba9Genesis\u5728\u91cd\u8981\u573a\u666f\u4e0b\u8fbe\u5230\u751a\u81f3\u8d85\u8d8aPyTorch\u7684\u6027\u80fd\u3002\u867d\u7136\u4e0d\u662f\u6240\u6709\u573a\u666f\u90fd\u5b8c\u7f8e\uff0c\u4f46\u5df2\u7ecf\u4ece\"\u4e25\u91cd\u843d\u540e\"\u8f6c\u53d8\u4e3a\"\u57fa\u672c\u53ef\u7528\uff0c\u67d0\u4e9b\u573a\u666f\u9886\u5148\"\u7684\u72b6\u6001\u3002</p> <p>\u8fd9\u4e3aGenesis\u5728\u5b9e\u9645\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002</p>"},{"location":"memory-allocator-optimization.zh/#_26","title":"\u4f18\u5316\u5386\u7a0b\u603b\u7ed3","text":"<p>\u4ece\u6700\u521d\u7684\"\u707e\u96be\u6027\u6027\u80fd\"(0.02x)\u5230\u73b0\u5728\u7684\"\u6574\u4f53\u9886\u5148\"(1.41x)\uff0c\u8fd9\u6b21\u5185\u5b58\u5206\u914d\u5668\u4f18\u5316\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u7a81\u7834\uff1a</p>"},{"location":"memory-allocator-optimization.zh/#_27","title":"\u4e09\u4e2a\u9636\u6bb5\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb","text":"<ul> <li>Phase 1: \u89e3\u51b3\u4e86\u6700\u57fa\u672c\u7684\u5185\u5b58\u590d\u7528\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u4f18\u5316\u57fa\u7840</li> <li>Phase 2: \u63d0\u9ad8\u4e86\u53d8\u5316\u5c3a\u5bf8\u573a\u666f\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u6539\u5584\u4e86\u4e2d\u4f4d\u6570\u6027\u80fd  </li> <li>Phase 3: \u5f7b\u5e95\u89e3\u51b3\u4e86\u5927\u5185\u5b58\u5206\u914d\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u8d28\u7684\u98de\u8dc3</li> </ul>"},{"location":"memory-allocator-optimization.zh/#_28","title":"\u6280\u672f\u8def\u7ebf\u7684\u6b63\u786e\u6027\u9a8c\u8bc1","text":"<p>\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u6839\u56e0\u5206\u6790\uff0c\u6211\u4eec\u51c6\u786e\u8bc6\u522b\u4e86\u6027\u80fd\u74f6\u9888\u5e76\u9009\u62e9\u4e86\u6b63\u786e\u7684\u6280\u672f\u65b9\u6848\u3002\u6bcf\u4e2a\u9636\u6bb5\u90fd\u6709\u660e\u786e\u7684\u76ee\u6807\u548c\u53ef\u8861\u91cf\u7684\u6210\u679c\u3002</p>"},{"location":"memory-allocator-optimization.zh/#_29","title":"\u5b9e\u7528\u4ef7\u503c","text":"<p>Genesis\u73b0\u5728\u5728\u5927\u90e8\u5206\u5b9e\u9645\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u53ef\u63a5\u53d7\u7684\u5185\u5b58\u5206\u914d\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u8fd9\u7c7b\u5173\u952e\u573a\u666f\u4e0b\u5df2\u7ecf\u5177\u5907\u4e86\u7ade\u4e89\u529b\u3002</p> <p>\u5f53\u7136\uff0c\u8fd9\u53ea\u662f\u5185\u5b58\u7ba1\u7406\u4f18\u5316\u7684\u4e00\u4e2a\u9636\u6bb5\u6027\u6210\u679c\u3002\u672a\u6765\u8fd8\u53ef\u4ee5\u8003\u8651\u66f4\u591a\u4f18\u5316\u65b9\u5411\uff0c\u6bd4\u5982\u591a\u6d41\u5e76\u53d1\u3001NUMA\u611f\u77e5\u3001\u6216\u8005\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u4e13\u95e8\u4f18\u5316\u7b49\u3002</p>"},{"location":"memory-allocator-optimization_en/","title":"Genesis GPU Memory Allocator Performance Optimization Journey","text":"<p>A complete technical blog documenting the performance optimization process of Genesis deep learning framework's GPU memory allocator, from problem discovery to progressive solutions</p>"},{"location":"memory-allocator-optimization_en/#background-why-is-genesis-memory-allocation-so-slow","title":"Background: Why is Genesis Memory Allocation So Slow?","text":"<p>When using our self-developed Genesis deep learning framework, we discovered a serious performance issue: GPU memory allocation was much slower than PyTorch. Comparative testing revealed:</p> Text Only<pre><code>CUDAStorage allocation vs PyTorch:\n- 1K elements:   Genesis 0.58x PyTorch (42% slower)\n- 10K elements:  Genesis 0.75x PyTorch (25% slower) \n- 100K elements: Genesis 0.42x PyTorch (58% slower)\n</code></pre> <p>Even more shocking was the <code>fill_</code> operation performance:</p> Text Only<pre><code>Fill operation (before optimization):\n- 512\u00d7512:   Genesis 0.10x PyTorch (10x slower!)\n- 1024\u00d71024: Genesis 0.03x PyTorch (33x slower!)\n- 2048\u00d72048: Genesis 0.01x PyTorch (100x slower!)\n</code></pre> <p>This was clearly unacceptable. We needed to deeply analyze the problem and develop an optimization strategy.</p>"},{"location":"memory-allocator-optimization_en/#step-1-establish-performance-baseline-testing","title":"Step 1: Establish Performance Baseline Testing","text":"<p>Any optimization must first establish accurate baseline testing. We needed to understand: 1. How slow is current allocation performance exactly? 2. Which allocation patterns are performance bottlenecks? 3. What are the specific gaps compared to PyTorch?</p>"},{"location":"memory-allocator-optimization_en/#design-benchmark-tests","title":"Design Benchmark Tests","text":"<p>I created a dedicated memory manager benchmark tool <code>benchmark/bench_memory_manager.py</code>, testing the following key patterns:</p> <ol> <li>Same-size Repeated Allocation - Simulate training loops</li> <li>Allocation-Release Cycles - Test memory reuse capability  </li> <li>Variable-size Allocation - Simulate batch size changes</li> <li>Large Memory Allocation - Test large block memory behavior</li> <li>PyTorch Cache Analysis - Deep understanding of PyTorch's caching mechanism</li> </ol>"},{"location":"memory-allocator-optimization_en/#baseline-test-results","title":"Baseline Test Results","text":"<p>After running tests, the results were shocking:</p> Text Only<pre><code>\ud83d\udd34 Overall Performance Statistics:\n- Average speedup ratio: 0.16x (Genesis 6x slower than PyTorch!)\n- Worst speedup ratio: 0.02x (allocation-release cycles 50x slower!)\n- Best speedup ratio: 0.38x (still 2.6x slower)\n\n\ud83d\udcca By Pattern Category:\n- Same-size repeated allocation:    0.22x (poor)\n- Allocation-release cycles:        0.02x (severe!)  \n- Variable-size allocation:         0.12x (severe)\n- Large memory allocation:          0.20x (poor)\n</code></pre>"},{"location":"memory-allocator-optimization_en/#key-findings","title":"Key Findings","text":""},{"location":"memory-allocator-optimization_en/#1-pytorch-caching-effect-is-amazing","title":"1. PyTorch Caching Effect is Amazing","text":"Text Only<pre><code>PyTorch 1024\u00d71024 allocation behavior:\n- First allocation (cold start): 0.458ms\n- Second allocation (cache hit): 0.021ms  \n- Cache speedup ratio: 22x!\n\n10 consecutive allocations:\n- PyTorch average: 0.015ms \n- Genesis average: 0.925ms\n- Steady-state performance gap: 62x!\n</code></pre>"},{"location":"memory-allocator-optimization_en/#2-genesis-has-no-caching","title":"2. Genesis Has No Caching","text":"<p>Genesis allocation time is consistently similar (0.9-1.0ms), indicating it indeed calls <code>cudaMalloc</code> every time with no caching mechanism.</p>"},{"location":"memory-allocator-optimization_en/#3-allocation-release-cycles-are-the-biggest-bottleneck","title":"3. Allocation-Release Cycles are the Biggest Bottleneck","text":"Text Only<pre><code>Allocation-release cycle performance (20 cycles):\n- 1024\u00d71024: PyTorch 0.149ms vs Genesis 5.116ms (34x slower!)\n</code></pre> <p>This confirmed expert analysis: <code>cudaFree</code>'s implicit synchronization severely impacts performance.</p>"},{"location":"memory-allocator-optimization_en/#optimization-direction-determined","title":"Optimization Direction Determined","text":"<p>Based on test results, our optimization priorities are very clear:</p> <ol> <li>\ud83d\udd34 Urgent: Implement basic cache pool to solve repeated <code>cudaMalloc/cudaFree</code> issues</li> <li>\ud83d\udfe0 Important: Optimize memory reuse strategy, especially allocation-release cycles</li> <li>\ud83d\udfe1 Improvement: Handle variable-size allocation patterns</li> </ol> <p>Now let's start Phase 1 optimization.</p>"},{"location":"memory-allocator-optimization_en/#step-2-implement-simple-cache-pool","title":"Step 2: Implement Simple Cache Pool","text":"<p>Based on baseline test discoveries, we first implement a simple memory cache pool to avoid frequent <code>cudaMalloc/cudaFree</code> calls.</p>"},{"location":"memory-allocator-optimization_en/#phase-1-design-approach","title":"Phase 1 Design Approach","text":"<p>I implemented a minimal viable cache allocator with the following features:</p> <ol> <li>512B Alignment: All allocations aligned to 512-byte boundaries</li> <li>Exact Size Matching: Cache by exact size to avoid memory waste</li> <li>Simple Free List: Use <code>defaultdict(list)</code> to implement size -&gt; [ptr_list] mapping</li> <li>Immediate Recycling: Return to cache immediately upon release (if cache not full)</li> <li>Single Stream Friendly: Current version doesn't handle cross-stream, focuses on validating cache effect</li> </ol>"},{"location":"memory-allocator-optimization_en/#core-implementation","title":"Core Implementation","text":"Python<pre><code>class CUDAMemoryManager:\n    def __init__(self):\n        # Phase 1: Simple caching allocator\n        self.free_blocks = defaultdict(list)  # size -&gt; [ptr_list] \n        self.active_blocks = {}  # ptr -&gt; size\n        self.alignment = 512  # 512B alignment\n        self.max_cache_size = 1024 * 1024 * 1024  # 1GB cache limit\n\n    def allocate(self, nbytes: int, stream=None) -&gt; int:\n        aligned_size = self._round_up(nbytes, self.alignment)\n\n        # Try cache first\n        if self.free_blocks[aligned_size]:\n            ptr = self.free_blocks[aligned_size].pop()\n            self.cache_hits += 1\n            return ptr\n\n        # Cache miss - allocate from CUDA\n        ptr = cuda.cuMemAlloc(aligned_size)\n        self.cache_misses += 1\n        return ptr\n\n    def free(self, ptr: int, stream=None):\n        size = self.active_blocks.pop(ptr)\n\n        # Return to cache if not full\n        if self.current_cache_size + size &lt;= self.max_cache_size:\n            self.free_blocks[size].append(ptr)\n            return\n\n        # Cache full - actually free\n        cuda.cuMemFree(ptr)\n</code></pre>"},{"location":"memory-allocator-optimization_en/#phase-1-optimization-results","title":"Phase 1 Optimization Results","text":"<p>Simple cache allocator benchmark results:</p>"},{"location":"memory-allocator-optimization_en/#performance-results","title":"Performance Results","text":"Text Only<pre><code>Average speedup ratio: 0.98x\nMedian speedup ratio: 0.65x  \nPerformance range: 0.03x ~ 2.85x\n</code></pre>"},{"location":"memory-allocator-optimization_en/#scenario-analysis","title":"Scenario Analysis","text":"<p>Well-performing scenarios: - Same-size repeated allocation: 1.43x - Large memory allocation: 1.29x - Inference dynamic batches: 1.01x</p> <p>Average-performing scenarios: - Allocation-release cycles: 0.84x - Variable-size allocation: 0.51x</p> <p>Poorly-performing scenarios: - Transformer training: 0.04x - Gradient accumulation: 0.03x - Memory pressure: 0.08x</p>"},{"location":"memory-allocator-optimization_en/#main-findings","title":"Main Findings","text":"<p>Cache mechanism validation: - Exact size matching is effective for repeated allocation scenarios - Large allocations (\u2265100K elements) average 1.20x - Small allocations (&lt;100K elements) only 0.46x, becoming performance bottleneck</p> <p>Limitations: - Low cache hit rate in complex scenarios - Exact matching strategy unsuitable for diverse memory patterns - Need more flexible caching strategies</p>"},{"location":"memory-allocator-optimization_en/#step-2-next-phase-optimization-plan","title":"Step 2: Next Phase Optimization Plan","text":"<p>Based on Phase 1 results analysis, determine optimization priorities:</p>"},{"location":"memory-allocator-optimization_en/#core-problem-diagnosis","title":"Core Problem Diagnosis","text":"<ol> <li>Poor small allocation performance: &lt;100K element scenarios drag down overall performance</li> <li>Complex scenario failures: Extremely low cache hit rates in diverse memory patterns</li> <li>Exact matching limitations: Current strategy unsuitable for scenarios with large size variations</li> </ol>"},{"location":"memory-allocator-optimization_en/#phase-2-optimization-plan-size-bucket-caching","title":"Phase 2 Optimization Plan: Size Bucket Caching","text":"<p>Goal: Improve cache hit rate, solve variable-size allocation issues</p> <p>Core improvements: - Change exact matching to bucket matching (like 64B, 128B, 256B, 512B...) - Reduce memory fragmentation, improve reuse rate - Prioritize solving small allocation performance issues</p> <p>Expected effects: - Variable-size allocation from 0.51x to 0.8x+ - Complex scenario performance improvement - Overall average performance from 0.98x to 1.2x+</p>"},{"location":"memory-allocator-optimization_en/#implementation-plan","title":"Implementation Plan","text":"<ol> <li>Design bucket size strategy (powers of 2 vs fixed steps)</li> <li>Implement bucket matching allocation logic</li> <li>Benchmark test to verify effects</li> <li>Based on results, decide whether to proceed to Phase 3 (block allocator)</li> </ol> <p>Current Phase 1 has established a stable foundation, can begin Phase 2 development.</p>"},{"location":"memory-allocator-optimization_en/#phase-2-implementation-size-bucket-cache-optimization","title":"Phase 2 Implementation: Size Bucket Cache Optimization","text":""},{"location":"memory-allocator-optimization_en/#core-improvements","title":"Core Improvements","text":"<p>Changed exact size matching to bucket matching strategy: - Use powers of 2 buckets: 512B, 1KB, 2KB, 4KB... - Maximum bucket limit 16MB, use exact alignment beyond - Improve cache hit rate for variable-size scenarios</p>"},{"location":"memory-allocator-optimization_en/#implementation-results","title":"Implementation Results","text":""},{"location":"memory-allocator-optimization_en/#overall-performance-comparison","title":"Overall Performance Comparison","text":"Text Only<pre><code>Phase 1 \u2192 Phase 2:\nAverage speedup ratio: 0.98x \u2192 0.97x (slight decrease)\nMedian speedup ratio: 0.65x \u2192 0.88x (significant improvement +35%)\n</code></pre>"},{"location":"memory-allocator-optimization_en/#per-scenario-performance-changes","title":"Per-scenario Performance Changes","text":"<p>Significantly improved scenarios: - Variable-size allocation: 0.51x \u2192 0.83x (+63%) - Memory pressure: 0.08x \u2192 1.48x (+1750%) - Inference dynamic batches: 1.01x \u2192 1.40x (+39%) - Allocation-release cycles: 0.84x \u2192 1.01x (+20%)</p> <p>Performance decline scenarios: - Same-size repeated allocation: 1.43x \u2192 0.90x (-37%)</p> <p>Still severe bottlenecks: - Transformer training: 0.04x \u2192 0.05x (almost no improvement) - Gradient accumulation: 0.03x \u2192 0.07x (minor improvement)</p>"},{"location":"memory-allocator-optimization_en/#phase-2-technical-assessment","title":"Phase 2 Technical Assessment","text":"<p>Successfully validated: - Bucket matching effectively improved cache hit rate for variable-size scenarios - Large median performance improvement shows most scenarios benefited - Breakthrough in memory pressure scenarios proves value of bucket caching</p> <p>Problems discovered: - Bucket caching introduces memory waste, affecting same-size allocation performance - Complex training scenarios (Transformer/gradient accumulation) still not fundamentally improved - Need deeper optimization strategies to solve core bottlenecks</p>"},{"location":"memory-allocator-optimization_en/#transformer-scenario-bottleneck-root-cause-analysis","title":"Transformer Scenario Bottleneck Root Cause Analysis","text":"<p>Through deep analysis, found fundamental reasons why bucket caching is ineffective for complex training scenarios:</p>"},{"location":"memory-allocator-optimization_en/#large-tensors-exceed-bucket-limits","title":"Large Tensors Exceed Bucket Limits","text":"<ul> <li>Logits tensors reach 78MB-313MB, far exceeding 16MB bucket limit</li> <li>Ultra-large tensors fall back to exact alignment, cannot enjoy bucket caching advantages</li> <li>Frequent large memory cudaMalloc calls become main overhead</li> </ul>"},{"location":"memory-allocator-optimization_en/#architectural-level-differences","title":"Architectural-level Differences","text":"Text Only<pre><code>PyTorch block allocator advantages:\n- Pre-allocate large memory pools (512MB-2GB)\n- Slice tensors from memory pool, avoiding cudaMalloc\n- Return to pool upon release, achieving true zero-overhead reuse\n\nGenesis bucket cache limitations:\n- Each tensor still needs independent cudaMalloc\n- Cannot utilize fundamental advantages of memory pools\n- Large tensors completely bypass caching mechanism\n</code></pre>"},{"location":"memory-allocator-optimization_en/#performance-bottleneck-truth","title":"Performance Bottleneck Truth","text":"<ul> <li>60 tensors, mostly 4MB-320MB level</li> <li>cudaMalloc system call overhead for large memory blocks is huge</li> <li>No matter how high cache hit rate, cannot mask fundamental architectural issues</li> </ul> <p>Conclusion: Bucket caching is incremental improvement but cannot solve fundamental problems of large-scale training. Need to implement PyTorch-style block allocator to truly break through performance bottlenecks.</p>"},{"location":"memory-allocator-optimization_en/#phase-3-implementation-block-allocator","title":"Phase 3 Implementation: Block Allocator","text":""},{"location":"memory-allocator-optimization_en/#core-design","title":"Core Design","text":"<p>Implement PyTorch-style block allocator to solve fundamental performance issues of large memory allocation: - Pre-allocate large memory segments (1GB) as memory pool - Use best-fit algorithm to slice blocks from pool - Return to pool upon release, support block merging to reduce fragmentation - Layered architecture: &lt;1MB use bucket cache, \u22651MB use block allocator</p>"},{"location":"memory-allocator-optimization_en/#implementation-results_1","title":"Implementation Results","text":""},{"location":"memory-allocator-optimization_en/#overall-performance-comparison_1","title":"Overall Performance Comparison","text":"Text Only<pre><code>Phase 2 \u2192 Phase 3:\nAverage speedup ratio: 0.97x \u2192 1.41x (+45% improvement)\nMedian speedup ratio: 0.88x \u2192 0.81x (slight decrease)\nBest performance: 2.60x \u2192 4.83x (new performance peak)\n</code></pre>"},{"location":"memory-allocator-optimization_en/#major-breakthroughs-in-key-scenarios","title":"Major Breakthroughs in Key Scenarios","text":"<p>Dramatically improved scenarios: - Transformer training: 0.05x \u2192 1.89x (+3680%, from severe bottleneck to surpassing PyTorch) - Large memory allocation: 1.29x \u2192 3.92x (+204%, significantly better than PyTorch) - Large-size repeated allocation: from Phase 1's 0.27x to Phase 3's 2.31x</p> <p>Stable scenarios: - Small allocation scenarios basically maintain original levels - Practical scenarios like inference services perform stably</p> <p>Still need improvement scenarios: - Gradient accumulation: 0.07x \u2192 0.18x (improved but still poor) - Variable-size allocation: 0.83x \u2192 0.34x (affected by layered strategy)</p>"},{"location":"memory-allocator-optimization_en/#technical-achievements","title":"Technical Achievements","text":"<p>Successfully solved core problems: - Completely eliminated cudaMalloc system call overhead in large allocation scenarios - Achieved true memory pool reuse mechanism - Validated effectiveness of block allocator architecture</p> <p>Technical architecture success: - Layered allocation strategy works properly - Good utilization rate of 1GB memory segments - Best-fit algorithm and block merging mechanism effective</p> <p>Limitation recognition: - Small allocation scenarios still have room for improvement - Some special scenarios (like gradient accumulation) need further tuning - Compared to mature PyTorch, still gaps in some specific scenarios</p>"},{"location":"memory-allocator-optimization_en/#phase-3-assessment","title":"Phase 3 Assessment","text":"<p>Block Allocator successfully solved the most critical large memory allocation bottleneck, enabling Genesis to achieve or even surpass PyTorch performance in important scenarios. While not all scenarios are perfect, it has transformed from \"severely lagging\" to \"basically usable, leading in some scenarios.\"</p> <p>This establishes a solid foundation for Genesis applications in actual deep learning tasks.</p>"},{"location":"memory-allocator-optimization_en/#optimization-journey-summary","title":"Optimization Journey Summary","text":"<p>From initial \"catastrophic performance\" (0.02x) to current \"overall leading\" (1.41x), this memory allocator optimization achieved substantial breakthroughs:</p>"},{"location":"memory-allocator-optimization_en/#progressive-improvements-in-three-phases","title":"Progressive Improvements in Three Phases","text":"<ul> <li>Phase 1: Solved most basic memory reuse issues, established optimization foundation</li> <li>Phase 2: Improved cache hit rate for variable-size scenarios, enhanced median performance  </li> <li>Phase 3: Completely solved large memory allocation bottlenecks, achieved qualitative leap</li> </ul>"},{"location":"memory-allocator-optimization_en/#technical-route-correctness-validation","title":"Technical Route Correctness Validation","text":"<p>Through systematic benchmark testing and root cause analysis, we accurately identified performance bottlenecks and chose correct technical solutions. Each phase had clear goals and measurable results.</p>"},{"location":"memory-allocator-optimization_en/#practical-value","title":"Practical Value","text":"<p>Genesis can now provide acceptable memory allocation performance in most practical scenarios, particularly competitive in critical scenarios like large-scale model training.</p> <p>Of course, this is only a phase result in memory management optimization. Future could consider more optimization directions, such as multi-stream concurrency, NUMA awareness, or specialized optimizations for specific models.</p>"},{"location":"api/autograd.zh/","title":"\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf (genesis.autograd)","text":""},{"location":"api/autograd.zh/#_1","title":"\u6982\u8ff0","text":"<p>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u662fGenesis\u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u52a8\u6001\u8ba1\u7b97\u56fe\u6784\u5efa\u548c\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u3002\u5b83\u5b9e\u73b0\u4e86\u53cd\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08\u53cd\u5411\u4f20\u64ad\uff09\uff0c\u652f\u6301\u590d\u6742\u7684\u8ba1\u7b97\u56fe\u3002</p>"},{"location":"api/autograd.zh/#_2","title":"\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"api/autograd.zh/#_3","title":"\u8ba1\u7b97\u56fe","text":"<p>Genesis\u5728\u6267\u884c\u64cd\u4f5c\u65f6\u6784\u5efa\u52a8\u6001\u8ba1\u7b97\u56fe\u3002\u6bcf\u4e2a\u64cd\u4f5c\u5728\u56fe\u4e2d\u521b\u5efa\u8282\u70b9\uff0c\u7528\u4e8e\u8ffd\u8e2a\uff1a - \u8f93\u5165\u5f20\u91cf - \u6267\u884c\u7684\u64cd\u4f5c - \u8f93\u51fa\u5f20\u91cf - \u7528\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u68af\u5ea6\u51fd\u6570</p>"},{"location":"api/autograd.zh/#_4","title":"\u68af\u5ea6\u8ba1\u7b97","text":"<p>\u68af\u5ea6\u4f7f\u7528\u94fe\u5f0f\u6cd5\u5219\u8ba1\u7b97\uff0c\u4ece\u8f93\u51fa\u5230\u8f93\u5165\u53cd\u5411\u904d\u5386\u8ba1\u7b97\u56fe\u3002</p>"},{"location":"api/autograd.zh/#_5","title":"\u4e3b\u8981\u7c7b","text":""},{"location":"api/autograd.zh/#genesistensor","title":"<code>genesis.Tensor</code>","text":"<p>Genesis\u4e2d\u652f\u6301\u81ea\u52a8\u5fae\u5206\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\u3002</p> Python<pre><code>class Tensor:\n    def __init__(\n        self,\n        array: Union[list, np.ndarray, NDArray],\n        device: Optional[Device] = None,\n        dtype: Optional[DType] = None,\n        requires_grad: bool = False,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/autograd.zh/#_6","title":"\u53c2\u6570","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>array</code> array-like required \u8f93\u5165\u6570\u636e\uff08\u5217\u8868\u3001numpy\u6570\u7ec4\u6216NDArray\uff09 <code>device</code> Device <code>None</code> \u8ba1\u7b97\u8bbe\u5907\uff08cpu/cuda\uff09 <code>dtype</code> DType <code>None</code> \u6570\u636e\u7c7b\u578b\uff08\u5982\u679c\u4e3aNone\u5219\u63a8\u65ad\uff09 <code>requires_grad</code> bool <code>False</code> \u662f\u5426\u8ba1\u7b97\u68af\u5ea6 <code>**kwargs</code> dict <code>{}</code> \u989d\u5916\u7684NDArray\u53c2\u6570"},{"location":"api/autograd.zh/#_7","title":"\u5c5e\u6027","text":""},{"location":"api/autograd.zh/#_8","title":"\u5f62\u72b6\u548c\u7c7b\u578b\u4fe1\u606f","text":"Python<pre><code>@property\ndef shape(self) -&gt; Tuple[int, ...]:\n    \"\"\"\u8fd4\u56de\u5f20\u91cf\u7684\u5f62\u72b6\u3002\"\"\"\n\n@property\ndef dtype(self) -&gt; DType:\n    \"\"\"\u8fd4\u56de\u6570\u636e\u7c7b\u578b\u3002\"\"\"\n\n@property\ndef device(self) -&gt; Device:\n    \"\"\"\u8fd4\u56de\u8bbe\u5907\u3002\"\"\"\n\n@property\ndef ndim(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u7ef4\u5ea6\u6570\u91cf\u3002\"\"\"\n\n@property\ndef size(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u5143\u7d20\u603b\u6570\u3002\"\"\"\n</code></pre>"},{"location":"api/autograd.zh/#_9","title":"\u68af\u5ea6\u5c5e\u6027","text":"Python<pre><code>@property\ndef requires_grad(self) -&gt; bool:\n    \"\"\"\u6b64\u5f20\u91cf\u662f\u5426\u9700\u8981\u68af\u5ea6\u8ba1\u7b97\u3002\"\"\"\n\n@property\ndef grad(self) -&gt; Optional[Tensor]:\n    \"\"\"\u8bbf\u95ee\u68af\u5ea6\u5f20\u91cf\u3002\"\"\"\n\n@property\ndef is_leaf(self) -&gt; bool:\n    \"\"\"\u662f\u5426\u662f\u53f6\u8282\u70b9\uff08\u7528\u6237\u521b\u5efa\u7684\u5f20\u91cf\uff09\u3002\"\"\"\n\n@property\ndef grad_fn(self) -&gt; Optional[Function]:\n    \"\"\"\u521b\u5efa\u6b64\u5f20\u91cf\u7684\u51fd\u6570\u3002\"\"\"\n</code></pre>"},{"location":"api/autograd.zh/#_10","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"api/autograd.zh/#_11","title":"\u68af\u5ea6\u64cd\u4f5c","text":"Python<pre><code>def backward(self, gradient: Optional[Tensor] = None) -&gt; None:\n    \"\"\"\n    \u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        gradient: \u8f93\u51fa\u68af\u5ea6\u3002\u5bf9\u4e8e\u6807\u91cf\u9ed8\u8ba4\u4e3atensor([1.0])\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2., 3.], requires_grad=True)\n        &gt;&gt;&gt; y = (x ** 2).sum()\n        &gt;&gt;&gt; y.backward()\n        &gt;&gt;&gt; print(x.grad)  # tensor([2., 4., 6.])\n    \"\"\"\n\ndef detach(self) -&gt; Tensor:\n    \"\"\"\n    \u8fd4\u56de\u4ece\u8ba1\u7b97\u56fe\u5206\u79bb\u7684\u65b0\u5f20\u91cf\u3002\n\n    \u8fd4\u56de:\n        requires_grad=False\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x.detach()\n        &gt;&gt;&gt; print(y.requires_grad)  # False\n    \"\"\"\n\ndef retain_grad(self) -&gt; None:\n    \"\"\"\n    \u4e3a\u975e\u53f6\u5f20\u91cf\u542f\u7528\u68af\u5ea6\u4fdd\u6301\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x * 2  # \u975e\u53f6\u5f20\u91cf\n        &gt;&gt;&gt; y.retain_grad()\n        &gt;&gt;&gt; z = y.sum()\n        &gt;&gt;&gt; z.backward()\n        &gt;&gt;&gt; print(y.grad)  # tensor([1., 1.])\n    \"\"\"\n\ndef zero_grad(self) -&gt; None:\n    \"\"\"\n    \u5c06\u68af\u5ea6\u5f20\u91cf\u6e05\u96f6\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x.sum()\n        &gt;&gt;&gt; y.backward()\n        &gt;&gt;&gt; x.zero_grad()\n        &gt;&gt;&gt; print(x.grad)  # None\n    \"\"\"\n</code></pre>"},{"location":"api/autograd.zh/#_12","title":"\u5f20\u91cf\u64cd\u4f5c","text":"<p>\u6240\u6709\u6807\u51c6\u6570\u5b66\u64cd\u4f5c\u90fd\u88ab\u652f\u6301\u5e76\u81ea\u52a8\u8ffd\u8e2a\u68af\u5ea6\u8ba1\u7b97\uff1a</p> Python<pre><code># \u7b97\u672f\u64cd\u4f5c\nz = x + y          # \u52a0\u6cd5\nz = x - y          # \u51cf\u6cd5\nz = x * y          # \u4e58\u6cd5\nz = x / y          # \u9664\u6cd5\nz = x ** y         # \u5e42\u8fd0\u7b97\nz = x @ y          # \u77e9\u9635\u4e58\u6cd5\n\n# \u4e00\u5143\u64cd\u4f5c\nz = -x             # \u53d6\u8d1f\nz = x.abs()        # \u7edd\u5bf9\u503c\nz = x.exp()        # \u6307\u6570\nz = x.log()        # \u81ea\u7136\u5bf9\u6570\nz = x.sqrt()       # \u5e73\u65b9\u6839\nz = x.sin()        # \u6b63\u5f26\nz = x.cos()        # \u4f59\u5f26\nz = x.tanh()       # \u53cc\u66f2\u6b63\u5207\n\n# \u5f52\u7ea6\u64cd\u4f5c\nz = x.sum()        # \u6c42\u548c\nz = x.mean()       # \u5e73\u5747\u503c\nz = x.max()        # \u6700\u5927\u503c\nz = x.min()        # \u6700\u5c0f\u503c\n\n# \u5f62\u72b6\u64cd\u4f5c\nz = x.reshape(shape)      # \u91cd\u5851\nz = x.transpose(dims)     # \u8f6c\u7f6e\nz = x.squeeze()           # \u79fb\u9664\u5355\u7ef4\u5ea6\nz = x.unsqueeze(dim)      # \u6dfb\u52a0\u5355\u7ef4\u5ea6\nz = x.view(shape)         # \u4ee5\u4e0d\u540c\u5f62\u72b6\u67e5\u770b\n</code></pre>"},{"location":"api/autograd.zh/#genesisfunction","title":"<code>genesis.Function</code>","text":"<p>\u6240\u6709\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\u3002</p> Python<pre><code>class Function:\n    \"\"\"\n    \u5b9e\u73b0\u81ea\u5b9a\u4e49\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\u3002\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx: Context, *args, **kwargs) -&gt; Tensor:\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\u5b9e\u73b0\u3002\n\n        \u53c2\u6570:\n            ctx: \u7528\u4e8e\u4e3a\u540e\u5411\u4f20\u64ad\u4fdd\u5b58\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u5bf9\u8c61\n            *args: \u8f93\u5165\u5f20\u91cf\n            **kwargs: \u989d\u5916\u53c2\u6570\n\n        \u8fd4\u56de:\n            \u8f93\u51fa\u5f20\u91cf\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def backward(ctx: Context, *grad_outputs) -&gt; Tuple[Optional[Tensor], ...]:\n        \"\"\"\n        \u540e\u5411\u4f20\u64ad\u5b9e\u73b0\u3002\n\n        \u53c2\u6570:\n            ctx: \u5305\u542b\u4fdd\u5b58\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u5bf9\u8c61\n            *grad_outputs: \u76f8\u5bf9\u4e8e\u8f93\u51fa\u7684\u68af\u5ea6\n\n        \u8fd4\u56de:\n            \u76f8\u5bf9\u4e8e\u8f93\u5165\u7684\u68af\u5ea6\uff08\u5bf9\u4e8e\u4e0d\u53ef\u5fae\u5206\u8f93\u5165\u4e3aNone\uff09\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def apply(cls, *args, **kwargs) -&gt; Tensor:\n        \"\"\"\n        \u5e94\u7528\u51fd\u6570\u5e76\u5728\u8ba1\u7b97\u56fe\u4e2d\u6ce8\u518c\u3002\n        \"\"\"\n</code></pre>"},{"location":"api/autograd.zh/#_13","title":"\u81ea\u5b9a\u4e49\u51fd\u6570\u793a\u4f8b","text":"Python<pre><code>import genesis\nfrom genesis import Function\n\nclass Exp(Function):\n    @staticmethod\n    def forward(ctx, x):\n        # \u4e3a\u540e\u5411\u4f20\u64ad\u4fdd\u5b58\u8f93\u5165\n        ctx.save_for_backward(x)\n        return genesis.tensor(x.data.exp(), requires_grad=x.requires_grad)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # \u68c0\u7d22\u4fdd\u5b58\u7684\u5f20\u91cf\n        x, = ctx.saved_tensors\n        # exp(x)\u7684\u68af\u5ea6\u662fexp(x)\n        return grad_output * x.exp()\n\n# \u4f7f\u7528\nexp = Exp.apply\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\ny = exp(x)\ny.sum().backward()\nprint(x.grad)  # \u901a\u8fc7\u81ea\u5b9a\u4e49\u51fd\u6570\u8ba1\u7b97\u7684\u68af\u5ea6\n</code></pre>"},{"location":"api/autograd.zh/#_14","title":"\u4e0a\u4e0b\u6587\u7ba1\u7406","text":""},{"location":"api/autograd.zh/#genesisno_grad","title":"<code>genesis.no_grad()</code>","text":"<p>\u5728\u63a8\u7406\u65f6\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\u4ee5\u63d0\u9ad8\u6548\u7387\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002</p> Python<pre><code>with genesis.no_grad():\n    # \u8fd9\u91cc\u7684\u64cd\u4f5c\u4e0d\u4f1a\u6784\u5efa\u8ba1\u7b97\u56fe\n    y = model(x)  # \u4e0d\u8ba1\u7b97\u68af\u5ea6\n</code></pre>"},{"location":"api/autograd.zh/#genesisenable_grad","title":"<code>genesis.enable_grad()</code>","text":"<p>\u542f\u7528\u68af\u5ea6\u8ba1\u7b97\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff08\u5728no_grad\u4e0a\u4e0b\u6587\u4e2d\u6709\u7528\uff09\u3002</p> Python<pre><code>with genesis.no_grad():\n    # \u5927\u591a\u6570\u64cd\u4f5c\u4e0d\u9700\u8981\u68af\u5ea6\n    y = model(x)\n\n    with genesis.enable_grad():\n        # \u8fd9\u4e2a\u7279\u5b9a\u64cd\u4f5c\u9700\u8981\u68af\u5ea6\n        z = y.sum()\n        z.backward()\n</code></pre>"},{"location":"api/autograd.zh/#genesisset_grad_enabledmode-bool","title":"<code>genesis.set_grad_enabled(mode: bool)</code>","text":"<p>\u5168\u5c40\u542f\u7528\u6216\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\u3002</p> Python<pre><code>genesis.set_grad_enabled(False)  # \u5168\u5c40\u7981\u7528\ny = x * 2  # \u65e0\u68af\u5ea6\n\ngenesis.set_grad_enabled(True)   # \u5168\u5c40\u542f\u7528\nz = x * 2  # \u8ba1\u7b97\u68af\u5ea6\n</code></pre>"},{"location":"api/autograd.zh/#_15","title":"\u68af\u5ea6\u94a9\u5b50","text":""},{"location":"api/autograd.zh/#_16","title":"\u524d\u7f6e\u548c\u540e\u7f6e\u94a9\u5b50","text":"<p>\u6ce8\u518c\u5728\u540e\u5411\u4f20\u64ad\u671f\u95f4\u8c03\u7528\u7684\u51fd\u6570\uff1a</p> Python<pre><code>def print_grad(grad):\n    print(f\"\u68af\u5ea6: {grad}\")\n    return grad  # \u53ef\u4ee5\u5728\u8fd9\u91cc\u4fee\u6539\u68af\u5ea6\n\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\nx.register_hook(print_grad)\ny = (x ** 2).sum()\ny.backward()  # \u5728\u540e\u5411\u4f20\u64ad\u671f\u95f4\u6253\u5370\u68af\u5ea6\n</code></pre>"},{"location":"api/autograd.zh/#_17","title":"\u5185\u5b58\u7ba1\u7406","text":""},{"location":"api/autograd.zh/#_18","title":"\u68af\u5ea6\u7d2f\u79ef","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u68af\u5ea6\u4f1a\u5728\u591a\u6b21\u540e\u5411\u4f20\u64ad\u4e2d\u7d2f\u79ef\uff1a</p> Python<pre><code>x = genesis.tensor([1., 2.], requires_grad=True)\n\ny1 = x.sum()\ny1.backward()\nprint(x.grad)  # tensor([1., 1.])\n\ny2 = (x * 2).sum()\ny2.backward()\nprint(x.grad)  # tensor([3., 3.]) - \u7d2f\u79ef\u4e86\uff01\n</code></pre>"},{"location":"api/autograd.zh/#_19","title":"\u6e05\u9664\u68af\u5ea6","text":"Python<pre><code># \u5728\u65b0\u8ba1\u7b97\u524d\u6e05\u9664\u68af\u5ea6\nx.grad = None  # \u6216 x.zero_grad()\n</code></pre>"},{"location":"api/autograd.zh/#_20","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"api/autograd.zh/#1","title":"1. \u9ad8\u6548\u63a8\u7406","text":"<p>\u5728\u63a8\u7406\u65f6\u59cb\u7ec8\u4f7f\u7528<code>no_grad()</code>\u4e0a\u4e0b\u6587\uff1a</p> Python<pre><code>model.eval()\nwith genesis.no_grad():\n    predictions = model(test_data)\n</code></pre>"},{"location":"api/autograd.zh/#2","title":"2. \u5185\u5b58\u4f18\u5316","text":"<p>\u5f53\u4e0d\u9700\u8981\u68af\u5ea6\u65f6\uff0c\u5206\u79bb\u4e2d\u95f4\u7ed3\u679c\uff1a</p> Python<pre><code># \u4e0d\u9700\u8981running_mean\u7684\u68af\u5ea6\nrunning_mean = (alpha * running_mean.detach() + \n                (1 - alpha) * batch_mean)\n</code></pre>"},{"location":"api/autograd.zh/#3","title":"3. \u68af\u5ea6\u88c1\u526a","text":"<p>\u9632\u6b62\u68af\u5ea6\u7206\u70b8\uff1a</p> Python<pre><code>genesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre>"},{"location":"api/autograd.zh/#4","title":"4. \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u52a0\u901f\u8bad\u7ec3\uff1a</p> Python<pre><code>genesis.enable_autocast = True\nwith genesis.autocast():\n    output = model(input)\n    loss = criterion(output, target)\n</code></pre>"},{"location":"api/autograd.zh/#_21","title":"\u5e38\u89c1\u6a21\u5f0f","text":""},{"location":"api/autograd.zh/#_22","title":"\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>model = MyModel()\noptimizer = genesis.optim.Adam(model.parameters())\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(batch.inputs)\n        loss = criterion(outputs, batch.targets)\n\n        # \u540e\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n\n        # \u66f4\u65b0\u6743\u91cd\n        optimizer.step()\n</code></pre>"},{"location":"api/autograd.zh/#_23","title":"\u68af\u5ea6\u68c0\u67e5\u70b9","text":"<p>\u901a\u8fc7\u91cd\u8ba1\u7b97\u6fc0\u6d3b\u8282\u7701\u5185\u5b58\uff1a</p> Python<pre><code># \u672a\u6765\u7248\u672c\u4e2d\u63d0\u4f9b\nfrom genesis.utils.checkpoint import checkpoint\n\ndef forward(self, x):\n    # \u68c0\u67e5\u70b9\u4e2d\u95f4\u8ba1\u7b97\n    x = checkpoint(self.layer1, x)\n    x = checkpoint(self.layer2, x)\n    return self.layer3(x)\n</code></pre>"},{"location":"api/autograd.zh/#_24","title":"\u8c03\u8bd5","text":""},{"location":"api/autograd.zh/#_25","title":"\u68af\u5ea6\u68c0\u67e5","text":"<p>\u4f7f\u7528\u6570\u503c\u5fae\u5206\u9a8c\u8bc1\u68af\u5ea6\uff1a</p> Python<pre><code>from genesis.autograd import gradcheck\n\ndef func(x):\n    return (x ** 2).sum()\n\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\ngradcheck(func, x, eps=1e-6)  # \u5982\u679c\u68af\u5ea6\u6b63\u786e\u8fd4\u56deTrue\n</code></pre>"},{"location":"api/autograd.zh/#_26","title":"\u68c0\u67e5\u8ba1\u7b97\u56fe","text":"Python<pre><code># \u6253\u5370\u8ba1\u7b97\u56fe\u7ed3\u6784\ny = x * 2 + 3\nprint(y.grad_fn)  # &lt;AddBackward&gt;\nprint(y.grad_fn.next_functions)  # \u8fde\u63a5\u7684\u64cd\u4f5c\n</code></pre>"},{"location":"api/autograd.zh/#_27","title":"\u6027\u80fd\u63d0\u793a","text":"<ol> <li>\u91cd\u7528\u5f20\u91cf: \u907f\u514d\u4e0d\u5fc5\u8981\u5730\u521b\u5efa\u65b0\u5f20\u91cf</li> <li>\u539f\u5730\u64cd\u4f5c: \u5c3d\u53ef\u80fd\u4f7f\u7528\uff08\u5982<code>x.add_(y)</code>\uff09</li> <li>\u6279\u91cf\u64cd\u4f5c: \u540c\u65f6\u5904\u7406\u591a\u4e2a\u6837\u672c</li> <li>\u7981\u7528\u68af\u5ea6: \u63a8\u7406\u65f6\u4f7f\u7528<code>no_grad()</code></li> <li>\u6e05\u9664\u68af\u5ea6: \u6bcf\u6b21\u540e\u5411\u4f20\u64ad\u524d\u5c06\u68af\u5ea6\u6e05\u96f6</li> </ol>"},{"location":"api/autograd.zh/#_28","title":"\u53e6\u8bf7\u53c2\u9605","text":"<ul> <li>\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 - \u4f7f\u7528Genesis\u6784\u5efa\u6a21\u578b</li> <li>\u4f18\u5316\u5668 - \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3</li> <li>\u5f20\u91cf\u64cd\u4f5c - \u4f4e\u7ea7\u5f20\u91cf\u64cd\u4f5c</li> <li>\u793a\u4f8b - \u5b8c\u6574\u5de5\u4f5c\u793a\u4f8b</li> </ul>"},{"location":"api/autograd_en/","title":"Automatic Differentiation System (genesis.autograd)","text":""},{"location":"api/autograd_en/#overview","title":"Overview","text":"<p>The automatic differentiation system is the core of Genesis, providing dynamic computational graph construction and automatic gradient computation. It implements reverse-mode automatic differentiation (backpropagation) with support for complex computational graphs.</p>"},{"location":"api/autograd_en/#core-concepts","title":"Core Concepts","text":""},{"location":"api/autograd_en/#computational-graph","title":"Computational Graph","text":"<p>Genesis builds a dynamic computational graph as operations are performed. Each operation creates nodes in the graph that track: - Input tensors - The operation performed - Output tensors - Gradient functions for backpropagation</p>"},{"location":"api/autograd_en/#gradient-computation","title":"Gradient Computation","text":"<p>Gradients are computed using the chain rule, traversing the computational graph in reverse order from outputs to inputs.</p>"},{"location":"api/autograd_en/#main-classes","title":"Main Classes","text":""},{"location":"api/autograd_en/#genesistensor","title":"<code>genesis.Tensor</code>","text":"<p>The fundamental data structure in Genesis that supports automatic differentiation.</p> Python<pre><code>class Tensor:\n    def __init__(\n        self,\n        array: Union[list, np.ndarray, NDArray],\n        device: Optional[Device] = None,\n        dtype: Optional[DType] = None,\n        requires_grad: bool = False,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/autograd_en/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>array</code> array-like required Input data (list, numpy array, or NDArray) <code>device</code> Device <code>None</code> Computation device (cpu/cuda) <code>dtype</code> DType <code>None</code> Data type (inferred if None) <code>requires_grad</code> bool <code>False</code> Whether to compute gradients <code>**kwargs</code> dict <code>{}</code> Additional NDArray parameters"},{"location":"api/autograd_en/#properties","title":"Properties","text":""},{"location":"api/autograd_en/#shape-and-type-information","title":"Shape and Type Information","text":"Python<pre><code>@property\ndef shape(self) -&gt; Tuple[int, ...]:\n    \"\"\"Returns the shape of the tensor.\"\"\"\n\n@property\ndef dtype(self) -&gt; DType:\n    \"\"\"Returns the data type.\"\"\"\n\n@property\ndef device(self) -&gt; Device:\n    \"\"\"Returns the device.\"\"\"\n\n@property\ndef ndim(self) -&gt; int:\n    \"\"\"Returns the number of dimensions.\"\"\"\n\n@property\ndef size(self) -&gt; int:\n    \"\"\"Returns the total number of elements.\"\"\"\n</code></pre>"},{"location":"api/autograd_en/#gradient-properties","title":"Gradient Properties","text":"Python<pre><code>@property\ndef requires_grad(self) -&gt; bool:\n    \"\"\"Whether this tensor requires gradient computation.\"\"\"\n\n@property\ndef grad(self) -&gt; Optional[Tensor]:\n    \"\"\"Access the gradient tensor.\"\"\"\n\n@property\ndef is_leaf(self) -&gt; bool:\n    \"\"\"Whether this is a leaf node (user-created tensor).\"\"\"\n\n@property\ndef grad_fn(self) -&gt; Optional[Function]:\n    \"\"\"The function that created this tensor.\"\"\"\n</code></pre>"},{"location":"api/autograd_en/#core-methods","title":"Core Methods","text":""},{"location":"api/autograd_en/#gradient-operations","title":"Gradient Operations","text":"Python<pre><code>def backward(self, gradient: Optional[Tensor] = None) -&gt; None:\n    \"\"\"\n    Compute gradients via backpropagation.\n\n    Args:\n        gradient: Output gradient. Defaults to tensor([1.0]) for scalars.\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2., 3.], requires_grad=True)\n        &gt;&gt;&gt; y = (x ** 2).sum()\n        &gt;&gt;&gt; y.backward()\n        &gt;&gt;&gt; print(x.grad)  # tensor([2., 4., 6.])\n    \"\"\"\n\ndef detach(self) -&gt; Tensor:\n    \"\"\"\n    Returns a new tensor detached from the computational graph.\n\n    Returns:\n        Tensor with requires_grad=False\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x.detach()\n        &gt;&gt;&gt; print(y.requires_grad)  # False\n    \"\"\"\n\ndef retain_grad(self) -&gt; None:\n    \"\"\"\n    Enable gradient retention for non-leaf tensors.\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x * 2  # Non-leaf tensor\n        &gt;&gt;&gt; y.retain_grad()\n        &gt;&gt;&gt; z = y.sum()\n        &gt;&gt;&gt; z.backward()\n        &gt;&gt;&gt; print(y.grad)  # tensor([1., 1.])\n    \"\"\"\n\ndef zero_grad(self) -&gt; None:\n    \"\"\"\n    Zero out the gradient tensor.\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x.sum()\n        &gt;&gt;&gt; y.backward()\n        &gt;&gt;&gt; x.zero_grad()\n        &gt;&gt;&gt; print(x.grad)  # None\n    \"\"\"\n</code></pre>"},{"location":"api/autograd_en/#tensor-operations","title":"Tensor Operations","text":"<p>All standard mathematical operations are supported and automatically tracked for gradient computation:</p> Python<pre><code># Arithmetic operations\nz = x + y          # Addition\nz = x - y          # Subtraction\nz = x * y          # Multiplication\nz = x / y          # Division\nz = x ** y         # Power\nz = x @ y          # Matrix multiplication\n\n# Unary operations\nz = -x             # Negation\nz = x.abs()        # Absolute value\nz = x.exp()        # Exponential\nz = x.log()        # Natural logarithm\nz = x.sqrt()       # Square root\nz = x.sin()        # Sine\nz = x.cos()        # Cosine\nz = x.tanh()       # Hyperbolic tangent\n\n# Reduction operations\nz = x.sum()        # Sum all elements\nz = x.mean()       # Mean of all elements\nz = x.max()        # Maximum element\nz = x.min()        # Minimum element\n\n# Shape operations\nz = x.reshape(shape)      # Reshape\nz = x.transpose(dims)     # Transpose\nz = x.squeeze()           # Remove singleton dimensions\nz = x.unsqueeze(dim)      # Add singleton dimension\nz = x.view(shape)         # View with different shape\n</code></pre>"},{"location":"api/autograd_en/#genesisfunction","title":"<code>genesis.Function</code>","text":"<p>Base class for all differentiable operations.</p> Python<pre><code>class Function:\n    \"\"\"\n    Base class for implementing custom differentiable operations.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx: Context, *args, **kwargs) -&gt; Tensor:\n        \"\"\"\n        Forward pass implementation.\n\n        Args:\n            ctx: Context object for saving information for backward pass\n            *args: Input tensors\n            **kwargs: Additional arguments\n\n        Returns:\n            Output tensor(s)\n        \"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def backward(ctx: Context, *grad_outputs) -&gt; Tuple[Optional[Tensor], ...]:\n        \"\"\"\n        Backward pass implementation.\n\n        Args:\n            ctx: Context object with saved information\n            *grad_outputs: Gradients w.r.t. outputs\n\n        Returns:\n            Gradients w.r.t. inputs (None for non-differentiable inputs)\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def apply(cls, *args, **kwargs) -&gt; Tensor:\n        \"\"\"\n        Apply the function and register it in the computational graph.\n        \"\"\"\n</code></pre>"},{"location":"api/autograd_en/#custom-function-example","title":"Custom Function Example","text":"Python<pre><code>import genesis\nfrom genesis import Function\n\nclass Exp(Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Save input for backward pass\n        ctx.save_for_backward(x)\n        return genesis.tensor(x.data.exp(), requires_grad=x.requires_grad)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retrieve saved tensor\n        x, = ctx.saved_tensors\n        # Gradient of exp(x) is exp(x)\n        return grad_output * x.exp()\n\n# Usage\nexp = Exp.apply\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\ny = exp(x)\ny.sum().backward()\nprint(x.grad)  # Gradients computed through custom function\n</code></pre>"},{"location":"api/autograd_en/#context-management","title":"Context Management","text":""},{"location":"api/autograd_en/#genesisno_grad","title":"<code>genesis.no_grad()</code>","text":"<p>Context manager to disable gradient computation for efficiency during inference.</p> Python<pre><code>with genesis.no_grad():\n    # Operations here won't build computational graph\n    y = model(x)  # No gradients computed\n</code></pre>"},{"location":"api/autograd_en/#genesisenable_grad","title":"<code>genesis.enable_grad()</code>","text":"<p>Context manager to enable gradient computation (useful within no_grad context).</p> Python<pre><code>with genesis.no_grad():\n    # Most operations without gradients\n    y = model(x)\n\n    with genesis.enable_grad():\n        # This specific operation needs gradients\n        z = y.sum()\n        z.backward()\n</code></pre>"},{"location":"api/autograd_en/#genesisset_grad_enabledmode-bool","title":"<code>genesis.set_grad_enabled(mode: bool)</code>","text":"<p>Globally enable or disable gradient computation.</p> Python<pre><code>genesis.set_grad_enabled(False)  # Disable globally\ny = x * 2  # No gradients\n\ngenesis.set_grad_enabled(True)   # Enable globally\nz = x * 2  # Gradients computed\n</code></pre>"},{"location":"api/autograd_en/#gradient-hooks","title":"Gradient Hooks","text":""},{"location":"api/autograd_en/#pre-and-post-hooks","title":"Pre and Post Hooks","text":"<p>Register functions to be called during backward pass:</p> Python<pre><code>def print_grad(grad):\n    print(f\"Gradient: {grad}\")\n    return grad  # Can modify gradient here\n\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\nx.register_hook(print_grad)\ny = (x ** 2).sum()\ny.backward()  # Will print gradients during backward\n</code></pre>"},{"location":"api/autograd_en/#memory-management","title":"Memory Management","text":""},{"location":"api/autograd_en/#gradient-accumulation","title":"Gradient Accumulation","text":"<p>Gradients accumulate by default across multiple backward passes:</p> Python<pre><code>x = genesis.tensor([1., 2.], requires_grad=True)\n\ny1 = x.sum()\ny1.backward()\nprint(x.grad)  # tensor([1., 1.])\n\ny2 = (x * 2).sum()\ny2.backward()\nprint(x.grad)  # tensor([3., 3.]) - accumulated!\n</code></pre>"},{"location":"api/autograd_en/#clearing-gradients","title":"Clearing Gradients","text":"Python<pre><code># Clear gradients before new computation\nx.grad = None  # or x.zero_grad()\n</code></pre>"},{"location":"api/autograd_en/#best-practices","title":"Best Practices","text":""},{"location":"api/autograd_en/#1-efficient-inference","title":"1. Efficient Inference","text":"<p>Always use <code>no_grad()</code> context for inference:</p> Python<pre><code>model.eval()\nwith genesis.no_grad():\n    predictions = model(test_data)\n</code></pre>"},{"location":"api/autograd_en/#2-memory-optimization","title":"2. Memory Optimization","text":"<p>Detach intermediate results when gradients aren't needed:</p> Python<pre><code># Don't need gradients for running_mean\nrunning_mean = (alpha * running_mean.detach() + \n                (1 - alpha) * batch_mean)\n</code></pre>"},{"location":"api/autograd_en/#3-gradient-clipping","title":"3. Gradient Clipping","text":"<p>Prevent gradient explosion:</p> Python<pre><code>genesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n</code></pre>"},{"location":"api/autograd_en/#4-mixed-precision-support","title":"4. Mixed Precision Support","text":"<p>Genesis supports mixed precision through automatic type casting in the Function class. When <code>genesis.enable_autocast</code> is set to <code>True</code>, operations automatically handle float16/float32 conversions:</p> Python<pre><code># Enable mixed precision globally\ngenesis.enable_autocast = True\n\n# The framework will automatically handle type conversions\n# during forward and backward passes\n</code></pre>"},{"location":"api/autograd_en/#common-patterns","title":"Common Patterns","text":""},{"location":"api/autograd_en/#training-loop","title":"Training Loop","text":"Python<pre><code>model = MyModel()\noptimizer = genesis.optim.Adam(model.parameters())\n\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass\n        outputs = model(batch.inputs)\n        loss = criterion(outputs, batch.targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Update weights\n        optimizer.step()\n</code></pre>"},{"location":"api/autograd_en/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Save memory by recomputing activations:</p> Python<pre><code># Available in future versions\nfrom genesis.utils.checkpoint import checkpoint\n\ndef forward(self, x):\n    # Checkpoint intermediate computations\n    x = checkpoint(self.layer1, x)\n    x = checkpoint(self.layer2, x)\n    return self.layer3(x)\n</code></pre>"},{"location":"api/autograd_en/#debugging","title":"Debugging","text":""},{"location":"api/autograd_en/#gradient-checking","title":"Gradient Checking","text":"<p>Verify gradients using numerical differentiation:</p> Python<pre><code>from genesis.autograd import gradcheck\n\ndef func(x):\n    return (x ** 2).sum()\n\nx = genesis.tensor([1., 2., 3.], requires_grad=True)\ngradcheck(func, x, eps=1e-6)  # Returns True if gradients are correct\n</code></pre>"},{"location":"api/autograd_en/#inspecting-computational-graph","title":"Inspecting Computational Graph","text":"Python<pre><code># Print computational graph structure\ny = x * 2 + 3\nprint(y.grad_fn)  # &lt;AddBackward&gt;\nprint(y.grad_fn.next_functions)  # Connected operations\n</code></pre>"},{"location":"api/autograd_en/#performance-tips","title":"Performance Tips","text":"<ol> <li>Reuse tensors: Avoid creating new tensors unnecessarily</li> <li>In-place operations: Use when possible (e.g., <code>x.add_(y)</code>)</li> <li>Batch operations: Process multiple samples together</li> <li>Disable gradients: Use <code>no_grad()</code> for inference</li> <li>Clear gradients: Zero gradients before each backward pass</li> </ol>"},{"location":"api/autograd_en/#see-also","title":"See Also","text":"<ul> <li>Neural Network Modules - Building models with Genesis</li> <li>Optimizers - Training with gradient descent</li> <li>Tensor Operations - Low-level tensor operations</li> <li>Examples - Complete working examples</li> </ul>"},{"location":"api/serialization.zh/","title":"\u6a21\u578b\u5e8f\u5217\u5316\u4e0e\u68c0\u67e5\u70b9","text":"<p>Genesis\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6a21\u578b\u5e8f\u5217\u5316\u548c\u68c0\u67e5\u70b9\u529f\u80fd\uff0c\u7528\u4e8e\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\u72b6\u6001\u3001\u4f18\u5316\u5668\u72b6\u6001\u548c\u8bad\u7ec3\u8fdb\u5ea6\u3002\u8fd9\u5bf9\u4e8e\u957f\u65f6\u95f4\u8bad\u7ec3\u3001\u6a21\u578b\u90e8\u7f72\u548c\u5b9e\u9a8c\u53ef\u91cd\u73b0\u6027\u81f3\u5173\u91cd\u8981\u3002</p>"},{"location":"api/serialization.zh/#_2","title":"\u6982\u8ff0","text":"<p>Genesis\u4e2d\u7684\u5e8f\u5217\u5316\u7cfb\u7edf\u5904\u7406\uff1a - \u6a21\u578b\u72b6\u6001\u5b57\u5178\uff08\u53c2\u6570\u548c\u7f13\u51b2\u533a\uff09 - \u4f18\u5316\u5668\u72b6\u6001\uff08\u52a8\u91cf\u3001\u8fd0\u884c\u5e73\u5747\u503c\u7b49\uff09 - \u8bad\u7ec3\u5143\u6570\u636e\uff08epoch\u3001\u635f\u5931\u3001\u6307\u6807\uff09 - \u539f\u5b50\u5199\u64cd\u4f5c\u548c\u5b89\u5168\u5907\u4efd</p>"},{"location":"api/serialization.zh/#_3","title":"\u6838\u5fc3\u51fd\u6570","text":""},{"location":"api/serialization.zh/#save","title":"save()","text":"Python<pre><code>import genesis\n\ndef save(state_dict, file_path):\n    \"\"\"\n    \u5c06\u72b6\u6001\u5b57\u5178\u4fdd\u5b58\u5230\u6587\u4ef6\uff0c\u4f7f\u7528\u539f\u5b50\u5199\u64cd\u4f5c\u3002\n\n    Args:\n        state_dict (dict): \u5305\u542b\u8981\u4fdd\u5b58\u72b6\u6001\u7684\u5b57\u5178\n        file_path (str): \u4fdd\u5b58\u6587\u4ef6\u7684\u8def\u5f84\n\n    Features:\n        - \u539f\u5b50\u5199\u64cd\u4f5c\u4e0e\u5907\u4efd\n        - \u6210\u529f\u65f6\u81ea\u52a8\u6e05\u7406\n        - \u5931\u8d25\u65f6\u56de\u6eda\n        - \u4fdd\u5b58\u540e\u5185\u5b58\u6e05\u7406\n    \"\"\"\n</code></pre>"},{"location":"api/serialization.zh/#load","title":"load()","text":"Python<pre><code>def load(file_path):\n    \"\"\"\n    \u4ece\u6587\u4ef6\u52a0\u8f7d\u72b6\u6001\u5b57\u5178\u3002\n\n    Args:\n        file_path (str): \u4fdd\u5b58\u6587\u4ef6\u7684\u8def\u5f84\n\n    Returns:\n        dict: \u52a0\u8f7d\u7684\u72b6\u6001\u5b57\u5178\n\n    Raises:\n        FileNotFoundError: \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\n        pickle.UnpicklingError: \u5982\u679c\u6587\u4ef6\u635f\u574f\n    \"\"\"\n</code></pre>"},{"location":"api/serialization.zh/#save_checkpoint","title":"save_checkpoint()","text":"Python<pre><code>def save_checkpoint(model_state_dict, optimizer_state_dict, file_path):\n    \"\"\"\n    \u4fdd\u5b58\u6a21\u578b\u548c\u4f18\u5316\u5668\u68c0\u67e5\u70b9\u3002\n\n    Args:\n        model_state_dict (dict): \u6a21\u578b\u72b6\u6001\u5b57\u5178\n        optimizer_state_dict (dict): \u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\n        file_path (str): \u4fdd\u5b58\u68c0\u67e5\u70b9\u7684\u8def\u5f84\n\n    \u521b\u5efa\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\u7684\u68c0\u67e5\u70b9\uff1a\n        - model_state_dict: \u6a21\u578b\u53c2\u6570\u548c\u7f13\u51b2\u533a\n        - optimizer_state_dict: \u4f18\u5316\u5668\u72b6\u6001\n    \"\"\"\n</code></pre>"},{"location":"api/serialization.zh/#load_checkpoint","title":"load_checkpoint()","text":"Python<pre><code>def load_checkpoint(file_path):\n    \"\"\"\n    \u52a0\u8f7d\u6a21\u578b\u548c\u4f18\u5316\u5668\u68c0\u67e5\u70b9\u3002\n\n    Args:\n        file_path (str): \u68c0\u67e5\u70b9\u6587\u4ef6\u8def\u5f84\n\n    Returns:\n        tuple: (model_state_dict, optimizer_state_dict)\n\n    Example:\n        &gt;&gt;&gt; model_state, optimizer_state = genesis.load_checkpoint('checkpoint.pth')\n        &gt;&gt;&gt; model.load_state_dict(model_state)\n        &gt;&gt;&gt; optimizer.load_state_dict(optimizer_state)\n    \"\"\"\n</code></pre>"},{"location":"api/serialization.zh/#_4","title":"\u57fa\u672c\u7528\u6cd5","text":""},{"location":"api/serialization.zh/#_5","title":"\u4fdd\u5b58\u7b80\u5355\u6a21\u578b","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# \u521b\u5efa\u548c\u8bad\u7ec3\u6a21\u578b\nmodel = nn.Linear(784, 10)\n\n# \u4fdd\u5b58\u6a21\u578b\u72b6\u6001\ngenesis.save(model.state_dict(), 'model.pth')\n\n# \u52a0\u8f7d\u6a21\u578b\u72b6\u6001\nstate_dict = genesis.load('model.pth')\nmodel.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/serialization.zh/#_6","title":"\u8bad\u7ec3\u68c0\u67e5\u70b9","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u8bbe\u7f6e\u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# \u5e26\u68c0\u67e5\u70b9\u7684\u8bad\u7ec3\u5faa\u73af\nfor epoch in range(100):\n    # \u8bad\u7ec3\u4ee3\u7801...\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n\n    # \u6bcf10\u4e2aepoch\u4fdd\u5b58\u68c0\u67e5\u70b9\n    if epoch % 10 == 0:\n        genesis.save_checkpoint(\n            model.state_dict(),\n            optimizer.state_dict(), \n            f'checkpoint_epoch_{epoch}.pth'\n        )\n        print(f\"\u68c0\u67e5\u70b9\u5df2\u4fdd\u5b58\u5728epoch {epoch}\")\n\n# \u52a0\u8f7d\u68c0\u67e5\u70b9\u6062\u590d\u8bad\u7ec3\nmodel_state, optimizer_state = genesis.load_checkpoint('checkpoint_epoch_90.pth')\nmodel.load_state_dict(model_state)\noptimizer.load_state_dict(optimizer_state)\n</code></pre>"},{"location":"api/serialization.zh/#_7","title":"\u9ad8\u7ea7\u68c0\u67e5\u70b9","text":""},{"location":"api/serialization.zh/#_8","title":"\u5b8c\u6574\u8bad\u7ec3\u72b6\u6001","text":"Python<pre><code>import genesis\n\ndef save_training_checkpoint(model, optimizer, scheduler, epoch, loss, metrics, file_path):\n    \"\"\"\u4fdd\u5b58\u5b8c\u6574\u8bad\u7ec3\u72b6\u6001\u3002\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n        'loss': loss,\n        'metrics': metrics,\n        'model_config': {\n            'input_size': model.input_size,\n            'hidden_size': model.hidden_size,\n            'num_classes': model.num_classes\n        }\n    }\n    genesis.save(checkpoint, file_path)\n\ndef load_training_checkpoint(file_path):\n    \"\"\"\u52a0\u8f7d\u5b8c\u6574\u8bad\u7ec3\u72b6\u6001\u3002\"\"\"\n    return genesis.load(file_path)\n\n# \u7528\u6cd5\nsave_training_checkpoint(\n    model, optimizer, scheduler, \n    epoch=50, loss=0.234, \n    metrics={'accuracy': 0.94, 'f1': 0.91},\n    file_path='complete_checkpoint.pth'\n)\n\n# \u6062\u590d\u8bad\u7ec3\ncheckpoint = load_training_checkpoint('complete_checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nif checkpoint['scheduler_state_dict']:\n    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\nstart_epoch = checkpoint['epoch'] + 1\nprint(f\"\u4eceepoch {start_epoch}\u6062\u590d\u8bad\u7ec3\uff0c\u635f\u5931: {checkpoint['loss']}\")\n</code></pre>"},{"location":"api/serialization.zh/#_9","title":"\u6700\u4f73\u6a21\u578b\u8ddf\u8e2a","text":"Python<pre><code>import genesis\n\nclass ModelCheckpointer:\n    def __init__(self, save_dir='checkpoints'):\n        self.save_dir = save_dir\n        self.best_loss = float('inf')\n        self.best_accuracy = 0.0\n\n    def save_checkpoint(self, model, optimizer, epoch, loss, accuracy, is_best=False):\n        \"\"\"\u4fdd\u5b58\u68c0\u67e5\u70b9\u5e76\u8ddf\u8e2a\u6700\u4f73\u6a21\u578b\u3002\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            'accuracy': accuracy\n        }\n\n        # \u4fdd\u5b58\u5e38\u89c4\u68c0\u67e5\u70b9\n        checkpoint_path = f'{self.save_dir}/checkpoint_epoch_{epoch}.pth'\n        genesis.save(checkpoint, checkpoint_path)\n\n        # \u57fa\u4e8e\u635f\u5931\u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n        if loss &lt; self.best_loss:\n            self.best_loss = loss\n            best_loss_path = f'{self.save_dir}/best_loss_model.pth'\n            genesis.save(checkpoint, best_loss_path)\n            print(f\"\u65b0\u7684\u6700\u4f73\u635f\u5931: {loss:.4f}\")\n\n        # \u57fa\u4e8e\u51c6\u786e\u7387\u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n        if accuracy &gt; self.best_accuracy:\n            self.best_accuracy = accuracy\n            best_acc_path = f'{self.save_dir}/best_accuracy_model.pth'\n            genesis.save(checkpoint, best_acc_path)\n            print(f\"\u65b0\u7684\u6700\u4f73\u51c6\u786e\u7387: {accuracy:.4f}\")\n\n    def load_best_model(self, model, metric='loss'):\n        \"\"\"\u52a0\u8f7d\u6700\u4f73\u6a21\u578b\u3002\"\"\"\n        if metric == 'loss':\n            path = f'{self.save_dir}/best_loss_model.pth'\n        elif metric == 'accuracy':\n            path = f'{self.save_dir}/best_accuracy_model.pth'\n        else:\n            raise ValueError(\"metric\u5fc5\u987b\u662f'loss'\u6216'accuracy'\")\n\n        checkpoint = genesis.load(path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        return checkpoint\n\n# \u7528\u6cd5\ncheckpointer = ModelCheckpointer()\n\nfor epoch in range(100):\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n    val_loss, val_accuracy = validate(model, val_loader)\n\n    checkpointer.save_checkpoint(\n        model, optimizer, epoch, val_loss, val_accuracy\n    )\n</code></pre>"},{"location":"api/serialization.zh/#_10","title":"\u6a21\u578b\u90e8\u7f72","text":""},{"location":"api/serialization.zh/#_11","title":"\u63a8\u7406\u6a21\u578b\u4fdd\u5b58","text":"Python<pre><code>import genesis\n\ndef save_for_inference(model, file_path, model_config=None):\n    \"\"\"\u4fdd\u5b58\u4f18\u5316\u7684\u63a8\u7406\u6a21\u578b\u3002\"\"\"\n    model.eval()  # \u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n\n    inference_state = {\n        'model_state_dict': model.state_dict(),\n        'model_config': model_config,\n        'genesis_version': genesis.__version__,\n        'inference_only': True\n    }\n\n    genesis.save(inference_state, file_path)\n\ndef load_for_inference(file_path, model_class):\n    \"\"\"\u52a0\u8f7d\u63a8\u7406\u6a21\u578b\u3002\"\"\"\n    checkpoint = genesis.load(file_path)\n\n    # \u521b\u5efa\u6a21\u578b\u5b9e\u4f8b\n    if 'model_config' in checkpoint and checkpoint['model_config']:\n        model = model_class(**checkpoint['model_config'])\n    else:\n        model = model_class()\n\n    # \u52a0\u8f7d\u72b6\u6001\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    return model\n\n# \u4fdd\u5b58\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u7528\u4e8e\u90e8\u7f72\nmodel_config = {\n    'input_size': 784,\n    'hidden_size': 256, \n    'num_classes': 10\n}\n\nsave_for_inference(model, 'deployed_model.pth', model_config)\n\n# \u5728\u751f\u4ea7\u73af\u5883\u4e2d\u52a0\u8f7d\ndeployed_model = load_for_inference('deployed_model.pth', MyModelClass)\n</code></pre>"},{"location":"api/serialization.zh/#_12","title":"\u6a21\u578b\u7248\u672c\u7ba1\u7406","text":"Python<pre><code>import genesis\nimport time\nfrom datetime import datetime\n\nclass VersionedCheckpoint:\n    def __init__(self, base_path='models'):\n        self.base_path = base_path\n\n    def save_version(self, model, optimizer, epoch, metrics, version_name=None):\n        \"\"\"\u4fdd\u5b58\u5e26\u6709\u7248\u672c\u4fe1\u606f\u7684\u6a21\u578b\u3002\"\"\"\n        if version_name is None:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            version_name = f\"v_{timestamp}\"\n\n        checkpoint = {\n            'version': version_name,\n            'timestamp': time.time(),\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'metrics': metrics,\n            'genesis_version': genesis.__version__\n        }\n\n        file_path = f'{self.base_path}/{version_name}.pth'\n        genesis.save(checkpoint, file_path)\n\n        # \u66f4\u65b0\u6700\u65b0\u7248\u672c\u94fe\u63a5\n        latest_path = f'{self.base_path}/latest.pth'\n        genesis.save(checkpoint, latest_path)\n\n        return version_name\n\n    def load_version(self, version_name='latest'):\n        \"\"\"\u52a0\u8f7d\u7279\u5b9a\u7248\u672c\u7684\u6a21\u578b\u3002\"\"\"\n        file_path = f'{self.base_path}/{version_name}.pth'\n        return genesis.load(file_path)\n\n    def list_versions(self):\n        \"\"\"\u5217\u51fa\u53ef\u7528\u7684\u6a21\u578b\u7248\u672c\u3002\"\"\"\n        import os\n        versions = []\n        for file in os.listdir(self.base_path):\n            if file.endswith('.pth') and file != 'latest.pth':\n                versions.append(file[:-4])  # \u79fb\u9664.pth\u6269\u5c55\u540d\n        return sorted(versions)\n\n# \u7528\u6cd5\nversioner = VersionedCheckpoint()\n\n# \u4fdd\u5b58\u65b0\u7248\u672c\nversion = versioner.save_version(\n    model, optimizer, epoch=100, \n    metrics={'accuracy': 0.95, 'loss': 0.15},\n    version_name='model_v1.2'\n)\n\n# \u52a0\u8f7d\u7279\u5b9a\u7248\u672c\ncheckpoint = versioner.load_version('model_v1.2')\n\n# \u52a0\u8f7d\u6700\u65b0\u7248\u672c\nlatest = versioner.load_version('latest')\n</code></pre>"},{"location":"api/serialization.zh/#_13","title":"\u9519\u8bef\u5904\u7406\u548c\u5b89\u5168\u6027","text":""},{"location":"api/serialization.zh/#_14","title":"\u7a33\u5065\u7684\u68c0\u67e5\u70b9\u52a0\u8f7d","text":"Python<pre><code>import genesis\nimport os\n\ndef safe_load_checkpoint(file_path, model, optimizer=None):\n    \"\"\"\u5b89\u5168\u52a0\u8f7d\u68c0\u67e5\u70b9\uff0c\u5e26\u9519\u8bef\u5904\u7406\u3002\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            print(f\"\u8b66\u544a: \u68c0\u67e5\u70b9 {file_path} \u672a\u627e\u5230\")\n            return False\n\n        checkpoint = genesis.load(file_path)\n\n        # \u9a8c\u8bc1\u68c0\u67e5\u70b9\u7ed3\u6784\n        required_keys = ['model_state_dict']\n        for key in required_keys:\n            if key not in checkpoint:\n                print(f\"\u9519\u8bef: \u68c0\u67e5\u70b9\u4e2d\u7f3a\u5c11\u952e '{key}'\")\n                return False\n\n        # \u52a0\u8f7d\u6a21\u578b\u72b6\u6001\n        try:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(\"\u6a21\u578b\u72b6\u6001\u52a0\u8f7d\u6210\u529f\")\n        except Exception as e:\n            print(f\"\u52a0\u8f7d\u6a21\u578b\u72b6\u6001\u65f6\u51fa\u9519: {e}\")\n            return False\n\n        # \u5982\u679c\u63d0\u4f9b\u4e86\u4f18\u5316\u5668\uff0c\u5219\u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001\n        if optimizer and 'optimizer_state_dict' in checkpoint:\n            try:\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                print(\"\u4f18\u5316\u5668\u72b6\u6001\u52a0\u8f7d\u6210\u529f\")\n            except Exception as e:\n                print(f\"\u8b66\u544a: \u65e0\u6cd5\u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001: {e}\")\n\n        # \u8fd4\u56de\u9644\u52a0\u4fe1\u606f\n        epoch = checkpoint.get('epoch', 0)\n        loss = checkpoint.get('loss', 'unknown')\n        print(f\"\u4eceepoch {epoch}\u52a0\u8f7d\u68c0\u67e5\u70b9\uff0c\u635f\u5931: {loss}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"\u52a0\u8f7d\u68c0\u67e5\u70b9\u65f6\u51fa\u9519: {e}\")\n        return False\n\n# \u7528\u6cd5\nsuccess = safe_load_checkpoint('checkpoint.pth', model, optimizer)\nif success:\n    print(\"\u68c0\u67e5\u70b9\u52a0\u8f7d\u6210\u529f\")\nelse:\n    print(\"\u52a0\u8f7d\u68c0\u67e5\u70b9\u5931\u8d25\uff0c\u4ece\u5934\u5f00\u59cb\")\n</code></pre>"},{"location":"api/serialization.zh/#_15","title":"\u68c0\u67e5\u70b9\u9a8c\u8bc1","text":"Python<pre><code>import genesis\n\ndef validate_checkpoint(file_path):\n    \"\"\"\u9a8c\u8bc1\u68c0\u67e5\u70b9\u6587\u4ef6\u5b8c\u6574\u6027\u3002\"\"\"\n    try:\n        checkpoint = genesis.load(file_path)\n\n        # \u57fa\u672c\u7ed3\u6784\u9a8c\u8bc1\n        if not isinstance(checkpoint, dict):\n            return False, \"\u68c0\u67e5\u70b9\u4e0d\u662f\u5b57\u5178\u7c7b\u578b\"\n\n        if 'model_state_dict' not in checkpoint:\n            return False, \"\u7f3a\u5c11model_state_dict\"\n\n        # \u68c0\u67e5\u6a21\u578b\u72b6\u6001\u7ed3\u6784\n        model_state = checkpoint['model_state_dict']\n        if not isinstance(model_state, dict):\n            return False, \"model_state_dict\u4e0d\u662f\u5b57\u5178\u7c7b\u578b\"\n\n        # \u68c0\u67e5\u7a7a\u72b6\u6001\n        if len(model_state) == 0:\n            return False, \"model_state_dict\u4e3a\u7a7a\"\n\n        # \u9a8c\u8bc1\u5f20\u91cf\u5f62\u72b6\uff08\u57fa\u672c\u68c0\u67e5\uff09\n        for key, tensor in model_state.items():\n            if not hasattr(tensor, 'shape'):\n                return False, f\"\u952e '{key}' \u7684\u5f20\u91cf\u65e0\u6548\"\n\n        return True, \"\u68c0\u67e5\u70b9\u6709\u6548\"\n\n    except Exception as e:\n        return False, f\"\u9a8c\u8bc1\u68c0\u67e5\u70b9\u65f6\u51fa\u9519: {e}\"\n\n# \u7528\u6cd5\nis_valid, message = validate_checkpoint('checkpoint.pth')\nprint(f\"\u68c0\u67e5\u70b9\u9a8c\u8bc1: {message}\")\n</code></pre>"},{"location":"api/serialization.zh/#_16","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"api/serialization.zh/#1","title":"1. \u68c0\u67e5\u70b9\u7b56\u7565","text":"<ul> <li>\u5b9a\u671f\u4fdd\u5b58\u68c0\u67e5\u70b9\uff08\u6bcfN\u4e2aepoch\uff09</li> <li>\u4fdd\u7559\u591a\u4e2a\u6700\u8fd1\u7684\u68c0\u67e5\u70b9</li> <li>\u5355\u72ec\u4fdd\u5b58\u6700\u4f73\u6a21\u578b</li> <li>\u5305\u542b\u8bad\u7ec3\u5143\u6570\u636e</li> </ul>"},{"location":"api/serialization.zh/#2","title":"2. \u6587\u4ef6\u7ec4\u7ec7","text":"Python<pre><code># \u63a8\u8350\u7684\u76ee\u5f55\u7ed3\u6784\ncheckpoints/\n\u251c\u2500\u2500 latest.pth                    # \u6700\u65b0\u68c0\u67e5\u70b9\n\u251c\u2500\u2500 best_model.pth               # \u6700\u4f73\u6027\u80fd\u6a21\u578b\n\u251c\u2500\u2500 epoch_000010.pth            # \u5e38\u89c4\u68c0\u67e5\u70b9\n\u251c\u2500\u2500 epoch_000020.pth\n\u2514\u2500\u2500 deployed/\n    \u2514\u2500\u2500 production_model.pth     # \u751f\u4ea7\u5c31\u7eea\u6a21\u578b\n</code></pre>"},{"location":"api/serialization.zh/#3","title":"3. \u5185\u5b58\u7ba1\u7406","text":"Python<pre><code>import genesis\nimport gc\n\ndef efficient_checkpoint_save(model, optimizer, file_path):\n    \"\"\"\u5e26\u5185\u5b58\u4f18\u5316\u7684\u68c0\u67e5\u70b9\u4fdd\u5b58\u3002\"\"\"\n    # \u521b\u5efa\u68c0\u67e5\u70b9\u5b57\u5178\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n\n    # \u4fdd\u5b58\u68c0\u67e5\u70b9\n    genesis.save(checkpoint, file_path)\n\n    # \u4ece\u5185\u5b58\u4e2d\u6e05\u9664\u68c0\u67e5\u70b9\u5b57\u5178\n    del checkpoint\n    gc.collect()\n\n    print(f\"\u68c0\u67e5\u70b9\u5df2\u4fdd\u5b58\u5230 {file_path}\")\n</code></pre>"},{"location":"api/serialization.zh/#4","title":"4. \u8de8\u8bbe\u5907\u517c\u5bb9\u6027","text":"Python<pre><code>def save_device_agnostic(model, file_path):\n    \"\"\"\u4fdd\u5b58\u53ef\u5728\u4efb\u4f55\u8bbe\u5907\u4e0a\u52a0\u8f7d\u7684\u6a21\u578b\u3002\"\"\"\n    # \u4fdd\u5b58\u524d\u79fb\u52a8\u5230CPU\n    model.cpu()\n    genesis.save(model.state_dict(), file_path)\n\ndef load_to_device(file_path, model, device):\n    \"\"\"\u5c06\u68c0\u67e5\u70b9\u52a0\u8f7d\u5230\u6307\u5b9a\u8bbe\u5907\u3002\"\"\"\n    # \u52a0\u8f7d\u68c0\u67e5\u70b9\n    state_dict = genesis.load(file_path)\n\n    # \u52a0\u8f7d\u5230\u6a21\u578b\n    model.load_state_dict(state_dict)\n\n    # \u79fb\u52a8\u5230\u76ee\u6807\u8bbe\u5907\n    model.to(device)\n</code></pre>"},{"location":"api/serialization.zh/#_17","title":"\u8fc1\u79fb\u548c\u517c\u5bb9\u6027","text":""},{"location":"api/serialization.zh/#pytorch","title":"\u4ecePyTorch\u8fc1\u79fb","text":"Python<pre><code>import genesis\nimport torch\n\ndef convert_pytorch_checkpoint(pytorch_file, genesis_file):\n    \"\"\"\u5c06PyTorch\u68c0\u67e5\u70b9\u8f6c\u6362\u4e3aGenesis\u683c\u5f0f\u3002\"\"\"\n    # \u52a0\u8f7dPyTorch\u68c0\u67e5\u70b9\n    torch_checkpoint = torch.load(pytorch_file, map_location='cpu')\n\n    # \u8f6c\u6362\u4e3aGenesis\u683c\u5f0f\uff08\u5982\u9700\u8981\uff09\n    genesis_checkpoint = {\n        'model_state_dict': torch_checkpoint['model_state_dict'],\n        'optimizer_state_dict': torch_checkpoint.get('optimizer_state_dict', {}),\n        'epoch': torch_checkpoint.get('epoch', 0),\n        'converted_from_pytorch': True\n    }\n\n    # \u4ee5Genesis\u683c\u5f0f\u4fdd\u5b58\n    genesis.save(genesis_checkpoint, genesis_file)\n    print(f\"\u5df2\u8f6c\u6362 {pytorch_file} -&gt; {genesis_file}\")\n</code></pre> <p>Genesis\u5e8f\u5217\u5316\u7cfb\u7edf\u4e3a\u751f\u4ea7\u7ea7\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u9ad8\u6548\u548c\u5b89\u5168\u7684\u6a21\u578b\u68c0\u67e5\u70b9\u529f\u80fd\u3002</p>"},{"location":"api/serialization_en/","title":"Model Serialization and Checkpointing","text":"<p>Genesis provides model serialization and checkpointing functionality to save and load model states, optimizer states, and training progress.</p>"},{"location":"api/serialization_en/#overview","title":"Overview","text":"<p>The serialization system in Genesis provides: - Model state dictionary saving/loading - Optimizer state saving/loading - Checkpoint management with atomic write operations - Backup file creation for safety</p>"},{"location":"api/serialization_en/#core-functions","title":"Core Functions","text":""},{"location":"api/serialization_en/#save","title":"save()","text":"Python<pre><code>def save(state_dict, file_path):\n    \"\"\"\n    Save state dictionary to file with atomic write operation.\n\n    Args:\n        state_dict (dict): Dictionary containing state to save\n        file_path (str): Path where to save the file\n\n    Features:\n        - Creates backup file (.genesis.bak) before overwriting\n        - Atomic write with automatic cleanup on success\n        - Rollback to backup on failure\n        - Clears state_dict from memory after save\n\n    Implementation:\n        - Uses dill (enhanced pickle) for serialization\n        - Supports lambda functions and complex Python objects\n    \"\"\"\n</code></pre>"},{"location":"api/serialization_en/#load","title":"load()","text":"Python<pre><code>def load(file_path):\n    \"\"\"\n    Load state dictionary from file.\n\n    Args:\n        file_path (str): Path to the saved file\n\n    Returns:\n        dict: Loaded state dictionary\n\n    Raises:\n        FileNotFoundError: If file doesn't exist\n        pickle.UnpicklingError: If file is corrupted\n\n    Implementation:\n        - Uses dill for deserialization\n        - Compatible with files saved by save()\n    \"\"\"\n</code></pre>"},{"location":"api/serialization_en/#save_checkpoint","title":"save_checkpoint()","text":"Python<pre><code>def save_checkpoint(model_state_dict, optimizer_state_dict, file_path):\n    \"\"\"\n    Save model and optimizer checkpoint.\n\n    Args:\n        model_state_dict (dict): Model state dictionary\n        optimizer_state_dict (dict): Optimizer state dictionary  \n        file_path (str): Path where to save checkpoint\n\n    Creates checkpoint containing:\n        - \"model_state_dict\": Model parameters and buffers\n        - \"optimizer_state_dict\": Optimizer state\n\n    Implementation:\n        - Internally calls save() with structured dictionary\n    \"\"\"\n</code></pre>"},{"location":"api/serialization_en/#load_checkpoint","title":"load_checkpoint()","text":"Python<pre><code>def load_checkpoint(file_path):\n    \"\"\"\n    Load model and optimizer checkpoint.\n\n    Args:\n        file_path (str): Path to checkpoint file\n\n    Returns:\n        tuple: (model_state_dict, optimizer_state_dict)\n\n    Implementation:\n        - Loads pickle file and extracts state dictionaries\n        - Returns tuple for convenient unpacking\n\n    Example:\n        &gt;&gt;&gt; model_state, optimizer_state = genesis.load_checkpoint('checkpoint.pth')\n        &gt;&gt;&gt; model.load_state_dict(model_state)\n        &gt;&gt;&gt; optimizer.load_state_dict(optimizer_state)\n    \"\"\"\n</code></pre>"},{"location":"api/serialization_en/#basic-usage","title":"Basic Usage","text":""},{"location":"api/serialization_en/#saving-a-model","title":"Saving a Model","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Create and train model\nmodel = nn.Linear(784, 10)\n\n# Save model state\ngenesis.save(model.state_dict(), 'model.pth')\n\n# Load model state\nstate_dict = genesis.load('model.pth')\nmodel.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/serialization_en/#training-checkpoints","title":"Training Checkpoints","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Setup model and optimizer\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop with checkpointing\nfor epoch in range(100):\n    # Training code...\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n\n    # Save checkpoint every 10 epochs\n    if epoch % 10 == 0:\n        genesis.save_checkpoint(\n            model.state_dict(),\n            optimizer.state_dict(), \n            f'checkpoint_epoch_{epoch}.pth'\n        )\n        print(f\"Checkpoint saved at epoch {epoch}\")\n\n# Load checkpoint to resume training\nmodel_state, optimizer_state = genesis.load_checkpoint('checkpoint_epoch_90.pth')\nmodel.load_state_dict(model_state)\noptimizer.load_state_dict(optimizer_state)\n</code></pre>"},{"location":"api/serialization_en/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/serialization_en/#complete-training-state","title":"Complete Training State","text":"Python<pre><code>import genesis\n\n# Save complete training state\ncheckpoint = {\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    'best_accuracy': best_accuracy,\n    'training_history': training_history\n}\ngenesis.save(checkpoint, 'full_checkpoint.pth')\n\n# Load complete training state\ncheckpoint = genesis.load('full_checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nstart_epoch = checkpoint['epoch'] + 1\nbest_accuracy = checkpoint['best_accuracy']\n</code></pre>"},{"location":"api/serialization_en/#best-model-tracking","title":"Best Model Tracking","text":"Python<pre><code>import genesis\nimport os\n\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    # Training\n    train_loss = train_one_epoch(model, train_loader, optimizer)\n    val_loss = validate(model, val_loader)\n\n    # Save regular checkpoint\n    genesis.save_checkpoint(\n        model.state_dict(),\n        optimizer.state_dict(),\n        f'checkpoint_epoch_{epoch}.pth'\n    )\n\n    # Save best model\n    if val_loss &lt; best_loss:\n        best_loss = val_loss\n        genesis.save(model.state_dict(), 'best_model.pth')\n        print(f\"New best model saved with loss: {val_loss:.4f}\")\n</code></pre>"},{"location":"api/serialization_en/#error-handling","title":"Error Handling","text":"Python<pre><code>import genesis\nimport os\n\ndef safe_load_checkpoint(file_path, model, optimizer=None):\n    \"\"\"Safely load checkpoint with error handling.\"\"\"\n    try:\n        if not os.path.exists(file_path):\n            print(f\"Checkpoint {file_path} not found\")\n            return False\n\n        # Load checkpoint\n        if file_path.endswith('.pth'):\n            # Try loading as checkpoint first\n            try:\n                model_state, optimizer_state = genesis.load_checkpoint(file_path)\n                model.load_state_dict(model_state)\n                if optimizer:\n                    optimizer.load_state_dict(optimizer_state)\n            except:\n                # Fall back to loading as state dict\n                state_dict = genesis.load(file_path)\n                if 'model_state_dict' in state_dict:\n                    model.load_state_dict(state_dict['model_state_dict'])\n                    if optimizer and 'optimizer_state_dict' in state_dict:\n                        optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n                else:\n                    model.load_state_dict(state_dict)\n\n        print(\"Checkpoint loaded successfully\")\n        return True\n\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        return False\n\n# Usage\nsuccess = safe_load_checkpoint('checkpoint.pth', model, optimizer)\n</code></pre>"},{"location":"api/serialization_en/#implementation-details","title":"Implementation Details","text":""},{"location":"api/serialization_en/#atomic-write-operation","title":"Atomic Write Operation","text":"<p>The save function implements atomic writes to prevent data corruption:</p> <ol> <li>Backup Creation: Before overwriting an existing file, creates a <code>.genesis.bak</code> backup</li> <li>Write Operation: Saves new data to the target file</li> <li>Memory Cleanup: Clears the state_dict from memory after successful save</li> <li>Cleanup/Rollback: On success, removes backup; on failure, restores from backup</li> </ol>"},{"location":"api/serialization_en/#serialization-format","title":"Serialization Format","text":"<ul> <li>Uses <code>dill</code> (enhanced pickle) for serialization</li> <li>Supports lambda functions and complex Python objects</li> <li>Binary format for efficient storage</li> <li>Compatible with standard pickle for basic objects</li> </ul>"},{"location":"api/serialization_en/#memory-management","title":"Memory Management","text":"<p>The save function includes automatic memory cleanup: Python<pre><code># After successful save, clear the dictionary to free memory\nfor key in list(state_dict.keys()):\n    del state_dict[key]\n</code></pre></p>"},{"location":"api/serialization_en/#best-practices","title":"Best Practices","text":""},{"location":"api/serialization_en/#1-checkpoint-frequency","title":"1. Checkpoint Frequency","text":"<ul> <li>Save checkpoints regularly (e.g., every epoch or N steps)</li> <li>Keep multiple recent checkpoints</li> <li>Save best model separately</li> </ul>"},{"location":"api/serialization_en/#2-file-organization","title":"2. File Organization","text":"Python<pre><code># Recommended structure\ncheckpoints/\n\u251c\u2500\u2500 checkpoint_epoch_10.pth     # Regular checkpoints\n\u251c\u2500\u2500 checkpoint_epoch_20.pth\n\u251c\u2500\u2500 best_model.pth              # Best performing model\n\u2514\u2500\u2500 final_model.pth             # Final trained model\n</code></pre>"},{"location":"api/serialization_en/#3-checkpoint-naming","title":"3. Checkpoint Naming","text":"Python<pre><code># Include useful information in filename\nfilename = f\"checkpoint_epoch_{epoch}_loss_{loss:.4f}.pth\"\ngenesis.save_checkpoint(model.state_dict(), optimizer.state_dict(), filename)\n</code></pre>"},{"location":"api/serialization_en/#4-resume-training","title":"4. Resume Training","text":"Python<pre><code># Check for existing checkpoint\ncheckpoint_path = 'checkpoint_latest.pth'\nif os.path.exists(checkpoint_path):\n    model_state, optimizer_state = genesis.load_checkpoint(checkpoint_path)\n    model.load_state_dict(model_state)\n    optimizer.load_state_dict(optimizer_state)\n    print(\"Resumed from checkpoint\")\nelse:\n    print(\"Starting fresh training\")\n</code></pre>"},{"location":"api/serialization_en/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ol> <li>File Format: Uses pickle/dill format, not compatible with PyTorch <code>.pt</code> files</li> <li>Memory Usage: The save function clears the input dictionary from memory</li> <li>No Compression: Files are not compressed (consider using external compression if needed)</li> <li>Single Device: No special handling for multi-GPU model states</li> </ol>"},{"location":"api/serialization_en/#migration-notes","title":"Migration Notes","text":""},{"location":"api/serialization_en/#from-pytorch","title":"From PyTorch","text":"<p>Genesis uses a similar API to PyTorch but with some differences: - Uses <code>dill</code> instead of standard <code>pickle</code> for better lambda support - Automatic backup file creation for safety - Memory cleanup after saving</p>"},{"location":"api/serialization_en/#loading-pytorch-models","title":"Loading PyTorch Models","text":"<p>To load PyTorch models in Genesis, you'll need to convert them: Python<pre><code>import torch\nimport genesis\n\n# Load PyTorch model\ntorch_state = torch.load('pytorch_model.pt')\n\n# Convert and save in Genesis format\n# Note: Tensor conversion may be needed depending on backend\ngenesis.save(torch_state, 'genesis_model.pth')\n</code></pre></p>"},{"location":"api/serialization_en/#advanced-checkpointing","title":"Advanced Checkpointing","text":""},{"location":"api/serialization_en/#robust-checkpoint-loading","title":"Robust Checkpoint Loading","text":"<p>Handle loading errors gracefully with validation:</p> Python<pre><code>import genesis\n\ndef safe_load_checkpoint(file_path, model, optimizer=None):\n    \"\"\"Load checkpoint with comprehensive error handling.\"\"\"\n    try:\n        checkpoint = genesis.load(file_path)\n\n        # Load model state\n        if 'model_state_dict' in checkpoint:\n            model.load_state_dict(checkpoint['model_state_dict'])\n            print(\"Model state loaded successfully\")\n        else:\n            print(\"Warning: No model state found in checkpoint\")\n            return False\n\n        # Load optimizer state (optional)\n        if optimizer and 'optimizer_state_dict' in checkpoint:\n            try:\n                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                print(\"Optimizer state loaded successfully\")\n            except Exception as e:\n                print(f\"Warning: Could not load optimizer state: {e}\")\n\n        # Return additional information\n        epoch = checkpoint.get('epoch', 0)\n        loss = checkpoint.get('loss', 'unknown')\n        print(f\"Loaded checkpoint from epoch {epoch}, loss: {loss}\")\n\n        return True\n\n    except Exception as e:\n        print(f\"Error loading checkpoint: {e}\")\n        return False\n\n# Usage\nsuccess = safe_load_checkpoint('checkpoint.pth', model, optimizer)\nif success:\n    print(\"Checkpoint loaded successfully\")\nelse:\n    print(\"Failed to load checkpoint, starting from scratch\")\n</code></pre>"},{"location":"api/serialization_en/#checkpoint-validation","title":"Checkpoint Validation","text":"Python<pre><code>import genesis\n\ndef validate_checkpoint(file_path):\n    \"\"\"Validate checkpoint file integrity.\"\"\"\n    try:\n        checkpoint = genesis.load(file_path)\n\n        # Basic structure validation\n        if not isinstance(checkpoint, dict):\n            return False, \"Checkpoint is not a dictionary\"\n\n        if 'model_state_dict' not in checkpoint:\n            return False, \"Missing model_state_dict\"\n\n        # Check model state structure\n        model_state = checkpoint['model_state_dict']\n        if not isinstance(model_state, dict):\n            return False, \"model_state_dict is not a dictionary\"\n\n        # Check for empty state\n        if len(model_state) == 0:\n            return False, \"model_state_dict is empty\"\n\n        # Validate tensor shapes (basic check)\n        for key, tensor in model_state.items():\n            if not hasattr(tensor, 'shape'):\n                return False, f\"Invalid tensor for key '{key}'\"\n\n        return True, \"Checkpoint is valid\"\n\n    except Exception as e:\n        return False, f\"Error validating checkpoint: {e}\"\n\n# Usage\nis_valid, message = validate_checkpoint('checkpoint.pth')\nprint(f\"Checkpoint validation: {message}\")\n</code></pre>"},{"location":"api/serialization_en/#best-practices_1","title":"Best Practices","text":""},{"location":"api/serialization_en/#1-checkpoint-strategy","title":"1. Checkpoint Strategy","text":"<ul> <li>Save checkpoints regularly (every N epochs)</li> <li>Keep multiple recent checkpoints</li> <li>Save best model separately</li> <li>Include training metadata</li> </ul>"},{"location":"api/serialization_en/#2-file-organization_1","title":"2. File Organization","text":"Python<pre><code># Recommended directory structure\ncheckpoints/\n\u251c\u2500\u2500 latest.pth                    # Latest checkpoint\n\u251c\u2500\u2500 best_model.pth               # Best performance model\n\u251c\u2500\u2500 epoch_000010.pth            # Regular checkpoints\n\u251c\u2500\u2500 epoch_000020.pth\n\u2514\u2500\u2500 deployed/\n    \u2514\u2500\u2500 production_model.pth     # Production-ready model\n</code></pre>"},{"location":"api/serialization_en/#3-memory-management","title":"3. Memory Management","text":"Python<pre><code>import genesis\nimport gc\n\ndef efficient_checkpoint_save(model, optimizer, file_path):\n    \"\"\"Memory-optimized checkpoint saving.\"\"\"\n    # Create checkpoint dictionary\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict()\n    }\n\n    # Save checkpoint\n    genesis.save(checkpoint, file_path)\n\n    # Clear checkpoint dictionary from memory\n    del checkpoint\n    gc.collect()\n</code></pre>"},{"location":"api/serialization_en/#4-version-management","title":"4. Version Management","text":"Python<pre><code>class CheckpointManager:\n    \"\"\"Manage checkpoint versions and cleanup.\"\"\"\n\n    def __init__(self, checkpoint_dir, max_checkpoints=5):\n        self.checkpoint_dir = checkpoint_dir\n        self.max_checkpoints = max_checkpoints\n\n    def save_checkpoint(self, model, optimizer, epoch, loss):\n        \"\"\"Save checkpoint with automatic cleanup.\"\"\"\n        file_path = f\"{self.checkpoint_dir}/epoch_{epoch:06d}.pth\"\n\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            'timestamp': time.time()\n        }\n\n        genesis.save(checkpoint, file_path)\n        self._cleanup_old_checkpoints()\n\n    def _cleanup_old_checkpoints(self):\n        \"\"\"Remove old checkpoints keeping only the most recent.\"\"\"\n        import os\n        import glob\n\n        checkpoints = glob.glob(f\"{self.checkpoint_dir}/epoch_*.pth\")\n        checkpoints.sort()\n\n        while len(checkpoints) &gt; self.max_checkpoints:\n            os.remove(checkpoints.pop(0))\n\n# Usage\nmanager = CheckpointManager('checkpoints/', max_checkpoints=3)\nmanager.save_checkpoint(model, optimizer, epoch, loss)\n</code></pre>"},{"location":"api/serialization_en/#see-also","title":"See Also","text":"<ul> <li>Optimizers - Optimizer state management</li> <li>Neural Network Modules - Model state_dict methods</li> <li>Training Examples - Complete training scripts with checkpointing</li> </ul>"},{"location":"api/ndarray/index.zh/","title":"NDArray\u7cfb\u7edf (genesis.ndarray)","text":""},{"location":"api/ndarray/index.zh/#_1","title":"\u6982\u8ff0","text":"<p><code>genesis.ndarray</code>\u6a21\u5757\u63d0\u4f9b\u4f4e\u7ea7\u5f20\u91cf\u64cd\u4f5c\u548c\u8bbe\u5907\u62bd\u8c61\u5c42\uff0c\u4e3aGenesis\u63d0\u4f9b\u652f\u6301\u3002\u5b83\u5b9e\u73b0\u4e86\u53cc\u540e\u7aef\u67b6\u6784\uff0c\u4e3aCPU\u548cGPU\u6267\u884c\u63d0\u4f9b\u4e86\u4f18\u5316\u64cd\u4f5c\u3002</p>"},{"location":"api/ndarray/index.zh/#_2","title":"\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"api/ndarray/index.zh/#_3","title":"\u53cc\u540e\u7aef\u67b6\u6784","text":"<p>Genesis\u4f7f\u7528\u72ec\u7279\u7684\u53cc\u540e\u7aef\u65b9\u6cd5\uff1a - CPU\u540e\u7aef: \u5229\u7528PyTorch\u8fdb\u884cCPU\u64cd\u4f5c\uff0c\u5177\u6709\u5b8c\u5168\u517c\u5bb9\u6027 - GPU\u540e\u7aef: \u7eafCUDA/Triton\u5b9e\u73b0\uff0c\u5b9e\u73b0\u6700\u5927\u6027\u80fd\u63a7\u5236</p>"},{"location":"api/ndarray/index.zh/#_4","title":"\u8bbe\u5907\u62bd\u8c61","text":"<p>\u6240\u6709\u8ba1\u7b97\u901a\u8fc7<code>Device</code>\u62bd\u8c61\u5b9e\u73b0\u8bbe\u5907\u65e0\u5173\uff1a - \u81ea\u52a8\u8bbe\u5907\u9009\u62e9\u548c\u5185\u5b58\u7ba1\u7406 - CPU\u548cGPU\u6267\u884c\u4e4b\u95f4\u7684\u65e0\u7f1d\u5207\u6362 - \u4f18\u5316\u7684\u5185\u5b58\u5206\u914d\u6a21\u5f0f</p>"},{"location":"api/ndarray/index.zh/#_5","title":"\u6027\u80fd\u4f18\u5316","text":"<p>ndarray\u7cfb\u7edf\u5305\u62ec\u51e0\u4e2a\u6027\u80fd\u4f18\u5316\uff1a - \u5185\u6838\u7f13\u5b58: \u7f16\u8bd1\u7684Triton\u5185\u6838\u88ab\u7f13\u5b58\u4ee5\u4f9b\u91cd\u7528 - \u81ea\u9002\u5e94\u914d\u7f6e: \u5757\u5927\u5c0f\u6839\u636e\u5f20\u91cf\u7ef4\u5ea6\u81ea\u52a8\u8c03\u6574 - \u5185\u5b58\u89c6\u56fe: \u65e0\u9700\u6570\u636e\u590d\u5236\u7684\u9ad8\u6548\u5f20\u91cf\u89c6\u56fe - \u5e7f\u64ad\u4f18\u5316: \u5143\u7d20\u7ea7\u64cd\u4f5c\u7684\u667a\u80fd\u5e7f\u64ad</p>"},{"location":"api/ndarray/index.zh/#_6","title":"\u4e3b\u8981\u7c7b","text":""},{"location":"api/ndarray/index.zh/#ndarray","title":"<code>NDArray</code>","text":"<p>Genesis\u4e2d\u7684\u57fa\u7840\u6570\u7ec4\u7c7b\u578b\uff0c\u63d0\u4f9b\u8bbe\u5907\u65e0\u5173\u7684\u5f20\u91cf\u64cd\u4f5c\u3002</p> Python<pre><code>class NDArray:\n    \"\"\"\n    \u652f\u6301\u8bbe\u5907\u7684N\u7ef4\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        data: \u8f93\u5165\u6570\u636e\uff08numpy\u6570\u7ec4\u3001\u5217\u8868\u6216\u5f20\u91cf\uff09\n        device: \u76ee\u6807\u8bbe\u5907\uff08cpu\u6216cuda\uff09\n        dtype: \u6570\u7ec4\u7684\u6570\u636e\u7c7b\u578b\n\n    \u5c5e\u6027:\n        shape: \u6570\u7ec4\u7ef4\u5ea6\u7684\u5143\u7ec4\n        dtype: \u5143\u7d20\u7684\u6570\u636e\u7c7b\u578b\n        device: \u6570\u7ec4\u5b58\u50a8\u7684\u8bbe\u5907\n        data: \u5e95\u5c42\u5f20\u91cf\u6570\u636e\n    \"\"\"\n\n    def __init__(\n        self, \n        data, \n        device: Optional[Device] = None, \n        dtype: Optional[DType] = None\n    ):\n</code></pre>"},{"location":"api/ndarray/index.zh/#_7","title":"\u521b\u5efa\u65b9\u6cd5","text":"Python<pre><code>@staticmethod\ndef make(\n    shape: Tuple[int, ...], \n    device: Optional[Device] = None, \n    dtype: DType = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u521b\u5efa\u6307\u5b9a\u5f62\u72b6\u7684\u672a\u521d\u59cb\u5316\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        shape: \u6570\u7ec4\u7684\u7ef4\u5ea6\n        device: \u76ee\u6807\u8bbe\u5907\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u65b0\u7684NDArray\u5b9e\u4f8b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; arr = NDArray.make((10, 20), device=genesis.cuda(), dtype=genesis.float32)\n        &gt;&gt;&gt; print(arr.shape)  # (10, 20)\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index.zh/#_8","title":"\u5c5e\u6027\u548c\u65b9\u6cd5","text":"Python<pre><code>@property\ndef shape(self) -&gt; Tuple[int, ...]:\n    \"\"\"\u6570\u7ec4\u7ef4\u5ea6\u3002\"\"\"\n\n@property\ndef dtype(self) -&gt; DType:\n    \"\"\"\u5143\u7d20\u6570\u636e\u7c7b\u578b\u3002\"\"\"\n\n@property\ndef device(self) -&gt; Device:\n    \"\"\"\u6570\u7ec4\u5b58\u50a8\u7684\u8bbe\u5907\u3002\"\"\"\n\ndef numel(self) -&gt; int:\n    \"\"\"\n    \u5143\u7d20\u603b\u6570\u3002\n\n    \u8fd4\u56de:\n        \u6240\u6709\u7ef4\u5ea6\u7684\u4e58\u79ef\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; arr = NDArray.make((3, 4, 5))\n        &gt;&gt;&gt; print(arr.numel())  # 60\n    \"\"\"\n\ndef is_contiguous(self) -&gt; bool:\n    \"\"\"\n    \u68c0\u67e5\u6570\u7ec4\u662f\u5426\u5177\u6709\u8fde\u7eed\u7684\u5185\u5b58\u5e03\u5c40\u3002\n\n    \u8fd4\u56de:\n        \u5982\u679c\u5185\u5b58\u8fde\u7eed\u5219\u4e3aTrue\n    \"\"\"\n\ndef fill(self, value: float) -&gt; None:\n    \"\"\"\n    \u7528\u5e38\u6570\u503c\u539f\u5730\u586b\u5145\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        value: \u586b\u5145\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; arr = NDArray.make((5, 5))\n        &gt;&gt;&gt; arr.fill(0.0)\n        &gt;&gt;&gt; # \u6570\u7ec4\u73b0\u5728\u5305\u542b\u5168\u96f6\n    \"\"\"\n\ndef numpy(self) -&gt; np.ndarray:\n    \"\"\"\n    \u8f6c\u6362\u4e3aNumPy\u6570\u7ec4\u3002\n\n    \u8fd4\u56de:\n        \u590d\u5236\u6570\u636e\u7684NumPy\u6570\u7ec4\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; arr = NDArray([1, 2, 3], device=genesis.cuda())\n        &gt;&gt;&gt; np_arr = arr.numpy()  # \u4eceGPU\u590d\u5236\u5230CPU\n    \"\"\"\n\ndef cpu(self):\n    \"\"\"\n    \u5c06\u6570\u7ec4\u4f20\u8f93\u5230CPU\u3002\n\n    \u8fd4\u56de:\n        \u6570\u7ec4\u6570\u636e\u7684CPU\u7248\u672c\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index.zh/#device","title":"<code>Device</code>","text":"<p>\u652f\u6301CPU\u548cCUDA\u6267\u884c\u7684\u62bd\u8c61\u8bbe\u5907\u63a5\u53e3\u3002</p> Python<pre><code>class Device:\n    \"\"\"\n    \u8ba1\u7b97\u540e\u7aef\u7684\u8bbe\u5907\u62bd\u8c61\u3002\n\n    \u53c2\u6570:\n        name: \u8bbe\u5907\u540d\u79f0\uff08'cpu'\u6216'cuda'\uff09\n        mod: \u64cd\u4f5c\u7684\u540e\u7aef\u6a21\u5757\n        device_id: GPU\u8bbe\u5907\u7d22\u5f15\uff08\u7528\u4e8eCUDA\u8bbe\u5907\uff09\n    \"\"\"\n\n    def __init__(\n        self, \n        name: str, \n        mod: Any, \n        device_id: Optional[int] = None\n    ):\n\n    def enabled(self) -&gt; bool:\n        \"\"\"\n        \u68c0\u67e5\u8bbe\u5907\u662f\u5426\u53ef\u7528\u3002\n\n        \u8fd4\u56de:\n            \u5982\u679c\u8bbe\u5907\u53ef\u4ee5\u4f7f\u7528\u5219\u4e3aTrue\n        \"\"\"\n</code></pre>"},{"location":"api/ndarray/index.zh/#_9","title":"\u5f20\u91cf\u521b\u5efa","text":"Python<pre><code>def randn(\n    self, \n    *shape: int, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u4ece\u6b63\u6001\u5206\u5e03\u521b\u5efa\u968f\u673a\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        *shape: \u6570\u7ec4\u7ef4\u5ea6\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u5177\u6709\u968f\u673a\u503c\u7684NDArray\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; device = genesis.cuda()\n        &gt;&gt;&gt; arr = device.randn(10, 10)  # 10x10\u968f\u673a\u6570\u7ec4\n    \"\"\"\n\ndef rand(\n    self, \n    *shape: int, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u4ece\u5747\u5300\u5206\u5e03[0, 1)\u521b\u5efa\u968f\u673a\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        *shape: \u6570\u7ec4\u7ef4\u5ea6\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u5177\u6709\u5747\u5300\u968f\u673a\u503c\u7684NDArray\n    \"\"\"\n\ndef empty(\n    self, \n    shape: Tuple[int, ...], \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u521b\u5efa\u672a\u521d\u59cb\u5316\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        shape: \u6570\u7ec4\u7ef4\u5ea6\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u672a\u521d\u59cb\u5316\u7684NDArray\n    \"\"\"\n\ndef full(\n    self, \n    shape: Tuple[int, ...], \n    fill_value: float, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u521b\u5efa\u7528\u6307\u5b9a\u503c\u586b\u5145\u7684\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        shape: \u6570\u7ec4\u7ef4\u5ea6\n        fill_value: \u586b\u5145\u6570\u7ec4\u7684\u503c\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u7528fill_value\u586b\u5145\u7684NDArray\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; device = genesis.cpu()\n        &gt;&gt;&gt; ones = device.full((5, 5), 1.0)  # 5x5\u51681\u6570\u7ec4\n    \"\"\"\n\ndef one_hot(\n    self, \n    n: int, \n    i: NDArray, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    \u521b\u5efa\u72ec\u70ed\u7f16\u7801\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        n: \u7c7b\u522b\u6570\u91cf\n        i: \u7d22\u5f15\u6570\u7ec4\n        dtype: \u5143\u7d20\u6570\u636e\u7c7b\u578b\n\n    \u8fd4\u56de:\n        \u72ec\u70ed\u7f16\u7801\u7684NDArray\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; device = genesis.cpu()\n        &gt;&gt;&gt; indices = NDArray([0, 2, 1], device=device)\n        &gt;&gt;&gt; one_hot = device.one_hot(3, indices)\n        &gt;&gt;&gt; # \u5f62\u72b6: (3, 3) \u72ec\u70ed\u7f16\u7801\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index.zh/#_10","title":"\u8bbe\u5907\u51fd\u6570","text":""},{"location":"api/ndarray/index.zh/#_11","title":"\u8bbe\u5907\u521b\u5efa","text":"Python<pre><code>def cpu() -&gt; Device:\n    \"\"\"\n    \u521b\u5efaCPU\u8bbe\u5907\u3002\n\n    \u8fd4\u56de:\n        CPU\u8bbe\u5907\u5b9e\u4f8b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; cpu_device = genesis.cpu()\n        &gt;&gt;&gt; arr = NDArray([1, 2, 3], device=cpu_device)\n    \"\"\"\n\ndef cuda(index: int = 0) -&gt; Device:\n    \"\"\"\n    \u521b\u5efaCUDA\u8bbe\u5907\u3002\n\n    \u53c2\u6570:\n        index: GPU\u8bbe\u5907\u7d22\u5f15\n\n    \u8fd4\u56de:\n        CUDA\u8bbe\u5907\u5b9e\u4f8b\uff0c\u5982\u679cCUDA\u4e0d\u53ef\u7528\u5219\u4e3aNone\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; gpu_device = genesis.cuda(0)  # \u7b2c\u4e00\u4e2aGPU\n        &gt;&gt;&gt; if gpu_device.enabled():\n        ...     arr = NDArray([1, 2, 3], device=gpu_device)\n    \"\"\"\n\ndef device(device_name: Union[str, int]) -&gt; Device:\n    \"\"\"\n    \u901a\u8fc7\u540d\u79f0\u6216\u7d22\u5f15\u521b\u5efa\u8bbe\u5907\u3002\n\n    \u53c2\u6570:\n        device_name: 'cpu', 'cuda', 'cuda:N', \u6216GPU\u7d22\u5f15\n\n    \u8fd4\u56de:\n        \u8bbe\u5907\u5b9e\u4f8b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; dev1 = genesis.device('cuda:1')  # \u7b2c\u4e8c\u4e2aGPU\n        &gt;&gt;&gt; dev2 = genesis.device(1)         # \u540c\u4e0a\n        &gt;&gt;&gt; dev3 = genesis.device('cpu')     # CPU\u8bbe\u5907\n    \"\"\"\n\ndef default_device() -&gt; Device:\n    \"\"\"\n    \u83b7\u53d6\u9ed8\u8ba4\u8bbe\u5907\uff08CPU\uff09\u3002\n\n    \u8fd4\u56de:\n        \u9ed8\u8ba4\u8bbe\u5907\u5b9e\u4f8b\n    \"\"\"\n\ndef all_devices() -&gt; List[Device]:\n    \"\"\"\n    \u83b7\u53d6\u6240\u6709\u53ef\u7528\u8bbe\u5907\u7684\u5217\u8868\u3002\n\n    \u8fd4\u56de:\n        \u8bbe\u5907\u5b9e\u4f8b\u5217\u8868\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index.zh/#_12","title":"\u64cd\u4f5c","text":"<p>ndarray\u7cfb\u7edf\u901a\u8fc7\u540e\u7aef\u6a21\u5757\u652f\u6301\u4e00\u5957\u5b8c\u6574\u7684\u64cd\u4f5c\u3002</p>"},{"location":"api/ndarray/index.zh/#_13","title":"\u7b97\u672f\u64cd\u4f5c","text":"Python<pre><code># \u4e8c\u5143\u64cd\u4f5c\nadd(x, y)           # \u5143\u7d20\u7ea7\u52a0\u6cd5\nsub(x, y)           # \u5143\u7d20\u7ea7\u51cf\u6cd5  \nmul(x, y)           # \u5143\u7d20\u7ea7\u4e58\u6cd5\ntruediv(x, y)       # \u5143\u7d20\u7ea7\u9664\u6cd5\npow(x, scalar)      # \u5143\u7d20\u7ea7\u5e42\u8fd0\u7b97\n\n# \u4e00\u5143\u64cd\u4f5c\nlog(x)              # \u81ea\u7136\u5bf9\u6570\nexp(x)              # \u6307\u6570\nsin(x)              # \u6b63\u5f26\ncos(x)              # \u4f59\u5f26\nsqrt(x)             # \u5e73\u65b9\u6839\n</code></pre>"},{"location":"api/ndarray/index.zh/#_14","title":"\u5f52\u7ea6\u64cd\u4f5c","text":"Python<pre><code>reduce_sum(x, axis=None, keepdims=False)    # \u6c42\u548c\u5f52\u7ea6\nreduce_max(x, axis=None, keepdims=False)    # \u6700\u5927\u503c\u5f52\u7ea6\nreduce_min(x, axis=None, keepdims=False)    # \u6700\u5c0f\u503c\u5f52\u7ea6\n</code></pre>"},{"location":"api/ndarray/index.zh/#_15","title":"\u6bd4\u8f83\u64cd\u4f5c","text":"Python<pre><code>maximum(x, y)       # \u5143\u7d20\u7ea7\u6700\u5927\u503c\nminimum(x, y)       # \u5143\u7d20\u7ea7\u6700\u5c0f\u503c\n</code></pre>"},{"location":"api/ndarray/index.zh/#_16","title":"\u77e9\u9635\u64cd\u4f5c","text":"Python<pre><code>matmul(x, y)        # \u77e9\u9635\u4e58\u6cd5\ntranspose(x, axes)  # \u5f20\u91cf\u8f6c\u7f6e\n</code></pre>"},{"location":"api/ndarray/index.zh/#_17","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"api/ndarray/index.zh/#_18","title":"\u5185\u6838\u7f13\u5b58","text":"<p>Triton\u5185\u6838\u81ea\u52a8\u7f13\u5b58\u4ee5\u4f9b\u91cd\u7528\uff1a</p> Python<pre><code>from genesis.ndarray.kernel_cache import cached_kernel_call\n\n# \u5185\u6838\u6309\u51fd\u6570\u7b7e\u540d\u548c\u53c2\u6570\u7f13\u5b58\ncached_kernel_call(kernel_func, grid_func, *args, **kwargs)\n</code></pre>"},{"location":"api/ndarray/index.zh/#_19","title":"\u81ea\u9002\u5e94\u914d\u7f6e","text":"<p>\u5757\u5927\u5c0f\u81ea\u52a8\u9002\u5e94\u5f20\u91cf\u7ef4\u5ea6\uff1a</p> Python<pre><code>from genesis.ndarray.adaptive_config import AdaptiveConfig\n\n# \u4e3a\u5f20\u91cf\u5f62\u72b6\u83b7\u53d6\u4f18\u5316\u914d\u7f6e\nconfig = AdaptiveConfig.get_elementwise_config(shape)\nblock_size = config['BLOCK_SIZE']\ngrid = config['grid']\n</code></pre>"},{"location":"api/ndarray/index.zh/#_20","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>\u9ad8\u6548\u7684\u5185\u5b58\u5206\u914d\u6a21\u5f0f\uff1a - \u5c3d\u53ef\u80fd\u8fde\u7eed\u7684\u5185\u5b58\u5e03\u5c40 - \u57fa\u4e8e\u89c6\u56fe\u7684\u64cd\u4f5c\u907f\u514d\u590d\u5236 - \u81ea\u52a8\u5185\u5b58\u6e05\u7406</p>"},{"location":"api/ndarray/index.zh/#gpucudatriton","title":"GPU\u540e\u7aef\uff08CUDA/Triton\uff09","text":""},{"location":"api/ndarray/index.zh/#gpu","title":"GPU\u64cd\u4f5c","text":"<p>\u7eafCUDA/Triton\u5b9e\u73b0\uff0c\u9488\u5bf9GPU\u64cd\u4f5c\u8fdb\u884c\u6027\u80fd\u4f18\u5316\u3002</p>"},{"location":"api/ndarray/index.zh/#triton","title":"Triton\u5185\u6838","text":"<p>\u4e3a\u6700\u5927\u6027\u80fd\u624b\u5de5\u4f18\u5316\u7684Triton\u5185\u6838\uff1a - \u5177\u6709\u5e7f\u64ad\u7684\u5143\u7d20\u7ea7\u64cd\u4f5c - \u5177\u6709\u5de5\u4f5c\u9ad8\u6548\u7b97\u6cd5\u7684\u5f52\u7ea6\u64cd\u4f5c - \u5177\u6709\u5206\u5757\u7684\u77e9\u9635\u4e58\u6cd5 - \u5185\u5b58\u4f18\u5316\u8bbf\u95ee\u6a21\u5f0f</p>"},{"location":"api/ndarray/index.zh/#_21","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"api/ndarray/index.zh/#_22","title":"\u57fa\u672c\u6570\u7ec4\u521b\u5efa","text":"Python<pre><code>import genesis\n\n# \u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u521b\u5efa\u6570\u7ec4\ncpu_arr = genesis.NDArray([1, 2, 3, 4], device=genesis.cpu())\ngpu_arr = genesis.NDArray([1, 2, 3, 4], device=genesis.cuda())\n\n# \u521b\u5efa\u7279\u5b9a\u5f62\u72b6\nzeros = genesis.NDArray.make((100, 100), device=genesis.cuda())\nzeros.fill(0.0)\n</code></pre>"},{"location":"api/ndarray/index.zh/#_23","title":"\u8bbe\u5907\u64cd\u4f5c","text":"Python<pre><code># \u968f\u673a\u6570\u7ec4\ndevice = genesis.cuda(0)\nrandom_normal = device.randn(1000, 1000)\nrandom_uniform = device.rand(1000, 1000)\n\n# \u72ec\u70ed\u7f16\u7801\nindices = genesis.NDArray([0, 2, 1, 3], device=device)\none_hot = device.one_hot(4, indices)\n</code></pre>"},{"location":"api/ndarray/index.zh/#_24","title":"\u5185\u5b58\u4f20\u8f93","text":"Python<pre><code># GPU\u5230CPU\u4f20\u8f93\ngpu_data = genesis.NDArray([1, 2, 3], device=genesis.cuda())\ncpu_data = gpu_data.cpu()\nnumpy_data = gpu_data.numpy()\n\n# CPU\u5230GPU\u4f20\u8f93  \ncpu_data = genesis.NDArray([1, 2, 3], device=genesis.cpu())\ngpu_data = genesis.NDArray(cpu_data, device=genesis.cuda())\n</code></pre>"},{"location":"api/ndarray/index.zh/#_25","title":"\u6027\u80fd\u76d1\u63a7","text":"Python<pre><code>import time\n\n# \u8ba1\u65f6\u64cd\u4f5c\nstart = time.time()\nresult = genesis.ndarray.add(x, y)\nend = time.time()\nprint(f\"\u64cd\u4f5c\u7528\u65f6 {(end - start) * 1000:.2f}ms\")\n</code></pre>"},{"location":"api/ndarray/index.zh/#_26","title":"\u540e\u7aef\u9009\u62e9","text":"<p>Genesis\u81ea\u52a8\u9009\u62e9\u5408\u9002\u7684\u540e\u7aef\uff1a</p> Python<pre><code># CPU\u64cd\u4f5c\u4f7f\u7528PyTorch\u540e\u7aef\ncpu_device = genesis.cpu()\nx = genesis.NDArray([1, 2, 3], device=cpu_device)\n\n# GPU\u64cd\u4f5c\u4f7f\u7528Triton/CUDA\u540e\u7aef\ngpu_device = genesis.cuda()\nif gpu_device.enabled():\n    x = genesis.NDArray([1, 2, 3], device=gpu_device)\n    # \u4f7f\u7528\u4f18\u5316\u7684Triton\u5185\u6838\n</code></pre>"},{"location":"api/ndarray/index.zh/#_27","title":"\u9519\u8bef\u5904\u7406","text":"<p>ndarray\u7cfb\u7edf\u63d0\u4f9b\u5168\u9762\u7684\u9519\u8bef\u5904\u7406\uff1a</p> Python<pre><code>try:\n    # \u5c1d\u8bd5GPU\u64cd\u4f5c\n    gpu_device = genesis.cuda()\n    if not gpu_device.enabled():\n        raise RuntimeError(\"CUDA\u4e0d\u53ef\u7528\")\n\n    arr = genesis.NDArray([1, 2, 3], device=gpu_device)\nexcept RuntimeError as e:\n    # \u56de\u9000\u5230CPU\n    print(f\"GPU\u9519\u8bef: {e}\uff0c\u4f7f\u7528CPU\")\n    cpu_device = genesis.cpu()\n    arr = genesis.NDArray([1, 2, 3], device=cpu_device)\n</code></pre>"},{"location":"api/ndarray/index.zh/#_28","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u8bbe\u5907\u9009\u62e9: \u4f7f\u7528\u524d\u68c0\u67e5\u8bbe\u5907\u53ef\u7528\u6027</li> <li>\u5185\u5b58\u7ba1\u7406: \u8c28\u614e\u5730\u5728\u8bbe\u5907\u95f4\u4f20\u8f93  </li> <li>\u6279\u91cf\u64cd\u4f5c: \u5c3d\u53ef\u80fd\u540c\u65f6\u5904\u7406\u591a\u4e2a\u5f20\u91cf</li> <li>\u8fde\u7eed\u5185\u5b58: \u786e\u4fdd\u6570\u7ec4\u8fde\u7eed\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd</li> <li>\u9519\u8bef\u5904\u7406: \u59cb\u7ec8\u4f18\u96c5\u5730\u5904\u7406CUDA\u53ef\u7528\u6027</li> </ol>"},{"location":"api/ndarray/index.zh/#_29","title":"\u6027\u80fd\u63d0\u793a","text":"<ol> <li>\u4f7f\u7528\u9002\u5f53\u7684\u5757\u5927\u5c0f \u7528\u4e8eGPU\u5185\u6838</li> <li>\u6700\u5c0f\u5316\u8bbe\u5907\u4f20\u8f93 \u5728CPU\u548cGPU\u4e4b\u95f4</li> <li>\u5229\u7528\u5185\u6838\u7f13\u5b58 \u901a\u8fc7\u91cd\u7528\u76f8\u4f3c\u64cd\u4f5c</li> <li>\u4f7f\u7528\u89c6\u56fe\u800c\u4e0d\u662f\u526f\u672c \u5c3d\u53ef\u80fd</li> <li>\u6279\u91cf\u76f8\u4f3c\u64cd\u4f5c \u644a\u9500\u5185\u6838\u542f\u52a8\u5f00\u9500</li> </ol>"},{"location":"api/ndarray/index.zh/#_30","title":"\u53e6\u8bf7\u53c2\u9605","text":"<ul> <li>\u5f20\u91cf\u64cd\u4f5c - \u9ad8\u7ea7\u5f20\u91cf\u63a5\u53e3</li> <li>\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 - \u5728ndarray\u57fa\u7840\u4e0a\u6784\u5efa</li> <li>\u6027\u80fd\u6307\u5357 - \u4f18\u5316\u6280\u672f</li> <li>CUDA\u5b58\u50a8 - \u4f4e\u7ea7CUDA\u5b9e\u73b0</li> </ul>"},{"location":"api/ndarray/index_en/","title":"NDArray System (genesis.ndarray)","text":""},{"location":"api/ndarray/index_en/#overview","title":"Overview","text":"<p>The <code>genesis.ndarray</code> module provides the low-level tensor operations and device abstraction layer that powers Genesis. It implements a dual-backend architecture with optimized operations for both CPU and GPU execution.</p>"},{"location":"api/ndarray/index_en/#core-concepts","title":"Core Concepts","text":""},{"location":"api/ndarray/index_en/#dual-backend-architecture","title":"Dual Backend Architecture","text":"<p>Genesis uses a unique dual-backend approach: - CPU Backend: Leverages PyTorch for CPU operations with full compatibility - GPU Backend: Pure CUDA/Triton implementation for maximum performance control</p>"},{"location":"api/ndarray/index_en/#device-abstraction","title":"Device Abstraction","text":"<p>All computations are device-agnostic through the <code>Device</code> abstraction: - Automatic device selection and memory management - Seamless switching between CPU and GPU execution - Optimized memory allocation patterns</p>"},{"location":"api/ndarray/index_en/#performance-optimization","title":"Performance Optimization","text":"<p>The ndarray system includes several performance optimizations: - Kernel Caching: Compiled Triton kernels are cached for reuse - Adaptive Configuration: Block sizes auto-tune based on tensor dimensions - Memory Views: Efficient tensor views without data copying - Broadcast Optimization: Smart broadcasting for elementwise operations</p>"},{"location":"api/ndarray/index_en/#main-classes","title":"Main Classes","text":""},{"location":"api/ndarray/index_en/#ndarray","title":"<code>NDArray</code>","text":"<p>The fundamental array type in Genesis, providing device-agnostic tensor operations.</p> Python<pre><code>class NDArray:\n    \"\"\"\n    N-dimensional array with device support.\n\n    Args:\n        data: Input data (numpy array, list, or tensor)\n        device: Target device (cpu or cuda)\n        dtype: Data type for the array\n\n    Properties:\n        shape: Tuple of array dimensions\n        dtype: Data type of elements\n        device: Device where array is stored\n        data: Underlying tensor data\n    \"\"\"\n\n    def __init__(\n        self, \n        data, \n        device: Optional[Device] = None, \n        dtype: Optional[DType] = None\n    ):\n</code></pre>"},{"location":"api/ndarray/index_en/#creation-methods","title":"Creation Methods","text":"Python<pre><code>@staticmethod\ndef make(\n    shape: Tuple[int, ...], \n    device: Optional[Device] = None, \n    dtype: DType = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create uninitialized array with specified shape.\n\n    Args:\n        shape: Dimensions of the array\n        device: Target device\n        dtype: Element data type\n\n    Returns:\n        New NDArray instance\n\n    Example:\n        &gt;&gt;&gt; arr = NDArray.make((10, 20), device=genesis.cuda(), dtype=genesis.float32)\n        &gt;&gt;&gt; print(arr.shape)  # (10, 20)\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index_en/#properties-and-methods","title":"Properties and Methods","text":"Python<pre><code>@property\ndef shape(self) -&gt; Tuple[int, ...]:\n    \"\"\"Array dimensions.\"\"\"\n\n@property\ndef dtype(self) -&gt; DType:\n    \"\"\"Element data type.\"\"\"\n\n@property\ndef device(self) -&gt; Device:\n    \"\"\"Device where array is stored.\"\"\"\n\ndef numel(self) -&gt; int:\n    \"\"\"\n    Total number of elements.\n\n    Returns:\n        Product of all dimensions\n\n    Example:\n        &gt;&gt;&gt; arr = NDArray.make((3, 4, 5))\n        &gt;&gt;&gt; print(arr.numel())  # 60\n    \"\"\"\n\ndef is_contiguous(self) -&gt; bool:\n    \"\"\"\n    Check if array has contiguous memory layout.\n\n    Returns:\n        True if memory is contiguous\n    \"\"\"\n\ndef fill(self, value: float) -&gt; None:\n    \"\"\"\n    Fill array with constant value in-place.\n\n    Args:\n        value: Fill value\n\n    Example:\n        &gt;&gt;&gt; arr = NDArray.make((5, 5))\n        &gt;&gt;&gt; arr.fill(0.0)\n        &gt;&gt;&gt; # Array now contains all zeros\n    \"\"\"\n\ndef numpy(self) -&gt; np.ndarray:\n    \"\"\"\n    Convert to NumPy array.\n\n    Returns:\n        NumPy array with copied data\n\n    Example:\n        &gt;&gt;&gt; arr = NDArray([1, 2, 3], device=genesis.cuda())\n        &gt;&gt;&gt; np_arr = arr.numpy()  # Copies from GPU to CPU\n    \"\"\"\n\ndef cpu(self):\n    \"\"\"\n    Transfer array to CPU.\n\n    Returns:\n        CPU version of the array data\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index_en/#device","title":"<code>Device</code>","text":"<p>Abstract device interface supporting CPU and CUDA execution.</p> Python<pre><code>class Device:\n    \"\"\"\n    Device abstraction for computation backends.\n\n    Args:\n        name: Device name ('cpu' or 'cuda')\n        mod: Backend module for operations\n        device_id: GPU device index (for CUDA devices)\n    \"\"\"\n\n    def __init__(\n        self, \n        name: str, \n        mod: Any, \n        device_id: Optional[int] = None\n    ):\n\n    def enabled(self) -&gt; bool:\n        \"\"\"\n        Check if device is available.\n\n        Returns:\n            True if device can be used\n        \"\"\"\n</code></pre>"},{"location":"api/ndarray/index_en/#tensor-creation","title":"Tensor Creation","text":"Python<pre><code>def randn(\n    self, \n    *shape: int, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create random array from normal distribution.\n\n    Args:\n        *shape: Array dimensions\n        dtype: Element data type\n\n    Returns:\n        NDArray with random values\n\n    Example:\n        &gt;&gt;&gt; device = genesis.cuda()\n        &gt;&gt;&gt; arr = device.randn(10, 10)  # 10x10 random array\n    \"\"\"\n\ndef rand(\n    self, \n    *shape: int, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create random array from uniform distribution [0, 1).\n\n    Args:\n        *shape: Array dimensions\n        dtype: Element data type\n\n    Returns:\n        NDArray with uniform random values\n    \"\"\"\n\ndef empty(\n    self, \n    shape: Tuple[int, ...], \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create uninitialized array.\n\n    Args:\n        shape: Array dimensions\n        dtype: Element data type\n\n    Returns:\n        Uninitialized NDArray\n    \"\"\"\n\ndef full(\n    self, \n    shape: Tuple[int, ...], \n    fill_value: float, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create array filled with specified value.\n\n    Args:\n        shape: Array dimensions\n        fill_value: Value to fill array with\n        dtype: Element data type\n\n    Returns:\n        NDArray filled with fill_value\n\n    Example:\n        &gt;&gt;&gt; device = genesis.cpu()\n        &gt;&gt;&gt; ones = device.full((5, 5), 1.0)  # 5x5 array of ones\n    \"\"\"\n\ndef one_hot(\n    self, \n    n: int, \n    i: NDArray, \n    dtype: Optional[DType] = genesis.float32\n) -&gt; NDArray:\n    \"\"\"\n    Create one-hot encoded array.\n\n    Args:\n        n: Number of classes\n        i: Index array\n        dtype: Element data type\n\n    Returns:\n        One-hot encoded NDArray\n\n    Example:\n        &gt;&gt;&gt; device = genesis.cpu()\n        &gt;&gt;&gt; indices = NDArray([0, 2, 1], device=device)\n        &gt;&gt;&gt; one_hot = device.one_hot(3, indices)\n        &gt;&gt;&gt; # Shape: (3, 3) with one-hot encoding\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index_en/#device-functions","title":"Device Functions","text":""},{"location":"api/ndarray/index_en/#device-creation","title":"Device Creation","text":"Python<pre><code>def cpu() -&gt; Device:\n    \"\"\"\n    Create CPU device.\n\n    Returns:\n        CPU device instance\n\n    Example:\n        &gt;&gt;&gt; cpu_device = genesis.cpu()\n        &gt;&gt;&gt; arr = NDArray([1, 2, 3], device=cpu_device)\n    \"\"\"\n\ndef cuda(index: int = 0) -&gt; Device:\n    \"\"\"\n    Create CUDA device.\n\n    Args:\n        index: GPU device index\n\n    Returns:\n        CUDA device instance or None if CUDA unavailable\n\n    Example:\n        &gt;&gt;&gt; gpu_device = genesis.cuda(0)  # First GPU\n        &gt;&gt;&gt; if gpu_device.enabled():\n        ...     arr = NDArray([1, 2, 3], device=gpu_device)\n    \"\"\"\n\ndef device(device_name: Union[str, int]) -&gt; Device:\n    \"\"\"\n    Create device by name or index.\n\n    Args:\n        device_name: 'cpu', 'cuda', 'cuda:N', or GPU index\n\n    Returns:\n        Device instance\n\n    Example:\n        &gt;&gt;&gt; dev1 = genesis.device('cuda:1')  # Second GPU\n        &gt;&gt;&gt; dev2 = genesis.device(1)         # Same as above\n        &gt;&gt;&gt; dev3 = genesis.device('cpu')     # CPU device\n    \"\"\"\n\ndef default_device() -&gt; Device:\n    \"\"\"\n    Get default device (CPU).\n\n    Returns:\n        Default device instance\n    \"\"\"\n\ndef all_devices() -&gt; List[Device]:\n    \"\"\"\n    Get list of all available devices.\n\n    Returns:\n        List of device instances\n    \"\"\"\n</code></pre>"},{"location":"api/ndarray/index_en/#operations","title":"Operations","text":"<p>The ndarray system supports a comprehensive set of operations through backend modules.</p>"},{"location":"api/ndarray/index_en/#arithmetic-operations","title":"Arithmetic Operations","text":"Python<pre><code># Binary operations\nadd(x, y)           # Element-wise addition\nsub(x, y)           # Element-wise subtraction  \nmul(x, y)           # Element-wise multiplication\ntruediv(x, y)       # Element-wise division\npow(x, scalar)      # Element-wise power\n\n# Unary operations\nlog(x)              # Natural logarithm\nexp(x)              # Exponential\nsin(x)              # Sine\ncos(x)              # Cosine\nsqrt(x)             # Square root\n</code></pre>"},{"location":"api/ndarray/index_en/#reduction-operations","title":"Reduction Operations","text":"Python<pre><code>reduce_sum(x, axis=None, keepdims=False)    # Sum reduction\nreduce_max(x, axis=None, keepdims=False)    # Max reduction\nreduce_min(x, axis=None, keepdims=False)    # Min reduction\n</code></pre>"},{"location":"api/ndarray/index_en/#comparison-operations","title":"Comparison Operations","text":"Python<pre><code>maximum(x, y)       # Element-wise maximum\nminimum(x, y)       # Element-wise minimum\n</code></pre>"},{"location":"api/ndarray/index_en/#matrix-operations","title":"Matrix Operations","text":"Python<pre><code>matmul(x, y)        # Matrix multiplication\ntranspose(x, axes)  # Tensor transpose\n</code></pre>"},{"location":"api/ndarray/index_en/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"api/ndarray/index_en/#kernel-caching","title":"Kernel Caching","text":"<p>Triton kernels are automatically cached for reuse:</p> Python<pre><code>from genesis.ndarray.kernel_cache import cached_kernel_call\n\n# Kernels are cached by function signature and parameters\ncached_kernel_call(kernel_func, grid_func, *args, **kwargs)\n</code></pre>"},{"location":"api/ndarray/index_en/#adaptive-configuration","title":"Adaptive Configuration","text":"<p>Block sizes automatically adapt to tensor dimensions:</p> Python<pre><code>from genesis.ndarray.adaptive_config import AdaptiveConfig\n\n# Get optimized configuration for tensor shape\nconfig = AdaptiveConfig.get_elementwise_config(shape)\nblock_size = config['BLOCK_SIZE']\ngrid = config['grid']\n</code></pre>"},{"location":"api/ndarray/index_en/#memory-management","title":"Memory Management","text":"<p>Efficient memory allocation patterns: - Contiguous memory layout when possible - View-based operations to avoid copying - Automatic memory cleanup</p>"},{"location":"api/ndarray/index_en/#gpu-backend-cudatriton","title":"GPU Backend (CUDA/Triton)","text":""},{"location":"api/ndarray/index_en/#gpu-operations","title":"GPU Operations","text":"<p>Pure CUDA/Triton implementation for GPU operations optimized for performance.</p>"},{"location":"api/ndarray/index_en/#triton-kernels","title":"Triton Kernels","text":"<p>Hand-optimized Triton kernels for maximum performance: - Elementwise operations with broadcasting - Reduction operations with work-efficient algorithms - Matrix multiplication with tiling - Memory-optimized access patterns</p>"},{"location":"api/ndarray/index_en/#usage-examples","title":"Usage Examples","text":""},{"location":"api/ndarray/index_en/#basic-array-creation","title":"Basic Array Creation","text":"Python<pre><code>import genesis\n\n# Create arrays on different devices\ncpu_arr = genesis.NDArray([1, 2, 3, 4], device=genesis.cpu())\ngpu_arr = genesis.NDArray([1, 2, 3, 4], device=genesis.cuda())\n\n# Create with specific shapes\nzeros = genesis.NDArray.make((100, 100), device=genesis.cuda())\nzeros.fill(0.0)\n</code></pre>"},{"location":"api/ndarray/index_en/#device-operations","title":"Device Operations","text":"Python<pre><code># Random arrays\ndevice = genesis.cuda(0)\nrandom_normal = device.randn(1000, 1000)\nrandom_uniform = device.rand(1000, 1000)\n\n# One-hot encoding\nindices = genesis.NDArray([0, 2, 1, 3], device=device)\none_hot = device.one_hot(4, indices)\n</code></pre>"},{"location":"api/ndarray/index_en/#memory-transfer","title":"Memory Transfer","text":"Python<pre><code># GPU to CPU transfer\ngpu_data = genesis.NDArray([1, 2, 3], device=genesis.cuda())\ncpu_data = gpu_data.cpu()\nnumpy_data = gpu_data.numpy()\n\n# CPU to GPU transfer  \ncpu_data = genesis.NDArray([1, 2, 3], device=genesis.cpu())\ngpu_data = genesis.NDArray(cpu_data, device=genesis.cuda())\n</code></pre>"},{"location":"api/ndarray/index_en/#performance-monitoring","title":"Performance Monitoring","text":"Python<pre><code>import time\n\n# Time operations\nstart = time.time()\nresult = genesis.ndarray.add(x, y)\nend = time.time()\nprint(f\"Operation took {(end - start) * 1000:.2f}ms\")\n</code></pre>"},{"location":"api/ndarray/index_en/#backend-selection","title":"Backend Selection","text":"<p>Genesis automatically selects the appropriate backend:</p> Python<pre><code># CPU operations use PyTorch backend\ncpu_device = genesis.cpu()\nx = genesis.NDArray([1, 2, 3], device=cpu_device)\n\n# GPU operations use Triton/CUDA backend\ngpu_device = genesis.cuda()\nif gpu_device.enabled():\n    x = genesis.NDArray([1, 2, 3], device=gpu_device)\n    # Uses optimized Triton kernels\n</code></pre>"},{"location":"api/ndarray/index_en/#error-handling","title":"Error Handling","text":"<p>The ndarray system provides comprehensive error handling:</p> Python<pre><code>try:\n    # Attempt GPU operation\n    gpu_device = genesis.cuda()\n    if not gpu_device.enabled():\n        raise RuntimeError(\"CUDA not available\")\n\n    arr = genesis.NDArray([1, 2, 3], device=gpu_device)\nexcept RuntimeError as e:\n    # Fall back to CPU\n    print(f\"GPU error: {e}, using CPU\")\n    cpu_device = genesis.cpu()\n    arr = genesis.NDArray([1, 2, 3], device=cpu_device)\n</code></pre>"},{"location":"api/ndarray/index_en/#best-practices","title":"Best Practices","text":"<ol> <li>Device Selection: Check device availability before use</li> <li>Memory Management: Transfer between devices judiciously  </li> <li>Batch Operations: Process multiple tensors together when possible</li> <li>Contiguous Memory: Ensure arrays are contiguous for optimal performance</li> <li>Error Handling: Always handle CUDA availability gracefully</li> </ol>"},{"location":"api/ndarray/index_en/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use appropriate block sizes for GPU kernels</li> <li>Minimize device transfers between CPU and GPU</li> <li>Leverage kernel caching by reusing similar operations</li> <li>Use views instead of copies when possible</li> <li>Batch similar operations to amortize kernel launch overhead</li> </ol>"},{"location":"api/ndarray/index_en/#see-also","title":"See Also","text":"<ul> <li>Tensor Operations - High-level tensor interface</li> <li>Neural Network Modules - Building on ndarray</li> <li>Performance Guide - Optimization techniques</li> <li>CUDA Storage - Low-level CUDA implementation</li> </ul>"},{"location":"api/nn/functional.zh/","title":"\u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3 (genesis.nn.functional)","text":"<p>Genesis\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u63d0\u4f9b\u4e86\u65e0\u72b6\u6001\u7684\u5f20\u91cf\u64cd\u4f5c\u51fd\u6570\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u8c03\u7528\u800c\u65e0\u9700\u521b\u5efa\u6a21\u5757\u5b9e\u4f8b\u3002</p>"},{"location":"api/nn/functional.zh/#_1","title":"\u6a21\u5757\u6982\u8ff0","text":"<p><code>genesis.nn.functional</code>\uff08\u901a\u5e38\u5bfc\u5165\u4e3a<code>F</code>\uff09\u5305\u542b\uff1a - \u57fa\u7840\u7b97\u672f\u8fd0\u7b97\uff08\u52a0\u3001\u51cf\u3001\u4e58\u3001\u9664\uff09 - \u6570\u5b66\u51fd\u6570\uff08sin\u3001cos\u3001log\u3001exp\u3001sqrt\u3001power\uff09 - \u5f20\u91cf\u5f62\u72b6\u64cd\u4f5c\uff08transpose\u3001reshape\u3001expand\u3001view\u3001flatten\uff09 - \u5f20\u91cf\u7d22\u5f15\u548c\u5207\u7247\uff08getitem\u3001setitem\u3001broadcast_to\uff09 - \u805a\u5408\u64cd\u4f5c\uff08sum\u3001max\u3001logsumexp\uff09 - \u77e9\u9635\u64cd\u4f5c\uff08matmul\u3001stack\u3001cat\u3001squeeze\u3001unsqueeze\uff09 - \u57fa\u7840\u6fc0\u6d3b\u51fd\u6570\uff08relu\uff09 - \u9ad8\u7ea7\u64cd\u4f5c\uff08softmax\u3001dropout\u6765\u81eatriton_ops\uff09</p>"},{"location":"api/nn/functional.zh/#_2","title":"\u57fa\u7840\u7b97\u672f\u8fd0\u7b97","text":""},{"location":"api/nn/functional.zh/#add","title":"add","text":"Python<pre><code>def add(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u4e24\u4e2a\u5f20\u91cf\u7684\u9010\u5143\u7d20\u52a0\u6cd5\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u7b2c\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\n        b: Tensor - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u9010\u5143\u7d20\u548c a + b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; y = genesis.tensor([[2.0, 1.0], [1.0, 2.0]])\n        &gt;&gt;&gt; z = F.add(x, y)\n        &gt;&gt;&gt; # \u7ed3\u679c: [[3.0, 3.0], [4.0, 6.0]]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#sub","title":"sub","text":"Python<pre><code>def sub(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u4e24\u4e2a\u5f20\u91cf\u7684\u9010\u5143\u7d20\u51cf\u6cd5\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u7b2c\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\uff08\u88ab\u51cf\u6570\uff09\n        b: Tensor - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf\uff08\u51cf\u6570\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u9010\u5143\u7d20\u5dee a - b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([5.0, 3.0, 8.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 1.0, 3.0])\n        &gt;&gt;&gt; z = F.sub(x, y)\n        &gt;&gt;&gt; # \u7ed3\u679c: [3.0, 2.0, 5.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#multiply","title":"multiply","text":"Python<pre><code>def multiply(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u4e24\u4e2a\u5f20\u91cf\u7684\u9010\u5143\u7d20\u4e58\u6cd5\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u7b2c\u4e00\u4e2a\u8f93\u5165\u5f20\u91cf\n        b: Tensor - \u7b2c\u4e8c\u4e2a\u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u9010\u5143\u7d20\u79ef a * b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([2.0, 3.0, 4.0])\n        &gt;&gt;&gt; y = genesis.tensor([1.5, 2.0, 0.5])\n        &gt;&gt;&gt; z = F.multiply(x, y)\n        &gt;&gt;&gt; # \u7ed3\u679c: [3.0, 6.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#divide","title":"divide","text":"Python<pre><code>def divide(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u4e24\u4e2a\u5f20\u91cf\u7684\u9010\u5143\u7d20\u9664\u6cd5\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u88ab\u9664\u6570\u5f20\u91cf\n        b: Tensor - \u9664\u6570\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u9010\u5143\u7d20\u5546 a / b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([6.0, 8.0, 9.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 4.0, 3.0])\n        &gt;&gt;&gt; z = F.divide(x, y)\n        &gt;&gt;&gt; # \u7ed3\u679c: [3.0, 2.0, 3.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_3","title":"\u6807\u91cf\u8fd0\u7b97","text":""},{"location":"api/nn/functional.zh/#add_scalar-mul_scalar-divide_scalar-pow_scalar","title":"add_scalar, mul_scalar, divide_scalar, pow_scalar","text":"Python<pre><code>def add_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef mul_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef divide_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\ndef pow_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\n    \"\"\"\n    \u5f20\u91cf\u4e0e\u6807\u91cf\u4e4b\u95f4\u7684\u9010\u5143\u7d20\u8fd0\u7b97\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        scalar: float - \u6807\u91cf\u503c\n        reverse: bool - \u5982\u679c\u4e3aTrue\uff0c\u5219\u6267\u884c scalar op tensor\uff08\u7528\u4e8edivide/pow\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u5f20\u91cf-\u6807\u91cf\u8fd0\u7b97\u7ed3\u679c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; y1 = F.add_scalar(x, 5.0)      # [6.0, 7.0, 8.0]\n        &gt;&gt;&gt; y2 = F.mul_scalar(x, 2.0)      # [2.0, 4.0, 6.0]\n        &gt;&gt;&gt; y3 = F.pow_scalar(x, 2.0)      # [1.0, 4.0, 9.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_4","title":"\u6570\u5b66\u51fd\u6570","text":""},{"location":"api/nn/functional.zh/#sin-cos-log-exp-sqrt","title":"sin, cos, log, exp, sqrt","text":"Python<pre><code>def sin(a: Tensor) -&gt; Tensor:\ndef cos(a: Tensor) -&gt; Tensor:\ndef log(a: Tensor) -&gt; Tensor:\ndef exp(a: Tensor) -&gt; Tensor:\ndef sqrt(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u9010\u5143\u7d20\u6570\u5b66\u51fd\u6570\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u6570\u5b66\u51fd\u6570\u7ed3\u679c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y1 = F.sin(x)   # [0.0, 0.841, 0.909]\n        &gt;&gt;&gt; y2 = F.exp(x)   # [1.0, 2.718, 7.389]\n        &gt;&gt;&gt; y3 = F.sqrt(genesis.tensor([4.0, 9.0, 16.0]))  # [2.0, 3.0, 4.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#negate","title":"negate","text":"Python<pre><code>def negate(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u9010\u5143\u7d20\u53d6\u8d1f: -a\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u53d6\u8d1f\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, -2.0, 3.0])\n        &gt;&gt;&gt; y = F.negate(x)\n        &gt;&gt;&gt; # \u7ed3\u679c: [-1.0, 2.0, -3.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_5","title":"\u5f62\u72b6\u64cd\u4f5c","text":""},{"location":"api/nn/functional.zh/#transpose","title":"transpose","text":"Python<pre><code>def transpose(a: Tensor, axis: tuple = None) -&gt; Tensor:\n    \"\"\"\n    \u8f6c\u7f6e\u5f20\u91cf\u7ef4\u5ea6\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        axis: tuple - \u8981\u4ea4\u6362\u7684\u7ef4\u5ea6\u5bf9\uff08\u9ed8\u8ba4\uff1a\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u8f6c\u7f6e\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y1 = F.transpose(x)           # \u4ea4\u6362\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6: (3, 5, 4)\n        &gt;&gt;&gt; y2 = F.transpose(x, (0, 2))   # \u4ea4\u6362\u7ef4\u5ea60,2: (5, 4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#reshape","title":"reshape","text":"Python<pre><code>def reshape(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    \u91cd\u65b0\u5851\u5f62\u5f20\u91cf\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        shape: tuple - \u65b0\u5f62\u72b6\uff08\u603b\u5143\u7d20\u6570\u5fc5\u987b\u76f8\u540c\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u91cd\u5851\u5f62\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 6)\n        &gt;&gt;&gt; y = F.reshape(x, (3, 4))\n        &gt;&gt;&gt; # \u5f62\u72b6\u4ece (2, 6) \u53d8\u4e3a (3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#view-expand-flatten","title":"view, expand, flatten","text":"Python<pre><code>def view(a: Tensor, shape: tuple) -&gt; Tensor:\ndef expand(a: Tensor, shape: tuple) -&gt; Tensor:\ndef flatten(a: Tensor, start_dim: int = 0, end_dim: int = None) -&gt; Tensor:\n    \"\"\"\n    \u5f20\u91cf\u89c6\u56fe\u548c\u5f62\u72b6\u64cd\u4f5c\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        shape: tuple - \u76ee\u6807\u5f62\u72b6\n        start_dim, end_dim: int - \u8981\u5c55\u5e73\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u53d8\u6362\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y1 = F.view(x, (6, 4))         # \u89c6\u56fe\u4e3a (6, 4)\n        &gt;&gt;&gt; y2 = F.expand(x, (2, 3, 4, 5)) # \u6269\u5c55\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\n        &gt;&gt;&gt; y3 = F.flatten(x, 1)           # \u4ece\u7ef4\u5ea61\u5f00\u59cb\u5c55\u5e73: (2, 12)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_6","title":"\u5f20\u91cf\u64cd\u4f5c","text":""},{"location":"api/nn/functional.zh/#matmul","title":"matmul","text":"Python<pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u77e9\u9635\u4e58\u6cd5\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u5de6\u77e9\u9635\n        b: Tensor - \u53f3\u77e9\u9635\n\n    \u8fd4\u56de:\n        Tensor - \u77e9\u9635\u4e58\u79ef\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y = genesis.randn(4, 5)\n        &gt;&gt;&gt; z = F.matmul(x, y)  # \u5f62\u72b6: (3, 5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#stack-cat","title":"stack, cat","text":"Python<pre><code>def stack(tensors: list, dim: int = 0) -&gt; Tensor:\ndef cat(tensors: list, dim: int = 0) -&gt; Tensor:\n    \"\"\"\n    \u6cbf\u6307\u5b9a\u7ef4\u5ea6\u5806\u53e0\u6216\u8fde\u63a5\u5f20\u91cf\u3002\n\n    \u53c2\u6570:\n        tensors: list - \u8981\u7ec4\u5408\u7684\u5f20\u91cf\u5217\u8868\n        dim: int - \u5806\u53e0/\u8fde\u63a5\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u7ec4\u5408\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = genesis.randn(2, 3)\n        &gt;&gt;&gt; z1 = F.stack([x, y], dim=0)  # \u5f62\u72b6: (2, 2, 3)\n        &gt;&gt;&gt; z2 = F.cat([x, y], dim=0)    # \u5f62\u72b6: (4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#squeeze-unsqueeze","title":"squeeze, unsqueeze","text":"Python<pre><code>def squeeze(tensor: Tensor, dim: int) -&gt; Tensor:\ndef unsqueeze(tensor: Tensor, dim: int) -&gt; Tensor:\n    \"\"\"\n    \u79fb\u9664\u6216\u6dfb\u52a0\u5355\u4e00\u7ef4\u5ea6\u3002\n\n    \u53c2\u6570:\n        tensor: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int - \u8981squeeze/unsqueeze\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u4fee\u6539\u7ef4\u5ea6\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(1, 3, 1, 4)\n        &gt;&gt;&gt; y1 = F.squeeze(x, 0)    # \u5f62\u72b6: (3, 1, 4)\n        &gt;&gt;&gt; y2 = F.unsqueeze(x, 2)  # \u5f62\u72b6: (1, 3, 1, 1, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_7","title":"\u805a\u5408\u64cd\u4f5c","text":""},{"location":"api/nn/functional.zh/#sum","title":"sum","text":"Python<pre><code>def sum(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\n    \"\"\"\n    \u6cbf\u6307\u5b9a\u7ef4\u5ea6\u6c42\u548c\u5f20\u91cf\u5143\u7d20\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        axis: int - \u6c42\u548c\u7684\u7ef4\u5ea6\uff08None\u8868\u793a\u6240\u6709\u7ef4\u5ea6\uff09\n        keepdims: bool - \u662f\u5426\u4fdd\u6301\u7f29\u51cf\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u6c42\u548c\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.sum(x)           # \u6c42\u6240\u6709\u5143\u7d20\u7684\u548c\uff1a\u6807\u91cf\n        &gt;&gt;&gt; y2 = F.sum(x, axis=0)   # \u6cbf\u884c\u6c42\u548c\uff1a\u5f62\u72b6 (4,)\n        &gt;&gt;&gt; y3 = F.sum(x, axis=1, keepdims=True)  # \u5f62\u72b6: (3, 1)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#max-logsumexp","title":"max, logsumexp","text":"Python<pre><code>def max(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\ndef logsumexp(a: Tensor, axis: int = None) -&gt; Tensor:\n    \"\"\"\n    \u6700\u5927\u503c\u548clog-sum-exp\u64cd\u4f5c\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        axis: int - \u7f29\u51cf\u7684\u7ef4\u5ea6\n        keepdims: bool - \u662f\u5426\u4fdd\u6301\u7f29\u51cf\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u7ed3\u679c\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.max(x, axis=1)      # \u6cbf\u884c\u6c42\u6700\u5927\u503c\n        &gt;&gt;&gt; y2 = F.logsumexp(x, axis=0) # \u6cbf\u5217LogSumExp\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_8","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"api/nn/functional.zh/#relu","title":"relu","text":"Python<pre><code>def relu(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    ReLU\u6fc0\u6d3b\u51fd\u6570: f(x) = max(0, x)\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - ReLU\u6fc0\u6d3b\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y = F.relu(x)\n        &gt;&gt;&gt; # \u7ed3\u679c: [0.0, 0.0, 0.0, 1.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#triton_ops","title":"\u9ad8\u7ea7\u64cd\u4f5c\uff08\u6765\u81eatriton_ops\uff09","text":""},{"location":"api/nn/functional.zh/#softmax","title":"softmax","text":"Python<pre><code># \u4ece genesis.nn.triton_ops \u5bfc\u5165\nfrom genesis.nn.triton_ops import softmax\n\ndef softmax(x: Tensor, dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    \u4f7f\u7528\u4f18\u5316\u7684Triton\u5185\u6838\u7684Softmax\u51fd\u6570\u3002\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int - \u5e94\u7528softmax\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - Softmax\u8f93\u51fa\uff08\u6cbfdim\u7ef4\u5ea6\u548c\u4e3a1\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = softmax(x, dim=1)\n        &gt;&gt;&gt; # \u6bcf\u884c\u548c\u4e3a1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#dropout","title":"dropout","text":"Python<pre><code># \u4ece genesis.nn.triton_ops \u5bfc\u5165\nfrom genesis.nn.triton_ops import dropout\n\ndef dropout(x: Tensor, p: float = 0.5, training: bool = True) -&gt; Tensor:\n    \"\"\"\n    \u4f7f\u7528Triton\u5185\u6838\u7684Dropout\u6b63\u5219\u5316\u3002\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        p: float - Dropout\u6982\u7387\n        training: bool - \u662f\u5426\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\n\n    \u8fd4\u56de:\n        Tensor - \u5e94\u7528dropout\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(100, 50)\n        &gt;&gt;&gt; y = dropout(x, p=0.2, training=True)\n        &gt;&gt;&gt; # 20%\u7684\u5143\u7d20\u8bbe\u4e3a0\uff0c\u5176\u4ed6\u5143\u7d20\u63091/(1-p)\u7f29\u653e\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_9","title":"\u7d22\u5f15\u548c\u5e7f\u64ad","text":""},{"location":"api/nn/functional.zh/#getitem-setitem-broadcast_to","title":"getitem, setitem, broadcast_to","text":"Python<pre><code>def getitem(a: Tensor, index) -&gt; Tensor:\ndef setitem(a: Tensor, index, value) -&gt; Tensor:\ndef broadcast_to(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    \u5f20\u91cf\u7d22\u5f15\u548c\u5e7f\u64ad\u64cd\u4f5c\u3002\n\n    \u53c2\u6570:\n        a: Tensor - \u8f93\u5165\u5f20\u91cf\n        index: Various - \u7d22\u5f15\uff08int\u3001slice\u3001list\u3001Tensor\uff09\n        value: Tensor/scalar - \u8981\u8bbe\u7f6e\u7684\u503c\n        shape: tuple - \u76ee\u6807\u5e7f\u64ad\u5f62\u72b6\n\n    \u8fd4\u56de:\n        Tensor - \u7d22\u5f15/\u5e7f\u64ad\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.getitem(x, [0, 2])      # \u9009\u62e9\u7b2c0\u548c\u7b2c2\u884c\n        &gt;&gt;&gt; y2 = F.broadcast_to(x, (2, 3, 4))  # \u5e7f\u64ad\u5230 (2, 3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional.zh/#_10","title":"\u6027\u80fd\u8bf4\u660e","text":"<ul> <li>GPU\u52a0\u901f\uff1a\u5f53\u5f20\u91cf\u5728CUDA\u8bbe\u5907\u4e0a\u65f6\uff0c\u64cd\u4f5c\u81ea\u52a8\u4f7f\u7528GPU</li> <li>Triton\u4f18\u5316\uff1aSoftmax\u548cdropout\u4f7f\u7528\u4f18\u5316\u7684Triton\u5185\u6838</li> <li>\u5185\u5b58\u6548\u7387\uff1aview\u64cd\u4f5c\u5728\u53ef\u80fd\u65f6\u5171\u4eab\u5185\u5b58</li> <li>\u6df7\u5408\u7cbe\u5ea6\uff1a\u542f\u7528\u65f6\u51fd\u6570\u652f\u6301\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6</li> </ul>"},{"location":"api/nn/functional.zh/#_11","title":"\u5e38\u7528\u6a21\u5f0f","text":"Python<pre><code>import genesis\nimport genesis.nn.functional as F\n\n# \u57fa\u7840\u64cd\u4f5c\nx = genesis.randn(100, 784)\ny = F.relu(F.matmul(x, weights) + bias)\n\n# \u5f62\u72b6\u64cd\u4f5c\nx = genesis.randn(32, 3, 224, 224)\nx_flat = F.flatten(x, start_dim=1)  # (32, 150528)\n\n# \u805a\u5408\nlogits = genesis.randn(32, 10)\nprobs = F.softmax(logits, dim=1)\nmax_vals = F.max(logits, axis=1)\n\n# \u9ad8\u7ea7\u7d22\u5f15\nindices = genesis.tensor([0, 2, 4])\nselected = F.getitem(x, indices)\n</code></pre>"},{"location":"api/nn/functional.zh/#_12","title":"\u672a\u6765\u529f\u80fd\uff08\u8def\u7ebf\u56fe\uff09","text":"<p>\u4ee5\u4e0b\u51fd\u6570\u8ba1\u5212\u5728\u672a\u6765\u7248\u672c\u4e2d\u5b9e\u73b0\uff1a - \u9ad8\u7ea7\u6fc0\u6d3b\u51fd\u6570\uff08gelu\u3001silu\u3001swish\uff09 - \u635f\u5931\u51fd\u6570\uff08cross_entropy\u3001mse_loss\u3001l1_loss\uff09 - \u5f52\u4e00\u5316\u51fd\u6570\uff08layer_norm\u3001batch_norm\uff09 - \u5377\u79ef\u64cd\u4f5c\uff08conv1d\u3001conv2d\uff09 - \u6ce8\u610f\u529b\u673a\u5236\uff08scaled_dot_product_attention\uff09</p> <p>\u8981\u8ddf\u8e2a\u8fd9\u4e9b\u529f\u80fd\u7684\u8fdb\u5c55\uff0c\u8bf7\u67e5\u770bGitHub\u4e0a\u7684\u9879\u76ee\u8def\u7ebf\u56fe\u3002</p>"},{"location":"api/nn/functional_en/","title":"Functional Operations Interface (genesis.nn.functional)","text":"<p>Genesis functional interface provides stateless tensor operation functions that can be called directly on tensors without creating module instances.</p>"},{"location":"api/nn/functional_en/#module-overview","title":"Module Overview","text":"<p><code>genesis.nn.functional</code> (commonly imported as <code>F</code>) includes: - Basic arithmetic operations (add, subtract, multiply, divide) - Mathematical functions (sin, cos, log, exp, sqrt, power) - Tensor shape operations (transpose, reshape, expand, view, flatten) - Tensor indexing and slicing (getitem, setitem, broadcast_to) - Aggregation operations (sum, max, logsumexp) - Matrix operations (matmul, stack, cat, squeeze, unsqueeze) - Basic activation functions (relu) - Advanced operations (softmax, dropout from triton_ops)</p>"},{"location":"api/nn/functional_en/#basic-arithmetic-operations","title":"Basic Arithmetic Operations","text":""},{"location":"api/nn/functional_en/#add","title":"add","text":"Python<pre><code>def add(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise addition of two tensors.\n\n    Args:\n        a: Tensor - First input tensor\n        b: Tensor - Second input tensor\n\n    Returns:\n        Tensor - Element-wise sum a + b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([[1.0, 2.0], [3.0, 4.0]])\n        &gt;&gt;&gt; y = genesis.tensor([[2.0, 1.0], [1.0, 2.0]])\n        &gt;&gt;&gt; z = F.add(x, y)\n        &gt;&gt;&gt; # Result: [[3.0, 3.0], [4.0, 6.0]]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#sub","title":"sub","text":"Python<pre><code>def sub(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise subtraction of two tensors.\n\n    Args:\n        a: Tensor - First input tensor (minuend)\n        b: Tensor - Second input tensor (subtrahend)\n\n    Returns:\n        Tensor - Element-wise difference a - b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([5.0, 3.0, 8.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 1.0, 3.0])\n        &gt;&gt;&gt; z = F.sub(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 2.0, 5.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#multiply","title":"multiply","text":"Python<pre><code>def multiply(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise multiplication of two tensors.\n\n    Args:\n        a: Tensor - First input tensor\n        b: Tensor - Second input tensor\n\n    Returns:\n        Tensor - Element-wise product a * b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([2.0, 3.0, 4.0])\n        &gt;&gt;&gt; y = genesis.tensor([1.5, 2.0, 0.5])\n        &gt;&gt;&gt; z = F.multiply(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 6.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#divide","title":"divide","text":"Python<pre><code>def divide(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise division of two tensors.\n\n    Args:\n        a: Tensor - Dividend tensor\n        b: Tensor - Divisor tensor\n\n    Returns:\n        Tensor - Element-wise quotient a / b\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([6.0, 8.0, 9.0])\n        &gt;&gt;&gt; y = genesis.tensor([2.0, 4.0, 3.0])\n        &gt;&gt;&gt; z = F.divide(x, y)\n        &gt;&gt;&gt; # Result: [3.0, 2.0, 3.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#scalar-operations","title":"Scalar Operations","text":""},{"location":"api/nn/functional_en/#add_scalar-mul_scalar-divide_scalar-pow_scalar","title":"add_scalar, mul_scalar, divide_scalar, pow_scalar","text":"Python<pre><code>def add_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef mul_scalar(a: Tensor, scalar: float) -&gt; Tensor:\ndef divide_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\ndef pow_scalar(a: Tensor, scalar: float, reverse: bool = False) -&gt; Tensor:\n    \"\"\"\n    Element-wise operations between tensor and scalar.\n\n    Args:\n        a: Tensor - Input tensor\n        scalar: float - Scalar value\n        reverse: bool - If True, applies scalar op tensor (for divide/pow)\n\n    Returns:\n        Tensor - Result of tensor-scalar operation\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, 2.0, 3.0])\n        &gt;&gt;&gt; y1 = F.add_scalar(x, 5.0)      # [6.0, 7.0, 8.0]\n        &gt;&gt;&gt; y2 = F.mul_scalar(x, 2.0)      # [2.0, 4.0, 6.0]\n        &gt;&gt;&gt; y3 = F.pow_scalar(x, 2.0)      # [1.0, 4.0, 9.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#mathematical-functions","title":"Mathematical Functions","text":""},{"location":"api/nn/functional_en/#sin-cos-log-exp-sqrt","title":"sin, cos, log, exp, sqrt","text":"Python<pre><code>def sin(a: Tensor) -&gt; Tensor:\ndef cos(a: Tensor) -&gt; Tensor:\ndef log(a: Tensor) -&gt; Tensor:\ndef exp(a: Tensor) -&gt; Tensor:\ndef sqrt(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise mathematical functions.\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - Result of mathematical function\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y1 = F.sin(x)   # [0.0, 0.841, 0.909]\n        &gt;&gt;&gt; y2 = F.exp(x)   # [1.0, 2.718, 7.389]\n        &gt;&gt;&gt; y3 = F.sqrt(genesis.tensor([4.0, 9.0, 16.0]))  # [2.0, 3.0, 4.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#negate","title":"negate","text":"Python<pre><code>def negate(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    Element-wise negation: -a\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - Negated tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([1.0, -2.0, 3.0])\n        &gt;&gt;&gt; y = F.negate(x)\n        &gt;&gt;&gt; # Result: [-1.0, 2.0, -3.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#shape-operations","title":"Shape Operations","text":""},{"location":"api/nn/functional_en/#transpose","title":"transpose","text":"Python<pre><code>def transpose(a: Tensor, axis: tuple = None) -&gt; Tensor:\n    \"\"\"\n    Transpose tensor dimensions.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: tuple - Pair of dimensions to swap (default: last two dims)\n\n    Returns:\n        Tensor - Transposed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y1 = F.transpose(x)           # Swap last two dims: (3, 5, 4)\n        &gt;&gt;&gt; y2 = F.transpose(x, (0, 2))   # Swap dims 0,2: (5, 4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#reshape","title":"reshape","text":"Python<pre><code>def reshape(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    Reshape tensor to new shape.\n\n    Args:\n        a: Tensor - Input tensor\n        shape: tuple - New shape (must have same total elements)\n\n    Returns:\n        Tensor - Reshaped tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 6)\n        &gt;&gt;&gt; y = F.reshape(x, (3, 4))\n        &gt;&gt;&gt; # Changes shape from (2, 6) to (3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#view-expand-flatten","title":"view, expand, flatten","text":"Python<pre><code>def view(a: Tensor, shape: tuple) -&gt; Tensor:\ndef expand(a: Tensor, shape: tuple) -&gt; Tensor:\ndef flatten(a: Tensor, start_dim: int = 0, end_dim: int = None) -&gt; Tensor:\n    \"\"\"\n    Tensor view and shape manipulation operations.\n\n    Args:\n        a: Tensor - Input tensor\n        shape: tuple - Target shape\n        start_dim, end_dim: int - Dimensions to flatten\n\n    Returns:\n        Tensor - Transformed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y1 = F.view(x, (6, 4))         # View as (6, 4)\n        &gt;&gt;&gt; y2 = F.expand(x, (2, 3, 4, 5)) # Expand last dim\n        &gt;&gt;&gt; y3 = F.flatten(x, 1)           # Flatten from dim 1: (2, 12)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#tensor-operations","title":"Tensor Operations","text":""},{"location":"api/nn/functional_en/#matmul","title":"matmul","text":"Python<pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    Matrix multiplication.\n\n    Args:\n        a: Tensor - Left matrix\n        b: Tensor - Right matrix\n\n    Returns:\n        Tensor - Matrix product\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y = genesis.randn(4, 5)\n        &gt;&gt;&gt; z = F.matmul(x, y)  # Shape: (3, 5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#stack-cat","title":"stack, cat","text":"Python<pre><code>def stack(tensors: list, dim: int = 0) -&gt; Tensor:\ndef cat(tensors: list, dim: int = 0) -&gt; Tensor:\n    \"\"\"\n    Stack or concatenate tensors along dimension.\n\n    Args:\n        tensors: list - List of tensors to combine\n        dim: int - Dimension along which to stack/concatenate\n\n    Returns:\n        Tensor - Combined tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = genesis.randn(2, 3)\n        &gt;&gt;&gt; z1 = F.stack([x, y], dim=0)  # Shape: (2, 2, 3)\n        &gt;&gt;&gt; z2 = F.cat([x, y], dim=0)    # Shape: (4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#squeeze-unsqueeze","title":"squeeze, unsqueeze","text":"Python<pre><code>def squeeze(tensor: Tensor, dim: int) -&gt; Tensor:\ndef unsqueeze(tensor: Tensor, dim: int) -&gt; Tensor:\n    \"\"\"\n    Remove or add singleton dimensions.\n\n    Args:\n        tensor: Tensor - Input tensor\n        dim: int - Dimension to squeeze/unsqueeze\n\n    Returns:\n        Tensor - Tensor with modified dimensions\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(1, 3, 1, 4)\n        &gt;&gt;&gt; y1 = F.squeeze(x, 0)    # Shape: (3, 1, 4)\n        &gt;&gt;&gt; y2 = F.unsqueeze(x, 2)  # Shape: (1, 3, 1, 1, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#aggregation-operations","title":"Aggregation Operations","text":""},{"location":"api/nn/functional_en/#sum","title":"sum","text":"Python<pre><code>def sum(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\n    \"\"\"\n    Sum tensor elements along specified dimensions.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: int - Dimension to sum over (None for all)\n        keepdims: bool - Whether to keep reduced dimensions\n\n    Returns:\n        Tensor - Summed tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.sum(x)           # Sum all elements: scalar\n        &gt;&gt;&gt; y2 = F.sum(x, axis=0)   # Sum over rows: shape (4,)\n        &gt;&gt;&gt; y3 = F.sum(x, axis=1, keepdims=True)  # Shape: (3, 1)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#max-logsumexp","title":"max, logsumexp","text":"Python<pre><code>def max(a: Tensor, axis: int = None, keepdims: bool = False) -&gt; Tensor:\ndef logsumexp(a: Tensor, axis: int = None) -&gt; Tensor:\n    \"\"\"\n    Maximum and log-sum-exp operations.\n\n    Args:\n        a: Tensor - Input tensor\n        axis: int - Dimension to reduce over\n        keepdims: bool - Whether to keep reduced dimensions\n\n    Returns:\n        Tensor - Result tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.max(x, axis=1)      # Max along rows\n        &gt;&gt;&gt; y2 = F.logsumexp(x, axis=0) # LogSumExp along cols\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#activation-functions","title":"Activation Functions","text":""},{"location":"api/nn/functional_en/#relu","title":"relu","text":"Python<pre><code>def relu(a: Tensor) -&gt; Tensor:\n    \"\"\"\n    ReLU activation function: f(x) = max(0, x)\n\n    Args:\n        a: Tensor - Input tensor\n\n    Returns:\n        Tensor - ReLU-activated tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n        &gt;&gt;&gt; y = F.relu(x)\n        &gt;&gt;&gt; # Result: [0.0, 0.0, 0.0, 1.0, 2.0]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#advanced-operations-from-triton_ops","title":"Advanced Operations (from triton_ops)","text":""},{"location":"api/nn/functional_en/#softmax","title":"softmax","text":"Python<pre><code># Imported from genesis.nn.triton_ops\nfrom genesis.nn.triton_ops import softmax\n\ndef softmax(x: Tensor, dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    Softmax function using optimized Triton kernel.\n\n    Args:\n        x: Tensor - Input tensor\n        dim: int - Dimension along which to apply softmax\n\n    Returns:\n        Tensor - Softmax output (sums to 1 along dim)\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = softmax(x, dim=1)\n        &gt;&gt;&gt; # Each row sums to 1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#dropout","title":"dropout","text":"Python<pre><code># Imported from genesis.nn.triton_ops\nfrom genesis.nn.triton_ops import dropout\n\ndef dropout(x: Tensor, p: float = 0.5, training: bool = True) -&gt; Tensor:\n    \"\"\"\n    Dropout regularization using Triton kernel.\n\n    Args:\n        x: Tensor - Input tensor\n        p: float - Dropout probability\n        training: bool - Whether in training mode\n\n    Returns:\n        Tensor - Tensor with dropout applied\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(100, 50)\n        &gt;&gt;&gt; y = dropout(x, p=0.2, training=True)\n        &gt;&gt;&gt; # 20% of elements set to 0, others scaled by 1/(1-p)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#indexing-and-broadcasting","title":"Indexing and Broadcasting","text":""},{"location":"api/nn/functional_en/#getitem-setitem-broadcast_to","title":"getitem, setitem, broadcast_to","text":"Python<pre><code>def getitem(a: Tensor, index) -&gt; Tensor:\ndef setitem(a: Tensor, index, value) -&gt; Tensor:\ndef broadcast_to(a: Tensor, shape: tuple) -&gt; Tensor:\n    \"\"\"\n    Tensor indexing and broadcasting operations.\n\n    Args:\n        a: Tensor - Input tensor\n        index: Various - Index (int, slice, list, Tensor)\n        value: Tensor/scalar - Value to set\n        shape: tuple - Target broadcast shape\n\n    Returns:\n        Tensor - Indexed/broadcast tensor\n\n    Example:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y1 = F.getitem(x, [0, 2])      # Select rows 0 and 2\n        &gt;&gt;&gt; y2 = F.broadcast_to(x, (2, 3, 4))  # Broadcast to (2, 3, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional_en/#performance-notes","title":"Performance Notes","text":"<ul> <li>GPU Acceleration: Operations automatically use GPU when tensors are on CUDA device</li> <li>Triton Optimization: Softmax and dropout use optimized Triton kernels</li> <li>Memory Efficiency: View operations share memory when possible</li> <li>Mixed Precision: Functions support automatic mixed precision when enabled</li> </ul>"},{"location":"api/nn/functional_en/#common-usage-patterns","title":"Common Usage Patterns","text":"Python<pre><code>import genesis\nimport genesis.nn.functional as F\n\n# Basic operations\nx = genesis.randn(100, 784)\ny = F.relu(F.matmul(x, weights) + bias)\n\n# Shape manipulation\nx = genesis.randn(32, 3, 224, 224)\nx_flat = F.flatten(x, start_dim=1)  # (32, 150528)\n\n# Aggregation\nlogits = genesis.randn(32, 10)\nprobs = F.softmax(logits, dim=1)\nmax_vals = F.max(logits, axis=1)\n\n# Advanced indexing\nindices = genesis.tensor([0, 2, 4])\nselected = F.getitem(x, indices)\n</code></pre>"},{"location":"api/nn/functional_en/#future-features-roadmap","title":"Future Features (Roadmap)","text":"<p>The following functions are planned for future releases: - Advanced activation functions (gelu, silu, swish) - Loss functions (cross_entropy, mse_loss, l1_loss) - Normalization functions (layer_norm, batch_norm) - Convolution operations (conv1d, conv2d) - Attention mechanisms (scaled_dot_product_attention)</p> <p>To track progress on these features, see the project roadmap on GitHub.</p>"},{"location":"api/nn/modules.zh/","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 (genesis.nn)","text":""},{"location":"api/nn/modules.zh/#_1","title":"\u6982\u8ff0","text":"<p><code>genesis.nn</code>\u6a21\u5757\u63d0\u4f9b\u4e86\u521b\u5efa\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6240\u9700\u7684\u6240\u6709\u6784\u5efa\u5757\u3002\u5b83\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ec4\u5408\u66f4\u7b80\u5355\u7684\u7ec4\u4ef6\u6765\u6784\u5efa\u590d\u6742\u6a21\u578b\u3002</p>"},{"location":"api/nn/modules.zh/#_2","title":"\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"api/nn/modules.zh/#_3","title":"\u6a21\u5757\u7cfb\u7edf","text":"<p>\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u90fd\u7ee7\u627f\u81ea<code>nn.Module</code>\uff0c\u5b83\u63d0\u4f9b\uff1a - \u53c2\u6570\u7ba1\u7406 - \u8bbe\u5907\u548cdtype\u5904\u7406 - \u72b6\u6001\u5e8f\u5217\u5316 - \u524d\u5411\u4f20\u64ad\u5b9a\u4e49</p>"},{"location":"api/nn/modules.zh/#_4","title":"\u53c2\u6570","text":"<p>\u53c2\u6570\u662f\u81ea\u52a8\u8ddf\u8e2a\u5e76\u5728\u8bad\u7ec3\u671f\u95f4\u66f4\u65b0\u7684\u5f20\u91cf\uff1a - \u5206\u914d\u4e3a\u6a21\u5757\u5c5e\u6027\u65f6\u81ea\u52a8\u6ce8\u518c - \u5305\u542b\u5728<code>module.parameters()</code>\u4e2d\u4f9b\u4f18\u5316\u5668\u4f7f\u7528 - \u4e0e\u6a21\u578b\u72b6\u6001\u4e00\u8d77\u4fdd\u5b58/\u52a0\u8f7d</p>"},{"location":"api/nn/modules.zh/#_5","title":"\u57fa\u7c7b","text":""},{"location":"api/nn/modules.zh/#nnmodule","title":"<code>nn.Module</code>","text":"<p>\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u7684\u57fa\u7c7b\u3002</p> Python<pre><code>class Module:\n    \"\"\"\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u7684\u57fa\u7c7b\u3002\"\"\"\n\n    def __init__(self):\n        \"\"\"\u521d\u59cb\u5316\u6a21\u5757\u3002\"\"\"\n        self._modules = {}\n        self._parameters = {}\n        self._buffers = {}\n        self.training = True\n</code></pre>"},{"location":"api/nn/modules.zh/#_6","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"api/nn/modules.zh/#_7","title":"\u524d\u5411\u4f20\u64ad","text":"Python<pre><code>def forward(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    \u5b9a\u4e49\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\u3002\n    \u5fc5\u987b\u7531\u5b50\u7c7b\u91cd\u5199\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def forward(self, x):\n        ...         return x * 2\n    \"\"\"\n    raise NotImplementedError\n\ndef __call__(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    \u4f7f\u6a21\u5757\u53ef\u8c03\u7528\u3002\u5185\u90e8\u8c03\u7528forward()\u3002\n\n    \u6ce8\u610f: \u59cb\u7ec8\u4f7f\u7528module(input)\u800c\u4e0d\u662fmodule.forward(input)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_8","title":"\u53c2\u6570\u7ba1\u7406","text":"Python<pre><code>def parameters(self) -&gt; List[Tensor]:\n    \"\"\"\n    \u8fd4\u56de\u6a21\u5757\u4e2d\u7684\u6240\u6709\u53c2\u6570\u3002\n\n    \u8fd4\u56de:\n        \u53c2\u6570\u5f20\u91cf\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Linear(10, 5)\n        &gt;&gt;&gt; params = model.parameters()\n        &gt;&gt;&gt; print(len(params))  # 2 (\u6743\u91cd\u548c\u504f\u7f6e)\n    \"\"\"\n\ndef named_parameters(self) -&gt; List[Tuple[str, Tensor]]:\n    \"\"\"\n    \u8fd4\u56de\u5e26\u540d\u79f0\u7684\u53c2\u6570\u3002\n\n    \u8fd4\u56de:\n        (\u540d\u79f0, \u53c2\u6570)\u5143\u7ec4\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; for name, param in model.named_parameters():\n        ...     print(f\"{name}: {param.shape}\")\n    \"\"\"\n\ndef zero_grad(self) -&gt; None:\n    \"\"\"\n    \u5c06\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\u6e05\u96f6\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model.zero_grad()  # \u6e05\u9664\u6240\u6709\u68af\u5ea6\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_9","title":"\u6a21\u5757\u5c42\u6b21\u7ed3\u6784","text":"Python<pre><code>def add_module(self, name: str, module: Optional[Module]) -&gt; None:\n    \"\"\"\n    \u6dfb\u52a0\u5b50\u6a21\u5757\u3002\n\n    \u53c2\u6570:\n        name: \u5b50\u6a21\u5757\u7684\u540d\u79f0\n        module: \u8981\u6dfb\u52a0\u7684\u6a21\u5757\u5b9e\u4f8b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Module()\n        &gt;&gt;&gt; model.add_module('fc', nn.Linear(10, 5))\n    \"\"\"\n\ndef modules(self) -&gt; Iterator[Module]:\n    \"\"\"\u8fd4\u56de\u6240\u6709\u6a21\u5757\uff08\u5305\u62ec\u81ea\u8eab\uff09\u7684\u8fed\u4ee3\u5668\u3002\"\"\"\n\ndef children(self) -&gt; Iterator[Module]:\n    \"\"\"\u8fd4\u56de\u76f4\u63a5\u5b50\u6a21\u5757\u7684\u8fed\u4ee3\u5668\u3002\"\"\"\n\ndef named_modules(self) -&gt; Iterator[Tuple[str, Module]]:\n    \"\"\"\u8fd4\u56de\u6240\u6709\u6a21\u5757\u53ca\u5176\u540d\u79f0\u7684\u8fed\u4ee3\u5668\u3002\"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_10","title":"\u8bad\u7ec3\u6a21\u5f0f","text":"Python<pre><code>def train(self, mode: bool = True) -&gt; Module:\n    \"\"\"\n    \u5c06\u6a21\u5757\u8bbe\u7f6e\u4e3a\u8bad\u7ec3\u6a21\u5f0f\u3002\n\n    \u53c2\u6570:\n        mode: \u662f\u5426\u542f\u7528\u8bad\u7ec3\u6a21\u5f0f\n\n    \u8fd4\u56de:\n        self\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model.train()  # \u542f\u7528\u8bad\u7ec3\u6a21\u5f0f\n        &gt;&gt;&gt; model.train(False)  # \u7b49\u4ef7\u4e8emodel.eval()\n    \"\"\"\n\ndef eval(self) -&gt; Module:\n    \"\"\"\n    \u5c06\u6a21\u5757\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\u3002\n\n    \u8fd4\u56de:\n        self\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model.eval()  # \u7981\u7528dropout\uff0c\u4f7f\u7528BN\u7684\u8fd0\u884c\u7edf\u8ba1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_11","title":"\u72b6\u6001\u7ba1\u7406","text":"Python<pre><code>def state_dict(self) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    \u8fd4\u56de\u5305\u542b\u6240\u6709\u53c2\u6570\u548c\u7f13\u51b2\u533a\u7684\u72b6\u6001\u5b57\u5178\u3002\n\n    \u8fd4\u56de:\n        \u53c2\u6570\u540d\u79f0\u5230\u5f20\u91cf\u7684\u6620\u5c04\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; state = model.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'model.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: Dict[str, Tensor]) -&gt; None:\n    \"\"\"\n    \u4ece\u72b6\u6001\u5b57\u5178\u52a0\u8f7d\u53c2\u6570\u3002\n\n    \u53c2\u6570:\n        state_dict: \u53c2\u6570\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; state = genesis.load('model.pth')\n        &gt;&gt;&gt; model.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnparameter","title":"<code>nn.Parameter</code>","text":"<p>\u81ea\u52a8\u6ce8\u518c\u4e3a\u6a21\u5757\u53c2\u6570\u7684\u7279\u6b8a\u5f20\u91cf\u3002</p> Python<pre><code>class Parameter(Tensor):\n    \"\"\"\n    \u81ea\u52a8\u6ce8\u518c\u4e3a\u6a21\u5757\u53c2\u6570\u7684\u5f20\u91cf\u3002\n\n    \u53c2\u6570:\n        data: \u5f20\u91cf\u6570\u636e\n        requires_grad: \u662f\u5426\u8ba1\u7b97\u68af\u5ea6\uff08\u9ed8\u8ba4: True\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def __init__(self):\n        ...         super().__init__()\n        ...         self.weight = nn.Parameter(genesis.randn(10, 5))\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_12","title":"\u5c42\u7c7b\u578b","text":""},{"location":"api/nn/modules.zh/#_13","title":"\u7ebf\u6027\u5c42","text":""},{"location":"api/nn/modules.zh/#nnlinear","title":"<code>nn.Linear</code>","text":"<p>\u6267\u884c\u7ebf\u6027\u53d8\u6362\u7684\u5168\u8fde\u63a5\u5c42\u3002</p> Python<pre><code>class Linear(Module):\n    \"\"\"\n    \u7ebf\u6027\u53d8\u6362: y = xW^T + b\n\n    \u53c2\u6570:\n        in_features: \u8f93\u5165\u7279\u5f81\u5927\u5c0f\n        out_features: \u8f93\u51fa\u7279\u5f81\u5927\u5c0f\n        bias: \u662f\u5426\u5305\u542b\u504f\u7f6e\u9879\uff08\u9ed8\u8ba4: True\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (*, in_features)\n        - \u8f93\u51fa: (*, out_features)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; layer = nn.Linear(20, 30)\n        &gt;&gt;&gt; x = genesis.randn(128, 20)\n        &gt;&gt;&gt; output = layer(x)  # \u5f62\u72b6: (128, 30)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_14","title":"\u5377\u79ef\u5c42","text":""},{"location":"api/nn/modules.zh/#nnconv2d","title":"<code>nn.Conv2d</code>","text":"<p>\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u76842D\u5377\u79ef\u5c42\u3002</p> Python<pre><code>class Conv2d(Module):\n    \"\"\"\n    \u8f93\u5165\u4fe1\u53f7\u76842D\u5377\u79ef\u3002\n\n    \u53c2\u6570:\n        in_channels: \u8f93\u5165\u901a\u9053\u6570\n        out_channels: \u8f93\u51fa\u901a\u9053\u6570\n        kernel_size: \u5377\u79ef\u6838\u5927\u5c0f\n        stride: \u5377\u79ef\u6b65\u957f\uff08\u9ed8\u8ba4: 1\uff09\n        padding: \u4e24\u4fa7\u6dfb\u52a0\u7684\u96f6\u586b\u5145\uff08\u9ed8\u8ba4: 0\uff09\n        bias: \u662f\u5426\u6dfb\u52a0\u504f\u7f6e\uff08\u9ed8\u8ba4: True\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (N, C_in, H, W)\n        - \u8f93\u51fa: (N, C_out, H_out, W_out)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        &gt;&gt;&gt; x = genesis.randn(32, 3, 224, 224)\n        &gt;&gt;&gt; output = conv(x)  # \u5f62\u72b6: (32, 64, 224, 224)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_15","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"api/nn/modules.zh/#nnrelu","title":"<code>nn.ReLU</code>","text":"<p>\u4fee\u6b63\u7ebf\u6027\u5355\u5143\u6fc0\u6d3b\u3002</p> Python<pre><code>class ReLU(Module):\n    \"\"\"\n    ReLU\u6fc0\u6d3b: f(x) = max(0, x)\n\n    \u53c2\u6570:\n        inplace: \u662f\u5426\u539f\u5730\u4fee\u6539\u8f93\u5165\uff08\u9ed8\u8ba4: False\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; relu = nn.ReLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = relu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnsigmoid","title":"<code>nn.Sigmoid</code>","text":"<p>Sigmoid\u6fc0\u6d3b\u51fd\u6570\u3002</p> Python<pre><code>class Sigmoid(Module):\n    \"\"\"\n    Sigmoid\u6fc0\u6d3b: f(x) = 1 / (1 + exp(-x))\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; sigmoid = nn.Sigmoid()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = sigmoid(x)  # \u503c\u5728(0, 1)\u8303\u56f4\u5185\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nntanh","title":"<code>nn.Tanh</code>","text":"<p>\u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u3002</p> Python<pre><code>class Tanh(Module):\n    \"\"\"\n    Tanh\u6fc0\u6d3b: f(x) = tanh(x)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; tanh = nn.Tanh()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = tanh(x)  # \u503c\u5728(-1, 1)\u8303\u56f4\u5185\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnsilu-swish","title":"<code>nn.SiLU</code> (Swish)","text":"<p>Sigmoid\u7ebf\u6027\u5355\u5143\u6fc0\u6d3b\u3002</p> Python<pre><code>class SiLU(Module):\n    \"\"\"\n    SiLU/Swish\u6fc0\u6d3b: f(x) = x * sigmoid(x)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; silu = nn.SiLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = silu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nngelu","title":"<code>nn.GELU</code>","text":"<p>\u9ad8\u65af\u8bef\u5dee\u7ebf\u6027\u5355\u5143\u6fc0\u6d3b\u3002</p> Python<pre><code>class GELU(Module):\n    \"\"\"\n    GELU\u6fc0\u6d3b: f(x) = x * \u03a6(x)\n    \u5176\u4e2d\u03a6(x)\u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\u3002\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; gelu = nn.GELU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = gelu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnsoftmax","title":"<code>nn.Softmax</code>","text":"<p>\u591a\u7c7b\u5206\u7c7b\u7684Softmax\u6fc0\u6d3b\u3002</p> Python<pre><code>class Softmax(Module):\n    \"\"\"\n    Softmax\u6fc0\u6d3b: softmax(x_i) = exp(x_i) / \u03a3 exp(x_j)\n\n    \u53c2\u6570:\n        dim: \u5e94\u7528softmax\u7684\u7ef4\u5ea6\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; softmax = nn.Softmax(dim=-1)\n        &gt;&gt;&gt; x = genesis.randn(10, 5)\n        &gt;&gt;&gt; output = softmax(x)  # \u6bcf\u884c\u548c\u4e3a1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_16","title":"\u5f52\u4e00\u5316\u5c42","text":""},{"location":"api/nn/modules.zh/#nnbatchnorm1d","title":"<code>nn.BatchNorm1d</code>","text":"<p>1D\u62162D\u8f93\u5165\u7684\u6279\u91cf\u5f52\u4e00\u5316\u3002</p> Python<pre><code>class BatchNorm1d(Module):\n    \"\"\"\n    2D\u62163D\u8f93\u5165\u7684\u6279\u91cf\u5f52\u4e00\u5316\u3002\n\n    \u53c2\u6570:\n        num_features: \u7279\u5f81\u6570\u91cf\uff08[N, C]\u6216[N, C, L]\u4e2d\u7684C\uff09\n        eps: \u6570\u503c\u7a33\u5b9a\u6027\u7684\u5c0f\u503c\uff08\u9ed8\u8ba4: 1e-5\uff09\n        momentum: \u8fd0\u884c\u7edf\u8ba1\u7684\u52a8\u91cf\uff08\u9ed8\u8ba4: 0.1\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (N, C)\u6216(N, C, L)\n        - \u8f93\u51fa: \u4e0e\u8f93\u5165\u76f8\u540c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; bn = nn.BatchNorm1d(100)\n        &gt;&gt;&gt; x = genesis.randn(20, 100)\n        &gt;&gt;&gt; output = bn(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnlayernorm","title":"<code>nn.LayerNorm</code>","text":"<p>\u5c42\u5f52\u4e00\u5316\u3002</p> Python<pre><code>class LayerNorm(Module):\n    \"\"\"\n    \u6700\u540e\u7ef4\u5ea6\u7684\u5c42\u5f52\u4e00\u5316\u3002\n\n    \u53c2\u6570:\n        normalized_shape: \u8981\u5f52\u4e00\u5316\u7684\u7ef4\u5ea6\u5f62\u72b6\n        eps: \u6570\u503c\u7a33\u5b9a\u6027\u7684\u5c0f\u503c\uff08\u9ed8\u8ba4: 1e-5\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (*, normalized_shape)\n        - \u8f93\u51fa: \u4e0e\u8f93\u5165\u76f8\u540c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; ln = nn.LayerNorm([768])\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ln(x)  # \u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u4e0a\u5f52\u4e00\u5316\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#dropout","title":"Dropout\u5c42","text":""},{"location":"api/nn/modules.zh/#nndropout","title":"<code>nn.Dropout</code>","text":"<p>\u6b63\u5219\u5316\u7684Dropout\u3002</p> Python<pre><code>class Dropout(Module):\n    \"\"\"\n    \u968f\u673a\u5c06\u5143\u7d20\u7f6e\u96f6\u8fdb\u884c\u6b63\u5219\u5316\u3002\n\n    \u53c2\u6570:\n        p: \u5c06\u5143\u7d20\u7f6e\u96f6\u7684\u6982\u7387\uff08\u9ed8\u8ba4: 0.5\uff09\n        inplace: \u662f\u5426\u539f\u5730\u4fee\u6539\u8f93\u5165\uff08\u9ed8\u8ba4: False\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; dropout = nn.Dropout(p=0.2)\n        &gt;&gt;&gt; x = genesis.randn(20, 16)\n        &gt;&gt;&gt; output = dropout(x)  # \u8bad\u7ec3\u6a21\u5f0f\uff1a\u968f\u673a\u5c0620%\u7684\u5143\u7d20\u7f6e\u96f6\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_17","title":"\u6c60\u5316\u5c42","text":""},{"location":"api/nn/modules.zh/#nnmaxpool2d","title":"<code>nn.MaxPool2d</code>","text":"<p>2D\u6700\u5927\u6c60\u5316\u3002</p> Python<pre><code>class MaxPool2d(Module):\n    \"\"\"\n    2D\u8f93\u5165\u7684\u6700\u5927\u6c60\u5316\u3002\n\n    \u53c2\u6570:\n        kernel_size: \u6c60\u5316\u7a97\u53e3\u5927\u5c0f\n        stride: \u6c60\u5316\u6b65\u957f\uff08\u9ed8\u8ba4: kernel_size\uff09\n        padding: \u96f6\u586b\u5145\uff08\u9ed8\u8ba4: 0\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (N, C, H, W)\n        - \u8f93\u51fa: (N, C, H_out, W_out)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        &gt;&gt;&gt; x = genesis.randn(1, 16, 32, 32)\n        &gt;&gt;&gt; output = pool(x)  # \u5f62\u72b6: (1, 16, 16, 16)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnavgpool2d","title":"<code>nn.AvgPool2d</code>","text":"<p>2D\u5e73\u5747\u6c60\u5316\u3002</p> Python<pre><code>class AvgPool2d(Module):\n    \"\"\"\n    2D\u8f93\u5165\u7684\u5e73\u5747\u6c60\u5316\u3002\n\n    \u53c2\u6570:\n        kernel_size: \u6c60\u5316\u7a97\u53e3\u5927\u5c0f\n        stride: \u6c60\u5316\u6b65\u957f\uff08\u9ed8\u8ba4: kernel_size\uff09\n        padding: \u96f6\u586b\u5145\uff08\u9ed8\u8ba4: 0\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; pool = nn.AvgPool2d(kernel_size=2, stride=2)\n        &gt;&gt;&gt; x = genesis.randn(1, 16, 32, 32)\n        &gt;&gt;&gt; output = pool(x)  # \u5f62\u72b6: (1, 16, 16, 16)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_18","title":"\u5d4c\u5165\u5c42","text":""},{"location":"api/nn/modules.zh/#nnembedding","title":"<code>nn.Embedding</code>","text":"<p>\u5d4c\u5165\u67e5\u627e\u8868\u3002</p> Python<pre><code>class Embedding(Module):\n    \"\"\"\n    \u5d4c\u5165\u67e5\u627e\u8868\u3002\n\n    \u53c2\u6570:\n        num_embeddings: \u8bcd\u6c47\u5927\u5c0f\n        embedding_dim: \u5d4c\u5165\u7ef4\u5ea6\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (*)\u5305\u542b\u7d22\u5f15\n        - \u8f93\u51fa: (*, embedding_dim)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; embed = nn.Embedding(10000, 300)  # 10k\u8bcd\u6c47\uff0c300\u7ef4\u5d4c\u5165\n        &gt;&gt;&gt; indices = genesis.tensor([1, 2, 3, 4])\n        &gt;&gt;&gt; output = embed(indices)  # \u5f62\u72b6: (4, 300)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_19","title":"\u6ce8\u610f\u529b\u5c42","text":""},{"location":"api/nn/modules.zh/#nnmultiheadattention","title":"<code>nn.MultiheadAttention</code>","text":"<p>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002</p> Python<pre><code>class MultiheadAttention(Module):\n    \"\"\"\n    \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3002\n\n    \u53c2\u6570:\n        embed_dim: \u5d4c\u5165\u7ef4\u5ea6\n        num_heads: \u6ce8\u610f\u529b\u5934\u6570\n        dropout: Dropout\u6982\u7387\uff08\u9ed8\u8ba4: 0.0\uff09\n        bias: \u662f\u5426\u6dfb\u52a0\u504f\u7f6e\uff08\u9ed8\u8ba4: True\uff09\n\n    \u5f62\u72b6:\n        - Query: (L, N, E)\u6216(N, L, E)\n        - Key: (S, N, E)\u6216(N, S, E)\n        - Value: (S, N, E)\u6216(N, S, E)\n        - Output: (L, N, E)\u6216(N, L, E)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; attn = nn.MultiheadAttention(embed_dim=512, num_heads=8)\n        &gt;&gt;&gt; x = genesis.randn(10, 32, 512)  # (seq_len, batch, embed_dim)\n        &gt;&gt;&gt; output, weights = attn(x, x, x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_20","title":"\u5bb9\u5668\u6a21\u5757","text":""},{"location":"api/nn/modules.zh/#nnsequential","title":"<code>nn.Sequential</code>","text":"<p>\u6a21\u5757\u7684\u5e8f\u5217\u5bb9\u5668\u3002</p> Python<pre><code>class Sequential(Module):\n    \"\"\"\n    \u6309\u987a\u5e8f\u8fd0\u884c\u6a21\u5757\u7684\u5e8f\u5217\u5bb9\u5668\u3002\n\n    \u53c2\u6570:\n        *modules: \u8981\u5e94\u7528\u7684\u6a21\u5757\u5e8f\u5217\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Sequential(\n        ...     nn.Linear(784, 256),\n        ...     nn.ReLU(),\n        ...     nn.Linear(256, 10)\n        ... )\n        &gt;&gt;&gt; x = genesis.randn(32, 784)\n        &gt;&gt;&gt; output = model(x)  # \u5f62\u72b6: (32, 10)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnmodulelist","title":"<code>nn.ModuleList</code>","text":"<p>\u6a21\u5757\u7684\u5217\u8868\u5bb9\u5668\u3002</p> Python<pre><code>class ModuleList(Module):\n    \"\"\"\n    \u6b63\u786e\u6ce8\u518c\u7684\u6a21\u5757\u5217\u8868\u3002\n\n    \u53c2\u6570:\n        modules: \u53ef\u9009\u7684\u6a21\u5757\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; layers = nn.ModuleList([\n        ...     nn.Linear(10, 10) for _ in range(5)\n        ... ])\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; for layer in layers:\n        ...     x = layer(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnmoduledict","title":"<code>nn.ModuleDict</code>","text":"<p>\u6a21\u5757\u7684\u5b57\u5178\u5bb9\u5668\u3002</p> Python<pre><code>class ModuleDict(Module):\n    \"\"\"\n    \u5e26\u5b57\u7b26\u4e32\u952e\u7684\u6a21\u5757\u5b57\u5178\u3002\n\n    \u53c2\u6570:\n        modules: \u53ef\u9009\u7684\u6a21\u5757\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; layers = nn.ModuleDict({\n        ...     'fc1': nn.Linear(10, 20),\n        ...     'fc2': nn.Linear(20, 10)\n        ... })\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; x = layers['fc1'](x)\n        &gt;&gt;&gt; x = layers['fc2'](x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_21","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"api/nn/modules.zh/#nnmseloss","title":"<code>nn.MSELoss</code>","text":"<p>\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002</p> Python<pre><code>class MSELoss(Module):\n    \"\"\"\n    \u5747\u65b9\u8bef\u5dee\u635f\u5931: L = mean((y_pred - y_true)^2)\n\n    \u53c2\u6570:\n        reduction: 'mean', 'sum', \u6216 'none'\uff08\u9ed8\u8ba4: 'mean'\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss_fn = nn.MSELoss()\n        &gt;&gt;&gt; pred = genesis.randn(32, 10)\n        &gt;&gt;&gt; target = genesis.randn(32, 10)\n        &gt;&gt;&gt; loss = loss_fn(pred, target)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nncrossentropyloss","title":"<code>nn.CrossEntropyLoss</code>","text":"<p>\u5206\u7c7b\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002</p> Python<pre><code>class CrossEntropyLoss(Module):\n    \"\"\"\n    \u591a\u7c7b\u5206\u7c7b\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002\n\n    \u53c2\u6570:\n        weight: \u6bcf\u4e2a\u7c7b\u7684\u624b\u52a8\u91cd\u7f29\u653e\u6743\u91cd\n        reduction: 'mean', 'sum', \u6216 'none'\uff08\u9ed8\u8ba4: 'mean'\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (N, C) \u5176\u4e2dC\u662f\u7c7b\u522b\u6570\n        - \u76ee\u6807: (N,) \u5305\u542b\u7c7b\u522b\u7d22\u5f15\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss_fn = nn.CrossEntropyLoss()\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)  # 32\u4e2a\u6837\u672c\uff0c10\u4e2a\u7c7b\u522b\n        &gt;&gt;&gt; targets = genesis.randint(0, 10, (32,))\n        &gt;&gt;&gt; loss = loss_fn(logits, targets)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#nnbceloss","title":"<code>nn.BCELoss</code>","text":"<p>\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u3002</p> Python<pre><code>class BCELoss(Module):\n    \"\"\"\n    \u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u3002\n\n    \u53c2\u6570:\n        reduction: 'mean', 'sum', \u6216 'none'\uff08\u9ed8\u8ba4: 'mean'\uff09\n\n    \u5f62\u72b6:\n        - \u8f93\u5165: (N, *) \u5176\u4e2d*\u8868\u793a\u4efb\u610f\u6570\u91cf\u7684\u7ef4\u5ea6\n        - \u76ee\u6807: \u4e0e\u8f93\u5165\u76f8\u540c\u5f62\u72b6\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss_fn = nn.BCELoss()\n        &gt;&gt;&gt; pred = genesis.sigmoid(genesis.randn(32, 1))\n        &gt;&gt;&gt; target = genesis.randint(0, 2, (32, 1)).float()\n        &gt;&gt;&gt; loss = loss_fn(pred, target)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_22","title":"\u5de5\u5177","text":""},{"location":"api/nn/modules.zh/#_23","title":"\u6743\u91cd\u521d\u59cb\u5316","text":"Python<pre><code>def init_weights(module: Module, init_type: str = 'xavier'):\n    \"\"\"\n    \u521d\u59cb\u5316\u6a21\u5757\u6743\u91cd\u3002\n\n    \u53c2\u6570:\n        module: \u8981\u521d\u59cb\u5316\u7684\u6a21\u5757\n        init_type: 'xavier', 'kaiming', 'normal', 'uniform'\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Linear(10, 5)\n        &gt;&gt;&gt; init_weights(model, 'xavier')\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_24","title":"\u68af\u5ea6\u88c1\u526a","text":"Python<pre><code>def clip_grad_norm_(parameters, max_norm: float, norm_type: float = 2.0):\n    \"\"\"\n    \u6309\u8303\u6570\u88c1\u526a\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        parameters: \u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        max_norm: \u6700\u5927\u8303\u6570\u503c\n        norm_type: \u8303\u6570\u7c7b\u578b\uff08\u9ed8\u8ba4: 2.0\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    \"\"\"\n\ndef clip_grad_value_(parameters, clip_value: float):\n    \"\"\"\n    \u6309\u503c\u88c1\u526a\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        parameters: \u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        clip_value: \u6700\u5927\u7edd\u5bf9\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules.zh/#_25","title":"\u6784\u5efa\u81ea\u5b9a\u4e49\u6a21\u5757","text":""},{"location":"api/nn/modules.zh/#_26","title":"\u793a\u4f8b\uff1a\u81ea\u5b9a\u4e49\u5c42","text":"Python<pre><code>class CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # \u53c2\u6570\u81ea\u52a8\u8ddf\u8e2a\n        self.weight = nn.Parameter(genesis.randn(out_features, in_features))\n        self.bias = nn.Parameter(genesis.zeros(out_features))\n\n        # \u5b50\u6a21\u5757\u81ea\u52a8\u8ddf\u8e2a\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        # \u5b9a\u4e49\u524d\u5411\u4f20\u64ad\n        x = genesis.matmul(x, self.weight.T) + self.bias\n        x = self.activation(x)\n        return x\n\n# \u4f7f\u7528\nlayer = CustomLayer(10, 5)\nx = genesis.randn(32, 10)\noutput = layer(x)\n</code></pre>"},{"location":"api/nn/modules.zh/#_27","title":"\u793a\u4f8b\uff1a\u81ea\u5b9a\u4e49\u6a21\u578b","text":"Python<pre><code>class ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.bn2(self.conv2(x))\n        x = x + residual  # \u8df3\u8dc3\u8fde\u63a5\n        x = self.relu(x)\n        return x\n\nclass ResNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        # \u6b8b\u5dee\u5757\n        self.layer1 = nn.Sequential(*[ResidualBlock(64) for _ in range(3)])\n        self.layer2 = nn.Sequential(*[ResidualBlock(64) for _ in range(4)])\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(64, num_classes)\n\n    def forward(self, x):\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"api/nn/modules.zh/#_28","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u59cb\u7ec8\u91cd\u5199<code>forward()</code>: \u5728forward\u65b9\u6cd5\u4e2d\u5b9a\u4e49\u8ba1\u7b97</li> <li>\u4f7f\u7528<code>module(input)</code>: \u7edd\u4e0d\u76f4\u63a5\u8c03\u7528forward()</li> <li>\u6ce8\u518c\u53c2\u6570: \u5bf9\u53ef\u5b66\u4e60\u53c2\u6570\u4f7f\u7528nn.Parameter</li> <li>\u8ddf\u8e2a\u5b50\u6a21\u5757: \u5c06\u6a21\u5757\u5206\u914d\u4e3a\u5c5e\u6027\u4ee5\u81ea\u52a8\u8ddf\u8e2a</li> <li>\u5904\u7406\u8bad\u7ec3/\u8bc4\u4f30: \u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u4f7f\u7528\u4e0d\u540c\u884c\u4e3a</li> <li>\u521d\u59cb\u5316\u6743\u91cd: \u9002\u5f53\u7684\u521d\u59cb\u5316\u6539\u5584\u6536\u655b</li> </ol>"},{"location":"api/nn/modules.zh/#_29","title":"\u53e6\u8bf7\u53c2\u9605","text":"<ul> <li>\u51fd\u6570\u5f0fAPI - \u51fd\u6570\u5f0f\u64cd\u4f5c</li> <li>\u4f18\u5316\u5668 - \u8bad\u7ec3\u4f18\u5316\u5668</li> <li>\u81ea\u52a8\u5fae\u5206 - \u81ea\u52a8\u5fae\u5206</li> <li>\u793a\u4f8b - \u5b8c\u6574\u793a\u4f8b</li> </ul>"},{"location":"api/nn/modules_en/","title":"Neural Network Modules (genesis.nn)","text":""},{"location":"api/nn/modules_en/#overview","title":"Overview","text":"<p>The <code>genesis.nn</code> module provides all the building blocks needed to create deep learning models. It follows a modular design where complex models are built by composing simpler components.</p>"},{"location":"api/nn/modules_en/#core-concepts","title":"Core Concepts","text":""},{"location":"api/nn/modules_en/#module-system","title":"Module System","text":"<p>All neural network components inherit from <code>nn.Module</code>, which provides: - Parameter management - Device and dtype handling - State serialization - Forward pass definition</p>"},{"location":"api/nn/modules_en/#parameters","title":"Parameters","text":"<p>Parameters are tensors that are automatically tracked and updated during training: - Automatically registered when assigned as module attributes - Included in <code>module.parameters()</code> for optimizer - Saved/loaded with model state</p>"},{"location":"api/nn/modules_en/#base-classes","title":"Base Classes","text":""},{"location":"api/nn/modules_en/#nnmodule","title":"<code>nn.Module</code>","text":"<p>The base class for all neural network modules.</p> Python<pre><code>class Module:\n    \"\"\"Base class for all neural network modules.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the module.\"\"\"\n        self._modules = {}\n        self._parameters = {}\n        self._buffers = {}\n        self.training = True\n</code></pre>"},{"location":"api/nn/modules_en/#core-methods","title":"Core Methods","text":""},{"location":"api/nn/modules_en/#forward-pass","title":"Forward Pass","text":"Python<pre><code>def forward(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    Define the forward pass computation.\n    Must be overridden by subclasses.\n\n    Example:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def forward(self, x):\n        ...         return x * 2\n    \"\"\"\n    raise NotImplementedError\n\ndef __call__(self, *args, **kwargs) -&gt; Tensor:\n    \"\"\"\n    Make module callable. Calls forward() internally.\n\n    Note: Always use module(input) instead of module.forward(input)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#parameter-management","title":"Parameter Management","text":"Python<pre><code>def parameters(self) -&gt; List[Tensor]:\n    \"\"\"\n    Return all parameters in the module.\n\n    Returns:\n        List of parameter tensors\n\n    Example:\n        &gt;&gt;&gt; model = nn.Linear(10, 5)\n        &gt;&gt;&gt; params = model.parameters()\n        &gt;&gt;&gt; print(len(params))  # 2 (weight and bias)\n    \"\"\"\n\ndef named_parameters(self) -&gt; List[Tuple[str, Tensor]]:\n    \"\"\"\n    Return parameters with their names.\n\n    Returns:\n        List of (name, parameter) tuples\n\n    Example:\n        &gt;&gt;&gt; for name, param in model.named_parameters():\n        ...     print(f\"{name}: {param.shape}\")\n    \"\"\"\n\ndef zero_grad(self) -&gt; None:\n    \"\"\"\n    Zero out gradients of all parameters.\n\n    Example:\n        &gt;&gt;&gt; model.zero_grad()  # Clear all gradients\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#module-hierarchy","title":"Module Hierarchy","text":"Python<pre><code>def add_module(self, name: str, module: Optional[Module]) -&gt; None:\n    \"\"\"\n    Add a child module.\n\n    Args:\n        name: Name for the submodule\n        module: Module instance to add\n\n    Example:\n        &gt;&gt;&gt; model = nn.Module()\n        &gt;&gt;&gt; model.add_module('fc', nn.Linear(10, 5))\n    \"\"\"\n\ndef modules(self) -&gt; Iterator[Module]:\n    \"\"\"Return iterator over all modules (including self).\"\"\"\n\ndef children(self) -&gt; Iterator[Module]:\n    \"\"\"Return iterator over immediate child modules.\"\"\"\n\ndef named_modules(self) -&gt; Iterator[Tuple[str, Module]]:\n    \"\"\"Return iterator over all modules with names.\"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#training-mode","title":"Training Mode","text":"Python<pre><code>def train(self, mode: bool = True) -&gt; Module:\n    \"\"\"\n    Set module to training mode.\n\n    Args:\n        mode: Whether to enable training mode\n\n    Returns:\n        self\n\n    Example:\n        &gt;&gt;&gt; model.train()  # Enable training mode\n        &gt;&gt;&gt; model.train(False)  # Equivalent to model.eval()\n    \"\"\"\n\ndef eval(self) -&gt; Module:\n    \"\"\"\n    Set module to evaluation mode.\n\n    Returns:\n        self\n\n    Example:\n        &gt;&gt;&gt; model.eval()  # Disable dropout, use running stats for BN\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#state-management","title":"State Management","text":"Python<pre><code>def state_dict(self) -&gt; Dict[str, Tensor]:\n    \"\"\"\n    Return state dictionary containing all parameters and buffers.\n\n    Returns:\n        Dictionary mapping parameter names to tensors\n\n    Example:\n        &gt;&gt;&gt; state = model.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'model.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: Dict[str, Tensor]) -&gt; None:\n    \"\"\"\n    Load parameters from state dictionary.\n\n    Args:\n        state_dict: Dictionary of parameters\n\n    Example:\n        &gt;&gt;&gt; state = genesis.load('model.pth')\n        &gt;&gt;&gt; model.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnparameter","title":"<code>nn.Parameter</code>","text":"<p>A special tensor that is automatically registered as a module parameter.</p> Python<pre><code>class Parameter(Tensor):\n    \"\"\"\n    A tensor that is automatically registered as a module parameter.\n\n    Args:\n        data: Tensor data\n        requires_grad: Whether to compute gradients (default: True)\n\n    Example:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def __init__(self):\n        ...         super().__init__()\n        ...         self.weight = nn.Parameter(genesis.randn(10, 5))\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#layer-types","title":"Layer Types","text":""},{"location":"api/nn/modules_en/#linear-layers","title":"Linear Layers","text":""},{"location":"api/nn/modules_en/#nnlinear","title":"<code>nn.Linear</code>","text":"<p>Fully connected layer performing linear transformation.</p> Python<pre><code>class Linear(Module):\n    \"\"\"\n    Linear transformation: y = xW^T + b\n\n    Args:\n        in_features: Size of input features\n        out_features: Size of output features\n        bias: Whether to include bias term (default: True)\n\n    Shape:\n        - Input: (*, in_features)\n        - Output: (*, out_features)\n\n    Example:\n        &gt;&gt;&gt; layer = nn.Linear(20, 30)\n        &gt;&gt;&gt; x = genesis.randn(128, 20)\n        &gt;&gt;&gt; output = layer(x)  # Shape: (128, 30)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#convolutional-layers","title":"Convolutional Layers","text":""},{"location":"api/nn/modules_en/#nnconv2d","title":"<code>nn.Conv2d</code>","text":"<p>2D convolution layer for image processing.</p> Python<pre><code>class Conv2d(Module):\n    \"\"\"\n    2D convolution over input signal.\n\n    Args:\n        in_channels: Number of input channels\n        out_channels: Number of output channels\n        kernel_size: Size of convolving kernel\n        stride: Stride of convolution (default: 1)\n        padding: Zero-padding added to both sides (default: 0)\n        bias: Whether to add bias (default: True)\n\n    Shape:\n        - Input: (N, C_in, H, W)\n        - Output: (N, C_out, H_out, W_out)\n\n    Example:\n        &gt;&gt;&gt; conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        &gt;&gt;&gt; x = genesis.randn(32, 3, 224, 224)\n        &gt;&gt;&gt; output = conv(x)  # Shape: (32, 64, 224, 224)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#activation-functions","title":"Activation Functions","text":""},{"location":"api/nn/modules_en/#nnrelu","title":"<code>nn.ReLU</code>","text":"<p>Rectified Linear Unit activation.</p> Python<pre><code>class ReLU(Module):\n    \"\"\"\n    ReLU activation: f(x) = max(0, x)\n\n    Args:\n        inplace: Whether to modify input in-place (default: False)\n\n    Example:\n        &gt;&gt;&gt; relu = nn.ReLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = relu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnsilu-swish","title":"<code>nn.SiLU</code> (Swish)","text":"<p>Sigmoid Linear Unit activation.</p> Python<pre><code>class SiLU(Module):\n    \"\"\"\n    SiLU/Swish activation: f(x) = x * sigmoid(x)\n\n    Example:\n        &gt;&gt;&gt; silu = nn.SiLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; output = silu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnsoftmax","title":"<code>nn.Softmax</code>","text":"<p>Softmax activation for multi-class classification.</p> Python<pre><code>class Softmax(Module):\n    \"\"\"\n    Softmax activation: softmax(x_i) = exp(x_i) / \u03a3 exp(x_j)\n\n    Args:\n        dim: Dimension along which to apply softmax\n\n    Example:\n        &gt;&gt;&gt; softmax = nn.Softmax(dim=-1)\n        &gt;&gt;&gt; x = genesis.randn(10, 5)\n        &gt;&gt;&gt; output = softmax(x)  # Each row sums to 1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#normalization-layers","title":"Normalization Layers","text":""},{"location":"api/nn/modules_en/#nnbatchnorm1d","title":"<code>nn.BatchNorm1d</code>","text":"<p>Batch normalization for 1D inputs.</p> Python<pre><code>class BatchNorm1d(Module):\n    \"\"\"\n    Batch normalization over 1D input.\n\n    Args:\n        dim: Number of features to normalize\n        eps: Small value for numerical stability (default: 1e-5)\n        momentum: Momentum for running stats (default: 0.1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Shape:\n        - Input: (N, C) where C = dim\n        - Output: Same as input\n\n    Example:\n        &gt;&gt;&gt; bn = nn.BatchNorm1d(100)\n        &gt;&gt;&gt; x = genesis.randn(20, 100)\n        &gt;&gt;&gt; output = bn(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnlayernorm","title":"<code>nn.LayerNorm</code>","text":"<p>Layer normalization.</p> Python<pre><code>class LayerNorm(Module):\n    \"\"\"\n    Layer normalization over last dimension.\n\n    Args:\n        dim: Dimension size to normalize\n        eps: Small value for numerical stability (default: 1e-5)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Shape:\n        - Input: (..., dim)\n        - Output: Same as input\n\n    Example:\n        &gt;&gt;&gt; ln = nn.LayerNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ln(x)  # Normalize over last dimension\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#dropout-layers","title":"Dropout Layers","text":""},{"location":"api/nn/modules_en/#nndropout","title":"<code>nn.Dropout</code>","text":"<p>Dropout for regularization.</p> Python<pre><code>class Dropout(Module):\n    \"\"\"\n    Randomly zero out elements for regularization.\n\n    Args:\n        p: Probability of zeroing an element (default: 0.5)\n        inplace: Whether to modify input in-place (default: False)\n\n    Example:\n        &gt;&gt;&gt; dropout = nn.Dropout(p=0.2)\n        &gt;&gt;&gt; x = genesis.randn(20, 16)\n        &gt;&gt;&gt; output = dropout(x)  # Training mode: randomly zero 20% of elements\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#embedding-layers","title":"Embedding Layers","text":""},{"location":"api/nn/modules_en/#nnembedding","title":"<code>nn.Embedding</code>","text":"<p>Embedding lookup table.</p> Python<pre><code>class Embedding(Module):\n    \"\"\"\n    Embedding lookup table.\n\n    Args:\n        num_embeddings: Size of vocabulary\n        embedding_dim: Dimension of embeddings\n\n    Shape:\n        - Input: (*) containing indices\n        - Output: (*, embedding_dim)\n\n    Example:\n        &gt;&gt;&gt; embed = nn.Embedding(10000, 300)  # 10k vocab, 300-dim embeddings\n        &gt;&gt;&gt; indices = genesis.tensor([1, 2, 3, 4])\n        &gt;&gt;&gt; output = embed(indices)  # Shape: (4, 300)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#attention-layers","title":"Attention Layers","text":""},{"location":"api/nn/modules_en/#nnmultiheadattention","title":"<code>nn.MultiheadAttention</code>","text":"<p>Multi-head attention mechanism.</p> Python<pre><code>class MultiheadAttention(Module):\n    \"\"\"\n    Multi-head attention mechanism.\n\n    Args:\n        dim: Feature dimension (default: 64)\n        heads: Number of attention heads (default: 1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Note: Uses QKV projection matrix internally.\n\n    Example:\n        &gt;&gt;&gt; attn = nn.MultiheadAttention(dim=64, heads=8)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 64)  # (batch, seq_len, dim)\n        &gt;&gt;&gt; output, attention_weights = attn(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnfusedmultiheadattention","title":"<code>nn.FusedMultiheadAttention</code>","text":"<p>Fused multi-head attention with optimized kernels.</p> Python<pre><code>class FusedMultiheadAttention(Module):\n    \"\"\"\n    Fused multi-head attention using optimized kernels.\n\n    Args:\n        dim: Feature dimension (default: 64)\n        heads: Number of attention heads (default: 1)\n        device: Device placement (optional)\n        dtype: Data type (default: \"float32\")\n\n    Note: Returns None for attention weights when using fused kernels.\n\n    Example:\n        &gt;&gt;&gt; attn = nn.FusedMultiheadAttention(dim=64, heads=8)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 64)  # (batch, seq_len, dim)\n        &gt;&gt;&gt; output, _ = attn(x)  # weights is None\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#container-modules","title":"Container Modules","text":""},{"location":"api/nn/modules_en/#nnsequential","title":"<code>nn.Sequential</code>","text":"<p>Sequential container for modules.</p> Python<pre><code>class Sequential(Module):\n    \"\"\"\n    Sequential container that runs modules in order.\n\n    Args:\n        *modules: Sequence of modules to apply\n\n    Example:\n        &gt;&gt;&gt; model = nn.Sequential(\n        ...     nn.Linear(784, 256),\n        ...     nn.ReLU(),\n        ...     nn.Linear(256, 10)\n        ... )\n        &gt;&gt;&gt; x = genesis.randn(32, 784)\n        &gt;&gt;&gt; output = model(x)  # Shape: (32, 10)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnmodulelist","title":"<code>nn.ModuleList</code>","text":"<p>List container for modules.</p> Python<pre><code>class ModuleList(Module):\n    \"\"\"\n    List of modules that are properly registered.\n\n    Args:\n        modules: Optional list of modules\n\n    Example:\n        &gt;&gt;&gt; layers = nn.ModuleList([\n        ...     nn.Linear(10, 10) for _ in range(5)\n        ... ])\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; for layer in layers:\n        ...     x = layer(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnmoduledict","title":"<code>nn.ModuleDict</code>","text":"<p>Dictionary container for modules.</p> Python<pre><code>class ModuleDict(Module):\n    \"\"\"\n    Dictionary of modules with string keys.\n\n    Args:\n        modules: Optional dict of modules\n\n    Example:\n        &gt;&gt;&gt; layers = nn.ModuleDict({\n        ...     'fc1': nn.Linear(10, 20),\n        ...     'fc2': nn.Linear(20, 10)\n        ... })\n        &gt;&gt;&gt; x = genesis.randn(32, 10)\n        &gt;&gt;&gt; x = layers['fc1'](x)\n        &gt;&gt;&gt; x = layers['fc2'](x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#loss-functions","title":"Loss Functions","text":""},{"location":"api/nn/modules_en/#nnsoftmaxloss","title":"<code>nn.SoftmaxLoss</code>","text":"<p>Softmax cross-entropy loss.</p> Python<pre><code>class SoftmaxLoss(Module):\n    \"\"\"\n    Softmax cross-entropy loss for classification.\n\n    Handles label masking with -1 values.\n\n    Shape:\n        - Input: (N, C) where C is number of classes\n        - Target: (N,) containing class indices or -1 for masked positions\n\n    Example:\n        &gt;&gt;&gt; loss_fn = nn.SoftmaxLoss()\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)  # 32 samples, 10 classes\n        &gt;&gt;&gt; targets = genesis.randint(0, 10, (32,))\n        &gt;&gt;&gt; loss = loss_fn(logits, targets)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#additional-modules","title":"Additional Modules","text":""},{"location":"api/nn/modules_en/#nnresidual","title":"<code>nn.Residual</code>","text":"<p>Residual connection wrapper.</p> Python<pre><code>class Residual(Module):\n    \"\"\"\n    Residual connection wrapper.\n\n    Args:\n        fn: Module to wrap with residual connection\n\n    Example:\n        &gt;&gt;&gt; layer = nn.Residual(nn.Linear(256, 256))\n        &gt;&gt;&gt; x = genesis.randn(32, 256)\n        &gt;&gt;&gt; output = layer(x)  # output = fn(x) + x\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnfusedlayernorm","title":"<code>nn.FusedLayerNorm</code>","text":"<p>Fused layer normalization with optimized kernels.</p> Python<pre><code>class FusedLayerNorm(Module):\n    \"\"\"\n    Fused layer normalization for better performance.\n\n    Args:\n        dim: Dimension to normalize\n        eps: Small value for stability (default: 1e-6)\n\n    Example:\n        &gt;&gt;&gt; ln = nn.FusedLayerNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ln(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnrmsnorm","title":"<code>nn.RMSNorm</code>","text":"<p>Root Mean Square normalization.</p> Python<pre><code>class RMSNorm(Module):\n    \"\"\"\n    RMS normalization (used in some LLMs).\n\n    Args:\n        dim: Dimension to normalize\n        eps: Small value for stability (default: 1e-6)\n\n    Example:\n        &gt;&gt;&gt; rms = nn.RMSNorm(768)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = rms(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnrotaryembedding","title":"<code>nn.RotaryEmbedding</code>","text":"<p>Rotary position embeddings.</p> Python<pre><code>class RotaryEmbedding(Module):\n    \"\"\"\n    Rotary position embeddings for transformers.\n\n    Args:\n        dim: Embedding dimension\n        max_position_embeddings: Maximum sequence length (default: 2048)\n        base: Base for frequency computation (default: 10000)\n\n    Example:\n        &gt;&gt;&gt; rope = nn.RotaryEmbedding(64, max_position_embeddings=2048)\n        &gt;&gt;&gt; x = genesis.randn(1, 8, 100, 64)  # (batch, heads, seq, dim)\n        &gt;&gt;&gt; cos, sin = rope(x, seq_len=100)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#nnfeedfowardswiglu","title":"<code>nn.FeedFowardSwiGLU</code>","text":"<p>SwiGLU feedforward network.</p> Python<pre><code>class FeedFowardSwiGLU(Module):\n    \"\"\"\n    SwiGLU feedforward network (https://arxiv.org/pdf/2002.05202.pdf).\n\n    Args:\n        dim: Input/output dimension\n        hidden_dim: Hidden layer dimension\n\n    Example:\n        &gt;&gt;&gt; ff = nn.FeedFowardSwiGLU(768, 3072)\n        &gt;&gt;&gt; x = genesis.randn(32, 100, 768)\n        &gt;&gt;&gt; output = ff(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules_en/#building-custom-modules","title":"Building Custom Modules","text":""},{"location":"api/nn/modules_en/#example-custom-layer","title":"Example: Custom Layer","text":"Python<pre><code>class CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        # Parameters are automatically tracked\n        self.weight = nn.Parameter(genesis.randn(out_features, in_features))\n        self.bias = nn.Parameter(genesis.zeros(out_features))\n\n        # Submodules are automatically tracked\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        # Define forward pass\n        x = genesis.matmul(x, self.weight.T) + self.bias\n        x = self.activation(x)\n        return x\n\n# Usage\nlayer = CustomLayer(10, 5)\nx = genesis.randn(32, 10)\noutput = layer(x)\n</code></pre>"},{"location":"api/nn/modules_en/#example-custom-model","title":"Example: Custom Model","text":"Python<pre><code>class SimpleBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear2 = nn.Linear(dim, dim)\n        self.norm = nn.LayerNorm(dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        x = self.relu(self.linear1(x))\n        x = self.linear2(x)\n        x = self.norm(x + residual)  # Skip connection\n        return x\n\nclass SimpleTransformer(nn.Module):\n    def __init__(self, dim=768, num_classes=10):\n        super().__init__()\n        self.embedding = nn.Embedding(10000, dim)\n        self.layers = nn.ModuleList([\n            SimpleBlock(dim) for _ in range(6)\n        ])\n        self.norm = nn.LayerNorm(dim)\n        self.fc = nn.Linear(dim, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)\n        x = x.mean(dim=1)  # Global average pooling\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"api/nn/modules_en/#best-practices","title":"Best Practices","text":"<ol> <li>Always override <code>forward()</code>: Define computation in forward method</li> <li>Use <code>module(input)</code>: Never call forward() directly</li> <li>Register parameters: Use nn.Parameter for learnable parameters</li> <li>Track submodules: Assign modules as attributes for automatic tracking</li> <li>Handle training/eval: Use different behavior for training vs evaluation</li> <li>Initialize weights: Proper initialization improves convergence</li> </ol>"},{"location":"api/nn/modules_en/#see-also","title":"See Also","text":"<ul> <li>Functional API - Functional operations</li> <li>Optimizers - Training optimizers</li> <li>Autograd - Automatic differentiation</li> <li>Examples - Complete examples</li> </ul>"},{"location":"api/optim/lr_scheduler.zh/","title":"\u5b66\u4e60\u7387\u8c03\u5ea6\u5668","text":"<p>Genesis\u63d0\u4f9b\u4e86\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u6765\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u7387\uff0c\u8fd9\u5bf9\u4e8e\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u5b9e\u73b0\u6700\u4f73\u6536\u655b\u81f3\u5173\u91cd\u8981\u3002</p>"},{"location":"api/optim/lr_scheduler.zh/#_2","title":"\u6982\u8ff0","text":"<p>\u5b66\u4e60\u7387\u8c03\u5ea6\u662f\u4e00\u79cd\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u6574\u5b66\u4e60\u7387\u7684\u6280\u672f\u3002Genesis\u63d0\u4f9b\u4e86\u4e0ePyTorch\u517c\u5bb9\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6a21\u578b\u6536\u655b\u3002</p>"},{"location":"api/optim/lr_scheduler.zh/#_3","title":"\u53ef\u7528\u8c03\u5ea6\u5668","text":""},{"location":"api/optim/lr_scheduler.zh/#lambdalr","title":"LambdaLR","text":"<p><code>LambdaLR</code>\u8c03\u5ea6\u5668\u5141\u8bb8\u4f60\u5b9a\u4e49\u81ea\u5b9a\u4e49\u51fd\u6570\u6765\u4fee\u6539\u6bcf\u4e2a\u8f6e\u6b21\u7684\u5b66\u4e60\u7387\u3002</p> Python<pre><code>import genesis.optim as optim\n\nclass LambdaLR:\n    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n        \"\"\"\n        \u901a\u8fc7lr_lambda\u51fd\u6570\u7ed9\u51fa\u7684\u56e0\u5b50\u4e58\u4ee5\u5b66\u4e60\u7387\u3002\n\n        Args:\n            optimizer: \u5305\u88c5\u7684\u4f18\u5316\u5668\n            lr_lambda: \u8ba1\u7b97\u4e58\u6cd5\u56e0\u5b50\u7684\u51fd\u6570\u6216\u51fd\u6570\u5217\u8868\n            last_epoch: \u6700\u540e\u4e00\u4e2a\u8f6e\u6b21\u7684\u7d22\u5f15\n            verbose: \u5982\u679c\u4e3aTrue\uff0c\u4e3a\u6bcf\u6b21\u66f4\u65b0\u6253\u5370\u6d88\u606f\n        \"\"\"\n</code></pre> <p>\u4f7f\u7528\u793a\u4f8b\uff1a Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u521b\u5efa\u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# \u5b9a\u4e49\u5b66\u4e60\u7387\u8c03\u5ea6\u51fd\u6570\ndef lr_lambda(epoch):\n    # \u6bcf10\u4e2a\u8f6e\u6b21\u5c06\u5b66\u4e60\u7387\u8870\u51cf0.95\u500d\n    return 0.95 ** (epoch // 10)\n\n# \u521b\u5efa\u8c03\u5ea6\u5668\nscheduler = optim.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(100):\n    # \u8fd9\u91cc\u662f\u8bad\u7ec3\u4ee3\u7801\n    loss = train_one_epoch(model, dataloader, optimizer)\n\n    # \u8c03\u5ea6\u5668\u6b65\u8fdb\n    scheduler.step()\n    print(f\"\u8f6e\u6b21 {epoch}: lr={scheduler.get_last_lr()}\")\n</code></pre></p>"},{"location":"api/optim/lr_scheduler.zh/#_4","title":"\u4f59\u5f26\u9000\u706b\u4e0e\u9884\u70ed","text":"<p><code>get_cosine_schedule_with_warmup</code>\u51fd\u6570\u521b\u5efa\u4e00\u4e2a\u5e26\u6709\u7ebf\u6027\u9884\u70ed\u7684\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\u3002</p> Python<pre><code>def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    \"\"\"\n    \u521b\u5efa\u5177\u6709\u7ebf\u6027\u9884\u70ed\u548c\u4f59\u5f26\u8870\u51cf\u7684\u8c03\u5ea6\u3002\n\n    Args:\n        optimizer: \u5305\u88c5\u7684\u4f18\u5316\u5668\n        num_warmup_steps: \u9884\u70ed\u9636\u6bb5\u7684\u6b65\u6570\n        num_training_steps: \u603b\u8bad\u7ec3\u6b65\u6570\n\n    Returns:\n        LambdaLR\u8c03\u5ea6\u5668\u5bf9\u8c61\n    \"\"\"\n</code></pre> <p>\u4f7f\u7528\u793a\u4f8b\uff1a Python<pre><code>import genesis.optim as optim\n\n# \u8bad\u7ec3\u914d\u7f6e\nnum_epochs = 100\nsteps_per_epoch = 1000\ntotal_steps = num_epochs * steps_per_epoch\nwarmup_steps = total_steps // 10  # 10%\u9884\u70ed\n\n# \u521b\u5efa\u4f18\u5316\u5668\u548c\u8c03\u5ea6\u5668\noptimizer = optim.AdamW(model.parameters(), lr=5e-4)\nscheduler = optim.get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# \u9010\u6b65\u8c03\u5ea6\u7684\u8bad\u7ec3\u5faa\u73af\nstep = 0\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # \u524d\u5411\u4f20\u64ad\u548c\u4f18\u5316\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # \u6bcf\u4e2a\u6279\u6b21\u8c03\u5ea6\u5668\u6b65\u8fdb\n        scheduler.step()\n        step += 1\n\n        if step % 100 == 0:\n            print(f\"\u6b65\u9aa4 {step}: lr={scheduler.get_last_lr():.6f}\")\n</code></pre></p>"},{"location":"api/optim/lr_scheduler.zh/#_5","title":"\u8c03\u5ea6\u5668\u65b9\u6cd5","text":"<p>\u6240\u6709\u8c03\u5ea6\u5668\u63d0\u4f9b\u4ee5\u4e0b\u65b9\u6cd5\uff1a</p>"},{"location":"api/optim/lr_scheduler.zh/#step","title":"step()","text":"Python<pre><code>def step(self, epoch=None):\n    \"\"\"\n    \u6839\u636e\u8c03\u5ea6\u66f4\u65b0\u5b66\u4e60\u7387\u3002\n\n    Args:\n        epoch: \u5f53\u524d\u8f6e\u6b21\uff08\u53ef\u9009\uff0c\u5982\u679c\u4e3aNone\u4f7f\u7528\u5185\u90e8\u8ba1\u6570\u5668\uff09\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#get_last_lr","title":"get_last_lr()","text":"Python<pre><code>def get_last_lr(self):\n    \"\"\"\n    \u8fd4\u56de\u6700\u540e\u8ba1\u7b97\u7684\u5b66\u4e60\u7387\u3002\n\n    Returns:\n        \u5f53\u524d\u5b66\u4e60\u7387\u503c\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#state_dict","title":"state_dict()","text":"Python<pre><code>def state_dict(self):\n    \"\"\"\n    \u5c06\u8c03\u5ea6\u5668\u7684\u72b6\u6001\u4f5c\u4e3a\u5b57\u5178\u8fd4\u56de\u3002\n\n    Returns:\n        \u5305\u542b\u8c03\u5ea6\u5668\u72b6\u6001\u7684\u5b57\u5178\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#load_state_dict","title":"load_state_dict()","text":"Python<pre><code>def load_state_dict(self, state_dict):\n    \"\"\"\n    \u4ece\u5b57\u5178\u52a0\u8f7d\u8c03\u5ea6\u5668\u72b6\u6001\u3002\n\n    Args:\n        state_dict: \u5305\u542b\u8c03\u5ea6\u5668\u72b6\u6001\u7684\u5b57\u5178\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_6","title":"\u5e38\u89c1\u6a21\u5f0f","text":""},{"location":"api/optim/lr_scheduler.zh/#_7","title":"\u6307\u6570\u8870\u51cf","text":"Python<pre><code># \u6bcf\u4e2a\u8f6e\u6b21\u5b66\u4e60\u7387\u8870\u51cf0.95\nscheduler = optim.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_8","title":"\u9636\u68af\u8870\u51cf","text":"Python<pre><code># \u6bcf30\u4e2a\u8f6e\u6b21\u5c06\u5b66\u4e60\u7387\u51cf\u534a\ndef step_decay(epoch):\n    return 0.5 ** (epoch // 30)\n\nscheduler = optim.LambdaLR(optimizer, lr_lambda=step_decay)\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_9","title":"\u591a\u9879\u5f0f\u8870\u51cf","text":"Python<pre><code># \u591a\u9879\u5f0f\u8870\u51cf\u5230\u96f6\ndef poly_decay(epoch, total_epochs=100, power=0.9):\n    return (1 - epoch / total_epochs) ** power\n\nscheduler = optim.LambdaLR(optimizer, lr_lambda=lambda epoch: poly_decay(epoch))\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_10","title":"\u4f59\u5f26\u91cd\u542f","text":"Python<pre><code>import math\n\ndef cosine_restart(epoch, restart_period=50):\n    epoch_in_cycle = epoch % restart_period\n    return 0.5 * (1 + math.cos(math.pi * epoch_in_cycle / restart_period))\n\nscheduler = optim.LambdaLR(optimizer, lr_lambda=cosine_restart)\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_11","title":"\u4e0e\u8bad\u7ec3\u96c6\u6210","text":""},{"location":"api/optim/lr_scheduler.zh/#_12","title":"\u57fa\u7840\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u8bbe\u7f6e\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.get_cosine_schedule_with_warmup(\n    optimizer, num_warmup_steps=1000, num_training_steps=10000\n)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(data)\n        loss = nn.CrossEntropyLoss()(outputs, targets)\n\n        # \u53cd\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()  # \u66f4\u65b0\u5b66\u4e60\u7387\n\n        # \u65e5\u5fd7\u8bb0\u5f55\n        if batch_idx % 100 == 0:\n            current_lr = scheduler.get_last_lr()\n            print(f'\u8f6e\u6b21: {epoch}, \u6279\u6b21: {batch_idx}, LR: {current_lr:.6f}, \u635f\u5931: {loss.item():.4f}')\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_13","title":"\u68c0\u67e5\u70b9\u96c6\u6210","text":"Python<pre><code>import genesis\n\n# \u4fdd\u5b58\u8c03\u5ea6\u5668\u72b6\u6001\u4e0e\u6a21\u578b\u68c0\u67e5\u70b9\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'epoch': epoch,\n    'loss': loss\n}\ngenesis.save_checkpoint(checkpoint, 'checkpoint.pth')\n\n# \u52a0\u8f7d\u8c03\u5ea6\u5668\u72b6\u6001\ncheckpoint = genesis.load_checkpoint('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_14","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u9009\u62e9\u6b63\u786e\u7684\u8c03\u5ea6\uff1a</li> <li>\u5927\u591a\u6570\u5e94\u7528\u4f7f\u7528\u4f59\u5f26\u9000\u706b</li> <li>\u4e3atransformer\u6a21\u578b\u6dfb\u52a0\u9884\u70ed</li> <li> <p>\u5fae\u8c03\u65f6\u4f7f\u7528\u9636\u68af\u8870\u51cf</p> </li> <li> <p>\u9884\u70ed\u9636\u6bb5\uff1a</p> </li> <li>\u5bf9\u5927\u6279\u91cf\u5927\u5c0f\u81f3\u5173\u91cd\u8981</li> <li>\u5efa\u8bae\u7528\u4e8etransformer\u67b6\u6784</li> <li> <p>\u901a\u5e38\u4e3a\u603b\u8bad\u7ec3\u6b65\u6570\u76845-10%</p> </li> <li> <p>\u76d1\u63a7\uff1a</p> </li> <li>\u8bb0\u5f55\u5b66\u4e60\u7387\u503c</li> <li>\u7ed8\u5236\u5b66\u4e60\u7387\u8c03\u5ea6</li> <li> <p>\u8bad\u7ec3\u671f\u95f4\u76d1\u63a7\u9a8c\u8bc1\u635f\u5931</p> </li> <li> <p>\u68c0\u67e5\u70b9\u4fdd\u5b58\uff1a</p> </li> <li>\u59cb\u7ec8\u4fdd\u5b58\u8c03\u5ea6\u5668\u72b6\u6001</li> <li>\u4ee5\u6b63\u786e\u7684\u5b66\u4e60\u7387\u6062\u590d\u8bad\u7ec3</li> <li>\u5bf9\u957f\u65f6\u95f4\u8bad\u7ec3\u8fd0\u884c\u81f3\u5173\u91cd\u8981</li> </ol>"},{"location":"api/optim/lr_scheduler.zh/#_15","title":"\u793a\u4f8b","text":""},{"location":"api/optim/lr_scheduler.zh/#transformer","title":"Transformer\u8bad\u7ec3\u8c03\u5ea6","text":"Python<pre><code># \u5178\u578b\u7684transformer\u8bad\u7ec3\u8c03\u5ea6\ndef get_transformer_schedule(optimizer, d_model=512, warmup_steps=4000):\n    def lr_lambda(step):\n        if step == 0:\n            return 0\n        return min(step ** -0.5, step * warmup_steps ** -1.5) * (d_model ** -0.5)\n\n    return optim.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\nscheduler = get_transformer_schedule(optimizer, d_model=512, warmup_steps=4000)\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#_16","title":"\u5b66\u4e60\u7387\u8303\u56f4\u6d4b\u8bd5","text":"Python<pre><code># \u627e\u5230\u6700\u4f73\u5b66\u4e60\u7387\u8303\u56f4\ndef lr_range_test(model, optimizer, start_lr=1e-7, end_lr=10, num_it=100):\n    lrs = []\n    losses = []\n\n    lr_lambda = lambda step: (end_lr / start_lr) ** (step / num_it)\n    scheduler = optim.LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n    for i in range(num_it):\n        # \u8bad\u7ec3\u6b65\u9aa4\n        loss = train_step(model, batch)\n        losses.append(loss)\n        lrs.append(scheduler.get_last_lr())\n\n        scheduler.step()\n\n        if loss &gt; 4 * min(losses):  # \u5982\u679c\u635f\u5931\u7206\u70b8\u5219\u505c\u6b62\n            break\n\n    return lrs, losses\n</code></pre>"},{"location":"api/optim/lr_scheduler.zh/#pytorch","title":"\u4ecePyTorch\u8fc1\u79fb","text":"<p>Genesis\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u8bbe\u8ba1\u4e3aPyTorch\u8c03\u5ea6\u5668\u7684\u76f4\u63a5\u66ff\u4ee3\uff1a</p> Python<pre><code># PyTorch\u4ee3\u7801\nimport torch.optim as optim\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n\n# Genesis\u7b49\u6548\u4ee3\u7801\nimport genesis.optim as optim\nscheduler = optim.LambdaLR(\n    optimizer, \n    lr_lambda=lambda epoch: 0.5 * (1 + math.cos(math.pi * epoch / 100))\n)\n</code></pre> <p>API\u517c\u5bb9\uff0c\u4f7f\u5f97\u5c06\u73b0\u6709PyTorch\u8bad\u7ec3\u811a\u672c\u8fc1\u79fb\u5230Genesis\u53d8\u5f97\u5bb9\u6613\u3002</p>"},{"location":"api/optim/lr_scheduler_en/","title":"Learning Rate Schedulers","text":"<p>Genesis provides learning rate schedulers to adjust the learning rate during training, which is crucial for achieving optimal convergence in deep learning models.</p>"},{"location":"api/optim/lr_scheduler_en/#overview","title":"Overview","text":"<p>Learning rate scheduling is a technique used to adjust the learning rate throughout the training process. Genesis provides a simple and effective scheduler implementation.</p>"},{"location":"api/optim/lr_scheduler_en/#available-schedulers","title":"Available Schedulers","text":""},{"location":"api/optim/lr_scheduler_en/#lambdalr","title":"LambdaLR","text":"<p>The <code>LambdaLR</code> scheduler allows you to define a custom function to modify the learning rate at each epoch.</p> Python<pre><code>class LambdaLR:\n    def __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False):\n        \"\"\"\n        Multiply learning rate by a factor given by lr_lambda function.\n\n        Args:\n            optimizer: Wrapped optimizer\n            lr_lambda: Function to compute multiplicative factor\n            last_epoch: The index of last epoch (default: -1)\n            verbose: If True, prints a message for each update (not used)\n\n        Attributes:\n            base_lrs: Base learning rate from optimizer\n        \"\"\"\n</code></pre> <p>Implementation Details: - The scheduler stores the base learning rate from the optimizer - Each step multiplies the base learning rate by the lambda function output - Directly modifies the optimizer's <code>lr</code> attribute</p> <p>Usage Example: Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.optim.lr_scheduler import LambdaLR\n\n# Create model and optimizer\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Define learning rate schedule function\ndef lr_lambda(epoch):\n    # Decay learning rate by factor of 0.95 every 10 epochs\n    return 0.95 ** (epoch // 10)\n\n# Create scheduler\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n# Training loop\nfor epoch in range(100):\n    # Training code here\n    loss = train_one_epoch(model, dataloader, optimizer)\n\n    # Step scheduler\n    scheduler.step()\n    print(f\"Epoch {epoch}: lr={scheduler.get_last_lr()}\")\n</code></pre></p>"},{"location":"api/optim/lr_scheduler_en/#cosine-annealing-with-warmup","title":"Cosine Annealing with Warmup","text":"<p>The <code>get_cosine_schedule_with_warmup</code> function creates a cosine annealing schedule with linear warmup.</p> Python<pre><code>def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n    \"\"\"\n    Create a schedule with linear warmup and cosine decay.\n\n    Args:\n        optimizer: Wrapped optimizer\n        num_warmup_steps: Number of steps for warmup phase\n        num_training_steps: Total number of training steps\n\n    Returns:\n        LambdaLR scheduler object\n\n    Schedule:\n        - Linear warmup from 0 to base_lr over num_warmup_steps\n        - Cosine annealing from base_lr to 0 over remaining steps\n    \"\"\"\n</code></pre> <p>Usage Example: Python<pre><code>import genesis.optim as optim\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\n# Training configuration\nnum_epochs = 100\nsteps_per_epoch = 1000\ntotal_steps = num_epochs * steps_per_epoch\nwarmup_steps = total_steps // 10  # 10% warmup\n\n# Create optimizer and scheduler\noptimizer = optim.AdamW(model.parameters(), lr=5e-4)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Training loop with per-step scheduling\nstep = 0\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Forward pass and optimization\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Step scheduler every batch\n        scheduler.step()\n        step += 1\n\n        if step % 100 == 0:\n            print(f\"Step {step}: lr={scheduler.get_last_lr():.6f}\")\n</code></pre></p>"},{"location":"api/optim/lr_scheduler_en/#scheduler-methods","title":"Scheduler Methods","text":""},{"location":"api/optim/lr_scheduler_en/#step","title":"step()","text":"Python<pre><code>def step(self, epoch=None):\n    \"\"\"\n    Update learning rate according to schedule.\n\n    Args:\n        epoch: Current epoch (optional, uses internal counter if None)\n\n    Updates:\n        - Increments last_epoch if epoch is None\n        - Computes new learning rate using lr_lambda\n        - Updates optimizer.lr directly\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#get_last_lr","title":"get_last_lr()","text":"Python<pre><code>def get_last_lr(self):\n    \"\"\"\n    Return the last computed learning rate.\n\n    Returns:\n        float: Current learning rate value\n\n    Note: Returns the value stored in _last_lr attribute\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#state_dict","title":"state_dict()","text":"Python<pre><code>def state_dict(self):\n    \"\"\"\n    Return the state of the scheduler as a dict.\n\n    Returns:\n        Dictionary containing:\n        - last_epoch: Current epoch counter\n        - base_lrs: Base learning rate\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#load_state_dict","title":"load_state_dict()","text":"Python<pre><code>def load_state_dict(self, state_dict):\n    \"\"\"\n    Load scheduler state from dict.\n\n    Args:\n        state_dict: Dictionary containing scheduler state\n\n    Loads:\n        - last_epoch: Restore epoch counter\n        - base_lrs: Restore base learning rate\n    \"\"\"\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#common-patterns","title":"Common Patterns","text":""},{"location":"api/optim/lr_scheduler_en/#exponential-decay","title":"Exponential Decay","text":"Python<pre><code># Decay learning rate by 0.95 every epoch\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#step-decay","title":"Step Decay","text":"Python<pre><code># Reduce learning rate by half every 30 epochs\ndef step_decay(epoch):\n    return 0.5 ** (epoch // 30)\n\nscheduler = LambdaLR(optimizer, lr_lambda=step_decay)\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#polynomial-decay","title":"Polynomial Decay","text":"Python<pre><code># Polynomial decay to zero\ndef poly_decay(epoch, total_epochs=100, power=0.9):\n    return (1 - epoch / total_epochs) ** power\n\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: poly_decay(epoch))\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#cosine-with-restarts","title":"Cosine with Restarts","text":"Python<pre><code>import math\n\ndef cosine_restart(epoch, restart_period=50):\n    epoch_in_cycle = epoch % restart_period\n    return 0.5 * (1 + math.cos(math.pi * epoch_in_cycle / restart_period))\n\nscheduler = LambdaLR(optimizer, lr_lambda=cosine_restart)\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#integration-with-training","title":"Integration with Training","text":""},{"location":"api/optim/lr_scheduler_en/#basic-training-loop","title":"Basic Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.optim.lr_scheduler import LambdaLR\n\n# Setup\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Simple exponential decay\nscheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Forward pass\n        outputs = model(data)\n        loss = nn.SoftmaxLoss()(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Update learning rate after each epoch\n    scheduler.step()\n    current_lr = scheduler.get_last_lr()\n    print(f'Epoch: {epoch}, LR: {current_lr:.6f}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#checkpoint-integration","title":"Checkpoint Integration","text":"Python<pre><code>import genesis\n\n# Save scheduler state with model checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'scheduler_state_dict': scheduler.state_dict(),\n    'epoch': epoch,\n    'loss': loss\n}\ngenesis.save(checkpoint, 'checkpoint.pth')\n\n# Load scheduler state\ncheckpoint = genesis.load('checkpoint.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Schedule: </li> <li>Use cosine annealing for most applications</li> <li>Add warmup for transformer models</li> <li> <p>Use step decay for fine-tuning</p> </li> <li> <p>Warmup Phase:</p> </li> <li>Essential for large batch sizes</li> <li>Recommended for transformer architectures</li> <li> <p>Typically 5-10% of total training steps</p> </li> <li> <p>Monitoring:</p> </li> <li>Log learning rate values</li> <li>Plot learning rate schedule</li> <li> <p>Monitor validation loss during training</p> </li> <li> <p>Checkpointing:</p> </li> <li>Always save scheduler state</li> <li>Resume training with correct learning rate</li> <li>Essential for long training runs</li> </ol>"},{"location":"api/optim/lr_scheduler_en/#examples","title":"Examples","text":""},{"location":"api/optim/lr_scheduler_en/#transformer-training-schedule","title":"Transformer Training Schedule","text":"Python<pre><code># Typical transformer training schedule with warmup\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\n# 10K warmup steps, 100K total steps\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=10000,\n    num_training_steps=100000\n)\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#custom-schedule-function","title":"Custom Schedule Function","text":"Python<pre><code>def custom_schedule(epoch, warmup_epochs=5, total_epochs=100):\n    \"\"\"Custom schedule with warmup and decay.\"\"\"\n    if epoch &lt; warmup_epochs:\n        # Linear warmup\n        return (epoch + 1) / warmup_epochs\n    else:\n        # Exponential decay after warmup\n        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n        return 0.5 ** (progress * 3)  # Decay to 1/8 of original\n\nscheduler = LambdaLR(optimizer, lr_lambda=lambda e: custom_schedule(e))\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#learning-rate-range-test","title":"Learning Rate Range Test","text":"Python<pre><code># Find optimal learning rate range\ndef lr_range_test(model, optimizer, start_lr=1e-7, end_lr=10, num_it=100):\n    lrs = []\n    losses = []\n\n    # Exponential growth from start_lr to end_lr\n    lr_lambda = lambda step: (end_lr / start_lr) ** (step / num_it)\n    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\n    # Set initial learning rate\n    optimizer.lr = start_lr\n\n    for i in range(num_it):\n        # Training step\n        loss = train_step(model, batch)\n        losses.append(loss)\n        lrs.append(optimizer.lr)\n\n        scheduler.step()\n\n        if loss &gt; 4 * min(losses):  # Stop if loss explodes\n            break\n\n    return lrs, losses\n</code></pre>"},{"location":"api/optim/lr_scheduler_en/#implementation-notes","title":"Implementation Notes","text":"<p>The Genesis LambdaLR scheduler is a simplified but effective implementation that: - Directly modifies the optimizer's learning rate attribute - Maintains minimal state (epoch counter and base learning rate) - Provides flexible scheduling through lambda functions - Is compatible with all Genesis optimizers</p> <p>The API is designed to be familiar to PyTorch users while being simpler and more direct in its implementation.</p>"},{"location":"api/optim/optimizers.zh/","title":"\u4f18\u5316\u5668 (genesis.optim)","text":""},{"location":"api/optim/optimizers.zh/#_1","title":"\u6982\u8ff0","text":"<p><code>genesis.optim</code>\u6a21\u5757\u4e3a\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4f18\u5316\u5668\u3002\u5b83\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u652f\u6301\u53c2\u6570\u7ec4\u3001\u68af\u5ea6\u88c1\u526a\u548c\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002</p>"},{"location":"api/optim/optimizers.zh/#_2","title":"\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"api/optim/optimizers.zh/#_3","title":"\u4f18\u5316\u8fc7\u7a0b","text":"<p>\u4f18\u5316\u5668\u4f7f\u7528\u5404\u79cd\u7b97\u6cd5\u57fa\u4e8e\u8ba1\u7b97\u7684\u68af\u5ea6\u66f4\u65b0\u6a21\u578b\u53c2\u6570\uff1a 1. \u68af\u5ea6\u4e0b\u964d: \u4f7f\u7528\u68af\u5ea6\u8fdb\u884c\u57fa\u672c\u53c2\u6570\u66f4\u65b0 2. \u52a8\u91cf: \u4f7f\u7528\u79fb\u52a8\u5e73\u5747\u52a0\u901f\u6536\u655b 3. \u81ea\u9002\u5e94\u5b66\u4e60\u7387: \u6bcf\u4e2a\u53c2\u6570\u7684\u4e0d\u540c\u5b66\u4e60\u7387 4. \u6b63\u5219\u5316: \u6743\u91cd\u8870\u51cf\u548c\u68af\u5ea6\u88c1\u526a</p>"},{"location":"api/optim/optimizers.zh/#_4","title":"\u53c2\u6570\u7ec4","text":"<p>\u53c2\u6570\u53ef\u4ee5\u7ec4\u7ec7\u6210\u5177\u6709\u4e0d\u540c\u8d85\u53c2\u6570\u7684\u7ec4\uff1a - \u4e0d\u540c\u5c42\u7684\u4e0d\u540c\u5b66\u4e60\u7387 - \u9009\u62e9\u6027\u6743\u91cd\u8870\u51cf\u5e94\u7528 - \u5c42\u7279\u5b9a\u7684\u4f18\u5316\u8bbe\u7f6e</p>"},{"location":"api/optim/optimizers.zh/#_5","title":"\u57fa\u7c7b","text":""},{"location":"api/optim/optimizers.zh/#optimoptimizer","title":"<code>optim.Optimizer</code>","text":"<p>\u6240\u6709\u4f18\u5316\u5668\u7684\u62bd\u8c61\u57fa\u7c7b\u3002</p> Python<pre><code>class Optimizer:\n    \"\"\"\n    \u6240\u6709\u4f18\u5316\u5668\u7684\u57fa\u7c7b\u3002\n\n    \u53c2\u6570:\n        params: \u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\u6216\u5b9a\u4e49\u53c2\u6570\u7ec4\u7684\u5b57\u5178\n        defaults: \u5305\u542b\u4f18\u5316\u9009\u9879\u9ed8\u8ba4\u503c\u7684\u5b57\u5178\n    \"\"\"\n\n    def __init__(self, params, defaults: dict):\n        \"\"\"\n        \u521d\u59cb\u5316\u4f18\u5316\u5668\u3002\n\n        \u53c2\u6570:\n            params: \u6a21\u578b\u53c2\u6570\u6216\u53c2\u6570\u7ec4\n            defaults: \u9ed8\u8ba4\u8d85\u53c2\u6570\u503c\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_6","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"api/optim/optimizers.zh/#_7","title":"\u4f18\u5316\u6b65\u9aa4","text":"Python<pre><code>def step(self, closure: Optional[Callable] = None) -&gt; Optional[float]:\n    \"\"\"\n    \u6267\u884c\u5355\u4e2a\u4f18\u5316\u6b65\u9aa4\u3002\n\n    \u53c2\u6570:\n        closure: \u91cd\u65b0\u8bc4\u4f30\u6a21\u578b\u5e76\u8fd4\u56de\u635f\u5931\u7684\u53ef\u9009\u51fd\u6570\n\n    \u8fd4\u56de:\n        \u5982\u679c\u63d0\u4f9bclosure\u5219\u8fd4\u56de\u635f\u5931\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss = criterion(output, target)\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef zero_grad(self, set_to_none: bool = True) -&gt; None:\n    \"\"\"\n    \u6e05\u9664\u6240\u6709\u4f18\u5316\u53c2\u6570\u7684\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        set_to_none: \u5982\u679c\u4e3aTrue\uff0c\u5c06\u68af\u5ea6\u8bbe\u7f6e\u4e3aNone\u800c\u4e0d\u662f\u96f6\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u524d\u6e05\u9664\u68af\u5ea6\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_8","title":"\u72b6\u6001\u7ba1\u7406","text":"Python<pre><code>def state_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    \u5c06\u4f18\u5316\u5668\u72b6\u6001\u8fd4\u56de\u4e3a\u5b57\u5178\u3002\n\n    \u8fd4\u56de:\n        \u5305\u542b\u4f18\u5316\u5668\u72b6\u6001\u548c\u53c2\u6570\u7ec4\u7684\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u4fdd\u5b58\u4f18\u5316\u5668\u72b6\u6001\n        &gt;&gt;&gt; state = optimizer.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'optimizer_checkpoint.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    \u4ece\u5b57\u5178\u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001\u3002\n\n    \u53c2\u6570:\n        state_dict: \u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u6062\u590d\u4f18\u5316\u5668\u72b6\u6001\n        &gt;&gt;&gt; state = genesis.load('optimizer_checkpoint.pth')\n        &gt;&gt;&gt; optimizer.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_9","title":"\u53c2\u6570\u7ec4","text":"Python<pre><code>def add_param_group(self, param_group: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    \u5411\u4f18\u5316\u5668\u6dfb\u52a0\u53c2\u6570\u7ec4\u3002\n\n    \u53c2\u6570:\n        param_group: \u6307\u5b9a\u53c2\u6570\u53ca\u5176\u9009\u9879\u7684\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u6dfb\u52a0\u5177\u6709\u4e0d\u540c\u5b66\u4e60\u7387\u7684\u65b0\u5c42\n        &gt;&gt;&gt; optimizer.add_param_group({\n        ...     'params': new_layer.parameters(),\n        ...     'lr': 0.001\n        ... })\n    \"\"\"\n\n@property\ndef param_groups(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    \u8bbf\u95ee\u53c2\u6570\u7ec4\u3002\n\n    \u8fd4\u56de:\n        \u53c2\u6570\u7ec4\u5b57\u5178\u7684\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u624b\u52a8\u8c03\u6574\u5b66\u4e60\u7387\n        &gt;&gt;&gt; for group in optimizer.param_groups:\n        ...     group['lr'] *= 0.9\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_10","title":"\u4f18\u5316\u5668","text":""},{"location":"api/optim/optimizers.zh/#optimsgd","title":"<code>optim.SGD</code>","text":"<p>\u5e26\u52a8\u91cf\u548c\u6743\u91cd\u8870\u51cf\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u3002</p> Python<pre><code>class SGD(Optimizer):\n    \"\"\"\n    \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u3002\n\n    \u53c2\u6570:\n        params: \u8981\u4f18\u5316\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        lr: \u5b66\u4e60\u7387\uff08\u5fc5\u9700\uff09\n        momentum: \u52a8\u91cf\u56e0\u5b50\uff08\u9ed8\u8ba4: 0\uff09\n        dampening: \u52a8\u91cf\u7684\u963b\u5c3c\uff08\u9ed8\u8ba4: 0\uff09\n        weight_decay: \u6743\u91cd\u8870\u51cf\u7cfb\u6570\uff08\u9ed8\u8ba4: 0\uff09\n        nesterov: \u662f\u5426\u4f7f\u7528Nesterov\u52a8\u91cf\uff08\u9ed8\u8ba4: False\uff09\n\n    \u7b97\u6cd5:\n        v_t = momentum * v_{t-1} + g_t\n        p_t = p_{t-1} - lr * v_t\n\n    \u5176\u4e2d:\n        g_t: \u65f6\u523bt\u7684\u68af\u5ea6\n        v_t: \u65f6\u523bt\u7684\u901f\u5ea6\n        p_t: \u65f6\u523bt\u7684\u53c2\u6570\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float,\n        momentum: float = 0,\n        dampening: float = 0,\n        weight_decay: float = 0,\n        nesterov: bool = False\n    ):\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_11","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code>import genesis.optim as optim\n\n# \u57fa\u672cSGD\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# \u5e26\u52a8\u91cf\u7684SGD\uff08\u5927\u591a\u6570\u4efb\u52a1\u63a8\u8350\uff09\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# \u5e26\u6743\u91cd\u8870\u51cf\u7684SGD\noptimizer = optim.SGD(model.parameters(), lr=0.01, \n                     momentum=0.9, weight_decay=1e-4)\n\n# Nesterov\u52a0\u901f\u68af\u5ea6\noptimizer = optim.SGD(model.parameters(), lr=0.01,\n                     momentum=0.9, nesterov=True)\n\n# \u4e0d\u540c\u5c42\u7684\u4e0d\u540c\u5b66\u4e60\u7387\noptimizer = optim.SGD([\n    {'params': model.features.parameters(), 'lr': 0.001},\n    {'params': model.classifier.parameters(), 'lr': 0.01}\n], momentum=0.9)\n</code></pre>"},{"location":"api/optim/optimizers.zh/#optimadam","title":"<code>optim.Adam</code>","text":"<p>\u7ed3\u5408RMSprop\u548c\u52a8\u91cf\u7684\u81ea\u9002\u5e94\u77e9\u4f30\u8ba1\u4f18\u5316\u5668\u3002</p> Python<pre><code>class Adam(Optimizer):\n    \"\"\"\n    Adam\u4f18\u5316\u5668\u3002\n\n    \u53c2\u6570:\n        params: \u8981\u4f18\u5316\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        lr: \u5b66\u4e60\u7387\uff08\u9ed8\u8ba4: 1e-3\uff09\n        betas: \u8ba1\u7b97\u68af\u5ea6\u53ca\u5176\u5e73\u65b9\u7684\u8fd0\u884c\u5e73\u5747\u7684\u7cfb\u6570\n               \uff08\u9ed8\u8ba4: (0.9, 0.999)\uff09\n        eps: \u6dfb\u52a0\u5230\u5206\u6bcd\u4ee5\u63d0\u9ad8\u6570\u503c\u7a33\u5b9a\u6027\u7684\u9879\uff08\u9ed8\u8ba4: 1e-8\uff09\n        weight_decay: \u6743\u91cd\u8870\u51cf\u7cfb\u6570\uff08\u9ed8\u8ba4: 0\uff09\n        amsgrad: \u662f\u5426\u4f7f\u7528AMSGrad\u53d8\u4f53\uff08\u9ed8\u8ba4: False\uff09\n\n    \u7b97\u6cd5:\n        m_t = \u03b2\u2081 * m_{t-1} + (1 - \u03b2\u2081) * g_t\n        v_t = \u03b2\u2082 * v_{t-1} + (1 - \u03b2\u2082) * g_t\u00b2\n        m\u0302_t = m_t / (1 - \u03b2\u2081\u1d57)\n        v\u0302_t = v_t / (1 - \u03b2\u2082\u1d57)\n        p_t = p_{t-1} - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n\n    \u5176\u4e2d:\n        g_t: \u68af\u5ea6\n        m_t: \u4e00\u9636\u77e9\u4f30\u8ba1\uff08\u52a8\u91cf\uff09\n        v_t: \u4e8c\u9636\u77e9\u4f30\u8ba1\uff08\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff09\n        m\u0302_t, v\u0302_t: \u504f\u5dee\u4fee\u6b63\u7684\u77e9\u4f30\u8ba1\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        amsgrad: bool = False\n    ):\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_12","title":"\u72b6\u6001\u53d8\u91cf","text":"<p>\u6bcf\u4e2a\u53c2\u6570\u7ef4\u62a4\u4ee5\u4e0b\u72b6\u6001\uff1a - <code>step</code>: \u5df2\u6267\u884c\u7684\u4f18\u5316\u6b65\u6570 - <code>exp_avg</code>: \u68af\u5ea6\u503c\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08\u52a8\u91cf\uff09 - <code>exp_avg_sq</code>: \u5e73\u65b9\u68af\u5ea6\u503c\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747 - <code>max_exp_avg_sq</code>: exp_avg_sq\u7684\u6700\u5927\u503c\uff08\u4ec5AMSGrad\uff09</p>"},{"location":"api/optim/optimizers.zh/#_13","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code># \u9ed8\u8ba4Adam\uff08\u6700\u5e38\u89c1\uff09\noptimizer = optim.Adam(model.parameters())\n\n# \u81ea\u5b9a\u4e49\u5b66\u4e60\u7387\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Transformer\u6a21\u578b\u8bbe\u7f6e\noptimizer = optim.Adam(model.parameters(), lr=0.0001,\n                      betas=(0.9, 0.98), eps=1e-9)\n\n# \u5e26\u6743\u91cd\u8870\u51cf\noptimizer = optim.Adam(model.parameters(), lr=0.001,\n                      weight_decay=1e-5)\n\n# \u4e0d\u540c\u5b66\u4e60\u7387\u7684\u5fae\u8c03\noptimizer = optim.Adam([\n    {'params': model.encoder.parameters(), 'lr': 1e-5},\n    {'params': model.decoder.parameters(), 'lr': 1e-4},\n    {'params': model.head.parameters(), 'lr': 1e-3}\n])\n\n# \u4f7f\u7528AMSGrad\u53d8\u4f53\noptimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=True)\n</code></pre>"},{"location":"api/optim/optimizers.zh/#optimadamw","title":"<code>optim.AdamW</code>","text":"<p>\u5e26\u89e3\u8026\u6743\u91cd\u8870\u51cf\u7684Adam\u4f18\u5316\u5668\u3002</p> Python<pre><code>class AdamW(Optimizer):\n    \"\"\"\n    AdamW\u4f18\u5316\u5668\uff08\u5e26\u89e3\u8026\u6743\u91cd\u8870\u51cf\u7684Adam\uff09\u3002\n\n    \u53c2\u6570:\n        params: \u8981\u4f18\u5316\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        lr: \u5b66\u4e60\u7387\uff08\u9ed8\u8ba4: 1e-3\uff09\n        betas: \u8ba1\u7b97\u8fd0\u884c\u5e73\u5747\u7684\u7cfb\u6570\uff08\u9ed8\u8ba4: (0.9, 0.999)\uff09\n        eps: \u6570\u503c\u7a33\u5b9a\u6027\u9879\uff08\u9ed8\u8ba4: 1e-8\uff09\n        weight_decay: \u6743\u91cd\u8870\u51cf\u7cfb\u6570\uff08\u9ed8\u8ba4: 0.01\uff09\n        amsgrad: \u662f\u5426\u4f7f\u7528AMSGrad\u53d8\u4f53\uff08\u9ed8\u8ba4: False\uff09\n\n    \u4e0eAdam\u7684\u533a\u522b:\n        Adam: p_t = p_{t-1} - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * p_{t-1})\n        AdamW: p_t = p_{t-1} * (1 - lr * wd) - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n\n    AdamW\u5c06\u6743\u91cd\u8870\u51cf\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u89e3\u8026\uff0c\u76f4\u63a5\u5e94\u7528\u5230\u53c2\u6570\u4e0a\n    \u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6b63\u5219\u5316\u3002\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0.01,\n        amsgrad: bool = False\n    ):\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_14","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code># \u9ed8\u8ba4AdamW\uff08Transformers\u63a8\u8350\uff09\noptimizer = optim.AdamW(model.parameters())\n\n# BERT/GPT\u6807\u51c6\u8bbe\u7f6e\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\n# \u5927\u6a21\u578b\u8bad\u7ec3\noptimizer = optim.AdamW(model.parameters(), lr=1e-4,\n                       betas=(0.9, 0.95), weight_decay=0.1)\n\n# \u4ece\u6743\u91cd\u8870\u51cf\u4e2d\u6392\u9664\u504f\u7f6e\u548c\u5f52\u4e00\u5316\ndecay_params = []\nno_decay_params = []\nfor name, param in model.named_parameters():\n    if any(nd in name for nd in ['bias', 'norm', 'ln']):\n        no_decay_params.append(param)\n    else:\n        decay_params.append(param)\n\noptimizer = optim.AdamW([\n    {'params': decay_params, 'weight_decay': 0.01},\n    {'params': no_decay_params, 'weight_decay': 0.0}\n], lr=1e-4)\n</code></pre>"},{"location":"api/optim/optimizers.zh/#optimrmsprop","title":"<code>optim.RMSprop</code>","text":"<p>\u5747\u65b9\u6839\u4f20\u64ad\u4f18\u5316\u5668\u3002</p> Python<pre><code>class RMSprop(Optimizer):\n    \"\"\"\n    RMSprop\u4f18\u5316\u5668\u3002\n\n    \u53c2\u6570:\n        params: \u8981\u4f18\u5316\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        lr: \u5b66\u4e60\u7387\uff08\u9ed8\u8ba4: 1e-2\uff09\n        alpha: \u5e73\u6ed1\u5e38\u6570\uff08\u9ed8\u8ba4: 0.99\uff09\n        eps: \u6570\u503c\u7a33\u5b9a\u6027\u9879\uff08\u9ed8\u8ba4: 1e-8\uff09\n        weight_decay: \u6743\u91cd\u8870\u51cf\u7cfb\u6570\uff08\u9ed8\u8ba4: 0\uff09\n        momentum: \u52a8\u91cf\u56e0\u5b50\uff08\u9ed8\u8ba4: 0\uff09\n        centered: \u662f\u5426\u6309\u4e2d\u5fc3\u5316\u7684\u4e8c\u9636\u77e9\u5f52\u4e00\u5316\uff08\u9ed8\u8ba4: False\uff09\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_15","title":"\u68af\u5ea6\u88c1\u526a","text":"<p>\u9632\u6b62\u68af\u5ea6\u7206\u70b8\u7684\u5b9e\u7528\u5de5\u5177\u3002</p> Python<pre><code>def clip_grad_norm_(\n    parameters: Iterable[Tensor],\n    max_norm: float,\n    norm_type: float = 2.0,\n    error_if_nonfinite: bool = False\n) -&gt; float:\n    \"\"\"\n    \u6309\u5168\u5c40\u8303\u6570\u88c1\u526a\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        parameters: \u6709\u68af\u5ea6\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        max_norm: \u6700\u5927\u68af\u5ea6\u8303\u6570\n        norm_type: \u8303\u6570\u7c7b\u578b\uff081, 2, \u6216 inf\uff09\n        error_if_nonfinite: \u5982\u679c\u603b\u8303\u6570\u975e\u6709\u9650\u5219\u62a5\u9519\n\n    \u8fd4\u56de:\n        \u88c1\u526a\u524d\u68af\u5ea6\u7684\u603b\u8303\u6570\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; # \u88c1\u526a\u68af\u5ea6\u4ee5\u9632\u6b62\u7206\u70b8\n        &gt;&gt;&gt; genesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef clip_grad_value_(\n    parameters: Iterable[Tensor],\n    clip_value: float\n) -&gt; None:\n    \"\"\"\n    \u6309\u503c\u88c1\u526a\u68af\u5ea6\u3002\n\n    \u53c2\u6570:\n        parameters: \u6709\u68af\u5ea6\u7684\u53c2\u6570\u7684\u53ef\u8fed\u4ee3\u5bf9\u8c61\n        clip_value: \u88c1\u526a\u9608\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; # \u5c06\u68af\u5ea6\u9650\u5236\u5728[-1, 1]\u8303\u56f4\u5185\n        &gt;&gt;&gt; genesis.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_16","title":"\u8bad\u7ec3\u793a\u4f8b","text":""},{"location":"api/optim/optimizers.zh/#_17","title":"\u57fa\u672c\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u6a21\u578b\u548c\u4f18\u5316\u5668\u8bbe\u7f6e\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # \u68af\u5ea6\u6e05\u96f6\n        optimizer.zero_grad()\n\n        # \u524d\u5411\u4f20\u64ad\n        output = model(data)\n        loss = criterion(output, target)\n\n        # \u540e\u5411\u4f20\u64ad\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\uff08\u53ef\u9009\uff09\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u66f4\u65b0\u53c2\u6570\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_18","title":"\u6df7\u5408\u7cbe\u5ea6\u7684\u9ad8\u7ea7\u8bad\u7ec3","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\n# \u6a21\u578b\u8bbe\u7f6e\nmodel = TransformerModel()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        # \u4f7f\u7528autocast\u8fdb\u884c\u6df7\u5408\u7cbe\u5ea6\n        with genesis.autocast():\n            outputs = model(batch['input'])\n            loss = criterion(outputs, batch['target'])\n\n        # \u540e\u5411\u4f20\u64ad\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u4f18\u5316\u5668\u6b65\u9aa4\n        optimizer.step()\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_19","title":"\u5b66\u4e60\u7387\u8c03\u5ea6","text":"Python<pre><code>from genesis.optim.lr_scheduler import CosineAnnealingLR\n\n# \u4f18\u5316\u5668\u548c\u8c03\u5ea6\u5668\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\nfor epoch in range(num_epochs):\n    # \u8bad\u7ec3\n    for batch in train_loader:\n        optimizer.zero_grad()\n        loss = compute_loss(model, batch)\n        loss.backward()\n        optimizer.step()\n\n    # \u66f4\u65b0\u5b66\u4e60\u7387\n    scheduler.step()\n    print(f'Epoch {epoch}, LR: {scheduler.get_last_lr()[0]:.6f}')\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_20","title":"\u68af\u5ea6\u7d2f\u79ef","text":"Python<pre><code># \u901a\u8fc7\u7d2f\u79ef\u6a21\u62df\u66f4\u5927\u7684\u6279\u5927\u5c0f\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(train_loader):\n    # \u524d\u5411\u4f20\u64ad\n    outputs = model(batch['input'])\n    loss = criterion(outputs, batch['target'])\n\n    # \u6309\u7d2f\u79ef\u6b65\u6570\u5f52\u4e00\u5316\u635f\u5931\n    loss = loss / accumulation_steps\n    loss.backward()\n\n    # \u6bcfaccumulation_steps\u66f4\u65b0\u6743\u91cd\n    if (i + 1) % accumulation_steps == 0:\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"api/optim/optimizers.zh/#_21","title":"\u4f18\u5316\u5668\u9009\u62e9\u6307\u5357","text":""},{"location":"api/optim/optimizers.zh/#sgd","title":"SGD","text":"<ul> <li>\u4f18\u70b9: \u7b80\u5355\u3001\u5185\u5b58\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u597d</li> <li>\u7f3a\u70b9: \u6536\u655b\u6162\u3001\u5bf9\u5b66\u4e60\u7387\u654f\u611f</li> <li>\u9002\u7528\u4e8e:</li> <li>\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08ResNet\u3001VGG\uff09</li> <li>\u5185\u5b58\u53d7\u9650\u73af\u5883</li> <li>\u9700\u8981\u6700\u4f73\u6cdb\u5316\u65f6</li> <li>\u63a8\u8350\u8bbe\u7f6e: <code>lr=0.1, momentum=0.9, weight_decay=1e-4</code></li> </ul>"},{"location":"api/optim/optimizers.zh/#adam","title":"Adam","text":"<ul> <li>\u4f18\u70b9: \u6536\u655b\u5feb\u3001\u81ea\u9002\u5e94\u3001\u5bf9\u8d85\u53c2\u6570\u4e0d\u654f\u611f</li> <li>\u7f3a\u70b9: \u5185\u5b58\u4f7f\u7528\u8f83\u9ad8\u3001\u53ef\u80fd\u8fc7\u62df\u5408</li> <li>\u9002\u7528\u4e8e:</li> <li>NLP\u4efb\u52a1</li> <li>\u5feb\u901f\u539f\u578b\u5236\u4f5c</li> <li>\u7a00\u758f\u68af\u5ea6</li> <li>\u63a8\u8350\u8bbe\u7f6e: <code>lr=1e-3, betas=(0.9, 0.999)</code></li> </ul>"},{"location":"api/optim/optimizers.zh/#adamw","title":"AdamW","text":"<ul> <li>\u4f18\u70b9: \u6bd4Adam\u6cdb\u5316\u6027\u66f4\u597d\u3001\u5bf9\u5927\u6a21\u578b\u4f18\u79c0</li> <li>\u7f3a\u70b9: \u5185\u5b58\u4f7f\u7528\u8f83\u9ad8</li> <li>\u9002\u7528\u4e8e:</li> <li>Transformer\u6a21\u578b\uff08BERT\u3001GPT\uff09</li> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3</li> <li>\u9700\u8981\u5f3a\u6b63\u5219\u5316\u65f6</li> <li>\u63a8\u8350\u8bbe\u7f6e: <code>lr=5e-5, weight_decay=0.01</code></li> </ul>"},{"location":"api/optim/optimizers.zh/#rmsprop","title":"RMSprop","text":"<ul> <li>\u4f18\u70b9: \u5bf9\u975e\u5e73\u7a33\u76ee\u6807\u51fd\u6570\u597d</li> <li>\u7f3a\u70b9: \u9ad8\u5b66\u4e60\u7387\u65f6\u53ef\u80fd\u4e0d\u7a33\u5b9a</li> <li>\u9002\u7528\u4e8e:</li> <li>RNN\u8bad\u7ec3</li> <li>\u5f3a\u5316\u5b66\u4e60</li> <li>\u975e\u5e73\u7a33\u95ee\u9898</li> </ul>"},{"location":"api/optim/optimizers.zh/#_22","title":"\u6027\u80fd\u63d0\u793a","text":"<ol> <li>\u68af\u5ea6\u7d2f\u79ef: \u5185\u5b58\u6709\u9650\u65f6\u6a21\u62df\u66f4\u5927\u6279\u5927\u5c0f</li> <li>\u68af\u5ea6\u88c1\u526a: \u5bf9RNN\u548cTransformers\u5fc5\u4e0d\u53ef\u5c11</li> <li>\u53c2\u6570\u7ec4: \u5bf9\u4e0d\u540c\u5c42\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u7387</li> <li>\u6743\u91cd\u8870\u51cf: AdamW\u901a\u5e38\u6bd4Adam + L2\u6b63\u5219\u5316\u8868\u73b0\u66f4\u597d</li> <li>\u5b66\u4e60\u7387\u9884\u70ed: \u5927\u6279\u91cf\u8bad\u7ec3\u65f6\u4f7f\u7528\u9884\u70ed</li> <li>\u6df7\u5408\u7cbe\u5ea6: \u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u52a0\u901f\u8bad\u7ec3</li> </ol>"},{"location":"api/optim/optimizers.zh/#_23","title":"\u5185\u5b58\u8003\u8651","text":"<ul> <li>\u4f18\u5316\u5668\u7ef4\u62a4\u6bcf\u4e2a\u53c2\u6570\u7684\u72b6\u6001\uff08Adam/AdamW\u4f7f\u75282\u500d\u53c2\u6570\u5185\u5b58\uff09</li> <li>\u4f7f\u7528<code>zero_grad(set_to_none=True)</code>\u51cf\u5c11\u5185\u5b58\u788e\u7247</li> <li>\u5728\u8bbe\u5907\u95f4\u79fb\u52a8\u6a21\u578b\u65f6\u8003\u8651\u4f18\u5316\u5668\u72b6\u6001</li> <li>\u5728\u68c0\u67e5\u70b9\u4e2d\u4fdd\u5b58\u4f18\u5316\u5668\u72b6\u6001\u4ee5\u6062\u590d\u8bad\u7ec3</li> </ul>"},{"location":"api/optim/optimizers.zh/#_24","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u59cb\u7ec8\u6e05\u9664\u68af\u5ea6 \u5728\u540e\u5411\u4f20\u64ad\u524d</li> <li>\u4f7f\u7528\u68af\u5ea6\u88c1\u526a \u5bf9RNN\u548cTransformers</li> <li>\u76d1\u63a7\u5b66\u4e60\u7387 \u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b</li> <li>\u4fdd\u5b58\u4f18\u5316\u5668\u72b6\u6001 \u5728\u68c0\u67e5\u70b9\u4e2d</li> <li>\u4e3a\u6a21\u578b\u7c7b\u578b\u4f7f\u7528\u9002\u5f53\u7684\u6743\u91cd\u8870\u51cf</li> <li>\u4e3a\u5927\u6a21\u578b\u8003\u8651\u6df7\u5408\u7cbe\u5ea6</li> </ol>"},{"location":"api/optim/optimizers.zh/#_25","title":"\u53e6\u8bf7\u53c2\u9605","text":"<ul> <li>\u5b66\u4e60\u7387\u8c03\u5ea6\u5668 - \u52a8\u6001\u5b66\u4e60\u7387\u8c03\u6574</li> <li>\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 - \u6784\u5efa\u6a21\u578b</li> <li>\u81ea\u52a8\u5fae\u5206 - \u81ea\u52a8\u5fae\u5206</li> <li>\u793a\u4f8b - \u5b8c\u6574\u8bad\u7ec3\u793a\u4f8b</li> </ul>"},{"location":"api/optim/optimizers_en/","title":"Optimizers (genesis.optim)","text":""},{"location":"api/optim/optimizers_en/#overview","title":"Overview","text":"<p>The <code>genesis.optim</code> module provides optimizers for training neural networks. It implements state-of-the-art optimization algorithms with support for momentum, weight decay, and adaptive learning rates.</p>"},{"location":"api/optim/optimizers_en/#core-concepts","title":"Core Concepts","text":""},{"location":"api/optim/optimizers_en/#optimization-process","title":"Optimization Process","text":"<p>Optimizers update model parameters based on computed gradients using various algorithms: 1. Gradient Descent: Basic parameter update using gradients 2. Momentum: Accelerated convergence using moving averages 3. Adaptive Learning Rates: Different learning rates per parameter 4. Weight Decay: L2 regularization</p>"},{"location":"api/optim/optimizers_en/#base-classes","title":"Base Classes","text":""},{"location":"api/optim/optimizers_en/#optimoptimizer","title":"<code>optim.Optimizer</code>","text":"<p>Abstract base class for all optimizers.</p> Python<pre><code>class Optimizer:\n    \"\"\"\n    Base class for all optimizers.\n\n    Provides common functionality for parameter updates, gradient zeroing,\n    and state management across different optimization algorithms.\n    \"\"\"\n\n    def __init__(self, params):\n        \"\"\"\n        Initialize optimizer with parameters to optimize.\n\n        Args:\n            params: Iterable of parameters to optimize\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers_en/#core-methods","title":"Core Methods","text":""},{"location":"api/optim/optimizers_en/#optimization-step","title":"Optimization Step","text":"Python<pre><code>def step(self):\n    \"\"\"\n    Perform a single optimization step (parameter update).\n\n    Must be implemented by subclasses.\n\n    Example:\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss = criterion(output, target)\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef zero_grad(self):\n    \"\"\"\n    Zero gradients of all optimized parameters.\n    Sets grad to None for all parameters.\n\n    Example:\n        &gt;&gt;&gt; # Clear gradients before each training step\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef reset_grad(self):\n    \"\"\"\n    Reset gradients of all optimized parameters.\n    Alias for zero_grad().\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers_en/#state-management","title":"State Management","text":"Python<pre><code>def state_dict(self):\n    \"\"\"\n    Return optimizer state as a dictionary.\n\n    Returns:\n        Dictionary containing optimizer state (excluding params)\n\n    Example:\n        &gt;&gt;&gt; # Save optimizer state\n        &gt;&gt;&gt; state = optimizer.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'optimizer_checkpoint.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict):\n    \"\"\"\n    Load optimizer state from dictionary.\n\n    Args:\n        state_dict: Optimizer state dictionary\n\n    Example:\n        &gt;&gt;&gt; # Restore optimizer state\n        &gt;&gt;&gt; state = genesis.load('optimizer_checkpoint.pth')\n        &gt;&gt;&gt; optimizer.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers_en/#optimizers","title":"Optimizers","text":""},{"location":"api/optim/optimizers_en/#optimsgd","title":"<code>optim.SGD</code>","text":"<p>Stochastic Gradient Descent optimizer with momentum and weight decay.</p> Python<pre><code>class SGD(Optimizer):\n    \"\"\"\n    Stochastic Gradient Descent optimizer with momentum and weight decay.\n\n    Implements the classical SGD algorithm with optional momentum for improved\n    convergence and weight decay for regularization.\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.01)\n        momentum: Momentum factor, 0 disables momentum (default: 0.0)\n        weight_decay: Weight decay (L2 penalty) factor (default: 0.0)\n\n    Algorithm:\n        grad = gradient + weight_decay * param\n        velocity = momentum * velocity + (1 - momentum) * grad\n        param = param - lr * velocity\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.01,\n        momentum=0.0,\n        weight_decay=0.0\n    ):\n</code></pre>"},{"location":"api/optim/optimizers_en/#usage-examples","title":"Usage Examples","text":"Python<pre><code>import genesis.optim as optim\n\n# Basic SGD\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# SGD with momentum (recommended for most tasks)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# SGD with weight decay\noptimizer = optim.SGD(model.parameters(), lr=0.01, \n                     momentum=0.9, weight_decay=1e-4)\n</code></pre>"},{"location":"api/optim/optimizers_en/#optimadam","title":"<code>optim.Adam</code>","text":"<p>Adaptive Moment Estimation optimizer combining RMSprop and momentum.</p> Python<pre><code>class Adam(Optimizer):\n    \"\"\"\n    Adam optimizer with adaptive learning rates.\n\n    Implements the Adam algorithm which computes adaptive learning rates\n    for each parameter using estimates of first and second moments of gradients.\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.01)\n        beta1: Coefficient for first moment estimate (default: 0.9)\n        beta2: Coefficient for second moment estimate (default: 0.999)\n        eps: Term added to denominator for numerical stability (default: 1e-8)\n        weight_decay: Weight decay coefficient (default: 0.0)\n\n    Algorithm:\n        grad = gradient + weight_decay * param\n        m_t = \u03b2\u2081 * m_{t-1} + (1 - \u03b2\u2081) * grad\n        v_t = \u03b2\u2082 * v_{t-1} + (1 - \u03b2\u2082) * grad\u00b2\n        m\u0302_t = m_t / (1 - \u03b2\u2081\u1d57)\n        v\u0302_t = v_t / (1 - \u03b2\u2082\u1d57)\n        param = param - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.01,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.0\n    ):\n</code></pre>"},{"location":"api/optim/optimizers_en/#state-variables","title":"State Variables","text":"<p>Each parameter maintains the following state: - <code>t</code>: Time step counter (incremented on each step) - <code>m</code>: First moment estimate (momentum) - <code>v</code>: Second moment estimate (adaptive learning rate)</p>"},{"location":"api/optim/optimizers_en/#usage-examples_1","title":"Usage Examples","text":"Python<pre><code># Default Adam\noptimizer = optim.Adam(model.parameters())\n\n# Custom learning rate\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# With weight decay\noptimizer = optim.Adam(model.parameters(), lr=0.001,\n                      weight_decay=1e-5)\n\n# Transformer model settings\noptimizer = optim.Adam(model.parameters(), lr=0.0001,\n                      beta1=0.9, beta2=0.98, eps=1e-9)\n</code></pre>"},{"location":"api/optim/optimizers_en/#optimadamw","title":"<code>optim.AdamW</code>","text":"<p>Adam optimizer with decoupled weight decay.</p> Python<pre><code>class AdamW(Optimizer):\n    \"\"\"\n    AdamW optimizer (Adam with decoupled weight decay).\n\n    Args:\n        params: Parameters to optimize\n        lr: Learning rate (default: 0.001)\n        beta1: Coefficient for computing running averages (default: 0.9)\n        beta2: Coefficient for computing running averages (default: 0.999)\n        eps: Term for numerical stability (default: 1e-8)\n        weight_decay: Weight decay coefficient (default: 0.01)\n\n    Difference from Adam:\n        Adam:  param = param - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * param)\n        AdamW: param = param - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * param)\n\n    AdamW applies weight decay directly to parameters rather than to gradients,\n    providing better regularization especially for adaptive learning rate methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=0.001,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        weight_decay=0.01\n    ):\n</code></pre>"},{"location":"api/optim/optimizers_en/#usage-examples_2","title":"Usage Examples","text":"Python<pre><code># Default AdamW (recommended for Transformers)\noptimizer = optim.AdamW(model.parameters())\n\n# BERT/GPT standard settings\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\n# Large model training\noptimizer = optim.AdamW(model.parameters(), lr=1e-4,\n                       beta1=0.9, beta2=0.95, weight_decay=0.1)\n</code></pre>"},{"location":"api/optim/optimizers_en/#training-examples","title":"Training Examples","text":""},{"location":"api/optim/optimizers_en/#basic-training-loop","title":"Basic Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Model and optimizer setup\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.SoftmaxLoss()\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        output = model(data)\n        loss = criterion(output, target)\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api/optim/optimizers_en/#training-with-mixed-precision","title":"Training with Mixed Precision","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Enable mixed precision\ngenesis.enable_autocast = True\n\n# Model setup\nmodel = TransformerModel()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        # Mixed precision is handled automatically in Function.apply()\n        outputs = model(batch['input'])\n        loss = criterion(outputs, batch['target'])\n\n        # Backward pass\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n</code></pre>"},{"location":"api/optim/optimizers_en/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"Python<pre><code>from genesis.optim.lr_scheduler import LambdaLR\n\n# Optimizer and scheduler\noptimizer = optim.AdamW(model.parameters(), lr=1e-3)\n\n# Define learning rate schedule\ndef lr_lambda(epoch):\n    return 0.95 ** epoch\n\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\nfor epoch in range(num_epochs):\n    # Training\n    for batch in train_loader:\n        optimizer.zero_grad()\n        loss = compute_loss(model, batch)\n        loss.backward()\n        optimizer.step()\n\n    # Update learning rate\n    scheduler.step()\n    print(f'Epoch {epoch}, LR: {scheduler.get_last_lr():.6f}')\n</code></pre>"},{"location":"api/optim/optimizers_en/#gradient-accumulation","title":"Gradient Accumulation","text":"Python<pre><code># Simulate larger batch size through accumulation\naccumulation_steps = 4\noptimizer.zero_grad()\n\nfor i, batch in enumerate(train_loader):\n    # Forward pass\n    outputs = model(batch['input'])\n    loss = criterion(outputs, batch['target'])\n\n    # Normalize loss by accumulation steps\n    loss = loss / accumulation_steps\n    loss.backward()\n\n    # Update weights every accumulation_steps\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"api/optim/optimizers_en/#optimizer-selection-guide","title":"Optimizer Selection Guide","text":""},{"location":"api/optim/optimizers_en/#sgd","title":"SGD","text":"<ul> <li>Advantages: Simple, memory efficient, good generalization</li> <li>Disadvantages: Slow convergence, sensitive to learning rate</li> <li>Best for:</li> <li>Computer vision tasks (ResNet, VGG)</li> <li>Memory-constrained environments</li> <li>When best generalization is needed</li> <li>Recommended settings: <code>lr=0.1, momentum=0.9, weight_decay=1e-4</code></li> </ul>"},{"location":"api/optim/optimizers_en/#adam","title":"Adam","text":"<ul> <li>Advantages: Fast convergence, adaptive, less sensitive to hyperparameters</li> <li>Disadvantages: Higher memory usage (2x parameters)</li> <li>Best for:</li> <li>NLP tasks</li> <li>Rapid prototyping</li> <li>Sparse gradients</li> <li>Recommended settings: <code>lr=1e-3, beta1=0.9, beta2=0.999</code></li> </ul>"},{"location":"api/optim/optimizers_en/#adamw","title":"AdamW","text":"<ul> <li>Advantages: Better generalization than Adam, excellent for large models</li> <li>Disadvantages: Higher memory usage (2x parameters)</li> <li>Best for:</li> <li>Transformer models (BERT, GPT)</li> <li>Large-scale pre-training</li> <li>When strong regularization is needed</li> <li>Recommended settings: <code>lr=5e-5, weight_decay=0.01</code></li> </ul>"},{"location":"api/optim/optimizers_en/#performance-tips","title":"Performance Tips","text":"<ol> <li>Gradient Accumulation: Simulate larger batch sizes when memory is limited</li> <li>Learning Rate Scheduling: Use schedulers to improve convergence</li> <li>Weight Decay: AdamW generally performs better than Adam + L2 regularization</li> <li>Mixed Precision: Reduces memory usage and speeds up training</li> <li>Zero Gradients: Always clear gradients before backward pass</li> </ol>"},{"location":"api/optim/optimizers_en/#memory-considerations","title":"Memory Considerations","text":"<ul> <li>Adam/AdamW maintain state per parameter (2x parameter memory for momentum and variance)</li> <li>Use <code>zero_grad()</code> to clear gradients and free memory</li> <li>Consider optimizer state when moving models between devices</li> <li>Save optimizer state in checkpoints for resuming training</li> </ul>"},{"location":"api/optim/optimizers_en/#best-practices","title":"Best Practices","text":"<ol> <li>Always clear gradients before backward pass</li> <li>Monitor learning rates throughout training</li> <li>Save optimizer state in checkpoints</li> <li>Use appropriate weight decay for your model type</li> <li>Consider mixed precision for large models</li> </ol>"},{"location":"api/optim/optimizers_en/#see-also","title":"See Also","text":"<ul> <li>Learning Rate Schedulers - Dynamic learning rate adjustment</li> <li>Neural Network Modules - Building models</li> <li>Autograd - Automatic differentiation</li> <li>Examples - Complete training examples</li> </ul>"},{"location":"api/utils/index.zh/","title":"\u5b9e\u7528\u5de5\u5177 (genesis.utils)","text":""},{"location":"api/utils/index.zh/#_1","title":"\u6982\u8ff0","text":"<p><code>genesis.utils</code>\u6a21\u5757\u4e3a\u5f00\u53d1\u3001\u8c03\u8bd5\u548c\u6570\u636e\u5904\u7406\u63d0\u4f9b\u5fc5\u8981\u7684\u5b9e\u7528\u5de5\u5177\u3002\u5b83\u5305\u62ec\u6027\u80fd\u5206\u6790\u5de5\u5177\u3001\u6570\u636e\u52a0\u8f7d\u5b9e\u7528\u7a0b\u5e8f\u548c\u5e2e\u52a9\u51fd\u6570\uff0c\u4ee5\u7b80\u5316\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u3002</p>"},{"location":"api/utils/index.zh/#_2","title":"\u6838\u5fc3\u7ec4\u4ef6","text":""},{"location":"api/utils/index.zh/#_3","title":"\u6027\u80fd\u5206\u6790","text":"<ul> <li>\u51fd\u6570\u548c\u65b9\u6cd5\u6267\u884c\u65f6\u95f4\u8ddf\u8e2a</li> <li>\u88c5\u9970\u5668\u81ea\u52a8\u5206\u6790</li> <li>\u6027\u80fd\u5206\u6790\u548c\u62a5\u544a</li> </ul>"},{"location":"api/utils/index.zh/#_4","title":"\u6570\u636e\u52a0\u8f7d","text":"<ul> <li>\u8bad\u7ec3\u6570\u636e\u7684\u6570\u636e\u96c6\u62bd\u8c61</li> <li>\u5177\u6709\u6279\u5904\u7406\u548c\u6d17\u724c\u7684DataLoader</li> <li>\u652f\u6301\u6620\u5c04\u5f0f\u548c\u53ef\u8fed\u4ee3\u6570\u636e\u96c6</li> </ul>"},{"location":"api/utils/index.zh/#_5","title":"\u6027\u80fd\u5206\u6790\u5de5\u5177","text":""},{"location":"api/utils/index.zh/#profile","title":"<code>@profile</code> \u88c5\u9970\u5668","text":"<p>\u51fd\u6570\u548c\u7c7b\u7684\u81ea\u52a8\u6027\u80fd\u5206\u6790\u3002</p> Python<pre><code>from genesis.utils import profile\n\n@profile\ndef expensive_function(x):\n    \"\"\"\n    \u5206\u6790\u6b64\u51fd\u6570\u7684\u6267\u884c\u65f6\u95f4\u548c\u8c03\u7528\u6b21\u6570\u3002\n    \"\"\"\n    # \u4f60\u7684\u8ba1\u7b97\u5728\u8fd9\u91cc\n    return x * 2\n\n@profile\nclass MyModel:\n    \"\"\"\n    \u5206\u6790\u6b64\u7c7b\u4e2d\u7684\u6240\u6709\u65b9\u6cd5\u3002\n    \"\"\"\n    def forward(self, x):\n        return x + 1\n\n    def backward(self, grad):\n        return grad\n</code></pre> <p>\u5206\u6790\u5668\u81ea\u52a8\u8ddf\u8e2a\uff1a - \u8c03\u7528\u6b21\u6570: \u6bcf\u4e2a\u51fd\u6570\u88ab\u8c03\u7528\u7684\u6b21\u6570 - \u603b\u65f6\u95f4: \u7d2f\u79ef\u6267\u884c\u65f6\u95f4 - \u5e73\u5747\u65f6\u95f4: \u6bcf\u6b21\u8c03\u7528\u7684\u5e73\u5747\u6267\u884c\u65f6\u95f4</p>"},{"location":"api/utils/index.zh/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code>import genesis.utils as utils\nimport time\n\n# \u5206\u6790\u51fd\u6570\n@utils.profile\ndef matrix_multiply(a, b):\n    \"\"\"\u865a\u62df\u77e9\u9635\u4e58\u6cd5\u3002\"\"\"\n    time.sleep(0.01)  # \u6a21\u62df\u8ba1\u7b97\n    return a @ b\n\n# \u5206\u6790\u7c7b\n@utils.profile\nclass NeuralNetwork:\n    def __init__(self):\n        pass\n\n    def forward(self, x):\n        time.sleep(0.005)  # \u6a21\u62df\u524d\u5411\u4f20\u64ad\n        return x * 2\n\n    def backward(self, grad):\n        time.sleep(0.003)  # \u6a21\u62df\u540e\u5411\u4f20\u64ad\n        return grad\n\n# \u4f7f\u7528\u88ab\u5206\u6790\u7684\u51fd\u6570\nmodel = NeuralNetwork()\nfor i in range(100):\n    x = matrix_multiply([[1, 2]], [[3], [4]])\n    y = model.forward(x)\n    model.backward([1, 1])\n\n# \u5206\u6790\u6570\u636e\u5728\u7a0b\u5e8f\u9000\u51fa\u65f6\u81ea\u52a8\u6253\u5370\n</code></pre>"},{"location":"api/utils/index.zh/#_7","title":"\u5206\u6790\u6570\u636e\u683c\u5f0f","text":"<p>\u7a0b\u5e8f\u9000\u51fa\u65f6\uff0c\u5206\u6790\u6570\u636e\u81ea\u52a8\u6253\u5370\uff1a</p> Text Only<pre><code>\u7a0b\u5e8f\u82b1\u8d39\u4e86 2.1456 \u79d2!\n__main__.matrix_multiply: 100\u6b21\u8c03\u7528, 1.0234\u603b\u79d2\u6570\n__main__.NeuralNetwork.forward: 100\u6b21\u8c03\u7528, 0.5123\u603b\u79d2\u6570\n__main__.NeuralNetwork.backward: 100\u6b21\u8c03\u7528, 0.3089\u603b\u79d2\u6570\n</code></pre>"},{"location":"api/utils/index.zh/#_8","title":"\u624b\u52a8\u5206\u6790","text":"<p>\u4e3a\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\uff0c\u4f60\u53ef\u4ee5\u7f16\u7a0b\u5f0f\u5730\u8bbf\u95ee\u5206\u6790\u6570\u636e\uff1a</p> Python<pre><code>from genesis.utils.profile import profile_data, print_profile_data\n\n# \u83b7\u53d6\u5f53\u524d\u5206\u6790\u6570\u636e\ncurrent_data = profile_data.copy()\nprint(f\"\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u51fd\u6570\u8c03\u7528: {sum(data['calls'] for data in current_data.values())}\")\n\n# \u624b\u52a8\u6253\u5370\u5206\u6790\u6458\u8981\nprint_profile_data()\n</code></pre>"},{"location":"api/utils/index.zh/#_9","title":"\u6570\u636e\u52a0\u8f7d","text":""},{"location":"api/utils/index.zh/#dataset","title":"<code>Dataset</code>","text":"<p>\u6240\u6709\u6570\u636e\u96c6\u7684\u62bd\u8c61\u57fa\u7c7b\u3002</p> Python<pre><code>from genesis.utils.data import Dataset\n\nclass Dataset:\n    \"\"\"\n    \u62bd\u8c61\u6570\u636e\u96c6\u7c7b\u3002\n\n    \u6240\u6709\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0 __len__ \u548c __getitem__\u3002\n    \"\"\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        \u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002\n\n        \u8fd4\u56de:\n            \u6570\u636e\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf\n        \"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        \u6309\u7d22\u5f15\u68c0\u7d22\u6837\u672c\u3002\n\n        \u53c2\u6570:\n            idx: \u6837\u672c\u7d22\u5f15\n\n        \u8fd4\u56de:\n            \u7ed9\u5b9a\u7d22\u5f15\u5904\u7684\u6570\u636e\u6837\u672c\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/utils/index.zh/#_10","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u793a\u4f8b","text":"Python<pre><code>import numpy as np\nfrom genesis.utils.data import Dataset\n\nclass MNIST(Dataset):\n    \"\"\"MNIST\u6570\u636e\u96c6\u5b9e\u73b0\u793a\u4f8b\u3002\"\"\"\n\n    def __init__(self, data_path, transform=None):\n        \"\"\"\n        \u521d\u59cb\u5316MNIST\u6570\u636e\u96c6\u3002\n\n        \u53c2\u6570:\n            data_path: MNIST\u6570\u636e\u6587\u4ef6\u8def\u5f84\n            transform: \u53ef\u9009\u7684\u6570\u636e\u53d8\u6362\u51fd\u6570\n        \"\"\"\n        self.data = self._load_data(data_path)\n        self.labels = self._load_labels(data_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n    def _load_data(self, path):\n        # \u5728\u8fd9\u91cc\u52a0\u8f7d\u4f60\u7684\u6570\u636e\n        return np.random.randn(10000, 28, 28)  # \u865a\u62df\u6570\u636e\n\n    def _load_labels(self, path):\n        # \u5728\u8fd9\u91cc\u52a0\u8f7d\u4f60\u7684\u6807\u7b7e\n        return np.random.randint(0, 10, 10000)  # \u865a\u62df\u6807\u7b7e\n</code></pre>"},{"location":"api/utils/index.zh/#iterabledataset","title":"<code>IterableDataset</code>","text":"<p>\u53ef\u8fed\u4ee3\u5f0f\u6570\u636e\u96c6\u7684\u57fa\u7c7b\u3002</p> Python<pre><code>from genesis.utils.data import IterableDataset\n\nclass IterableDataset(Dataset):\n    \"\"\"\n    \u53ef\u8fed\u4ee3\u6570\u636e\u96c6\u7684\u57fa\u7c7b\u3002\n\n    \u5bf9\u4e8e\u6d41\u5f0f\u6570\u636e\u6216\u968f\u673a\u8bbf\u95ee\u4e0d\u53ef\u884c\u65f6\u5f88\u6709\u7528\u3002\n    \"\"\"\n\n    def __iter__(self):\n        \"\"\"\n        \u8fd4\u56de\u6570\u636e\u96c6\u7684\u8fed\u4ee3\u5668\u3002\n\n        \u8fd4\u56de:\n            \u4ea7\u751f\u6570\u636e\u6837\u672c\u7684\u8fed\u4ee3\u5668\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/utils/index.zh/#_11","title":"\u53ef\u8fed\u4ee3\u6570\u636e\u96c6\u793a\u4f8b","text":"Python<pre><code>import random\nfrom genesis.utils.data import IterableDataset\n\nclass RandomDataStream(IterableDataset):\n    \"\"\"\u6d41\u5f0f\u6570\u636e\u96c6\u793a\u4f8b\u3002\"\"\"\n\n    def __init__(self, num_samples, feature_dim):\n        \"\"\"\n        \u521d\u59cb\u5316\u6d41\u5f0f\u6570\u636e\u96c6\u3002\n\n        \u53c2\u6570:\n            num_samples: \u8981\u751f\u6210\u7684\u6837\u672c\u6570\n            feature_dim: \u6bcf\u4e2a\u6837\u672c\u7684\u7ef4\u5ea6\n        \"\"\"\n        self.num_samples = num_samples\n        self.feature_dim = feature_dim\n\n    def __iter__(self):\n        \"\"\"\u5373\u65f6\u751f\u6210\u968f\u673a\u6837\u672c\u3002\"\"\"\n        for _ in range(self.num_samples):\n            # \u751f\u6210\u968f\u673a\u6570\u636e\n            data = [random.random() for _ in range(self.feature_dim)]\n            label = random.randint(0, 9)\n            yield data, label\n</code></pre>"},{"location":"api/utils/index.zh/#dataloader","title":"<code>DataLoader</code>","text":"<p>\u5177\u6709\u6279\u5904\u7406\u548c\u6d17\u724c\u7684\u9ad8\u6548\u6570\u636e\u52a0\u8f7d\u3002</p> Python<pre><code>from genesis.utils.data import DataLoader\n\nclass DataLoader:\n    \"\"\"\n    \u7528\u4e8e\u6279\u5904\u7406\u548c\u6d17\u724c\u6570\u636e\u96c6\u7684\u6570\u636e\u52a0\u8f7d\u5668\u3002\n\n    \u53c2\u6570:\n        dataset: \u6570\u636e\u96c6\u5b9e\u4f8b\uff08Dataset\u6216IterableDataset\uff09\n        batch_size: \u6bcf\u6279\u6837\u672c\u6570\uff08\u9ed8\u8ba4: 1\uff09\n        shuffle: \u662f\u5426\u5728\u6bcf\u4e2aepoch\u6d17\u724c\u6570\u636e\uff08\u9ed8\u8ba4: False\uff09\n    \"\"\"\n\n    def __init__(\n        self, \n        dataset, \n        batch_size: int = 1, \n        shuffle: bool = False\n    ):\n</code></pre>"},{"location":"api/utils/index.zh/#dataloader_1","title":"DataLoader\u793a\u4f8b","text":"Python<pre><code>from genesis.utils.data import Dataset, DataLoader\nimport numpy as np\n\n# \u521b\u5efa\u7b80\u5355\u6570\u636e\u96c6\nclass SimpleDataset(Dataset):\n    def __init__(self, size):\n        self.data = np.random.randn(size, 10)\n        self.labels = np.random.randint(0, 2, size)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# \u521b\u5efa\u6570\u636e\u96c6\u548c\u6570\u636e\u52a0\u8f7d\u5668\ndataset = SimpleDataset(1000)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(5):\n    print(f\"Epoch {epoch + 1}\")\n    for batch_idx, batch in enumerate(dataloader):\n        # batch\u662f(data, label)\u5143\u7ec4\u7684\u5217\u8868\n        batch_data = [item[0] for item in batch]\n        batch_labels = [item[1] for item in batch]\n\n        # \u5982\u9700\u8981\u8f6c\u6362\u4e3a\u6570\u7ec4\n        batch_data = np.array(batch_data)\n        batch_labels = np.array(batch_labels)\n\n        print(f\"  Batch {batch_idx}: data shape {batch_data.shape}\")\n\n        # \u4f60\u7684\u8bad\u7ec3\u4ee3\u7801\u5728\u8fd9\u91cc\n        pass\n</code></pre>"},{"location":"api/utils/index.zh/#dataloader_2","title":"\u9ad8\u7ea7DataLoader\u4f7f\u7528","text":"Python<pre><code># \u5e26\u6d17\u724c\u7684\u5927\u6570\u636e\u96c6\nlarge_dataset = SimpleDataset(50000)\ntrain_loader = DataLoader(large_dataset, batch_size=128, shuffle=True)\n\n# \u53ef\u8fed\u4ee3\u6570\u636e\u96c6\nstream_dataset = RandomDataStream(1000, 20)\nstream_loader = DataLoader(stream_dataset, batch_size=16)\n\n# \u8c03\u8bd5\u7528\u5c0f\u6279\u91cf\ndebug_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n\n# \u591a\u6570\u636e\u52a0\u8f7d\u5668\u7684\u8bad\u7ec3\u5faa\u73af\ndef train_model(model, train_loader, val_loader):\n    for epoch in range(num_epochs):\n        # \u8bad\u7ec3\u9636\u6bb5\n        model.train()\n        for batch in train_loader:\n            # \u8bad\u7ec3\u4ee3\u7801\n            pass\n\n        # \u9a8c\u8bc1\u9636\u6bb5\n        model.eval()\n        for batch in val_loader:\n            # \u9a8c\u8bc1\u4ee3\u7801\n            pass\n</code></pre>"},{"location":"api/utils/index.zh/#genesis","title":"\u4e0eGenesis\u8bad\u7ec3\u7684\u96c6\u6210","text":""},{"location":"api/utils/index.zh/#_12","title":"\u5b8c\u6574\u8bad\u7ec3\u793a\u4f8b","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.utils.data import Dataset, DataLoader\nfrom genesis.utils import profile\nimport numpy as np\n\n# \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\nclass TrainingDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# \u88ab\u5206\u6790\u7684\u6a21\u578b\n@profile\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# \u751f\u6210\u865a\u62df\u6570\u636e\nX = np.random.randn(1000, 20).astype(np.float32)\ny = np.random.randint(0, 3, 1000)\n\n# \u521b\u5efa\u6570\u636e\u96c6\u548c\u6570\u636e\u52a0\u8f7d\u5668\ndataset = TrainingDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# \u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = SimpleModel(20, 64, 3)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u5e26\u5206\u6790\u7684\u8bad\u7ec3\u5faa\u73af\n@profile\ndef train_epoch(model, dataloader, optimizer, criterion):\n    \"\"\"\u8bad\u7ec3\u4e00\u4e2aepoch\u3002\"\"\"\n    total_loss = 0.0\n    for batch in dataloader:\n        # \u63d0\u53d6\u6279\u6570\u636e\n        batch_x = [item[0] for item in batch]\n        batch_y = [item[1] for item in batch]\n\n        # \u8f6c\u6362\u4e3aGenesis\u5f20\u91cf\n        x = genesis.tensor(batch_x)\n        y = genesis.tensor(batch_y)\n\n        # \u524d\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        # \u540e\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n# \u8bad\u7ec3\u6a21\u578b\nfor epoch in range(10):\n    avg_loss = train_epoch(model, dataloader, optimizer, criterion)\n    print(f\"Epoch {epoch + 1}, \u5e73\u5747\u635f\u5931: {avg_loss:.4f}\")\n\n# \u5206\u6790\u6570\u636e\u5c06\u5728\u7a0b\u5e8f\u9000\u51fa\u65f6\u81ea\u52a8\u6253\u5370\n</code></pre>"},{"location":"api/utils/index.zh/#_13","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"api/utils/index.zh/#_14","title":"\u5206\u6790\u6307\u5357","text":"<ol> <li>\u7528\u4e8e\u5f00\u53d1: \u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u542f\u7528\u5206\u6790\u4ee5\u8bc6\u522b\u74f6\u9888</li> <li>\u751f\u4ea7\u73af\u5883\u7981\u7528: \u5728\u751f\u4ea7\u4ee3\u7801\u4e2d\u5220\u9664\u5206\u6790\u88c5\u9970\u5668</li> <li>\u9009\u62e9\u6027\u5206\u6790: \u53ea\u5206\u6790\u4f60\u6000\u7591\u6162\u7684\u51fd\u6570</li> <li>\u6279\u91cf\u5206\u6790: \u5206\u6790\u6574\u4e2a\u8bad\u7ec3\u5faa\u73af\u800c\u4e0d\u662f\u5355\u4e2a\u64cd\u4f5c</li> </ol>"},{"location":"api/utils/index.zh/#_15","title":"\u6570\u636e\u52a0\u8f7d\u6307\u5357","text":"<ol> <li>\u9002\u5f53\u7684\u6279\u5927\u5c0f: \u5e73\u8861\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u6548\u7387</li> <li>\u6d17\u724c\u8bad\u7ec3\u6570\u636e: \u5728epochs\u4e4b\u95f4\u59cb\u7ec8\u6d17\u724c\u8bad\u7ec3\u6570\u636e</li> <li>\u4e0d\u6d17\u724c\u9a8c\u8bc1: \u4fdd\u6301\u9a8c\u8bc1\u6570\u636e\u4e00\u81f4\u7684\u987a\u5e8f</li> <li>\u5185\u5b58\u8003\u8651: \u5bf9\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\u4f7f\u7528\u53ef\u8fed\u4ee3\u6570\u636e\u96c6</li> <li>\u6570\u636e\u9884\u5904\u7406: \u5728\u6570\u636e\u96c6\u7684<code>__getitem__</code>\u65b9\u6cd5\u4e2d\u5e94\u7528\u53d8\u6362</li> </ol>"},{"location":"api/utils/index.zh/#_16","title":"\u6027\u80fd\u63d0\u793a","text":""},{"location":"api/utils/index.zh/#_17","title":"\u9ad8\u6548\u6570\u636e\u52a0\u8f7d","text":"Python<pre><code># \u597d\uff1a\u9ad8\u6548\u6279\u5904\u7406\nclass EfficientDataset(Dataset):\n    def __init__(self, data):\n        # \u9884\u5904\u7406\u6570\u636e\u4e00\u6b21\n        self.data = self._preprocess(data)\n\n    def _preprocess(self, data):\n        # \u6602\u8d35\u7684\u9884\u5904\u7406\u53ea\u505a\u4e00\u6b21\n        return data * 2 + 1\n\n    def __getitem__(self, idx):\n        # \u5feb\u901f\u8bbf\u95ee\n        return self.data[idx]\n\n# \u597d\uff1a\u5c3d\u53ef\u80fd\u4f7f\u7528\u5927\u6279\u5927\u5c0f\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n\n# \u597d\uff1a\u4f7f\u7528\u9002\u5f53\u7684\u6570\u636e\u7c7b\u578b\ndata = np.array(data, dtype=np.float32)  # \u4f7f\u7528float32\u800c\u4e0d\u662ffloat64\n</code></pre>"},{"location":"api/utils/index.zh/#_18","title":"\u5185\u5b58\u7ba1\u7406","text":"Python<pre><code># \u597d\uff1a\u5b8c\u6210\u540e\u5220\u9664\u5927\u5bf9\u8c61\ndel large_dataset\ndel temporary_data\n\n# \u597d\uff1a\u5bf9\u5927\u6570\u636e\u96c6\u4f7f\u7528\u751f\u6210\u5668\ndef data_generator():\n    for file in file_list:\n        data = load_file(file)\n        yield data\n\n# \u597d\uff1a\u5982\u9700\u8981\u7528\u8f83\u5c0f\u6279\u91cf\u9650\u5236\u5185\u5b58\u4f7f\u7528\nsmall_batch_loader = DataLoader(dataset, batch_size=16)\n</code></pre>"},{"location":"api/utils/index.zh/#_19","title":"\u53e6\u8bf7\u53c2\u9605","text":"<ul> <li>\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 - \u6784\u5efa\u6a21\u578b</li> <li>\u4f18\u5316\u5668 - \u8bad\u7ec3\u7b97\u6cd5  </li> <li>\u81ea\u52a8\u5fae\u5206 - \u81ea\u52a8\u5fae\u5206</li> <li>\u6027\u80fd\u6307\u5357 - \u4f18\u5316\u6280\u672f</li> </ul>"},{"location":"api/utils/index_en/","title":"Utilities (genesis.utils)","text":""},{"location":"api/utils/index_en/#overview","title":"Overview","text":"<p>The <code>genesis.utils</code> module provides essential utilities for development, debugging, and data handling. It includes profiling tools, data loading utilities, and helper functions to streamline the deep learning workflow.</p>"},{"location":"api/utils/index_en/#core-components","title":"Core Components","text":""},{"location":"api/utils/index_en/#performance-profiling","title":"Performance Profiling","text":"<ul> <li>Function and method execution time tracking</li> <li>Automatic profiling with decorators</li> <li>Performance analysis and reporting</li> </ul>"},{"location":"api/utils/index_en/#data-loading","title":"Data Loading","text":"<ul> <li>Dataset abstraction for training data</li> <li>DataLoader with batching and shuffling</li> <li>Support for both map-style and iterable datasets</li> </ul>"},{"location":"api/utils/index_en/#profiling-tools","title":"Profiling Tools","text":""},{"location":"api/utils/index_en/#profile-decorator","title":"<code>@profile</code> Decorator","text":"<p>Automatic performance profiling for functions and classes.</p> Python<pre><code>from genesis.utils import profile\n\n@profile\ndef expensive_function(x):\n    \"\"\"\n    Profile this function's execution time and call count.\n    \"\"\"\n    # Your computation here\n    return x * 2\n\n@profile\nclass MyModel:\n    \"\"\"\n    Profile all methods in this class.\n    \"\"\"\n    def forward(self, x):\n        return x + 1\n\n    def backward(self, grad):\n        return grad\n</code></pre> <p>The profiler automatically tracks: - Call Count: Number of times each function is called - Total Time: Cumulative execution time - Average Time: Mean execution time per call</p>"},{"location":"api/utils/index_en/#usage-examples","title":"Usage Examples","text":"Python<pre><code>import genesis.utils as utils\nimport time\n\n# Profile a function\n@utils.profile\ndef matrix_multiply(a, b):\n    \"\"\"Dummy matrix multiplication.\"\"\"\n    time.sleep(0.01)  # Simulate computation\n    return a @ b\n\n# Profile a class\n@utils.profile\nclass NeuralNetwork:\n    def __init__(self):\n        pass\n\n    def forward(self, x):\n        time.sleep(0.005)  # Simulate forward pass\n        return x * 2\n\n    def backward(self, grad):\n        time.sleep(0.003)  # Simulate backward pass\n        return grad\n\n# Use the profiled functions\nmodel = NeuralNetwork()\nfor i in range(100):\n    x = matrix_multiply([[1, 2]], [[3], [4]])\n    y = model.forward(x)\n    model.backward([1, 1])\n\n# Profile data is automatically printed at program exit\n</code></pre>"},{"location":"api/utils/index_en/#profile-data-format","title":"Profile Data Format","text":"<p>When the program exits, profiling data is automatically printed:</p> Text Only<pre><code>Program cost 2.1456 seconds!\n__main__.matrix_multiply: 100 calls, 1.0234 total seconds\n__main__.NeuralNetwork.forward: 100 calls, 0.5123 total seconds\n__main__.NeuralNetwork.backward: 100 calls, 0.3089 total seconds\n</code></pre>"},{"location":"api/utils/index_en/#manual-profiling","title":"Manual Profiling","text":"<p>For more granular control, you can access profiling data programmatically:</p> Python<pre><code>from genesis.utils.profile import profile_data, print_profile_data\n\n# Get current profile data\ncurrent_data = profile_data.copy()\nprint(f\"Function calls so far: {sum(data['calls'] for data in current_data.values())}\")\n\n# Print profile summary manually\nprint_profile_data()\n</code></pre>"},{"location":"api/utils/index_en/#data-loading_1","title":"Data Loading","text":""},{"location":"api/utils/index_en/#dataset","title":"<code>Dataset</code>","text":"<p>Abstract base class for all datasets.</p> Python<pre><code>from genesis.utils.data import Dataset\n\nclass Dataset:\n    \"\"\"\n    Abstract dataset class.\n\n    All subclasses must implement __len__ and __getitem__.\n    \"\"\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the size of the dataset.\n\n        Returns:\n            Number of samples in dataset\n        \"\"\"\n        raise NotImplementedError\n\n    def __getitem__(self, idx: int):\n        \"\"\"\n        Retrieve a sample by index.\n\n        Args:\n            idx: Sample index\n\n        Returns:\n            Data sample at the given index\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/utils/index_en/#custom-dataset-example","title":"Custom Dataset Example","text":"Python<pre><code>import numpy as np\nfrom genesis.utils.data import Dataset\n\nclass MNIST(Dataset):\n    \"\"\"Example MNIST dataset implementation.\"\"\"\n\n    def __init__(self, data_path, transform=None):\n        \"\"\"\n        Initialize MNIST dataset.\n\n        Args:\n            data_path: Path to MNIST data files\n            transform: Optional data transformation function\n        \"\"\"\n        self.data = self._load_data(data_path)\n        self.labels = self._load_labels(data_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n    def _load_data(self, path):\n        # Load your data here\n        return np.random.randn(10000, 28, 28)  # Dummy data\n\n    def _load_labels(self, path):\n        # Load your labels here\n        return np.random.randint(0, 10, 10000)  # Dummy labels\n</code></pre>"},{"location":"api/utils/index_en/#iterabledataset","title":"<code>IterableDataset</code>","text":"<p>Base class for iterable-style datasets.</p> Python<pre><code>from genesis.utils.data import IterableDataset\n\nclass IterableDataset(Dataset):\n    \"\"\"\n    Base class for iterable datasets.\n\n    Useful for streaming data or when random access is not feasible.\n    \"\"\"\n\n    def __iter__(self):\n        \"\"\"\n        Return an iterator over the dataset.\n\n        Returns:\n            Iterator that yields data samples\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/utils/index_en/#iterable-dataset-example","title":"Iterable Dataset Example","text":"Python<pre><code>import random\nfrom genesis.utils.data import IterableDataset\n\nclass RandomDataStream(IterableDataset):\n    \"\"\"Example streaming dataset.\"\"\"\n\n    def __init__(self, num_samples, feature_dim):\n        \"\"\"\n        Initialize streaming dataset.\n\n        Args:\n            num_samples: Number of samples to generate\n            feature_dim: Dimension of each sample\n        \"\"\"\n        self.num_samples = num_samples\n        self.feature_dim = feature_dim\n\n    def __iter__(self):\n        \"\"\"Generate random samples on-the-fly.\"\"\"\n        for _ in range(self.num_samples):\n            # Generate random data\n            data = [random.random() for _ in range(self.feature_dim)]\n            label = random.randint(0, 9)\n            yield data, label\n</code></pre>"},{"location":"api/utils/index_en/#dataloader","title":"<code>DataLoader</code>","text":"<p>Efficient data loading with batching and shuffling.</p> Python<pre><code>from genesis.utils.data import DataLoader\n\nclass DataLoader:\n    \"\"\"\n    Data loader for batching and shuffling datasets.\n\n    Args:\n        dataset: Dataset instance (Dataset or IterableDataset)\n        batch_size: Number of samples per batch (default: 1)\n        shuffle: Whether to shuffle data each epoch (default: False)\n    \"\"\"\n\n    def __init__(\n        self, \n        dataset, \n        batch_size: int = 1, \n        shuffle: bool = False\n    ):\n</code></pre>"},{"location":"api/utils/index_en/#dataloader-examples","title":"DataLoader Examples","text":"Python<pre><code>from genesis.utils.data import Dataset, DataLoader\nimport numpy as np\n\n# Create a simple dataset\nclass SimpleDataset(Dataset):\n    def __init__(self, size):\n        self.data = np.random.randn(size, 10)\n        self.labels = np.random.randint(0, 2, size)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n\n# Create dataset and dataloader\ndataset = SimpleDataset(1000)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training loop\nfor epoch in range(5):\n    print(f\"Epoch {epoch + 1}\")\n    for batch_idx, batch in enumerate(dataloader):\n        # batch is a list of (data, label) tuples\n        batch_data = [item[0] for item in batch]\n        batch_labels = [item[1] for item in batch]\n\n        # Convert to arrays if needed\n        batch_data = np.array(batch_data)\n        batch_labels = np.array(batch_labels)\n\n        print(f\"  Batch {batch_idx}: data shape {batch_data.shape}\")\n\n        # Your training code here\n        pass\n</code></pre>"},{"location":"api/utils/index_en/#advanced-dataloader-usage","title":"Advanced DataLoader Usage","text":"Python<pre><code># Large dataset with shuffling\nlarge_dataset = SimpleDataset(50000)\ntrain_loader = DataLoader(large_dataset, batch_size=128, shuffle=True)\n\n# Iterable dataset\nstream_dataset = RandomDataStream(1000, 20)\nstream_loader = DataLoader(stream_dataset, batch_size=16)\n\n# Small batch for debugging\ndebug_loader = DataLoader(dataset, batch_size=4, shuffle=False)\n\n# Training loop with multiple dataloaders\ndef train_model(model, train_loader, val_loader):\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        for batch in train_loader:\n            # Training code\n            pass\n\n        # Validation phase\n        model.eval()\n        for batch in val_loader:\n            # Validation code\n            pass\n</code></pre>"},{"location":"api/utils/index_en/#integration-with-genesis-training","title":"Integration with Genesis Training","text":""},{"location":"api/utils/index_en/#complete-training-example","title":"Complete Training Example","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.utils.data import Dataset, DataLoader\nfrom genesis.utils import profile\nimport numpy as np\n\n# Custom dataset\nclass TrainingDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n# Profiled model\n@profile\nclass SimpleModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Generate dummy data\nX = np.random.randn(1000, 20).astype(np.float32)\ny = np.random.randint(0, 3, 1000)\n\n# Create dataset and dataloader\ndataset = TrainingDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Model and optimizer\nmodel = SimpleModel(20, 64, 3)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop with profiling\n@profile\ndef train_epoch(model, dataloader, optimizer, criterion):\n    \"\"\"Train for one epoch.\"\"\"\n    total_loss = 0.0\n    for batch in dataloader:\n        # Extract batch data\n        batch_x = [item[0] for item in batch]\n        batch_y = [item[1] for item in batch]\n\n        # Convert to Genesis tensors\n        x = genesis.tensor(batch_x)\n        y = genesis.tensor(batch_y)\n\n        # Forward pass\n        optimizer.zero_grad()\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        # Backward pass\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n# Train the model\nfor epoch in range(10):\n    avg_loss = train_epoch(model, dataloader, optimizer, criterion)\n    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n\n# Profiling data will be printed automatically at program exit\n</code></pre>"},{"location":"api/utils/index_en/#best-practices","title":"Best Practices","text":""},{"location":"api/utils/index_en/#profiling-guidelines","title":"Profiling Guidelines","text":"<ol> <li>Use for Development: Enable profiling during development to identify bottlenecks</li> <li>Disable for Production: Remove profiling decorators in production code</li> <li>Selective Profiling: Profile only the functions you suspect are slow</li> <li>Batch Profiling: Profile entire training loops rather than individual operations</li> </ol>"},{"location":"api/utils/index_en/#data-loading-guidelines","title":"Data Loading Guidelines","text":"<ol> <li>Appropriate Batch Size: Balance memory usage and training efficiency</li> <li>Shuffle Training Data: Always shuffle training data between epochs</li> <li>Don't Shuffle Validation: Keep validation data in consistent order</li> <li>Memory Considerations: Use iterable datasets for very large datasets</li> <li>Data Preprocessing: Apply transforms in the dataset's <code>__getitem__</code> method</li> </ol>"},{"location":"api/utils/index_en/#performance-tips","title":"Performance Tips","text":""},{"location":"api/utils/index_en/#efficient-data-loading","title":"Efficient Data Loading","text":"Python<pre><code># Good: Efficient batch processing\nclass EfficientDataset(Dataset):\n    def __init__(self, data):\n        # Pre-process data once\n        self.data = self._preprocess(data)\n\n    def _preprocess(self, data):\n        # Expensive preprocessing done once\n        return data * 2 + 1\n\n    def __getitem__(self, idx):\n        # Fast access\n        return self.data[idx]\n\n# Good: Large batch sizes when possible\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n\n# Good: Use appropriate data types\ndata = np.array(data, dtype=np.float32)  # Use float32 instead of float64\n</code></pre>"},{"location":"api/utils/index_en/#memory-management","title":"Memory Management","text":"Python<pre><code># Good: Delete large objects when done\ndel large_dataset\ndel temporary_data\n\n# Good: Use generators for large datasets\ndef data_generator():\n    for file in file_list:\n        data = load_file(file)\n        yield data\n\n# Good: Limit memory usage with smaller batches if needed\nsmall_batch_loader = DataLoader(dataset, batch_size=16)\n</code></pre>"},{"location":"api/utils/index_en/#see-also","title":"See Also","text":"<ul> <li>Neural Network Modules - Building models</li> <li>Optimizers - Training algorithms  </li> <li>Autograd - Automatic differentiation</li> <li>Performance Guide - Optimization techniques</li> </ul>"},{"location":"api-reference/index.zh/","title":"API \u53c2\u8003\u6587\u6863","text":"<p>Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684API\u63a5\u53e3\uff0c\u672c\u8282\u63d0\u4f9b\u8be6\u7ec6\u7684\u4ee3\u7801\u7ea7\u6587\u6863\u548c\u4f7f\u7528\u793a\u4f8b\u3002</p>"},{"location":"api-reference/index.zh/#_1","title":"\u6838\u5fc3\u6a21\u5757\u7ed3\u6784","text":""},{"location":"api-reference/index.zh/#_2","title":"\u4e3b\u8981\u547d\u540d\u7a7a\u95f4","text":"<ul> <li><code>genesis</code>: \u6838\u5fc3\u5f20\u91cf\u548c\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf</li> <li><code>genesis.nn</code>: \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u548c\u5c42</li> <li><code>genesis.optim</code>: \u4f18\u5316\u5668\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\u5668</li> <li><code>genesis.functional</code>: \u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3</li> <li><code>genesis.utils</code>: \u5de5\u5177\u51fd\u6570\u548c\u8f85\u52a9\u7c7b</li> </ul>"},{"location":"api-reference/index.zh/#_3","title":"\u5feb\u901f\u5bfc\u822a","text":"\u6a21\u5757 \u63cf\u8ff0 \u4e3b\u8981\u7c7b/\u51fd\u6570 genesis \u6838\u5fc3\u5f20\u91cf\u7cfb\u7edf <code>Tensor</code>, <code>autocast</code>, <code>no_grad</code> nn \u795e\u7ecf\u7f51\u7edc\u5c42 <code>Module</code>, <code>Linear</code>, <code>MultiHeadAttention</code> optim \u4f18\u5316\u5668 <code>SGD</code>, <code>Adam</code>, <code>AdamW</code> functional \u51fd\u6570\u5f0f\u64cd\u4f5c <code>relu</code>, <code>softmax</code>, <code>matmul</code> utils \u5de5\u5177\u51fd\u6570 <code>profile</code>, <code>DataLoader</code>"},{"location":"api-reference/index.zh/#_4","title":"\u4ee3\u7801\u7ea6\u5b9a","text":""},{"location":"api-reference/index.zh/#_5","title":"\u5bfc\u5165\u89c4\u8303","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport genesis.nn.functional as F\n</code></pre>"},{"location":"api-reference/index.zh/#_6","title":"\u8bbe\u5907\u7ba1\u7406","text":"Python<pre><code># \u8bbe\u7f6e\u9ed8\u8ba4\u8bbe\u5907\ngenesis.set_default_device(genesis.cuda())\n\n# \u68c0\u67e5CUDA\u53ef\u7528\u6027\nif genesis.cuda.is_available():\n    device = genesis.cuda()\nelse:\n    device = genesis.cpu()\n</code></pre>"},{"location":"api-reference/index.zh/#_7","title":"\u6570\u636e\u7c7b\u578b","text":"Python<pre><code># \u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\ngenesis.float32  # \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\ngenesis.float16  # \u534a\u7cbe\u5ea6\u6d6e\u70b9\ngenesis.int32    # 32\u4f4d\u6574\u6570\ngenesis.bool     # \u5e03\u5c14\u7c7b\u578b\n</code></pre>"},{"location":"api-reference/index.zh/#_8","title":"\u5feb\u901f\u793a\u4f8b","text":""},{"location":"api-reference/index.zh/#_9","title":"\u57fa\u7840\u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([[1, 2], [3, 4]], dtype=genesis.float32)\ny = genesis.randn(2, 2)\n\n# \u57fa\u7840\u8fd0\u7b97\nz = x + y\nresult = genesis.matmul(x, y.T)\n\n# \u68af\u5ea6\u8ba1\u7b97\nx.requires_grad_(True)\nloss = (x ** 2).sum()\nloss.backward()\nprint(x.grad)  # \u6253\u5370\u68af\u5ea6\n</code></pre>"},{"location":"api-reference/index.zh/#_10","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u578b","text":"Python<pre><code>import genesis.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n# \u4f7f\u7528\u6a21\u578b\nmodel = MLP(784, 256, 10)\nx = genesis.randn(32, 784)\noutput = model(x)\n</code></pre>"},{"location":"api-reference/index.zh/#_11","title":"\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis.optim as optim\n\n# \u521d\u59cb\u5316\nmodel = MLP(784, 256, 10)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u8bad\u7ec3\u6b65\u9aa4\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"api-reference/index.zh/#_12","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":""},{"location":"api-reference/index.zh/#_13","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    output = model(input_tensor)\n    loss = criterion(output, target)\n</code></pre>"},{"location":"api-reference/index.zh/#gpu","title":"GPU\u5185\u5b58\u4f18\u5316","text":"Python<pre><code># \u4f7f\u7528inplace\u64cd\u4f5c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\nx.relu_()  # inplace ReLU\nx.add_(y)  # inplace \u52a0\u6cd5\n\n# \u91ca\u653e\u4e0d\u9700\u8981\u7684\u68af\u5ea6\nwith genesis.no_grad():\n    inference_result = model(data)\n</code></pre>"},{"location":"api-reference/index.zh/#_14","title":"\u6279\u91cf\u64cd\u4f5c\u4f18\u5316","text":"Python<pre><code># \u6279\u91cf\u77e9\u9635\u4e58\u6cd5\nbatch_result = genesis.bmm(batch_a, batch_b)\n\n# \u5411\u91cf\u5316\u64cd\u4f5c\u66ff\u4ee3\u5faa\u73af\nresult = genesis.sum(tensor, dim=1, keepdim=True)\n</code></pre>"},{"location":"api-reference/index_en/","title":"API Reference","text":"<p>Genesis deep learning framework provides complete API interfaces. This section provides detailed code-level documentation and usage examples.</p>"},{"location":"api-reference/index_en/#core-module-structure","title":"Core Module Structure","text":""},{"location":"api-reference/index_en/#main-namespaces","title":"Main Namespaces","text":"<ul> <li><code>genesis</code>: Core tensor and automatic differentiation system</li> <li><code>genesis.nn</code>: Neural network modules and layers  </li> <li><code>genesis.optim</code>: Optimizers and learning rate schedulers</li> <li><code>genesis.functional</code>: Functional operation interfaces</li> <li><code>genesis.utils</code>: Utility functions and helper classes</li> </ul>"},{"location":"api-reference/index_en/#quick-navigation","title":"Quick Navigation","text":"Module Description Main Classes/Functions genesis Core tensor system <code>Tensor</code>, <code>autocast</code>, <code>no_grad</code> nn Neural network layers <code>Module</code>, <code>Linear</code>, <code>MultiHeadAttention</code> optim Optimizers <code>SGD</code>, <code>Adam</code>, <code>AdamW</code> functional Functional operations <code>relu</code>, <code>softmax</code>, <code>matmul</code> utils Utility functions <code>profile</code>, <code>DataLoader</code>"},{"location":"api-reference/index_en/#code-conventions","title":"Code Conventions","text":""},{"location":"api-reference/index_en/#import-standards","title":"Import Standards","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport genesis.nn.functional as F\n</code></pre>"},{"location":"api-reference/index_en/#device-management","title":"Device Management","text":"Python<pre><code># Set default device\ngenesis.set_default_device(genesis.cuda())\n\n# Check CUDA availability\nif genesis.cuda.is_available():\n    device = genesis.cuda()\nelse:\n    device = genesis.cpu()\n</code></pre>"},{"location":"api-reference/index_en/#data-types","title":"Data Types","text":"Python<pre><code># Supported data types\ngenesis.float32  # Default float type\ngenesis.float16  # Half precision float\ngenesis.int32    # 32-bit integer\ngenesis.bool     # Boolean type\n</code></pre>"},{"location":"api-reference/index_en/#quick-examples","title":"Quick Examples","text":""},{"location":"api-reference/index_en/#basic-tensor-operations","title":"Basic Tensor Operations","text":"Python<pre><code>import genesis\n\n# Create tensors\nx = genesis.tensor([[1, 2], [3, 4]], dtype=genesis.float32)\ny = genesis.randn(2, 2)\n\n# Basic operations\nz = x + y\nresult = genesis.matmul(x, y.T)\n\n# Gradient computation\nx.requires_grad_(True)\nloss = (x ** 2).sum()\nloss.backward()\nprint(x.grad)  # Print gradients\n</code></pre>"},{"location":"api-reference/index_en/#neural-network-model","title":"Neural Network Model","text":"Python<pre><code>import genesis.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n# Use model\nmodel = MLP(784, 256, 10)\nx = genesis.randn(32, 784)\noutput = model(x)\n</code></pre>"},{"location":"api-reference/index_en/#training-loop","title":"Training Loop","text":"Python<pre><code>import genesis.optim as optim\n\n# Initialize\nmodel = MLP(784, 256, 10)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Training step\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"api-reference/index_en/#performance-optimization-tips","title":"Performance Optimization Tips","text":""},{"location":"api-reference/index_en/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable automatic mixed precision\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    output = model(input_tensor)\n    loss = criterion(output, target)\n</code></pre>"},{"location":"api-reference/index_en/#gpu-memory-optimization","title":"GPU Memory Optimization","text":"Python<pre><code># Use inplace operations to reduce memory usage\nx.relu_()  # inplace ReLU\nx.add_(y)  # inplace addition\n\n# Release unnecessary gradients\nwith genesis.no_grad():\n    inference_result = model(data)\n</code></pre>"},{"location":"api-reference/index_en/#batch-operation-optimization","title":"Batch Operation Optimization","text":"Python<pre><code># Batch matrix multiplication\nbatch_result = genesis.bmm(batch_a, batch_b)\n\n# Vectorized operations instead of loops\nresult = genesis.sum(tensor, dim=1, keepdim=True)\n</code></pre>"},{"location":"architecture/index.zh/","title":"\u67b6\u6784\u6982\u89c8","text":"<p>Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u91c7\u7528\u4e86\u5206\u5c42\u6a21\u5757\u5316\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u4ee3\u7801\u6e05\u6670\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\u80fd\u529b\u3002</p>"},{"location":"architecture/index.zh/#_2","title":"\ud83c\udfaf \u8bbe\u8ba1\u539f\u5219","text":"<ol> <li>\u6e05\u6670\u7684\u5c42\u6b21\u5206\u79bb: \u6bcf\u4e00\u5c42\u90fd\u6709\u5355\u4e00\u3001\u660e\u786e\u7684\u804c\u8d23</li> <li>\u65e0\u6cc4\u6f0f\u7684\u62bd\u8c61: \u4e0a\u5c42\u4e0d\u4e86\u89e3\u4e0b\u5c42\u5b9e\u73b0\u7ec6\u8282</li> <li>\u8bbe\u5907\u65e0\u5173: \u8ba1\u7b97\u903b\u8f91\u4e0e\u8bbe\u5907\u7279\u5b9a\u5b9e\u73b0\u5206\u79bb</li> <li>\u53ef\u6269\u5c55\u6027: \u6613\u4e8e\u6dfb\u52a0\u65b0\u64cd\u4f5c\u6216\u540e\u7aef\u5b9e\u73b0</li> </ol>"},{"location":"architecture/index.zh/#_3","title":"\ud83c\udfd7\ufe0f \u5206\u5c42\u67b6\u6784","text":""},{"location":"architecture/index.zh/#_4","title":"\u56db\u5c42\u8bbe\u8ba1","text":"<ol> <li>\u5f20\u91cf\u5c42 (\u7528\u6237\u63a5\u53e3 + \u81ea\u52a8\u5fae\u5206)</li> <li>\u9762\u5411\u7528\u6237\u7684API</li> <li>\u81ea\u52a8\u5fae\u5206</li> <li> <p>\u8ba1\u7b97\u56fe\u7ba1\u7406</p> </li> <li> <p>\u51fd\u6570\u5c42 (\u68af\u5ea6\u5b9a\u4e49)</p> </li> <li>\u524d\u5411\u8ba1\u7b97\u903b\u8f91</li> <li>\u53cd\u5411\u68af\u5ea6\u89c4\u5219</li> <li> <p>\u8fde\u63a5\u5f20\u91cf\u5c42\u548cNDArray\u5c42</p> </li> <li> <p>NDArray\u5c42 (\u8bbe\u5907\u62bd\u8c61)</p> </li> <li>\u8bbe\u5907\u65e0\u5173\u7684\u8ba1\u7b97\u63a5\u53e3</li> <li>CPU/GPU\u7edf\u4e00\u64cd\u4f5c</li> <li> <p>\u8bbe\u5907\u7ba1\u7406\u548c\u5207\u6362</p> </li> <li> <p>\u540e\u7aef\u5c42 (\u5b9e\u9645\u8ba1\u7b97)</p> </li> <li>CPU: PyTorch\u5f20\u91cf</li> <li>GPU: CUDAStorage\u4e0eTriton\u5185\u6838</li> </ol>"},{"location":"architecture/index.zh/#_5","title":"\ud83d\udd04 \u8ba1\u7b97\u6d41\u7a0b","text":"<pre><code>graph TB\n    subgraph \"\u7b2c1\u5c42: \u7528\u6237\u63a5\u53e3\"\n        User[\"\u7528\u6237\u4ee3\u7801&lt;br/&gt;c = a + b\"]\n    end\n\n    subgraph \"\u7b2c2\u5c42: \u5f20\u91cf (\u81ea\u52a8\u5fae\u5206)\"\n        Tensor[\"Tensor.__add__()&lt;br/&gt;\u2022 \u7ba1\u7406\u68af\u5ea6&lt;br/&gt;\u2022 \u6784\u5efa\u8ba1\u7b97\u56fe\"]\n    end\n\n    subgraph \"\u7b2c3\u5c42: \u51fd\u6570\u5c42\"\n        Func[\"nn.functional.Add&lt;br/&gt;\u2022 forward(): \u5b9a\u4e49\u8ba1\u7b97&lt;br/&gt;\u2022 backward(): \u5b9a\u4e49\u68af\u5ea6\"]\n    end\n\n    subgraph \"\u7b2c4\u5c42: NDArray\"\n        NDArray[\"NDArray.__add__()&lt;br/&gt;\u2022 \u8bbe\u5907\u65e0\u5173\u63a5\u53e3&lt;br/&gt;\u2022 \u5206\u53d1\u5230\u540e\u7aef\"]\n    end\n\n    subgraph \"\u7b2c5\u5c42: \u540e\u7aef\"\n        CPU[\"CPU\u540e\u7aef&lt;br/&gt;PyTorch\u5f20\u91cf\"]\n        GPU[\"GPU\u540e\u7aef&lt;br/&gt;CUDAStorage + Triton\"]\n    end\n\n    User --&gt; Tensor\n    Tensor --&gt;|\"\u8c03\u7528\"| Func\n    Func --&gt;|\"a.data + b.data\"| NDArray\n    NDArray --&gt;|\"device.add()\"| CPU\n    NDArray --&gt;|\"device.add()\"| GPU\n\n    style User fill:#e1f5fe\n    style Tensor fill:#f3e5f5\n    style Func fill:#fff3e0\n    style NDArray fill:#e8f5e8\n    style CPU fill:#fce4ec\n    style GPU fill:#e3f2fd</code></pre>"},{"location":"architecture/index.zh/#_6","title":"\u793a\u4f8b: \u52a0\u6cd5\u64cd\u4f5c","text":"Python<pre><code># \u7528\u6237\u4ee3\u7801\nc = a + b  # a, b \u662f\u5f20\u91cf\n\n# \u7b2c1\u5c42: \u5f20\u91cf\ndef __add__(self, other):\n    return genesis.nn.functional.add(self, other)\n\n# \u7b2c2\u5c42: \u51fd\u6570\u5c42\nclass Add(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        ctx.save_for_backward(a, b)\n        # \u53ea\u4f7f\u7528NDArray\u63a5\u53e3\uff0c\u4e0d\u6d89\u53ca\u540e\u7aef\u7ec6\u8282\n        return Tensor(a.data + b.data)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # \u68af\u5ea6\u89c4\u5219\n        return grad_output, grad_output\n\n# \u7b2c3\u5c42: NDArray\ndef __add__(self, other):\n    # \u5206\u53d1\u5230\u8bbe\u5907\u7279\u5b9a\u5b9e\u73b0\n    return self.device.add(self, other)\n\n# \u7b2c4\u5c42: \u540e\u7aef (GPU\u793a\u4f8b)\ndef add(x, y):\n    # \u5b9e\u9645\u7684Triton\u5185\u6838\u6267\u884c\n    output = empty(x.shape)\n    add_kernel[grid](x.ptr, y.ptr, output.ptr, n_elements)\n    return output\n</code></pre>"},{"location":"architecture/index.zh/#_7","title":"\ud83d\udd11 \u5173\u952e\u8bbe\u8ba1\u539f\u5219","text":""},{"location":"architecture/index.zh/#1","title":"1. \u6e05\u6670\u7684\u62bd\u8c61\u5c42\u6b21","text":"<p>\u539f\u5219: \u6bcf\u4e00\u5c42\u53ea\u4e86\u89e3\u76f4\u63a5\u4e0b\u5c42\u7684\u4fe1\u606f\u3002</p> <ul> <li>\u5f20\u91cf \u2192 \u4e86\u89e3 nn.functional (\u7528\u4e8e\u64cd\u4f5c)</li> <li>nn.functional \u2192 \u4e86\u89e3 NDArray (\u7528\u4e8e\u8ba1\u7b97)</li> <li>NDArray \u2192 \u4e86\u89e3 \u540e\u7aef (\u7528\u4e8e\u8bbe\u5907\u7279\u5b9a\u64cd\u4f5c)</li> <li>\u540e\u7aef \u2192 \u5b9e\u73b0\u5b9e\u9645\u8ba1\u7b97</li> </ul> <p>\u53cd\u6a21\u5f0f (\u6211\u4eec\u6b63\u5728\u4fee\u590d\u7684): Python<pre><code># \u9519\u8bef: nn.functional\u4e0d\u5e94\u8be5\u4e86\u89e3CUDAStorage\nif hasattr(tensor.data.data, 'to_numpy'):  # \u89e6\u53ca\u8fc7\u6df1\u5c42\u6b21!\n    # CUDAStorage\u7279\u5b9a\u4ee3\u7801\n</code></pre></p> <p>\u6b63\u786e\u6a21\u5f0f: Python<pre><code># \u6b63\u786e: nn.functional\u53ea\u4f7f\u7528NDArray\u63a5\u53e3\nresult = a.data + b.data  # \u6e05\u6670\u7684\u62bd\u8c61\n</code></pre></p>"},{"location":"architecture/index.zh/#2","title":"2. \u5355\u4e00\u804c\u8d23","text":"<ul> <li>\u5f20\u91cf: \u81ea\u52a8\u5fae\u5206\u548c\u68af\u5ea6\u7ba1\u7406</li> <li>nn.functional: \u5b9a\u4e49\u524d\u5411/\u540e\u5411\u8ba1\u7b97\u89c4\u5219</li> <li>NDArray: \u8bbe\u5907\u62bd\u8c61\u548c\u7edf\u4e00\u64cd\u4f5c</li> <li>\u540e\u7aef: \u5b9e\u9645\u8ba1\u7b97\u5b9e\u73b0</li> </ul>"},{"location":"architecture/index.zh/#3","title":"3. \u53cc\u540e\u7aef\u67b6\u6784","text":"<ul> <li>CPU\u540e\u7aef: \u5229\u7528PyTorch\u6210\u719f\u7684CPU\u5f20\u91cf\u5b9e\u73b0</li> <li>GPU\u540e\u7aef: \u5b8c\u5168\u72ec\u7acb\u7684CUDA\u5b9e\u73b0\uff0c\u4f7f\u7528CUDAStorage</li> </ul>"},{"location":"architecture/index.zh/#_8","title":"\ud83d\udcca \u7ec4\u4ef6\u804c\u8d23","text":""},{"location":"architecture/index.zh/#1-autogradpy","title":"\u7b2c1\u5c42: \u5f20\u91cf (<code>autograd.py</code>)","text":"Python<pre><code>class Tensor:\n    data: NDArray          # \u5e95\u5c42\u6570\u636e (\u59d4\u6258\u8ba1\u7b97)\n    requires_grad: bool    # \u662f\u5426\u9700\u8981\u68af\u5ea6\n    grad: Tensor          # \u7d2f\u79ef\u68af\u5ea6\n    creator: Function     # \u521b\u5efa\u6b64\u5f20\u91cf\u7684\u64cd\u4f5c\n\n    # \u9762\u5411\u7528\u6237\u7684\u64cd\u4f5c\n    def __add__(self, other):\n        return nn.functional.add(self, other)  # \u59d4\u6258\u7ed9\u51fd\u6570\u5c42\n</code></pre> <p>\u804c\u8d23: - \u7ba1\u7406\u8ba1\u7b97\u56fe - \u5b58\u50a8\u548c\u7d2f\u79ef\u68af\u5ea6 - \u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684API - \u59d4\u6258\u5b9e\u9645\u8ba1\u7b97\u7ed9nn.functional</p>"},{"location":"architecture/index.zh/#2-nnfunctional-nnfunctionalpy","title":"\u7b2c2\u5c42: nn.functional (<code>nn/functional.py</code>)","text":"Python<pre><code>class Add(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        ctx.save_for_backward(a, b)\n        # \u53ea\u4f7f\u7528NDArray\u63a5\u53e3\n        return Tensor(a.data + b.data)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # \u5b9a\u4e49\u68af\u5ea6\u89c4\u5219\n        return grad_output, grad_output\n</code></pre> <p>\u804c\u8d23: - \u5b9a\u4e49\u524d\u5411\u8ba1\u7b97\u903b\u8f91 - \u5b9a\u4e49\u540e\u5411\u68af\u5ea6\u89c4\u5219 - \u4fdd\u5b58\u540e\u5411\u4f20\u64ad\u6240\u9700\u4fe1\u606f - **\u4e0d**\u8d1f\u8d23\u5b9e\u9645\u8ba1\u7b97\u5b9e\u73b0</p>"},{"location":"architecture/index.zh/#3-ndarray-ndarrayndarraypy","title":"\u7b2c3\u5c42: NDArray (<code>ndarray/ndarray.py</code>)","text":"Python<pre><code>class NDArray:\n    device: Device        # CPU\u6216CUDA\n    data: Union[torch.Tensor, CUDAStorage]  # \u5b9e\u9645\u6570\u636e\n\n    def __add__(self, other):\n        # \u5206\u53d1\u5230\u8bbe\u5907\u7279\u5b9a\u5b9e\u73b0\n        return self.device.add(self, other)\n</code></pre> <p>\u804c\u8d23: - \u63d0\u4f9b\u8bbe\u5907\u65e0\u5173\u7684\u8ba1\u7b97\u63a5\u53e3 - \u5904\u7406\u8bbe\u5907\u5207\u6362 (CPU \u2194 GPU) - \u5206\u53d1\u64cd\u4f5c\u5230\u6b63\u786e\u7684\u540e\u7aef - \u6570\u636e\u683c\u5f0f\u8f6c\u6362 (numpy\u7b49)</p>"},{"location":"architecture/index.zh/#4-ndarray_ops_cpupy-ndarray_ops_gpupy","title":"\u7b2c4\u5c42: \u540e\u7aef (<code>ndarray_ops_cpu.py</code>, <code>ndarray_ops_gpu.py</code>)","text":"Python<pre><code># GPU\u540e\u7aef\u793a\u4f8b\ndef add(x, y):\n    output = empty(x.shape)\n    add_kernel[grid](x.ptr, y.ptr, output.ptr, n_elements)\n    return output\n</code></pre> <p>\u804c\u8d23: - \u5b9e\u9645\u8ba1\u7b97\u5b9e\u73b0 - \u5185\u5b58\u7ba1\u7406 - \u8bbe\u5907\u7279\u5b9a\u4f18\u5316 - \u5185\u6838\u6267\u884c</p>"},{"location":"architecture/index.zh/#_9","title":"\ud83d\udd04 \u68af\u5ea6\u6d41\u793a\u4f8b","text":"<p>\u8ba9\u6211\u4eec\u8ffd\u8e2a\u4e00\u6b21\u5b8c\u6574\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\uff1a</p> Python<pre><code># \u7528\u6237\u4ee3\u7801\na = Tensor([1, 2, 3], requires_grad=True)\nb = Tensor([4, 5, 6], requires_grad=True)\nc = a + b\nc.backward(Tensor([1, 1, 1]))\n</code></pre>"},{"location":"architecture/index.zh/#_10","title":"\u524d\u5411\u4f20\u64ad","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Tensor\n    participant Functional\n    participant NDArray\n    participant Backend\n\n    User-&gt;&gt;Tensor: c = a + b\n    Tensor-&gt;&gt;Functional: functional.add(a, b)\n    Functional-&gt;&gt;Functional: ctx.save_for_backward(a, b)\n    Functional-&gt;&gt;NDArray: a.data + b.data\n    NDArray-&gt;&gt;Backend: device.add(x, y)\n    Backend--&gt;&gt;NDArray: result_data\n    NDArray--&gt;&gt;Functional: result_ndarray\n    Functional--&gt;&gt;Tensor: Tensor(result_ndarray)\n    Tensor--&gt;&gt;User: c</code></pre>"},{"location":"architecture/index.zh/#_11","title":"\u540e\u5411\u4f20\u64ad","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Tensor\n    participant Functional\n\n    User-&gt;&gt;Tensor: c.backward(grad)\n    Tensor-&gt;&gt;Functional: Add.backward(ctx, grad)\n    Functional-&gt;&gt;Functional: \u8ba1\u7b97\u68af\u5ea6\n    Functional--&gt;&gt;Tensor: grad_a, grad_b\n    Tensor-&gt;&gt;Tensor: a.grad += grad_a\n    Tensor-&gt;&gt;Tensor: b.grad += grad_b</code></pre>"},{"location":"architecture/index.zh/#_12","title":"\u26a0\ufe0f \u5f53\u524d\u6b63\u5728\u4fee\u590d\u7684\u95ee\u9898","text":""},{"location":"architecture/index.zh/#nnfunctional","title":"\u95ee\u9898: nn.functional\u4e2d\u7684\u62bd\u8c61\u6cc4\u6f0f","text":"<p>\u5f53\u524d\uff0c<code>nn.functional</code>\u6709\u5982\u4e0b\u4ee3\u7801: Python<pre><code># \u9519\u8bef: \u89e6\u53ca\u5b9e\u73b0\u7ec6\u8282\nif hasattr(t.data.data, 'to_numpy'):  # \u68c0\u67e5CUDAStorage\n    # \u7279\u6b8aGPU\u5904\u7406\n</code></pre></p>"},{"location":"architecture/index.zh/#_13","title":"\u89e3\u51b3\u65b9\u6848: \u6e05\u6670\u62bd\u8c61","text":"<p>\u6211\u4eec\u6b63\u5728\u91cd\u6784\u4e3a: Python<pre><code># \u6b63\u786e: \u53ea\u4f7f\u7528NDArray\u63a5\u53e3\nresult = a.data + b.data  # NDArray\u5904\u7406\u8bbe\u5907\u7ec6\u8282\n</code></pre></p> <p>\u8fd9\u786e\u4fdd: 1. nn.functional\u4e0d\u4e86\u89e3CUDAStorage 2. \u6bcf\u4e00\u5c42\u53ea\u4e86\u89e3\u5176\u76f4\u63a5\u90bb\u5c45 3. \u6613\u4e8e\u6dfb\u52a0\u65b0\u540e\u7aef\u800c\u4e0d\u66f4\u6539\u4e0a\u5c42</p> <p>\u5173\u952e\u7279\u6027\uff1a - \u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362 - \u7075\u6d3b\u7684\u8ba1\u7b97\u56fe\u6784\u5efa\u548c\u904d\u5386 - \u5185\u7f6e\u7684\u68af\u5ea6\u7d2f\u79ef\u548c\u6e05\u96f6\u673a\u5236</p>"},{"location":"architecture/index.zh/#_14","title":"\u5f20\u91cf\u540e\u7aef\u7cfb\u7edf","text":""},{"location":"architecture/index.zh/#cpu-ndarray_ops_cpupy","title":"CPU\u540e\u7aef (<code>ndarray_ops_cpu.py</code>)","text":"Python<pre><code># \u76f4\u63a5\u4f7f\u7528PyTorch\u64cd\u4f5c\ndef add(x, y):\n    return x + y\n\ndef matmul(x, y):\n    return torch.matmul(x, y)\n</code></pre>"},{"location":"architecture/index.zh/#gpu-ndarray_ops_gpupy","title":"GPU\u540e\u7aef (<code>ndarray_ops_gpu.py</code>)","text":"Python<pre><code># \u4f7f\u7528Triton\u5b9e\u73b0\u7684GPU\u5185\u6838\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"architecture/index.zh/#cuda-cuda_storagepy","title":"CUDA\u5185\u5b58\u7ba1\u7406 (<code>cuda_storage.py</code>)","text":"Python<pre><code>class CUDAStorage:\n    \"\"\"\u7eafCUDA\u5b9e\u73b0\u7684\u5b58\u50a8\uff0c\u4e0d\u4f9d\u8d56PyTorch\"\"\"\n    def __init__(self, shape, dtype):\n        self._cuda_device, self._cuda_context = _ensure_cuda_initialized()\n        self._allocate_memory(shape, dtype)\n\n    def _allocate_memory(self, shape, dtype):\n        # \u4f7f\u7528CUDA Python API\u76f4\u63a5\u5206\u914dGPU\u5185\u5b58\n        size_bytes = prod(shape) * dtype.itemsize\n        result = cuda.cuMemAlloc(size_bytes)\n        self._data_ptr = check_cuda_error(result)\n</code></pre>"},{"location":"architecture/index.zh/#nnmodulespy","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 (<code>nn/modules.py</code>)","text":"Python<pre><code>class Module:\n    \"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u57fa\u7c7b\"\"\"\n    def parameters(self) -&gt; List[Tensor]:\n        # \u9012\u5f52\u6536\u96c6\u6240\u6709\u53c2\u6570\n        return _unpack_params(self.__dict__)\n\n    def forward(self, *args, **kwargs):\n        # \u5b50\u7c7b\u5b9e\u73b0\u5177\u4f53\u7684\u524d\u5411\u4f20\u64ad\u903b\u8f91\n        raise NotImplementedError\n\nclass Linear(Module):\n    \"\"\"\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0\"\"\"\n    def __init__(self, in_features, out_features):\n        self.weight = Parameter(genesis.randn(out_features, in_features))\n        self.bias = Parameter(genesis.zeros(out_features))\n</code></pre>"},{"location":"architecture/index.zh/#_15","title":"\ud83d\udd27 \u5173\u952e\u6280\u672f\u5b9e\u73b0","text":""},{"location":"architecture/index.zh/#1_1","title":"1. \u5185\u5b58\u7ba1\u7406\u7b56\u7565","text":"<p>CPU\u5185\u5b58\u7ba1\u7406\uff1a - \u4f9d\u8d56PyTorch\u7684\u5185\u5b58\u6c60\u548c\u5783\u573e\u56de\u6536 - \u81ea\u52a8\u5904\u7406\u5185\u5b58\u5bf9\u9f50\u548c\u7f13\u5b58\u4f18\u5316</p> <p>GPU\u5185\u5b58\u7ba1\u7406\uff1a Python<pre><code>class CUDAStorage:\n    def __init__(self, shape, dtype, base=None):\n        if base is not None:\n            # \u89c6\u56fe\u5b58\u50a8\uff1a\u5171\u4eab\u5185\u5b58\u4f46\u4fdd\u6301\u5bf9\u539f\u5b58\u50a8\u7684\u5f15\u7528\n            self.base = base\n            self._data_ptr = base._data_ptr + offset\n        else:\n            # \u65b0\u5b58\u50a8\uff1a\u5206\u914d\u72ec\u7acb\u5185\u5b58\n            self.base = None\n            self._data_ptr = cuda.cuMemAlloc(size_bytes)\n\n    def __del__(self):\n        # \u53ea\u6709\u57fa\u7840\u5b58\u50a8\u624d\u91ca\u653e\u5185\u5b58\n        if self.base is None and self._data_ptr:\n            cuda.cuMemFree(self._data_ptr)\n</code></pre></p>"},{"location":"architecture/index.zh/#2_1","title":"2. \u8bbe\u5907\u62bd\u8c61","text":"Python<pre><code>class Device:\n    def __init__(self, name: str, mod: Any, device_id: Optional[int] = None):\n        self.name = name        # \"cpu\" \u6216 \"cuda\"\n        self.mod = mod          # \u5bf9\u5e94\u7684\u64cd\u4f5c\u6a21\u5757\n        self.device_id = device_id  # GPU\u8bbe\u5907ID\n\n    def randn(self, *shape, dtype=genesis.float32):\n        if self.name == \"cuda\":\n            return NDArray(CUDAStorage(shape, dtype), device=self)\n        else:\n            return NDArray(torch.randn(*shape), device=self)\n</code></pre>"},{"location":"architecture/index.zh/#3_1","title":"3. \u7c7b\u578b\u7cfb\u7edf","text":"Python<pre><code># dtypes.py - \u7edf\u4e00\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\nclass DType:\n    def __init__(self, name: str, torch_dtype, numpy_dtype, itemsize: int):\n        self.name = name\n        self.torch_dtype = torch_dtype\n        self.numpy_dtype = numpy_dtype  \n        self.itemsize = itemsize\n\n# \u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\nfloat32 = DType(\"float32\", torch.float32, np.float32, 4)\nfloat16 = DType(\"float16\", torch.float16, np.float16, 2)\nbfloat16 = DType(\"bfloat16\", torch.bfloat16, np.dtype('uint16'), 2)\n</code></pre>"},{"location":"architecture/index.zh/#_16","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316\u7b56\u7565","text":""},{"location":"architecture/index.zh/#1-triton","title":"1. Triton\u5185\u6838\u4f18\u5316","text":"<p>Softmax\u5b9e\u73b0\uff1a Python<pre><code>@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride, output_row_stride, \n                  n_cols, BLOCK_SIZE: tl.constexpr):\n    # \u9ad8\u6548\u7684\u5e76\u884csoftmax\u5b9e\u73b0\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets &lt; n_cols, other=-float('inf'))\n\n    # \u6570\u503c\u7a33\u5b9a\u7684softmax\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets &lt; n_cols)\n</code></pre></p>"},{"location":"architecture/index.zh/#2_2","title":"2. \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># amp.py - \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\nenable_autocast = False\n\ndef _cast(value, dtype):\n    \"\"\"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n</code></pre>"},{"location":"architecture/index.zh/#_17","title":"\ud83d\udd0d \u67b6\u6784\u4f18\u52bf","text":""},{"location":"architecture/index.zh/#_18","title":"\u6559\u80b2\u4ef7\u503c","text":"<ol> <li>\u6e10\u8fdb\u5f0f\u590d\u6742\u5ea6\uff1a\u4ece\u7b80\u5355\u7684CPU\u5b9e\u73b0\u5230\u590d\u6742\u7684GPU\u4f18\u5316</li> <li>\u5b8c\u6574\u5b9e\u73b0\u5c55\u793a\uff1a\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u5b8c\u6574\u6784\u5efa\u8fc7\u7a0b  </li> <li>\u6e05\u6670\u7684\u6a21\u5757\u8fb9\u754c\uff1a\u6bcf\u4e2a\u7ec4\u4ef6\u804c\u8d23\u660e\u786e\uff0c\u4fbf\u4e8e\u7406\u89e3</li> </ol>"},{"location":"architecture/index.zh/#_19","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ol> <li>\u53cc\u540e\u7aef\u8bbe\u8ba1\uff1aCPU\u7a33\u5b9a\u6027 + GPU\u9ad8\u6027\u80fd</li> <li>\u5185\u5b58\u5b89\u5168\uff1aRAII\u6a21\u5f0f\u7684\u5185\u5b58\u7ba1\u7406\uff0c\u9632\u6b62\u5185\u5b58\u6cc4\u6f0f</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7edf\u4e00\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u907f\u514d\u7c7b\u578b\u9519\u8bef</li> </ol>"},{"location":"architecture/index.zh/#_20","title":"\u6027\u80fd\u7279\u6027","text":"<ol> <li>Triton\u4f18\u5316\uff1a\u73b0\u4ee3GPU\u5185\u6838\u7f16\u5199\u65b9\u5f0f</li> <li>\u96f6\u62f7\u8d1d\u89c6\u56fe\uff1a\u9ad8\u6548\u7684\u5f20\u91cf\u89c6\u56fe\u64cd\u4f5c</li> <li>\u5e76\u884c\u8ba1\u7b97\uff1a\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u80fd\u529b</li> </ol>"},{"location":"architecture/index.zh/#_21","title":"\ud83c\udfaf \u8bbe\u8ba1\u6743\u8861","text":""},{"location":"architecture/index.zh/#cpu-vs-gpu","title":"CPU vs GPU \u5b9e\u73b0\u9009\u62e9","text":"<ul> <li>CPU\uff1a\u4f7f\u7528PyTorch\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u517c\u5bb9\u6027</li> <li>GPU\uff1a\u72ec\u7acb\u5b9e\u73b0\u5c55\u793a\u5b8c\u6574\u7684GPU\u7f16\u7a0b\u6808</li> </ul>"},{"location":"architecture/index.zh/#vs","title":"\u7b80\u6d01\u6027 vs \u6027\u80fd","text":"<ul> <li>\u4fdd\u6301API\u7b80\u6d01\u7684\u540c\u65f6\uff0c\u5e95\u5c42\u5b9e\u73b0\u9ad8\u5ea6\u4f18\u5316</li> <li>\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u5c06\u590d\u6742\u6027\u9694\u79bb\u5728\u5e95\u5c42</li> </ul>"},{"location":"architecture/index.zh/#vs_1","title":"\u6559\u80b2 vs \u751f\u4ea7","text":"<ul> <li>\u4ee3\u7801\u6ce8\u91cd\u53ef\u8bfb\u6027\u548c\u6559\u80b2\u4ef7\u503c</li> <li>\u6027\u80fd\u4ecd\u7136\u8fbe\u5230\u5b9e\u7528\u7ea7\u522b</li> </ul> <p>\u8fd9\u79cd\u67b6\u6784\u8bbe\u8ba1\u4f7f\u5f97Genesis\u65e2\u662f\u4e00\u4e2a\u4f18\u79c0\u7684\u5b66\u4e60\u8d44\u6e90\uff0c\u4e5f\u662f\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002</p>"},{"location":"architecture/index_en/","title":"Architecture Overview","text":"<p>The Genesis deep learning framework adopts a layered modular architecture design that maintains code clarity while achieving high-performance computational capabilities.</p>"},{"location":"architecture/index_en/#overall-architecture","title":"\ud83c\udfd7\ufe0f Overall Architecture","text":"<pre><code>graph TB\n    subgraph \"User API Layer\"\n        A[genesis.Tensor] --&gt; B[genesis.nn.*]\n        A --&gt; C[genesis.optim.*]\n        A --&gt; D[genesis.functional.*]\n    end\n\n    subgraph \"Automatic Differentiation Layer\"\n        E[autograd.Tensor] --&gt; F[Function Base Class]\n        F --&gt; G[Context]\n    end\n\n    subgraph \"Tensor System\"\n        H[backend.py] --&gt; I[NDArray Interface]\n    end\n\n    subgraph \"Backend Implementation\"\n        I --&gt; J[CPU Backend&lt;br/&gt;PyTorch Tensors]\n        I --&gt; K[GPU Backend&lt;br/&gt;CUDA + Triton]\n    end\n\n    subgraph \"GPU Independent Implementation\"\n        K --&gt; L[cuda_storage.py&lt;br/&gt;Pure CUDA Memory Management]\n        K --&gt; M[ndarray_ops_gpu.py&lt;br/&gt;Triton kernels]\n        L --&gt; N[CUDA Python API]\n        M --&gt; O[Triton Compiler]\n    end\n\n    subgraph \"CPU Implementation\"\n        J --&gt; P[ndarray_ops_cpu.py&lt;br/&gt;PyTorch Operations]\n        P --&gt; Q[PyTorch Backend]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    E --&gt; H\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style H fill:#fff3e0\n    style K fill:#e8f5e8\n    style J fill:#fce4ec</code></pre>"},{"location":"architecture/index_en/#core-design-philosophy","title":"\ud83d\udd11 Core Design Philosophy","text":""},{"location":"architecture/index_en/#1-dual-backend-architecture","title":"1. Dual Backend Architecture","text":"<p>Genesis adopts an innovative dual backend design:</p> <ul> <li>CPU Backend: Leverages PyTorch's mature CPU tensor implementation to ensure stability and compatibility</li> <li>GPU Backend: Completely independent CUDA implementation that demonstrates the complete process of building a GPU compute stack from scratch</li> </ul>"},{"location":"architecture/index_en/#2-balance-between-education-and-performance","title":"2. Balance Between Education and Performance","text":"<ul> <li>Code Readability: Each module has clear responsibility separation and detailed documentation</li> <li>Performance Optimization: GPU backend uses Triton to implement high-performance kernels</li> <li>Progressive Learning: From simple CPU implementation to complex GPU optimization</li> </ul>"},{"location":"architecture/index_en/#3-modular-design","title":"3. Modular Design","text":"<p>Each component can be understood and extended independently: - Automatic differentiation system is independent of specific tensor implementations - Neural network modules are based on general tensor operations - Backend abstraction allows easy switching between different implementations</p>"},{"location":"architecture/index_en/#main-component-details","title":"\ud83d\udcca Main Component Details","text":""},{"location":"architecture/index_en/#automatic-differentiation-system-autogradpy","title":"Automatic Differentiation System (<code>autograd.py</code>)","text":"Python<pre><code># Core class structure\nclass Tensor:\n    data: NDArray          # Underlying data storage\n    requires_grad: bool    # Whether gradients are required\n    creator: Function      # The operation that created this tensor\n    grad: Tensor          # Gradient tensor\n\nclass Function:\n    @staticmethod\n    def forward(ctx, *args)    # Forward propagation\n    @staticmethod \n    def backward(ctx, grad)    # Backward propagation\n</code></pre> <p>Key Features: - Supports automatic type conversion for mixed precision training - Flexible computation graph construction and traversal - Built-in gradient accumulation and zeroing mechanisms</p>"},{"location":"architecture/index_en/#tensor-backend-system","title":"Tensor Backend System","text":""},{"location":"architecture/index_en/#cpu-backend-ndarray_ops_cpupy","title":"CPU Backend (<code>ndarray_ops_cpu.py</code>)","text":"Python<pre><code># Direct use of PyTorch operations\ndef add(x, y):\n    return x + y\n\ndef matmul(x, y):\n    return torch.matmul(x, y)\n</code></pre>"},{"location":"architecture/index_en/#gpu-backend-ndarray_ops_gpupy","title":"GPU Backend (<code>ndarray_ops_gpu.py</code>)","text":"Python<pre><code># GPU kernels implemented with Triton\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"architecture/index_en/#cuda-memory-management-cuda_storagepy","title":"CUDA Memory Management (<code>cuda_storage.py</code>)","text":"Python<pre><code>class CUDATensor:\n    \"\"\"Pure CUDA tensor implementation, independent of PyTorch\"\"\"\n    def __init__(self, shape, dtype):\n        self._cuda_device, self._cuda_context = _ensure_cuda_initialized()\n        self._allocate_memory(shape, dtype)\n\n    def _allocate_memory(self, shape, dtype):\n        # Allocate GPU memory directly using CUDA Python API\n        size_bytes = prod(shape) * dtype.itemsize\n        result = cuda.cuMemAlloc(size_bytes)\n        self._data_ptr = check_cuda_error(result)\n</code></pre>"},{"location":"architecture/index_en/#neural-network-modules-nnmodulespy","title":"Neural Network Modules (<code>nn/modules.py</code>)","text":"Python<pre><code>class Module:\n    \"\"\"Base class for neural network modules\"\"\"\n    def parameters(self) -&gt; List[Tensor]:\n        # Recursively collect all parameters\n        return _unpack_params(self.__dict__)\n\n    def forward(self, *args, **kwargs):\n        # Subclasses implement specific forward propagation logic\n        raise NotImplementedError\n\nclass Linear(Module):\n    \"\"\"Fully connected layer implementation\"\"\"\n    def __init__(self, in_features, out_features):\n        self.weight = Parameter(genesis.randn(out_features, in_features))\n        self.bias = Parameter(genesis.zeros(out_features))\n</code></pre>"},{"location":"architecture/index_en/#key-technical-implementations","title":"\ud83d\udd27 Key Technical Implementations","text":""},{"location":"architecture/index_en/#1-memory-management-strategy","title":"1. Memory Management Strategy","text":"<p>CPU Memory Management: - Relies on PyTorch's memory pool and garbage collection - Automatically handles memory alignment and cache optimization</p> <p>GPU Memory Management: Python<pre><code>class CUDATensor:\n    def __init__(self, shape, dtype, base=None):\n        if base is not None:\n            # View tensor: shares memory but maintains reference to original tensor\n            self.base = base\n            self._data_ptr = base._data_ptr + offset\n        else:\n            # New tensor: allocate independent memory\n            self.base = None\n            self._data_ptr = cuda.cuMemAlloc(size_bytes)\n\n    def __del__(self):\n        # Only base tensors free memory\n        if self.base is None and self._data_ptr:\n            cuda.cuMemFree(self._data_ptr)\n</code></pre></p>"},{"location":"architecture/index_en/#2-device-abstraction","title":"2. Device Abstraction","text":"Python<pre><code>class Device:\n    def __init__(self, name: str, mod: Any, device_id: Optional[int] = None):\n        self.name = name        # \"cpu\" or \"cuda\"\n        self.mod = mod          # Corresponding operation module\n        self.device_id = device_id  # GPU device ID\n\n    def randn(self, *shape, dtype=genesis.float32):\n        if self.name == \"cuda\":\n            return NDArray(CUDATensor(shape, dtype), device=self)\n        else:\n            return NDArray(torch.randn(*shape), device=self)\n</code></pre>"},{"location":"architecture/index_en/#3-type-system","title":"3. Type System","text":"Python<pre><code># dtypes.py - Unified data type system\nclass DType:\n    def __init__(self, name: str, torch_dtype, numpy_dtype, itemsize: int):\n        self.name = name\n        self.torch_dtype = torch_dtype\n        self.numpy_dtype = numpy_dtype  \n        self.itemsize = itemsize\n\n# Supported data types\nfloat32 = DType(\"float32\", torch.float32, np.float32, 4)\nfloat16 = DType(\"float16\", torch.float16, np.float16, 2)\nbfloat16 = DType(\"bfloat16\", torch.bfloat16, np.dtype('uint16'), 2)\n</code></pre>"},{"location":"architecture/index_en/#performance-optimization-strategies","title":"\ud83d\ude80 Performance Optimization Strategies","text":""},{"location":"architecture/index_en/#1-triton-kernel-optimization","title":"1. Triton Kernel Optimization","text":"<p>Softmax Implementation: Python<pre><code>@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride, output_row_stride, \n                  n_cols, BLOCK_SIZE: tl.constexpr):\n    # Efficient parallel softmax implementation\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets &lt; n_cols, other=-float('inf'))\n\n    # Numerically stable softmax\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets &lt; n_cols)\n</code></pre></p>"},{"location":"architecture/index_en/#2-mixed-precision-training","title":"2. Mixed Precision Training","text":"Python<pre><code># amp.py - Automatic mixed precision\nenable_autocast = False\n\ndef _cast(value, dtype):\n    \"\"\"Automatic type conversion\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n</code></pre>"},{"location":"architecture/index_en/#architectural-advantages","title":"\ud83d\udd0d Architectural Advantages","text":""},{"location":"architecture/index_en/#educational-value","title":"Educational Value","text":"<ol> <li>Progressive Complexity: From simple CPU implementation to complex GPU optimization</li> <li>Complete Implementation Showcase: Demonstrates the complete construction process of a deep learning framework  </li> <li>Clear Module Boundaries: Each component has clear responsibilities, making it easy to understand</li> </ol>"},{"location":"architecture/index_en/#engineering-practice","title":"Engineering Practice","text":"<ol> <li>Dual Backend Design: CPU stability + GPU high performance</li> <li>Memory Safety: RAII pattern memory management prevents memory leaks</li> <li>Type Safety: Unified type system avoids type errors</li> </ol>"},{"location":"architecture/index_en/#performance-characteristics","title":"Performance Characteristics","text":"<ol> <li>Triton Optimization: Modern GPU kernel development approach</li> <li>Zero-Copy Views: Efficient tensor view operations</li> <li>Parallel Computing: Fully utilizes GPU parallel capabilities</li> </ol>"},{"location":"architecture/index_en/#design-trade-offs","title":"\ud83c\udfaf Design Trade-offs","text":""},{"location":"architecture/index_en/#cpu-vs-gpu-implementation-choice","title":"CPU vs GPU Implementation Choice","text":"<ul> <li>CPU: Uses PyTorch to ensure stability and compatibility</li> <li>GPU: Independent implementation showcases complete GPU programming stack</li> </ul>"},{"location":"architecture/index_en/#simplicity-vs-performance","title":"Simplicity vs Performance","text":"<ul> <li>Maintains simple APIs while implementing highly optimized underlying layers</li> <li>Isolates complexity in the lower layers through layered architecture</li> </ul>"},{"location":"architecture/index_en/#education-vs-production","title":"Education vs Production","text":"<ul> <li>Code emphasizes readability and educational value</li> <li>Performance still reaches production-level standards</li> </ul> <p>This architectural design makes Genesis both an excellent learning resource and a fully functional deep learning framework.</p>"},{"location":"benchmark/index.zh/","title":"Genesis \u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a","text":"<p>\u6b22\u8fce\u8bbf\u95ee Genesis \u6846\u67b6\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a\u3002\u6b64\u76ee\u5f55\u5305\u542b Genesis \u4e0e PyTorch \u7684\u5168\u9762\u6027\u80fd\u5bf9\u6bd4\u5206\u6790\u3002</p>"},{"location":"benchmark/index.zh/#genesis_1","title":"\u5173\u4e8e Genesis \u57fa\u51c6\u6d4b\u8bd5","text":"<p>Genesis \u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u8be6\u7ec6\u7684\u6027\u80fd\u5206\u6790\uff0c\u5e2e\u52a9\u7528\u6237\u4e86\u89e3\uff1a - \u64cd\u4f5c\u7ea7\u6027\u80fd\uff1a\u5355\u4e2a\u64cd\u4f5c\u4e0e PyTorch \u7684\u6bd4\u8f83 - \u7aef\u5230\u7aef\u6a21\u578b\u6027\u80fd\uff1a\u5305\u542b\u5185\u5b58\u4f7f\u7528\u7684\u5b8c\u6574\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5 - \u4f18\u5316\u673a\u4f1a\uff1a\u8be6\u7ec6\u7684\u6027\u80fd\u6539\u8fdb\u5efa\u8bae</p>"},{"location":"benchmark/index.zh/#_1","title":"\u57fa\u51c6\u6d4b\u8bd5\u7c7b\u522b","text":""},{"location":"benchmark/index.zh/#_2","title":"\ud83d\udcca \u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u5355\u4e2a\u64cd\u4f5c\u548c\u64cd\u4f5c\u7c7b\u522b\u7684\u8be6\u7ec6\u6027\u80fd\u5206\u6790\uff1a - \u9010\u5143\u7d20\u64cd\u4f5c\uff1a\u52a0\u6cd5\u3001\u51cf\u6cd5\u3001\u4e58\u6cd5\u3001\u9664\u6cd5\u7b49 - \u6fc0\u6d3b\u51fd\u6570\uff1aReLU\u3001sigmoid\u3001tanh \u7b49 - \u5f52\u7ea6\u64cd\u4f5c\uff1a\u6c42\u548c\u3001\u5e73\u5747\u3001\u6700\u5927\u503c\u7b49 - \u77e9\u9635\u64cd\u4f5c\uff1a\u77e9\u9635\u4e58\u6cd5 - \u5185\u5b58\u64cd\u4f5c\uff1a\u8f6c\u7f6e\u3001\u91cd\u5851\u7b49</p>"},{"location":"benchmark/index.zh/#_3","title":"\ud83e\udd16 \u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u5b8c\u6574\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6027\u80fd\u5206\u6790\uff1a - Qwen \u8bed\u8a00\u6a21\u578b\uff1a\u524d\u5411/\u53cd\u5411\u4f20\u64ad\u65f6\u95f4\u548c\u5185\u5b58\u5206\u6790 - \u53ef\u6269\u5c55\u6027\u6d4b\u8bd5\uff1a\u4e0d\u540c\u6279\u91cf\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u6027\u80fd\u8868\u73b0</p>"},{"location":"benchmark/index.zh/#_4","title":"\u6027\u80fd\u6307\u6807","text":"<p>\u6211\u4eec\u4f7f\u7528\u6807\u51c6\u5316\u7684\u6027\u80fd\u8bc4\u5206\u7cfb\u7edf\uff1a</p> <ul> <li>\ud83d\udfe2 \u4f18\u79c0 (\u226590%)\uff1aGenesis \u6027\u80fd\u8fbe\u5230 PyTorch \u7684 90% \u6216\u66f4\u9ad8</li> <li>\ud83d\udfe1 \u826f\u597d (70-90%)\uff1a\u53ef\u63a5\u53d7\u7684\u6027\u80fd\u5dee\u8ddd</li> <li>\ud83d\udfe0 \u4e00\u822c (50-70%)\uff1a\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5efa\u8bae\u4f18\u5316</li> <li>\ud83d\udd34 \u8f83\u5dee (20-50%)\uff1a\u9700\u8981\u91cd\u5927\u4f18\u5316</li> <li>\u274c \u4e25\u91cd (&lt;20%)\uff1a\u5b58\u5728\u9700\u8981\u5173\u6ce8\u7684\u91cd\u5927\u6027\u80fd\u95ee\u9898</li> </ul>"},{"location":"benchmark/index.zh/#_5","title":"\u5982\u4f55\u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"benchmark/index.zh/#_6","title":"\u5feb\u901f\u5f00\u59cb","text":"Bash<pre><code># \u5bfc\u822a\u5230\u57fa\u51c6\u6d4b\u8bd5\u76ee\u5f55\ncd benchmark\n\n# \u8fd0\u884c\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5728 docs/benchmark/ \u751f\u6210\u62a5\u544a\uff09\n./run.sh\n</code></pre>"},{"location":"benchmark/index.zh/#_7","title":"\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"benchmark/index.zh/#_8","title":"\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5","text":"Bash<pre><code># \u6d4b\u8bd5\u6240\u6709\u64cd\u4f5c\uff08\u5168\u9762\uff09\npython bench_ops.py\n\n# \u6d4b\u8bd5\u7279\u5b9a\u64cd\u4f5c\u7c7b\u522b\npython bench_ops.py --category element\n\n# \u6d4b\u8bd5\u7279\u5b9a\u64cd\u4f5c\npython bench_ops.py --op add\n\n# \u5feb\u901f\u6a21\u5f0f\uff08\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\uff09\npython bench_ops.py --fast\n</code></pre>"},{"location":"benchmark/index.zh/#_9","title":"\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5","text":"Bash<pre><code># \u6d4b\u8bd5 Qwen \u6a21\u578b\uff08\u5982\u679c\u53ef\u7528\uff09\npython bench_qwen.py --size 0.5B --batch-size 1,2,4 --seq-len 128,256,512\n\n# \u5feb\u901f\u6d4b\u8bd5\npython bench_qwen.py --size 0.5B --batch-size 1,2 --seq-len 128,256 --fast\n</code></pre>"},{"location":"benchmark/index.zh/#_10","title":"\u7406\u89e3\u62a5\u544a","text":""},{"location":"benchmark/index.zh/#_11","title":"\u64cd\u4f5c\u62a5\u544a","text":"<p>\u6bcf\u4e2a\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a\u5305\u62ec\uff1a - \u7cfb\u7edf\u4fe1\u606f\uff1aGPU \u8be6\u60c5\u548c\u7406\u8bba\u6027\u80fd\u9650\u5236 - \u6027\u80fd\u6458\u8981\uff1a\u603b\u4f53\u7edf\u8ba1\u548c\u6210\u529f\u7387 - \u7c7b\u522b\u7ec6\u5206\uff1a\u6309\u64cd\u4f5c\u7c7b\u578b\u7684\u6027\u80fd\u8868\u73b0 - \u8be6\u7ec6\u7ed3\u679c\uff1a\u5305\u542b\u52a0\u901f\u6bd4\u548c\u5e26\u5bbd\u7684\u5355\u4e2a\u64cd\u4f5c\u7ed3\u679c - \u6700\u4f73\u8868\u73b0\uff1a\u6027\u80fd\u6700\u597d\u548c\u6700\u5dee\u7684\u64cd\u4f5c - \u4f18\u5316\u5efa\u8bae\uff1a\u5177\u4f53\u7684\u6539\u8fdb\u5efa\u8bae</p>"},{"location":"benchmark/index.zh/#_12","title":"\u6a21\u578b\u62a5\u544a","text":"<p>\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a\u63d0\u4f9b\uff1a - \u6a21\u578b\u914d\u7f6e\uff1a\u6a21\u578b\u5927\u5c0f\u3001\u6d4b\u8bd5\u7684\u6279\u91cf\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6 - \u6027\u80fd\u6458\u8981\uff1a\u603b\u4f53\u52a0\u901f\u6bd4\u548c\u5185\u5b58\u6548\u7387 - \u64cd\u4f5c\u5206\u6790\uff1a\u6309\u6a21\u578b\u64cd\u4f5c\u7684\u6027\u80fd\u7ec6\u5206 - \u53ef\u6269\u5c55\u6027\u5206\u6790\uff1a\u4e0d\u540c\u8f93\u5165\u5927\u5c0f\u7684\u6027\u80fd\u8868\u73b0 - \u5185\u5b58\u5206\u6790\uff1a\u5185\u5b58\u4f7f\u7528\u6a21\u5f0f\u548c\u6548\u7387 - \u4f18\u5316\u4f18\u5148\u7ea7\uff1a\u96c6\u4e2d\u7684\u6539\u8fdb\u5efa\u8bae</p>"},{"location":"benchmark/index.zh/#_13","title":"\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd","text":""},{"location":"benchmark/index.zh/#_14","title":"\u53ef\u9760\u6027\u529f\u80fd","text":"<ul> <li>\u7edf\u8ba1\u5f02\u5e38\u503c\u68c0\u6d4b\uff1a\u7a33\u5065\u7684\u65f6\u95f4\u6d4b\u91cf</li> <li>\u81ea\u9002\u5e94\u8fed\u4ee3\uff1a\u81ea\u52a8\u8c03\u6574\u8fed\u4ee3\u6b21\u6570</li> <li>\u5185\u5b58\u5e26\u5bbd\u5206\u6790\uff1a\u7406\u8bba\u6027\u80fd\u6bd4\u8f83</li> <li>\u9519\u8bef\u5904\u7406\uff1a\u4f18\u96c5\u5904\u7406\u5931\u8d25\u7684\u64cd\u4f5c</li> </ul>"},{"location":"benchmark/index.zh/#_15","title":"\u8ba1\u65f6\u6a21\u5f0f","text":"<ul> <li>\u5b9e\u9645\u8ba1\u65f6\uff1a\u5305\u542b\u6240\u6709\u5f00\u9500\uff08\u771f\u5b9e\u7528\u6237\u4f53\u9a8c\uff09</li> <li>\u7eaf\u8ba1\u65f6\uff1a\u6700\u5c0f\u5316\u5f00\u9500\uff08\u5cf0\u503c\u8ba1\u7b97\u6027\u80fd\uff09</li> </ul>"},{"location":"benchmark/index.zh/#_16","title":"\u5f00\u59cb\u4f7f\u7528","text":"<ol> <li>\u5b89\u88c5\u4f9d\u8d56\uff1a\u786e\u4fdd\u6b63\u786e\u5b89\u88c5 Genesis\u3001PyTorch \u548c CUDA</li> <li>\u8fd0\u884c\u5feb\u901f\u6d4b\u8bd5\uff1a<code>cd benchmark &amp;&amp; python bench_ops.py --fast</code></li> <li>\u751f\u6210\u62a5\u544a\uff1a<code>./run.sh</code> \u8fd0\u884c\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u5e76\u751f\u6210\u6587\u6863</li> <li>\u67e5\u770b\u7ed3\u679c\uff1a\u68c0\u67e5\u6b64\u76ee\u5f55\u4e2d\u751f\u6210\u7684\u62a5\u544a</li> </ol>"},{"location":"benchmark/index.zh/#_17","title":"\u6301\u7eed\u6539\u8fdb","text":"<p>\u8fd9\u4e9b\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\uff1a - \u8ddf\u8e2a Genesis \u6027\u80fd\u6539\u8fdb\u7684\u8fdb\u5c55 - \u8bc6\u522b\u4f18\u5316\u673a\u4f1a - \u9a8c\u8bc1\u65b0\u529f\u80fd\u548c\u4f18\u5316 - \u63d0\u4f9b\u6027\u80fd\u7279\u5f81\u7684\u900f\u660e\u5ea6</p> <p>\u6ce8\u610f\uff1a\u8981\u751f\u6210\u6700\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a\uff0c\u8bf7\u4ece benchmark \u76ee\u5f55\u8fd0\u884c <code>./run.sh</code>\u3002\u8fd9\u5c06\u521b\u5efa\u5e26\u6709\u6700\u65b0\u6027\u80fd\u6570\u636e\u7684\u65f6\u95f4\u6233\u62a5\u544a\u3002</p> <p>\u6709\u5173\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u73b0\u7684\u6280\u672f\u7ec6\u8282\uff0c\u8bf7\u53c2\u89c1 <code>benchmark/</code> \u76ee\u5f55\u4e2d\u7684\u6e90\u4ee3\u7801\u3002</p>"},{"location":"benchmark/index_en/","title":"Genesis Benchmark Reports","text":"<p>Welcome to the Genesis framework performance benchmark reports. This directory contains comprehensive performance analysis comparing Genesis with PyTorch.</p>"},{"location":"benchmark/index_en/#about-genesis-benchmarks","title":"About Genesis Benchmarks","text":"<p>Genesis benchmarks provide detailed performance analysis to help users understand: - Operation-level performance: How individual operations compare to PyTorch - End-to-end model performance: Complete model benchmarking including memory usage - Optimization opportunities: Detailed recommendations for performance improvements</p>"},{"location":"benchmark/index_en/#benchmark-categories","title":"Benchmark Categories","text":""},{"location":"benchmark/index_en/#operations-benchmarks","title":"\ud83d\udcca Operations Benchmarks","text":"<p>Detailed performance analysis of individual operations and operation categories: - Element-wise operations: add, subtract, multiply, divide, etc. - Activation functions: ReLU, sigmoid, tanh, etc. - Reduction operations: sum, mean, max, etc. - Matrix operations: matrix multiplication - Memory operations: transpose, reshape, etc.</p>"},{"location":"benchmark/index_en/#model-benchmarks","title":"\ud83e\udd16 Model Benchmarks","text":"<p>End-to-end performance analysis of complete models: - Qwen Language Models: Forward/backward pass timing and memory analysis - Scalability testing: Performance across different batch sizes and sequence lengths</p>"},{"location":"benchmark/index_en/#performance-indicators","title":"Performance Indicators","text":"<p>We use a standardized performance grading system:</p> <ul> <li>\ud83d\udfe2 Excellent (\u226590%): Genesis performs at 90% or better vs PyTorch</li> <li>\ud83d\udfe1 Good (70-90%): Acceptable performance gap</li> <li>\ud83d\udfe0 Fair (50-70%): Notable performance gap, optimization recommended</li> <li>\ud83d\udd34 Poor (20-50%): Significant optimization needed</li> <li>\u274c Critical (&lt;20%): Major performance issues requiring attention</li> </ul>"},{"location":"benchmark/index_en/#how-to-run-benchmarks","title":"How to Run Benchmarks","text":""},{"location":"benchmark/index_en/#quick-start","title":"Quick Start","text":"Bash<pre><code># Navigate to benchmark directory\ncd benchmark\n\n# Run all benchmarks (generates reports in docs/benchmark/)\n./run.sh\n</code></pre>"},{"location":"benchmark/index_en/#specific-benchmarks","title":"Specific Benchmarks","text":""},{"location":"benchmark/index_en/#operations-benchmarks_1","title":"Operations Benchmarks","text":"Bash<pre><code># Test all operations (comprehensive)\npython bench_ops.py\n\n# Test specific operation category\npython bench_ops.py --category element\n\n# Test specific operation\npython bench_ops.py --op add\n\n# Fast mode (reduced iterations)\npython bench_ops.py --fast\n</code></pre>"},{"location":"benchmark/index_en/#model-benchmarks_1","title":"Model Benchmarks","text":"Bash<pre><code># Test Qwen model (if available)\npython bench_qwen.py --size 0.5B --batch-size 1,2,4 --seq-len 128,256,512\n\n# Quick test\npython bench_qwen.py --size 0.5B --batch-size 1,2 --seq-len 128,256 --fast\n</code></pre>"},{"location":"benchmark/index_en/#understanding-the-reports","title":"Understanding the Reports","text":""},{"location":"benchmark/index_en/#operations-reports","title":"Operations Reports","text":"<p>Each operations benchmark report includes: - System Information: GPU details and theoretical performance limits - Performance Summary: Overall statistics and success rates - Category Breakdown: Performance by operation type - Detailed Results: Individual operation results with speedup and bandwidth - Top Performers: Best and worst performing operations - Optimization Recommendations: Specific improvement suggestions</p>"},{"location":"benchmark/index_en/#model-reports","title":"Model Reports","text":"<p>Model benchmark reports provide: - Model Configuration: Model size, batch sizes, sequence lengths tested - Performance Summary: Overall speedup and memory efficiency - Operation Analysis: Performance breakdown by model operation - Scalability Analysis: Performance across different input sizes - Memory Analysis: Memory usage patterns and efficiency - Optimization Priorities: Focused recommendations for improvement</p>"},{"location":"benchmark/index_en/#benchmark-infrastructure","title":"Benchmark Infrastructure","text":""},{"location":"benchmark/index_en/#reliability-features","title":"Reliability Features","text":"<ul> <li>Statistical outlier detection: Robust timing measurements</li> <li>Adaptive iterations: Automatic iteration count adjustment</li> <li>Memory bandwidth analysis: Theoretical performance comparison</li> <li>Error handling: Graceful handling of failed operations</li> </ul>"},{"location":"benchmark/index_en/#timing-modes","title":"Timing Modes","text":"<ul> <li>Real timing: Includes all overheads (realistic user experience)</li> <li>Pure timing: Minimized overhead (peak computational performance)</li> </ul>"},{"location":"benchmark/index_en/#getting-started","title":"Getting Started","text":"<ol> <li>Install Requirements: Ensure Genesis, PyTorch, and CUDA are properly installed</li> <li>Run Quick Test: <code>cd benchmark &amp;&amp; python bench_ops.py --fast</code></li> <li>Generate Reports: <code>./run.sh</code> to run all benchmarks and generate documentation</li> <li>Review Results: Check this directory for generated reports</li> </ol>"},{"location":"benchmark/index_en/#continuous-improvement","title":"Continuous Improvement","text":"<p>These benchmarks are designed to: - Track Genesis performance improvements over time - Identify optimization opportunities - Validate new features and optimizations - Provide transparency in performance characteristics</p> <p>Note: To generate fresh benchmark reports, run <code>./run.sh</code> from the benchmark directory. This will create timestamped reports with the latest performance data.</p> <p>For technical details about the benchmark implementation, see the source code in the <code>benchmark/</code> directory.</p>"},{"location":"benchmark/operations_element.zh/","title":"Genesis \u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u62a5\u544a","text":"<p>\u751f\u6210\u65f6\u95f4\uff1a2025-08-15 16:07:03</p>"},{"location":"benchmark/operations_element.zh/#_1","title":"\u7cfb\u7edf\u4fe1\u606f","text":"<ul> <li>GPU\uff1aNVIDIA A100-SXM4-40GB</li> <li>\u5185\u5b58\uff1a39.4 GB</li> <li>\u7406\u8bba\u5e26\u5bbd\uff1a1555 GB/s</li> <li>\u591a\u5904\u7406\u5668\uff1a108</li> </ul>"},{"location":"benchmark/operations_element.zh/#_2","title":"\u6d4b\u8bd5\u914d\u7f6e","text":"<ul> <li>\u6a21\u5f0f\uff1a\u5feb\u901f</li> <li>\u8ba1\u65f6\uff1a\u5b9e\u9645</li> <li>\u6570\u636e\u7c7b\u578b\uff1afloat32</li> <li>\u7c7b\u522b\uff1aelement</li> </ul>"},{"location":"benchmark/operations_element.zh/#_3","title":"\u6027\u80fd\u6458\u8981","text":"\u6307\u6807 \u503c \u603b\u6d4b\u8bd5\u6570 28 \u6210\u529f\u6d4b\u8bd5 28 \u5931\u8d25\u6d4b\u8bd5 0 \u6210\u529f\u7387 100.0% \u5e73\u5747\u52a0\u901f\u6bd4 0.63x \u4e2d\u4f4d\u6570\u52a0\u901f\u6bd4 0.19x \u6700\u4f73\u52a0\u901f\u6bd4 3.62x \u6700\u5dee\u52a0\u901f\u6bd4 0.11x"},{"location":"benchmark/operations_element.zh/#_4","title":"\u6309\u7c7b\u522b\u7684\u6027\u80fd","text":"\u7c7b\u522b \u6d4b\u8bd5\u6570 \u6210\u529f\u7387 \u5e73\u5747\u52a0\u901f\u6bd4 \u6700\u4f73\u52a0\u901f\u6bd4 \u72b6\u6001 element 28 100.0% 0.63x 3.62x \ud83d\udfe1 \u826f\u597d"},{"location":"benchmark/operations_element.zh/#_5","title":"\u8be6\u7ec6\u7ed3\u679c","text":"\u64cd\u4f5c \u7c7b\u522b \u5f62\u72b6 PyTorch (ms) Genesis (ms) \u52a0\u901f\u6bd4 \u5e26\u5bbd (GB/s) \u72b6\u6001 cos element 256\u00d7256 0.039 0.011 3.62x 51.2 \ud83d\udfe2 \u4f18\u79c0 add_scalar element 512\u00d7512 0.024 0.011 2.20x 186.2 \ud83d\udfe2 \u4f18\u79c0 sub element 256\u00d7256 0.021 0.010 2.15x 76.8 \ud83d\udfe2 \u4f18\u79c0 negate element 256\u00d7256 0.021 0.010 2.12x 51.2 \ud83d\udfe2 \u4f18\u79c0 log element 256\u00d7256 0.014 0.010 1.43x 51.2 \ud83d\udfe2 \u4f18\u79c0 multiply element 256\u00d7256 0.012 0.010 1.22x 76.8 \ud83d\udfe2 \u4f18\u79c0 divide_scalar element 256\u00d7256 0.010 0.010 0.99x 51.2 \ud83d\udfe2 \u4f18\u79c0 sqrt element 256\u00d7256 0.020 0.041 0.49x 51.2 \ud83d\udd34 \u8f83\u5dee mul_scalar element 256\u00d7256 0.024 0.107 0.23x 4.9 \ud83d\udd34 \u8f83\u5dee add_scalar element 256\u00d7256 0.024 0.108 0.22x 4.9 \ud83d\udd34 \u8f83\u5dee mul_scalar element 512\u00d7512 0.026 0.121 0.21x 17.7 \ud83d\udd34 \u8f83\u5dee add element 256\u00d7256 0.016 0.076 0.21x 8.2 \ud83d\udd34 \u8f83\u5dee divide element 256\u00d7256 0.017 0.089 0.19x 6.9 \u274c \u4e25\u91cd sin element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c \u4e25\u91cd exp element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c \u4e25\u91cd sin element 512\u00d7512 0.013 0.069 0.19x 18.1 \u274c \u4e25\u91cd negate element 512\u00d7512 0.011 0.060 0.18x 186.2 \u274c \u4e25\u91cd sqrt element 512\u00d7512 0.021 0.117 0.18x 18.2 \u274c \u4e25\u91cd exp element 512\u00d7512 0.014 0.079 0.18x 21.2 \u274c \u4e25\u91cd multiply element 512\u00d7512 0.020 0.116 0.17x 16.4 \u274c \u4e25\u91cd log element 512\u00d7512 0.020 0.116 0.17x 18.3 \u274c \u4e25\u91cd pow_scalar element 256\u00d7256 0.023 0.139 0.16x 3.8 \u274c \u4e25\u91cd add element 512\u00d7512 0.021 0.132 0.16x 24.4 \u274c \u4e25\u91cd pow_scalar element 512\u00d7512 0.011 0.068 0.16x 186.2 \u274c \u4e25\u91cd divide element 512\u00d7512 0.017 0.130 0.13x 24.5 \u274c \u4e25\u91cd divide_scalar element 512\u00d7512 0.011 0.083 0.13x 17.1 \u274c \u4e25\u91cd sub element 512\u00d7512 0.015 0.132 0.11x 24.4 \u274c \u4e25\u91cd cos element 512\u00d7512 0.013 0.120 0.11x 17.8 \u274c \u4e25\u91cd"},{"location":"benchmark/operations_element.zh/#_6","title":"\u6027\u80fd\u5206\u5e03","text":"<ul> <li>\ud83d\udfe2 \u4f18\u79c0 (\u226590%)\uff1a7 \u4e2a\u6d4b\u8bd5 (25.0%)</li> <li>\ud83d\udfe1 \u826f\u597d (70-90%)\uff1a0 \u4e2a\u6d4b\u8bd5 (0.0%)</li> <li>\ud83d\udfe0 \u4e00\u822c (50-70%)\uff1a0 \u4e2a\u6d4b\u8bd5 (0.0%)</li> <li>\ud83d\udd34 \u8f83\u5dee (20-50%)\uff1a5 \u4e2a\u6d4b\u8bd5 (17.9%)</li> <li>\u274c \u4e25\u91cd (&lt;20%)\uff1a16 \u4e2a\u6d4b\u8bd5 (57.1%)</li> </ul>"},{"location":"benchmark/operations_element.zh/#10","title":"\u524d 10 \u540d\u8868\u73b0","text":"\u6392\u540d \u64cd\u4f5c \u5f62\u72b6 \u52a0\u901f\u6bd4 \u72b6\u6001 1 cos 256\u00d7256 3.62x \ud83d\udfe2 \u4f18\u79c0 2 add_scalar 512\u00d7512 2.20x \ud83d\udfe2 \u4f18\u79c0 3 sub 256\u00d7256 2.15x \ud83d\udfe2 \u4f18\u79c0 4 negate 256\u00d7256 2.12x \ud83d\udfe2 \u4f18\u79c0 5 log 256\u00d7256 1.43x \ud83d\udfe2 \u4f18\u79c0 6 multiply 256\u00d7256 1.22x \ud83d\udfe2 \u4f18\u79c0 7 divide_scalar 256\u00d7256 0.99x \ud83d\udfe2 \u4f18\u79c0 8 sqrt 256\u00d7256 0.49x \ud83d\udd34 \u8f83\u5dee 9 mul_scalar 256\u00d7256 0.23x \ud83d\udd34 \u8f83\u5dee 10 add_scalar 256\u00d7256 0.22x \ud83d\udd34 \u8f83\u5dee"},{"location":"benchmark/operations_element_en/","title":"Genesis Operations Benchmark Report","text":"<p>Generated on: 2025-08-15 16:07:03</p>"},{"location":"benchmark/operations_element_en/#system-information","title":"System Information","text":"<ul> <li>GPU: NVIDIA A100-SXM4-40GB</li> <li>Memory: 39.4 GB</li> <li>Theoretical Bandwidth: 1555 GB/s</li> <li>Multi-processors: 108</li> </ul>"},{"location":"benchmark/operations_element_en/#test-configuration","title":"Test Configuration","text":"<ul> <li>Mode: Fast</li> <li>Timing: real</li> <li>Data Type: float32</li> <li>Category: element</li> </ul>"},{"location":"benchmark/operations_element_en/#performance-summary","title":"Performance Summary","text":"Metric Value Total Tests 28 Successful Tests 28 Failed Tests 0 Success Rate 100.0% Average Speedup 0.63x Median Speedup 0.19x Best Speedup 3.62x Worst Speedup 0.11x"},{"location":"benchmark/operations_element_en/#performance-by-category","title":"Performance by Category","text":"Category Tests Success Rate Avg Speedup Best Speedup Status element 28 100.0% 0.63x 3.62x \ud83d\udfe1 Good"},{"location":"benchmark/operations_element_en/#detailed-results","title":"Detailed Results","text":"Operation Category Shape PyTorch (ms) Genesis (ms) Speedup Bandwidth (GB/s) Status cos element 256\u00d7256 0.039 0.011 3.62x 51.2 \ud83d\udfe2 EXCELLENT add_scalar element 512\u00d7512 0.024 0.011 2.20x 186.2 \ud83d\udfe2 EXCELLENT sub element 256\u00d7256 0.021 0.010 2.15x 76.8 \ud83d\udfe2 EXCELLENT negate element 256\u00d7256 0.021 0.010 2.12x 51.2 \ud83d\udfe2 EXCELLENT log element 256\u00d7256 0.014 0.010 1.43x 51.2 \ud83d\udfe2 EXCELLENT multiply element 256\u00d7256 0.012 0.010 1.22x 76.8 \ud83d\udfe2 EXCELLENT divide_scalar element 256\u00d7256 0.010 0.010 0.99x 51.2 \ud83d\udfe2 EXCELLENT sqrt element 256\u00d7256 0.020 0.041 0.49x 51.2 \ud83d\udd34 POOR mul_scalar element 256\u00d7256 0.024 0.107 0.23x 4.9 \ud83d\udd34 POOR add_scalar element 256\u00d7256 0.024 0.108 0.22x 4.9 \ud83d\udd34 POOR mul_scalar element 512\u00d7512 0.026 0.121 0.21x 17.7 \ud83d\udd34 POOR add element 256\u00d7256 0.016 0.076 0.21x 8.2 \ud83d\udd34 POOR divide element 256\u00d7256 0.017 0.089 0.19x 6.9 \u274c CRITICAL sin element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c CRITICAL exp element 256\u00d7256 0.020 0.106 0.19x 5.0 \u274c CRITICAL sin element 512\u00d7512 0.013 0.069 0.19x 18.1 \u274c CRITICAL negate element 512\u00d7512 0.011 0.060 0.18x 186.2 \u274c CRITICAL sqrt element 512\u00d7512 0.021 0.117 0.18x 18.2 \u274c CRITICAL exp element 512\u00d7512 0.014 0.079 0.18x 21.2 \u274c CRITICAL multiply element 512\u00d7512 0.020 0.116 0.17x 16.4 \u274c CRITICAL log element 512\u00d7512 0.020 0.116 0.17x 18.3 \u274c CRITICAL pow_scalar element 256\u00d7256 0.023 0.139 0.16x 3.8 \u274c CRITICAL add element 512\u00d7512 0.021 0.132 0.16x 24.4 \u274c CRITICAL pow_scalar element 512\u00d7512 0.011 0.068 0.16x 186.2 \u274c CRITICAL divide element 512\u00d7512 0.017 0.130 0.13x 24.5 \u274c CRITICAL divide_scalar element 512\u00d7512 0.011 0.083 0.13x 17.1 \u274c CRITICAL sub element 512\u00d7512 0.015 0.132 0.11x 24.4 \u274c CRITICAL cos element 512\u00d7512 0.013 0.120 0.11x 17.8 \u274c CRITICAL"},{"location":"benchmark/operations_element_en/#performance-distribution","title":"Performance Distribution","text":"<ul> <li>\ud83d\udfe2 Excellent (\u226590%): 7 tests (25.0%)</li> <li>\ud83d\udfe1 Good (70-90%): 0 tests (0.0%)</li> <li>\ud83d\udfe0 Fair (50-70%): 0 tests (0.0%)</li> <li>\ud83d\udd34 Poor (20-50%): 5 tests (17.9%)</li> <li>\u274c Critical (&lt;20%): 16 tests (57.1%)</li> </ul>"},{"location":"benchmark/operations_element_en/#top-10-performers","title":"Top 10 Performers","text":"Rank Operation Shape Speedup Status 1 cos 256\u00d7256 3.62x \ud83d\udfe2 EXCELLENT 2 add_scalar 512\u00d7512 2.20x \ud83d\udfe2 EXCELLENT 3 sub 256\u00d7256 2.15x \ud83d\udfe2 EXCELLENT 4 negate 256\u00d7256 2.12x \ud83d\udfe2 EXCELLENT 5 log 256\u00d7256 1.43x \ud83d\udfe2 EXCELLENT 6 multiply 256\u00d7256 1.22x \ud83d\udfe2 EXCELLENT 7 divide_scalar 256\u00d7256 0.99x \ud83d\udfe2 EXCELLENT 8 sqrt 256\u00d7256 0.49x \ud83d\udd34 POOR 9 mul_scalar 256\u00d7256 0.23x \ud83d\udd34 POOR 10 add_scalar 256\u00d7256 0.22x \ud83d\udd34 POOR"},{"location":"contributing/development.zh/","title":"\u5f00\u53d1\u73af\u5883\u914d\u7f6e","text":"<p>\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u642d\u5efaGenesis\u5f00\u53d1\u73af\u5883\uff0c\u5305\u62ec\u4ee3\u7801\u7f16\u8f91\u3001\u8c03\u8bd5\u3001\u6d4b\u8bd5\u7b49\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002</p>"},{"location":"contributing/development.zh/#_2","title":"\ud83d\udee0\ufe0f \u7cfb\u7edf\u8981\u6c42","text":""},{"location":"contributing/development.zh/#_3","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: x86_64\u67b6\u6784\uff0c\u652f\u6301AVX\u6307\u4ee4\u96c6</li> <li>\u5185\u5b58: \u6700\u5c1116GB\uff0c\u63a8\u835032GB+</li> <li>GPU: NVIDIA GPU with CUDA\u652f\u6301 (\u5f00\u53d1GPU\u7b97\u5b50\u65f6\u9700\u8981)</li> <li>\u5b58\u50a8: 20GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"contributing/development.zh/#_4","title":"\u8f6f\u4ef6\u8981\u6c42","text":"<ul> <li>\u64cd\u4f5c\u7cfb\u7edf: Linux (\u63a8\u8350Ubuntu 20.04+), macOS 10.15+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>Git: \u6700\u65b0\u7248\u672c</li> <li>CUDA: 11.8+ (GPU\u5f00\u53d1\u9700\u8981)</li> </ul>"},{"location":"contributing/development.zh/#_5","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"contributing/development.zh/#1","title":"1. \u514b\u9686\u4ed3\u5e93","text":"Bash<pre><code># \u514b\u9686\u4f60\u7684fork (\u63a8\u8350)\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# \u6216\u514b\u9686\u4e3b\u4ed3\u5e93\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u6dfb\u52a0\u4e0a\u6e38\u4ed3\u5e93 (\u5982\u679cfork\u7684\u8bdd)\ngit remote add upstream https://github.com/phonism/genesis.git\n</code></pre>"},{"location":"contributing/development.zh/#2-python","title":"2. \u521b\u5efaPython\u73af\u5883","text":"\u4f7f\u7528conda\u4f7f\u7528venv Bash<pre><code># \u521b\u5efa\u73af\u5883\nconda create -n genesis-dev python=3.9\nconda activate genesis-dev\n\n# \u5b89\u88c5\u57fa\u7840\u4f9d\u8d56\nconda install numpy matplotlib ipython jupyter\n</code></pre> Bash<pre><code># \u521b\u5efa\u73af\u5883\npython -m venv genesis-dev\nsource genesis-dev/bin/activate  # Linux/macOS\n# genesis-dev\\\\Scripts\\\\activate  # Windows\n\n# \u5347\u7ea7pip\npip install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"contributing/development.zh/#3","title":"3. \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56","text":"Bash<pre><code># \u5b89\u88c5PyTorch (\u6839\u636e\u4f60\u7684CUDA\u7248\u672c\u9009\u62e9)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU\u7248\u672c\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# \u5b89\u88c5Triton\npip install triton\n\n# \u5b89\u88c5\u5f00\u53d1\u5de5\u5177\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/development.zh/#4-genesis","title":"4. \u5b89\u88c5Genesis (\u5f00\u53d1\u6a21\u5f0f)","text":"Bash<pre><code># \u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5 (\u63a8\u8350)\npip install -e .\n\n# \u9a8c\u8bc1\u5b89\u88c5\npython -c \"import genesis; print('Genesis\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u6210\u529f\uff01')\"\n</code></pre>"},{"location":"contributing/development.zh/#_6","title":"\ud83d\udce6 \u4f9d\u8d56\u7ba1\u7406","text":""},{"location":"contributing/development.zh/#requirementstxt","title":"\u6838\u5fc3\u4f9d\u8d56 (requirements.txt)","text":"Text Only<pre><code>torch&gt;=2.0.0\ntriton&gt;=2.0.0\nnumpy&gt;=1.21.0\ncuda-python&gt;=11.8.0\n</code></pre>"},{"location":"contributing/development.zh/#requirements-devtxt","title":"\u5f00\u53d1\u4f9d\u8d56 (requirements-dev.txt)","text":"Text Only<pre><code>pytest&gt;=7.0.0\npytest-cov&gt;=4.0.0\nblack&gt;=22.0.0\nflake8&gt;=5.0.0\nmypy&gt;=1.0.0\nisort&gt;=5.0.0\npre-commit&gt;=2.20.0\nsphinx&gt;=5.0.0\nmatplotlib&gt;=3.5.0\njupyter&gt;=1.0.0\nipython&gt;=8.0.0\n</code></pre>"},{"location":"contributing/development.zh/#_7","title":"\ud83d\udd27 \u5f00\u53d1\u5de5\u5177\u914d\u7f6e","text":""},{"location":"contributing/development.zh/#1-git","title":"1. Git\u914d\u7f6e","text":"Bash<pre><code># \u914d\u7f6e\u7528\u6237\u4fe1\u606f\ngit config user.name \"Your Name\"\ngit config user.email \"your.email@example.com\"\n\n# \u914d\u7f6e\u63d0\u4ea4\u6a21\u677f\necho \"feat: brief description\n\nMore detailed explanation (optional)\n\n- Change 1\n- Change 2\n\nFixes #123\" &gt; ~/.gitmessage\ngit config commit.template ~/.gitmessage\n</code></pre>"},{"location":"contributing/development.zh/#2-pre-commit","title":"2. Pre-commit\u94a9\u5b50","text":"Bash<pre><code># \u5b89\u88c5pre-commit\npip install pre-commit\n\n# \u5b89\u88c5\u94a9\u5b50\npre-commit install\n\n# \u624b\u52a8\u8fd0\u884c\u68c0\u67e5\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/development.zh/#3-ide","title":"3. IDE\u914d\u7f6e","text":"VS CodePyCharm <p>\u63a8\u8350\u5b89\u88c5\u4ee5\u4e0b\u6269\u5c55\uff1a</p> JSON<pre><code>// .vscode/extensions.json\n{\n    \"recommendations\": [\n        \"ms-python.python\",\n        \"ms-python.black-formatter\", \n        \"ms-python.flake8\",\n        \"ms-python.mypy-type-checker\",\n        \"ms-toolsai.jupyter\",\n        \"ms-vscode.cpptools\"\n    ]\n}\n</code></pre> <p>\u914d\u7f6e\u6587\u4ef6\uff1a JSON<pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./genesis-dev/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackArgs\": [\"--line-length=88\"],\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\"tests/\"]\n}\n</code></pre></p> <ol> <li>\u6253\u5f00\u9879\u76ee\u8bbe\u7f6e (File -&gt; Settings)</li> <li>\u914d\u7f6ePython\u89e3\u91ca\u5668\u6307\u5411\u865a\u62df\u73af\u5883</li> <li>\u542f\u7528\u4ee3\u7801\u683c\u5f0f\u5316\u5de5\u5177 (Black, isort)</li> <li>\u914d\u7f6e\u6d4b\u8bd5\u8fd0\u884c\u5668\u4e3apytest</li> </ol>"},{"location":"contributing/development.zh/#4","title":"4. \u73af\u5883\u53d8\u91cf","text":"Bash<pre><code># \u5f00\u53d1\u73af\u5883\u53d8\u91cf\nexport GENESIS_DEV=1\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\nexport CUDA_VISIBLE_DEVICES=0  # \u6307\u5b9aGPU\u8bbe\u5907\n\n# \u6dfb\u52a0\u5230 ~/.bashrc \u6216 ~/.zshrc\necho 'export GENESIS_DEV=1' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"contributing/development.zh/#_8","title":"\ud83e\uddea \u6d4b\u8bd5\u6846\u67b6","text":""},{"location":"contributing/development.zh/#_9","title":"\u6d4b\u8bd5\u76ee\u5f55\u7ed3\u6784","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # pytest\u914d\u7f6e\n\u251c\u2500\u2500 test_autograd.py         # \u81ea\u52a8\u5fae\u5206\u6d4b\u8bd5\n\u251c\u2500\u2500 test_nn.py              # \u795e\u7ecf\u7f51\u7edc\u6d4b\u8bd5\n\u251c\u2500\u2500 test_cuda_tensor.py     # CUDA\u5f20\u91cf\u6d4b\u8bd5\n\u251c\u2500\u2500 test_functional.py      # \u51fd\u6570\u5f0f\u63a5\u53e3\u6d4b\u8bd5\n\u251c\u2500\u2500 benchmarks/             # \u6027\u80fd\u6d4b\u8bd5\n\u2502   \u251c\u2500\u2500 bench_matmul.py\n\u2502   \u2514\u2500\u2500 bench_attention.py\n\u2514\u2500\u2500 integration/            # \u96c6\u6210\u6d4b\u8bd5\n    \u251c\u2500\u2500 test_training.py\n    \u2514\u2500\u2500 test_models.py\n</code></pre>"},{"location":"contributing/development.zh/#_10","title":"\u8fd0\u884c\u6d4b\u8bd5","text":"Bash<pre><code># \u8fd0\u884c\u6240\u6709\u6d4b\u8bd5\npytest tests/ -v\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\u6587\u4ef6\npytest tests/test_nn.py -v\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\u51fd\u6570\npytest tests/test_nn.py::test_linear_layer -v\n\n# \u8fd0\u884c\u5e26\u8986\u76d6\u7387\u7684\u6d4b\u8bd5\npytest tests/ --cov=genesis --cov-report=html\n\n# \u8fd0\u884c\u6027\u80fd\u6d4b\u8bd5\npytest tests/benchmarks/ -v --benchmark-only\n</code></pre>"},{"location":"contributing/development.zh/#_11","title":"\u7f16\u5199\u6d4b\u8bd5","text":"Python<pre><code># tests/test_example.py\nimport pytest\nimport genesis\nimport genesis.nn as nn\n\nclass TestExample:\n    \"\"\"Example test class.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Setup before each test method.\"\"\"\n        self.device = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\n\n    def test_basic_operation(self):\n        \"\"\"Test basic tensor operations.\"\"\"\n        x = genesis.randn(3, 4, device=self.device)\n        y = genesis.randn(3, 4, device=self.device)\n        z = x + y\n\n        assert z.shape == (3, 4)\n        assert z.device == self.device\n\n    @pytest.mark.parametrize(\"input_size,output_size\", [\n        (10, 5),\n        (128, 64),\n        (512, 256)\n    ])\n    def test_linear_layers(self, input_size, output_size):\n        \"\"\"Test linear layers with different sizes.\"\"\"\n        layer = nn.Linear(input_size, output_size).to(self.device)\n        x = genesis.randn(32, input_size, device=self.device)\n\n        output = layer(x)\n        assert output.shape == (32, output_size)\n\n    @pytest.mark.cuda\n    def test_cuda_specific(self):\n        \"\"\"Test CUDA-specific functionality.\"\"\"\n        if not genesis.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n\n        x = genesis.randn(10, 10, device='cuda')\n        assert x.is_cuda\n</code></pre>"},{"location":"contributing/development.zh/#_12","title":"\ud83d\udcca \u6027\u80fd\u5206\u6790","text":""},{"location":"contributing/development.zh/#1-profiler","title":"1. \u5185\u7f6eprofiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# \u4f7f\u7528context manager\nwith profiler.profile() as prof:\n    # \u4f60\u7684\u4ee3\u7801\n    x = genesis.randn(1000, 1000)\n    y = genesis.matmul(x, x)\n\n# \u6253\u5370\u7ed3\u679c\nprof.print_stats()\n\n# \u4fdd\u5b58\u7ed3\u679c\nprof.export_chrome_trace(\"profile.json\")\n</code></pre>"},{"location":"contributing/development.zh/#2","title":"2. \u5185\u5b58\u5206\u6790","text":"Python<pre><code>import genesis\n\n# \u542f\u7528\u5185\u5b58\u8ddf\u8e2a\ngenesis.cuda.memory.enable_debug()\n\n# \u4f60\u7684\u4ee3\u7801\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.matmul(x, x)\n\n# \u67e5\u770b\u5185\u5b58\u4f7f\u7528\nprint(f\"\u5185\u5b58\u4f7f\u7528: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"\u7f13\u5b58\u5185\u5b58: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# \u5185\u5b58\u5feb\u7167\nsnapshot = genesis.cuda.memory.memory_snapshot()\n</code></pre>"},{"location":"contributing/development.zh/#3_1","title":"3. \u57fa\u51c6\u6d4b\u8bd5","text":"Python<pre><code># benchmark/bench_example.py\nimport time\nimport genesis\nimport torch\n\ndef benchmark_matmul():\n    \"\"\"Benchmark matrix multiplication.\"\"\"\n    sizes = [128, 256, 512, 1024]\n\n    for size in sizes:\n        # Genesis\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis_time = time.time() - start_time\n\n        # PyTorch\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch_time = time.time() - start_time\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  Genesis: {genesis_time:.4f}s\")\n        print(f\"  PyTorch: {torch_time:.4f}s\")\n        print(f\"  Ratio: {genesis_time/torch_time:.2f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_matmul()\n</code></pre>"},{"location":"contributing/development.zh/#_13","title":"\ud83d\udc1b \u8c03\u8bd5\u6280\u5de7","text":""},{"location":"contributing/development.zh/#1_1","title":"1. \u8c03\u8bd5\u73af\u5883\u53d8\u91cf","text":"Bash<pre><code># \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f\nexport GENESIS_DEBUG=1\nexport CUDA_LAUNCH_BLOCKING=1  # \u540c\u6b65CUDA\u6267\u884c\nexport PYTHONFAULTHANDLER=1    # Python\u9519\u8bef\u5904\u7406\n</code></pre>"},{"location":"contributing/development.zh/#2_1","title":"2. \u65e5\u5fd7\u914d\u7f6e","text":"Python<pre><code>import logging\nimport genesis\n\n# \u914d\u7f6e\u65e5\u5fd7\nlogging.basicConfig(level=logging.DEBUG)\ngenesis.set_log_level('DEBUG')\n\n# \u4f7f\u7528\u65e5\u5fd7\nlogger = logging.getLogger(__name__)\nlogger.debug(\"\u8c03\u8bd5\u4fe1\u606f\")\n</code></pre>"},{"location":"contributing/development.zh/#3_2","title":"3. \u65ad\u70b9\u8c03\u8bd5","text":"Python<pre><code>import pdb\n\ndef buggy_function(x):\n    pdb.set_trace()  # \u8bbe\u7f6e\u65ad\u70b9\n    y = x * 2\n    return y\n\n# \u6216\u4f7f\u7528ipdb (\u9700\u8981\u5b89\u88c5: pip install ipdb)\nimport ipdb\nipdb.set_trace()\n</code></pre>"},{"location":"contributing/development.zh/#_14","title":"\ud83d\udcda \u6587\u6863\u5f00\u53d1","text":""},{"location":"contributing/development.zh/#_15","title":"\u6784\u5efa\u6587\u6863","text":"Bash<pre><code># \u5b89\u88c5\u6587\u6863\u4f9d\u8d56\npip install -r docs/requirements.txt\n\n# \u672c\u5730\u670d\u52a1\u5668\nmkdocs serve\n\n# \u6784\u5efa\u9759\u6001\u6587\u4ef6\nmkdocs build\n\n# \u90e8\u7f72\u5230GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/development.zh/#api","title":"API\u6587\u6863\u751f\u6210","text":"Bash<pre><code># \u81ea\u52a8\u751f\u6210API\u6587\u6863\npython scripts/generate_api_docs.py\n\n# \u68c0\u67e5docstring\u683c\u5f0f\npydocstyle genesis/\n</code></pre>"},{"location":"contributing/development.zh/#_16","title":"\ud83d\ude80 \u63d0\u4ea4\u4ee3\u7801","text":""},{"location":"contributing/development.zh/#1_2","title":"1. \u4ee3\u7801\u68c0\u67e5","text":"Bash<pre><code># \u683c\u5f0f\u5316\u4ee3\u7801\nblack genesis/ tests/\nisort genesis/ tests/\n\n# \u7c7b\u578b\u68c0\u67e5\nmypy genesis/\n\n# \u4ee3\u7801\u8d28\u91cf\u68c0\u67e5\nflake8 genesis/ tests/\n\n# \u8fd0\u884c\u6d4b\u8bd5\npytest tests/ -x\n</code></pre>"},{"location":"contributing/development.zh/#2_2","title":"2. \u63d0\u4ea4\u6d41\u7a0b","text":"Bash<pre><code># 1. \u540c\u6b65\u6700\u65b0\u4ee3\u7801\ngit fetch upstream\ngit rebase upstream/main\n\n# 2. \u521b\u5efa\u529f\u80fd\u5206\u652f\ngit checkout -b feature/your-feature\n\n# 3. \u5f00\u53d1\u548c\u6d4b\u8bd5\n# ... \u4f60\u7684\u5f00\u53d1\u5de5\u4f5c ...\n\n# 4. \u63d0\u4ea4\u4ee3\u7801\ngit add .\ngit commit -m \"feat: add your feature\"\n\n# 5. \u63a8\u9001\u5206\u652f\ngit push origin feature/your-feature\n\n# 6. \u521b\u5efaPull Request\n</code></pre>"},{"location":"contributing/development.zh/#_17","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"contributing/development.zh/#q-cuda","title":"Q: CUDA\u76f8\u5173\u9519\u8bef\uff1f","text":"<p>A: \u68c0\u67e5CUDA\u7248\u672c\u517c\u5bb9\u6027\uff0c\u786e\u4fddPyTorch\u548cTriton\u7248\u672c\u5339\u914d\u3002</p>"},{"location":"contributing/development.zh/#q","title":"Q: \u6d4b\u8bd5\u5931\u8d25\uff1f","text":"<p>A: \u8fd0\u884c <code>pytest tests/ -v</code> \u67e5\u770b\u8be6\u7ec6\u9519\u8bef\u4fe1\u606f\uff0c\u68c0\u67e5\u73af\u5883\u914d\u7f6e\u3002</p>"},{"location":"contributing/development.zh/#q_1","title":"Q: \u6027\u80fd\u95ee\u9898\uff1f","text":"<p>A: \u4f7f\u7528profiler\u5206\u6790\u74f6\u9888\uff0c\u68c0\u67e5\u662f\u5426\u542f\u7528\u4e86GPU\u52a0\u901f\u3002</p>"},{"location":"contributing/development.zh/#q_2","title":"Q: \u5185\u5b58\u4e0d\u8db3\uff1f","text":"<p>A: \u51cf\u5c0f\u6d4b\u8bd5\u7528\u4f8b\u7684\u6570\u636e\u89c4\u6a21\uff0c\u542f\u7528CPU\u56de\u9000\u6a21\u5f0f\u3002</p> <p>\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u5b8c\u6210</p> <p>\u73b0\u5728\u4f60\u53ef\u4ee5\u5f00\u59cb\u4e3aGenesis\u8d21\u732e\u4ee3\u7801\u4e86\uff01</p> <p>\u4e0b\u4e00\u6b65\uff1a\u4e86\u89e3\u6d4b\u8bd5\u89c4\u8303 \u8fd4\u56de\u8d21\u732e\u6307\u5357</p>"},{"location":"contributing/development_en/","title":"Development Environment Setup","text":"<p>This guide will help you set up a Genesis development environment, including code editing, debugging, testing, and other development workflows.</p>"},{"location":"contributing/development_en/#system-requirements","title":"\ud83d\udee0\ufe0f System Requirements","text":""},{"location":"contributing/development_en/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: x86_64 architecture with AVX instruction set support</li> <li>Memory: Minimum 16GB, recommended 32GB+</li> <li>GPU: NVIDIA GPU with CUDA support (required for GPU operator development)</li> <li>Storage: 20GB available space</li> </ul>"},{"location":"contributing/development_en/#software-requirements","title":"Software Requirements","text":"<ul> <li>Operating System: Linux (Ubuntu 20.04+ recommended), macOS 10.15+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>Git: Latest version</li> <li>CUDA: 11.8+ (required for GPU development)</li> </ul>"},{"location":"contributing/development_en/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"contributing/development_en/#1-clone-repository","title":"1. Clone Repository","text":"Bash<pre><code># Clone your fork (recommended)\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# Or clone the main repository\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# Add upstream repository (if forked)\ngit remote add upstream https://github.com/phonism/genesis.git\n</code></pre>"},{"location":"contributing/development_en/#2-create-python-environment","title":"2. Create Python Environment","text":"Using condaUsing venv Bash<pre><code># Create environment\nconda create -n genesis-dev python=3.9\nconda activate genesis-dev\n\n# Install base dependencies\nconda install numpy matplotlib ipython jupyter\n</code></pre> Bash<pre><code># Create environment\npython -m venv genesis-dev\nsource genesis-dev/bin/activate  # Linux/macOS\n# genesis-dev\\\\Scripts\\\\activate  # Windows\n\n# Upgrade pip\npip install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"contributing/development_en/#3-install-development-dependencies","title":"3. Install Development Dependencies","text":"Bash<pre><code># Install PyTorch (choose based on your CUDA version)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU version\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Install Triton\npip install triton\n\n# Install development tools\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/development_en/#4-install-genesis-development-mode","title":"4. Install Genesis (Development Mode)","text":"Bash<pre><code># Development mode installation (recommended)\npip install -e .\n\n# Verify installation\npython -c \"import genesis; print('Genesis development environment setup successful!')\"\n</code></pre>"},{"location":"contributing/development_en/#dependency-management","title":"\ud83d\udce6 Dependency Management","text":""},{"location":"contributing/development_en/#core-dependencies-requirementstxt","title":"Core Dependencies (requirements.txt)","text":"Text Only<pre><code>torch&gt;=2.0.0\ntriton&gt;=2.0.0\nnumpy&gt;=1.21.0\ncuda-python&gt;=11.8.0\n</code></pre>"},{"location":"contributing/development_en/#development-dependencies-requirements-devtxt","title":"Development Dependencies (requirements-dev.txt)","text":"Text Only<pre><code>pytest&gt;=7.0.0\npytest-cov&gt;=4.0.0\nblack&gt;=22.0.0\nflake8&gt;=5.0.0\nmypy&gt;=1.0.0\nisort&gt;=5.0.0\npre-commit&gt;=2.20.0\nsphinx&gt;=5.0.0\nmatplotlib&gt;=3.5.0\njupyter&gt;=1.0.0\nipython&gt;=8.0.0\n</code></pre>"},{"location":"contributing/development_en/#development-tools-configuration","title":"\ud83d\udd27 Development Tools Configuration","text":""},{"location":"contributing/development_en/#1-git-configuration","title":"1. Git Configuration","text":"Bash<pre><code># Configure user information\ngit config user.name \"Your Name\"\ngit config user.email \"your.email@example.com\"\n\n# Configure commit template\necho \"feat: brief description\n\nMore detailed explanation (optional)\n\n- Change 1\n- Change 2\n\nFixes #123\" &gt; ~/.gitmessage\ngit config commit.template ~/.gitmessage\n</code></pre>"},{"location":"contributing/development_en/#2-pre-commit-hooks","title":"2. Pre-commit Hooks","text":"Bash<pre><code># Install pre-commit\npip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run checks manually\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/development_en/#3-ide-configuration","title":"3. IDE Configuration","text":"VS CodePyCharm <p>Recommended extensions to install:</p> JSON<pre><code>// .vscode/extensions.json\n{\n    \"recommendations\": [\n        \"ms-python.python\",\n        \"ms-python.black-formatter\", \n        \"ms-python.flake8\",\n        \"ms-python.mypy-type-checker\",\n        \"ms-toolsai.jupyter\",\n        \"ms-vscode.cpptools\"\n    ]\n}\n</code></pre> <p>Configuration file: JSON<pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./genesis-dev/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackArgs\": [\"--line-length=88\"],\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\"tests/\"]\n}\n</code></pre></p> <ol> <li>Open project settings (File -&gt; Settings)</li> <li>Configure Python interpreter to point to virtual environment</li> <li>Enable code formatting tools (Black, isort)</li> <li>Configure test runner to pytest</li> </ol>"},{"location":"contributing/development_en/#4-environment-variables","title":"4. Environment Variables","text":"Bash<pre><code># Development environment variables\nexport GENESIS_DEV=1\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\nexport CUDA_VISIBLE_DEVICES=0  # Specify GPU device\n\n# Add to ~/.bashrc or ~/.zshrc\necho 'export GENESIS_DEV=1' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"contributing/development_en/#testing-framework","title":"\ud83e\uddea Testing Framework","text":""},{"location":"contributing/development_en/#test-directory-structure","title":"Test Directory Structure","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # pytest configuration\n\u251c\u2500\u2500 test_autograd.py         # Autograd tests\n\u251c\u2500\u2500 test_nn.py              # Neural network tests\n\u251c\u2500\u2500 test_cuda_tensor.py     # CUDA tensor tests\n\u251c\u2500\u2500 test_functional.py      # Functional interface tests\n\u251c\u2500\u2500 benchmarks/             # Performance tests\n\u2502   \u251c\u2500\u2500 bench_matmul.py\n\u2502   \u2514\u2500\u2500 bench_attention.py\n\u2514\u2500\u2500 integration/            # Integration tests\n    \u251c\u2500\u2500 test_training.py\n    \u2514\u2500\u2500 test_models.py\n</code></pre>"},{"location":"contributing/development_en/#running-tests","title":"Running Tests","text":"Bash<pre><code># Run all tests\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_nn.py -v\n\n# Run specific test function\npytest tests/test_nn.py::test_linear_layer -v\n\n# Run tests with coverage\npytest tests/ --cov=genesis --cov-report=html\n\n# Run performance tests\npytest tests/benchmarks/ -v --benchmark-only\n</code></pre>"},{"location":"contributing/development_en/#writing-tests","title":"Writing Tests","text":"Python<pre><code># tests/test_example.py\nimport pytest\nimport genesis\nimport genesis.nn as nn\n\nclass TestExample:\n    \"\"\"Example test class.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Setup before each test method.\"\"\"\n        self.device = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\n\n    def test_basic_operation(self):\n        \"\"\"Test basic tensor operations.\"\"\"\n        x = genesis.randn(3, 4, device=self.device)\n        y = genesis.randn(3, 4, device=self.device)\n        z = x + y\n\n        assert z.shape == (3, 4)\n        assert z.device == self.device\n\n    @pytest.mark.parametrize(\"input_size,output_size\", [\n        (10, 5),\n        (128, 64),\n        (512, 256)\n    ])\n    def test_linear_layers(self, input_size, output_size):\n        \"\"\"Test linear layers with different sizes.\"\"\"\n        layer = nn.Linear(input_size, output_size).to(self.device)\n        x = genesis.randn(32, input_size, device=self.device)\n\n        output = layer(x)\n        assert output.shape == (32, output_size)\n\n    @pytest.mark.cuda\n    def test_cuda_specific(self):\n        \"\"\"Test CUDA-specific functionality.\"\"\"\n        if not genesis.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n\n        x = genesis.randn(10, 10, device='cuda')\n        assert x.is_cuda\n</code></pre>"},{"location":"contributing/development_en/#performance-analysis","title":"\ud83d\udcca Performance Analysis","text":""},{"location":"contributing/development_en/#1-built-in-profiler","title":"1. Built-in Profiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# Using context manager\nwith profiler.profile() as prof:\n    # Your code\n    x = genesis.randn(1000, 1000)\n    y = genesis.matmul(x, x)\n\n# Print results\nprof.print_stats()\n\n# Save results\nprof.export_chrome_trace(\"profile.json\")\n</code></pre>"},{"location":"contributing/development_en/#2-memory-analysis","title":"2. Memory Analysis","text":"Python<pre><code>import genesis\n\n# Enable memory tracking\ngenesis.cuda.memory.enable_debug()\n\n# Your code\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.matmul(x, x)\n\n# Check memory usage\nprint(f\"Memory used: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"Cached memory: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# Memory snapshot\nsnapshot = genesis.cuda.memory.memory_snapshot()\n</code></pre>"},{"location":"contributing/development_en/#3-benchmarking","title":"3. Benchmarking","text":"Python<pre><code># benchmark/bench_example.py\nimport time\nimport genesis\nimport torch\n\ndef benchmark_matmul():\n    \"\"\"Benchmark matrix multiplication.\"\"\"\n    sizes = [128, 256, 512, 1024]\n\n    for size in sizes:\n        # Genesis\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis_time = time.time() - start_time\n\n        # PyTorch\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch_time = time.time() - start_time\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  Genesis: {genesis_time:.4f}s\")\n        print(f\"  PyTorch: {torch_time:.4f}s\")\n        print(f\"  Ratio: {genesis_time/torch_time:.2f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_matmul()\n</code></pre>"},{"location":"contributing/development_en/#debugging-tips","title":"\ud83d\udc1b Debugging Tips","text":""},{"location":"contributing/development_en/#1-debug-environment-variables","title":"1. Debug Environment Variables","text":"Bash<pre><code># Enable debug mode\nexport GENESIS_DEBUG=1\nexport CUDA_LAUNCH_BLOCKING=1  # Synchronous CUDA execution\nexport PYTHONFAULTHANDLER=1    # Python error handling\n</code></pre>"},{"location":"contributing/development_en/#2-logging-configuration","title":"2. Logging Configuration","text":"Python<pre><code>import logging\nimport genesis\n\n# Configure logging\nlogging.basicConfig(level=logging.DEBUG)\ngenesis.set_log_level('DEBUG')\n\n# Use logging\nlogger = logging.getLogger(__name__)\nlogger.debug(\"Debug information\")\n</code></pre>"},{"location":"contributing/development_en/#3-breakpoint-debugging","title":"3. Breakpoint Debugging","text":"Python<pre><code>import pdb\n\ndef buggy_function(x):\n    pdb.set_trace()  # Set breakpoint\n    y = x * 2\n    return y\n\n# Or use ipdb (install with: pip install ipdb)\nimport ipdb\nipdb.set_trace()\n</code></pre>"},{"location":"contributing/development_en/#documentation-development","title":"\ud83d\udcda Documentation Development","text":""},{"location":"contributing/development_en/#building-documentation","title":"Building Documentation","text":"Bash<pre><code># Install documentation dependencies\npip install -r docs/requirements.txt\n\n# Local server\nmkdocs serve\n\n# Build static files\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/development_en/#api-documentation-generation","title":"API Documentation Generation","text":"Bash<pre><code># Auto-generate API documentation\npython scripts/generate_api_docs.py\n\n# Check docstring format\npydocstyle genesis/\n</code></pre>"},{"location":"contributing/development_en/#code-submission","title":"\ud83d\ude80 Code Submission","text":""},{"location":"contributing/development_en/#1-code-checks","title":"1. Code Checks","text":"Bash<pre><code># Format code\nblack genesis/ tests/\nisort genesis/ tests/\n\n# Type checking\nmypy genesis/\n\n# Code quality checks\nflake8 genesis/ tests/\n\n# Run tests\npytest tests/ -x\n</code></pre>"},{"location":"contributing/development_en/#2-submission-process","title":"2. Submission Process","text":"Bash<pre><code># 1. Sync latest code\ngit fetch upstream\ngit rebase upstream/main\n\n# 2. Create feature branch\ngit checkout -b feature/your-feature\n\n# 3. Development and testing\n# ... your development work ...\n\n# 4. Commit code\ngit add .\ngit commit -m \"feat: add your feature\"\n\n# 5. Push branch\ngit push origin feature/your-feature\n\n# 6. Create Pull Request\n</code></pre>"},{"location":"contributing/development_en/#common-issues","title":"\u2753 Common Issues","text":""},{"location":"contributing/development_en/#q-cuda-related-errors","title":"Q: CUDA-related errors?","text":"<p>A: Check CUDA version compatibility, ensure PyTorch and Triton versions match.</p>"},{"location":"contributing/development_en/#q-test-failures","title":"Q: Test failures?","text":"<p>A: Run <code>pytest tests/ -v</code> to see detailed error information, check environment configuration.</p>"},{"location":"contributing/development_en/#q-performance-issues","title":"Q: Performance issues?","text":"<p>A: Use profiler to analyze bottlenecks, check if GPU acceleration is enabled.</p>"},{"location":"contributing/development_en/#q-out-of-memory","title":"Q: Out of memory?","text":"<p>A: Reduce test case data size, enable CPU fallback mode.</p> <p>Development Environment Setup Complete</p> <p>You can now start contributing code to Genesis!</p> <p>Next: Testing Guidelines Back to Contributing Guide</p>"},{"location":"contributing/index.zh/","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u6b22\u8fce\u4e3aGenesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8d21\u732e\u4ee3\u7801\uff01\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u4e86\u89e3\u5982\u4f55\u53c2\u4e0e\u9879\u76ee\u5f00\u53d1\u3002</p>"},{"location":"contributing/index.zh/#_2","title":"\ud83e\udd1d \u8d21\u732e\u65b9\u5f0f","text":""},{"location":"contributing/index.zh/#_3","title":"\u4ee3\u7801\u8d21\u732e","text":"<ul> <li>\u4fee\u590dbug</li> <li>\u6dfb\u52a0\u65b0\u7279\u6027</li> <li>\u6027\u80fd\u4f18\u5316</li> <li>\u6d4b\u8bd5\u5b8c\u5584</li> </ul>"},{"location":"contributing/index.zh/#_4","title":"\u6587\u6863\u8d21\u732e","text":"<ul> <li>\u6539\u8fdb\u73b0\u6709\u6587\u6863</li> <li>\u6dfb\u52a0\u6559\u7a0b\u548c\u793a\u4f8b</li> <li>\u7ffb\u8bd1\u6587\u6863</li> <li>API\u6587\u6863\u5b8c\u5584</li> </ul>"},{"location":"contributing/index.zh/#_5","title":"\u793e\u533a\u8d21\u732e","text":"<ul> <li>\u56de\u7b54\u95ee\u9898</li> <li>\u4ee3\u7801\u5ba1\u67e5</li> <li>\u95ee\u9898\u62a5\u544a</li> <li>\u529f\u80fd\u5efa\u8bae</li> </ul>"},{"location":"contributing/index.zh/#_6","title":"\ud83d\udccb \u5f00\u53d1\u6d41\u7a0b","text":""},{"location":"contributing/index.zh/#1","title":"1. \u51c6\u5907\u5de5\u4f5c","text":"Bash<pre><code># Fork\u9879\u76ee\u5230\u4f60\u7684GitHub\u8d26\u6237\n# Clone\u4f60\u7684fork\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# \u6dfb\u52a0\u4e0a\u6e38\u4ed3\u5e93\ngit remote add upstream https://github.com/phonism/genesis.git\n\n# \u521b\u5efa\u5f00\u53d1\u5206\u652f\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/index.zh/#2","title":"2. \u5f00\u53d1\u73af\u5883\u642d\u5efa","text":"<p>\u8be6\u89c1\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u6587\u6863\u3002</p>"},{"location":"contributing/index.zh/#3","title":"3. \u4ee3\u7801\u5f00\u53d1","text":"<ul> <li>\u9075\u5faa\u4ee3\u7801\u89c4\u8303</li> <li>\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5</li> <li>\u66f4\u65b0\u76f8\u5173\u6587\u6863</li> <li>\u63d0\u4ea4\u6e05\u6670\u7684commit\u6d88\u606f</li> </ul>"},{"location":"contributing/index.zh/#4","title":"4. \u6d4b\u8bd5\u9a8c\u8bc1","text":"Bash<pre><code># \u8fd0\u884c\u6d4b\u8bd5\u5957\u4ef6\npython -m pytest tests/ -v\n\n# \u8fd0\u884c\u4ee3\u7801\u683c\u5f0f\u68c0\u67e5\nblack genesis/ tests/\nflake8 genesis/ tests/\n\n# \u8fd0\u884c\u7c7b\u578b\u68c0\u67e5\nmypy genesis/\n</code></pre>"},{"location":"contributing/index.zh/#5-pr","title":"5. \u63d0\u4ea4PR","text":"<ul> <li>\u786e\u4fdd\u6240\u6709\u6d4b\u8bd5\u901a\u8fc7</li> <li>\u586b\u5199\u8be6\u7ec6\u7684PR\u63cf\u8ff0</li> <li>\u94fe\u63a5\u76f8\u5173\u7684Issue</li> <li>\u7b49\u5f85\u4ee3\u7801\u5ba1\u67e5</li> </ul>"},{"location":"contributing/index.zh/#_7","title":"\ud83d\udcdd \u4ee3\u7801\u89c4\u8303","text":""},{"location":"contributing/index.zh/#python","title":"Python\u98ce\u683c\u6307\u5357","text":"<p>\u6211\u4eec\u9075\u5faaPEP 8\u89c4\u8303\uff1a</p> Python<pre><code># \u597d\u7684\u793a\u4f8b\ndef compute_attention_weights(query, key, scale_factor):\n    \"\"\"Compute scaled dot-product attention weights.\n\n    Args:\n        query: Query tensor of shape [batch, seq_len, hidden_dim]\n        key: Key tensor of shape [batch, seq_len, hidden_dim] \n        scale_factor: Scaling factor for attention scores\n\n    Returns:\n        Attention weights of shape [batch, seq_len, seq_len]\n    \"\"\"\n    scores = genesis.matmul(query, key.transpose(-2, -1))\n    scaled_scores = scores * scale_factor\n    return genesis.softmax(scaled_scores, dim=-1)\n</code></pre>"},{"location":"contributing/index.zh/#_8","title":"\u6587\u6863\u5b57\u7b26\u4e32","text":"<p>\u4f7f\u7528Google\u98ce\u683c\u7684docstring\uff1a</p> Python<pre><code>def example_function(param1: int, param2: str = \"default\") -&gt; bool:\n    \"\"\"One line summary of the function.\n\n    More detailed description if needed. Can span multiple lines.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2 with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is negative\n\n    Example:\n        &gt;&gt;&gt; result = example_function(42, \"test\")\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n    if param1 &lt; 0:\n        raise ValueError(\"param1 must be non-negative\")\n    return param1 &gt; 0\n</code></pre>"},{"location":"contributing/index.zh/#_9","title":"\u6d4b\u8bd5\u7f16\u5199","text":"Python<pre><code>import pytest\nimport genesis\n\nclass TestAttention:\n    \"\"\"Test attention mechanisms.\"\"\"\n\n    def test_basic_attention(self):\n        \"\"\"Test basic attention computation.\"\"\"\n        batch_size, seq_len, hidden_dim = 2, 4, 8\n\n        query = genesis.randn(batch_size, seq_len, hidden_dim)\n        key = genesis.randn(batch_size, seq_len, hidden_dim)\n        value = genesis.randn(batch_size, seq_len, hidden_dim)\n\n        attention = genesis.nn.MultiHeadAttention(hidden_dim, num_heads=2)\n        output = attention(query, key, value)\n\n        assert output.shape == (batch_size, seq_len, hidden_dim)\n\n    @pytest.mark.parametrize(\"num_heads\", [1, 2, 4, 8])\n    def test_different_head_counts(self, num_heads):\n        \"\"\"Test attention with different head counts.\"\"\"\n        # \u6d4b\u8bd5\u5b9e\u73b0\n        pass\n</code></pre>"},{"location":"contributing/index.zh/#_10","title":"\ud83d\ude80 \u5f00\u53d1\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"contributing/index.zh/#1_1","title":"1. \u5206\u652f\u7ba1\u7406","text":"Bash<pre><code># \u4e3b\u8981\u5206\u652f\nmain          # \u7a33\u5b9a\u7248\u672c\ndevelop       # \u5f00\u53d1\u7248\u672c\n\n# \u529f\u80fd\u5206\u652f\nfeature/xxx   # \u65b0\u529f\u80fd\u5f00\u53d1\nbugfix/xxx    # bug\u4fee\u590d\nhotfix/xxx    # \u7d27\u6025\u4fee\u590d\n</code></pre>"},{"location":"contributing/index.zh/#2-commit","title":"2. Commit\u6d88\u606f\u683c\u5f0f","text":"Text Only<pre><code>type(scope): brief description\n\nDetailed description (optional)\n\nFixes #123\n</code></pre> <p>\u7c7b\u578b\u8bf4\u660e\uff1a - <code>feat</code>: \u65b0\u529f\u80fd - <code>fix</code>: bug\u4fee\u590d - <code>docs</code>: \u6587\u6863\u66f4\u65b0 - <code>style</code>: \u4ee3\u7801\u683c\u5f0f\u8c03\u6574 - <code>refactor</code>: \u91cd\u6784 - <code>perf</code>: \u6027\u80fd\u4f18\u5316 - <code>test</code>: \u6d4b\u8bd5\u76f8\u5173 - <code>chore</code>: \u6784\u5efa\u5de5\u5177\u7b49</p>"},{"location":"contributing/index.zh/#3_1","title":"3. \u6027\u80fd\u8003\u8651","text":"<ul> <li>\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\u62f7\u8d1d</li> <li>\u4f7f\u7528in-place\u64cd\u4f5cwhen\u53ef\u80fd</li> <li>\u8003\u8651CUDA kernel\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f</li> <li>\u6dfb\u52a0\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5</li> </ul>"},{"location":"contributing/index.zh/#bug","title":"\ud83d\udc1b Bug\u62a5\u544a","text":"<p>\u63d0\u4ea4bug\u65f6\u8bf7\u5305\u542b\uff1a</p> <ol> <li>\u73af\u5883\u4fe1\u606f</li> <li>Genesis\u7248\u672c</li> <li>Python\u7248\u672c</li> <li>CUDA\u7248\u672c</li> <li> <p>\u64cd\u4f5c\u7cfb\u7edf</p> </li> <li> <p>\u590d\u73b0\u6b65\u9aa4</p> </li> <li>\u6700\u5c0f\u53ef\u590d\u73b0\u4ee3\u7801</li> <li>\u9884\u671f\u884c\u4e3a</li> <li>\u5b9e\u9645\u884c\u4e3a</li> <li> <p>\u9519\u8bef\u4fe1\u606f</p> </li> <li> <p>\u76f8\u5173\u65e5\u5fd7</p> </li> <li>\u5b8c\u6574\u7684\u9519\u8bef\u5806\u6808</li> <li>\u76f8\u5173\u914d\u7f6e\u4fe1\u606f</li> </ol> <p>\u793a\u4f8b\uff1a Python<pre><code># \u6700\u5c0f\u590d\u73b0\u6848\u4f8b\nimport genesis\n\nmodel = genesis.nn.Linear(10, 5)\nx = genesis.randn(3, 10)\ny = model(x)  # \u8fd9\u91cc\u51fa\u73b0\u9519\u8bef\n\n# \u9519\u8bef\u4fe1\u606f\uff1a\n# RuntimeError: CUDA kernel launch failed\n</code></pre></p>"},{"location":"contributing/index.zh/#_11","title":"\ud83c\udfaf \u8d21\u732e\u91cd\u70b9\u9886\u57df","text":"<p>\u5f53\u524d\u6211\u4eec\u7279\u522b\u6b22\u8fce\u4ee5\u4e0b\u9886\u57df\u7684\u8d21\u732e\uff1a</p>"},{"location":"contributing/index.zh/#_12","title":"\u9ad8\u4f18\u5148\u7ea7","text":"<ul> <li> \u6027\u80fd\u4f18\u5316\u548c\u57fa\u51c6\u6d4b\u8bd5</li> <li> CUDA\u7b97\u5b50\u5b9e\u73b0</li> <li> \u6587\u6863\u548c\u6559\u7a0b\u5b8c\u5584</li> <li> \u6d4b\u8bd5\u8986\u76d6\u7387\u63d0\u5347</li> </ul>"},{"location":"contributing/index.zh/#_13","title":"\u4e2d\u4f18\u5148\u7ea7","text":"<ul> <li> \u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42</li> <li> \u6570\u636e\u52a0\u8f7d\u5668\u4f18\u5316</li> <li> \u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301</li> <li> \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> </ul>"},{"location":"contributing/index.zh/#_14","title":"\u4f4e\u4f18\u5148\u7ea7","text":"<ul> <li> \u53ef\u89c6\u5316\u5de5\u5177</li> <li> \u6a21\u578b\u90e8\u7f72\u652f\u6301</li> <li> \u7b2c\u4e09\u65b9\u6846\u67b6\u96c6\u6210</li> </ul>"},{"location":"contributing/index.zh/#_15","title":"\ud83d\udcde \u8054\u7cfb\u6211\u4eec","text":"<ul> <li>GitHub Issues: \u62a5\u544a\u95ee\u9898\u548c\u529f\u80fd\u8bf7\u6c42</li> <li>GitHub Discussions: \u6280\u672f\u8ba8\u8bba\u548c\u95ee\u7b54</li> <li>Email: genesis-dev@example.com</li> </ul>"},{"location":"contributing/index.zh/#_16","title":"\ud83c\udfc6 \u8d21\u732e\u8005\u8ba4\u53ef","text":"<p>\u6211\u4eec\u91cd\u89c6\u6bcf\u4e00\u4f4d\u8d21\u732e\u8005\u7684\u52aa\u529b\uff1a</p> <ul> <li>\u8d21\u732e\u8005\u5c06\u5217\u5728\u9879\u76eeREADME\u4e2d</li> <li>\u91cd\u5927\u8d21\u732e\u8005\u5c06\u83b7\u5f97\u7ef4\u62a4\u8005\u6743\u9650</li> <li>\u5b9a\u671f\u53d1\u5e03\u8d21\u732e\u8005\u901a\u8baf</li> </ul>"},{"location":"contributing/index.zh/#_17","title":"\ud83d\udcc4 \u8bb8\u53ef\u8bc1","text":"<p>\u901a\u8fc7\u8d21\u732e\u4ee3\u7801\uff0c\u4f60\u540c\u610f\u4f60\u7684\u8d21\u732e\u5c06\u5728MIT\u8bb8\u53ef\u8bc1\u4e0b\u53d1\u5e03\u3002</p> <p>\u5f00\u59cb\u8d21\u732e</p> <p>\u51c6\u5907\u597d\u5f00\u59cb\u8d21\u732e\u4e86\u5417\uff1f\u5148\u4ece\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u5f00\u59cb\u5427\uff01</p> <p>\u611f\u8c22\u4f60\u4e3aGenesis\u9879\u76ee\u7684\u8d21\u732e\uff01\ud83c\udf89</p>"},{"location":"contributing/index_en/","title":"Contributing Guide","text":"<p>Welcome to contribute code to the Genesis deep learning framework! This guide will help you understand how to participate in project development.</p>"},{"location":"contributing/index_en/#ways-to-contribute","title":"\ud83e\udd1d Ways to Contribute","text":""},{"location":"contributing/index_en/#code-contributions","title":"Code Contributions","text":"<ul> <li>Fix bugs</li> <li>Add new features</li> <li>Performance optimization</li> <li>Test improvements</li> </ul>"},{"location":"contributing/index_en/#documentation-contributions","title":"Documentation Contributions","text":"<ul> <li>Improve existing documentation</li> <li>Add tutorials and examples</li> <li>Translate documentation</li> <li>API documentation improvements</li> </ul>"},{"location":"contributing/index_en/#community-contributions","title":"Community Contributions","text":"<ul> <li>Answer questions</li> <li>Code review</li> <li>Issue reporting</li> <li>Feature suggestions</li> </ul>"},{"location":"contributing/index_en/#development-workflow","title":"\ud83d\udccb Development Workflow","text":""},{"location":"contributing/index_en/#1-preparation","title":"1. Preparation","text":"Bash<pre><code># Fork the project to your GitHub account\n# Clone your fork\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# Add upstream repository\ngit remote add upstream https://github.com/phonism/genesis.git\n\n# Create development branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/index_en/#2-development-environment-setup","title":"2. Development Environment Setup","text":"<p>See the Development Environment Configuration documentation for details.</p>"},{"location":"contributing/index_en/#3-code-development","title":"3. Code Development","text":"<ul> <li>Follow coding standards</li> <li>Add unit tests</li> <li>Update relevant documentation</li> <li>Write clear commit messages</li> </ul>"},{"location":"contributing/index_en/#4-testing-and-verification","title":"4. Testing and Verification","text":"Bash<pre><code># Run test suite\npython -m pytest tests/ -v\n\n# Run code format checks\nblack genesis/ tests/\nflake8 genesis/ tests/\n\n# Run type checking\nmypy genesis/\n</code></pre>"},{"location":"contributing/index_en/#5-submit-pull-request","title":"5. Submit Pull Request","text":"<ul> <li>Ensure all tests pass</li> <li>Write detailed PR description</li> <li>Link related issues</li> <li>Wait for code review</li> </ul>"},{"location":"contributing/index_en/#code-standards","title":"\ud83d\udcdd Code Standards","text":""},{"location":"contributing/index_en/#python-style-guide","title":"Python Style Guide","text":"<p>We follow PEP 8 standards:</p> Python<pre><code># Good example\ndef compute_attention_weights(query, key, scale_factor):\n    \"\"\"Compute scaled dot-product attention weights.\n\n    Args:\n        query: Query tensor of shape [batch, seq_len, hidden_dim]\n        key: Key tensor of shape [batch, seq_len, hidden_dim] \n        scale_factor: Scaling factor for attention scores\n\n    Returns:\n        Attention weights of shape [batch, seq_len, seq_len]\n    \"\"\"\n    scores = genesis.matmul(query, key.transpose(-2, -1))\n    scaled_scores = scores * scale_factor\n    return genesis.softmax(scaled_scores, dim=-1)\n</code></pre>"},{"location":"contributing/index_en/#documentation-strings","title":"Documentation Strings","text":"<p>Use Google-style docstrings:</p> Python<pre><code>def example_function(param1: int, param2: str = \"default\") -&gt; bool:\n    \"\"\"One line summary of the function.\n\n    More detailed description if needed. Can span multiple lines.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2 with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is negative\n\n    Example:\n        &gt;&gt;&gt; result = example_function(42, \"test\")\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n    if param1 &lt; 0:\n        raise ValueError(\"param1 must be non-negative\")\n    return param1 &gt; 0\n</code></pre>"},{"location":"contributing/index_en/#writing-tests","title":"Writing Tests","text":"Python<pre><code>import pytest\nimport genesis\n\nclass TestAttention:\n    \"\"\"Test attention mechanisms.\"\"\"\n\n    def test_basic_attention(self):\n        \"\"\"Test basic attention computation.\"\"\"\n        batch_size, seq_len, hidden_dim = 2, 4, 8\n\n        query = genesis.randn(batch_size, seq_len, hidden_dim)\n        key = genesis.randn(batch_size, seq_len, hidden_dim)\n        value = genesis.randn(batch_size, seq_len, hidden_dim)\n\n        attention = genesis.nn.MultiHeadAttention(hidden_dim, num_heads=2)\n        output = attention(query, key, value)\n\n        assert output.shape == (batch_size, seq_len, hidden_dim)\n\n    @pytest.mark.parametrize(\"num_heads\", [1, 2, 4, 8])\n    def test_different_head_counts(self, num_heads):\n        \"\"\"Test attention with different head counts.\"\"\"\n        # Test implementation\n        pass\n</code></pre>"},{"location":"contributing/index_en/#development-best-practices","title":"\ud83d\ude80 Development Best Practices","text":""},{"location":"contributing/index_en/#1-branch-management","title":"1. Branch Management","text":"Bash<pre><code># Main branches\nmain          # Stable version\ndevelop       # Development version\n\n# Feature branches\nfeature/xxx   # New feature development\nbugfix/xxx    # Bug fixes\nhotfix/xxx    # Hotfixes\n</code></pre>"},{"location":"contributing/index_en/#2-commit-message-format","title":"2. Commit Message Format","text":"Text Only<pre><code>type(scope): brief description\n\nDetailed description (optional)\n\nFixes #123\n</code></pre> <p>Type descriptions: - <code>feat</code>: New features - <code>fix</code>: Bug fixes - <code>docs</code>: Documentation updates - <code>style</code>: Code formatting adjustments - <code>refactor</code>: Refactoring - <code>perf</code>: Performance optimization - <code>test</code>: Test-related - <code>chore</code>: Build tools, etc.</p>"},{"location":"contributing/index_en/#3-performance-considerations","title":"3. Performance Considerations","text":"<ul> <li>Avoid unnecessary memory copies</li> <li>Use in-place operations when possible</li> <li>Consider CUDA kernel memory access patterns</li> <li>Add performance benchmarks</li> </ul>"},{"location":"contributing/index_en/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>When submitting bugs, please include:</p> <ol> <li>Environment Information</li> <li>Genesis version</li> <li>Python version</li> <li>CUDA version</li> <li> <p>Operating system</p> </li> <li> <p>Reproduction Steps</p> </li> <li>Minimal reproducible code</li> <li>Expected behavior</li> <li>Actual behavior</li> <li> <p>Error messages</p> </li> <li> <p>Related Logs</p> </li> <li>Complete error stack trace</li> <li>Relevant configuration information</li> </ol> <p>Example: Python<pre><code># Minimal reproduction case\nimport genesis\n\nmodel = genesis.nn.Linear(10, 5)\nx = genesis.randn(3, 10)\ny = model(x)  # Error occurs here\n\n# Error message:\n# RuntimeError: CUDA kernel launch failed\n</code></pre></p>"},{"location":"contributing/index_en/#key-contribution-areas","title":"\ud83c\udfaf Key Contribution Areas","text":"<p>We particularly welcome contributions in the following areas:</p>"},{"location":"contributing/index_en/#high-priority","title":"High Priority","text":"<ul> <li> Performance optimization and benchmarking</li> <li> CUDA operator implementation</li> <li> Documentation and tutorial improvements</li> <li> Test coverage enhancement</li> </ul>"},{"location":"contributing/index_en/#medium-priority","title":"Medium Priority","text":"<ul> <li> New neural network layers</li> <li> Data loader optimization</li> <li> Distributed training support</li> <li> Mixed precision training</li> </ul>"},{"location":"contributing/index_en/#low-priority","title":"Low Priority","text":"<ul> <li> Visualization tools</li> <li> Model deployment support</li> <li> Third-party framework integration</li> </ul>"},{"location":"contributing/index_en/#contact-us","title":"\ud83d\udcde Contact Us","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Technical discussions and Q&amp;A</li> <li>Email: genesis-dev@example.com</li> </ul>"},{"location":"contributing/index_en/#contributor-recognition","title":"\ud83c\udfc6 Contributor Recognition","text":"<p>We value every contributor's effort:</p> <ul> <li>Contributors will be listed in the project README</li> <li>Major contributors will receive maintainer privileges</li> <li>Regular contributor newsletters</li> </ul>"},{"location":"contributing/index_en/#license","title":"\ud83d\udcc4 License","text":"<p>By contributing code, you agree that your contributions will be licensed under the MIT License.</p> <p>Start Contributing</p> <p>Ready to start contributing? Begin with Development Environment Configuration!</p> <p>Thank you for your contribution to the Genesis project! \ud83c\udf89</p>"},{"location":"contributing/testing.zh/","title":"\u6d4b\u8bd5\u89c4\u8303","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>\u672c\u6587\u6863\u89c4\u5b9a\u4e86Genesis\u9879\u76ee\u7684\u6d4b\u8bd5\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"contributing/testing.zh/#_2","title":"\ud83c\udfaf \u6d4b\u8bd5\u539f\u5219","text":""},{"location":"contributing/testing.zh/#1","title":"1. \u6d4b\u8bd5\u91d1\u5b57\u5854","text":"<ul> <li>\u5355\u5143\u6d4b\u8bd5 (70%): \u6d4b\u8bd5\u5355\u4e2a\u51fd\u6570\u548c\u7c7b</li> <li>\u96c6\u6210\u6d4b\u8bd5 (20%): \u6d4b\u8bd5\u7ec4\u4ef6\u95f4\u4ea4\u4e92</li> <li>\u7aef\u5230\u7aef\u6d4b\u8bd5 (10%): \u6d4b\u8bd5\u5b8c\u6574\u5de5\u4f5c\u6d41</li> </ul>"},{"location":"contributing/testing.zh/#2","title":"2. \u6d4b\u8bd5\u8986\u76d6\u7387","text":"<ul> <li>\u76ee\u6807\u8986\u76d6\u7387: 90%+</li> <li>\u5173\u952e\u6a21\u5757\u8981\u6c42: 95%+</li> <li>\u65b0\u4ee3\u7801\u8981\u6c42: 100%</li> </ul>"},{"location":"contributing/testing.zh/#_3","title":"\ud83d\udccb \u6d4b\u8bd5\u5206\u7c7b","text":""},{"location":"contributing/testing.zh/#_4","title":"\u5355\u5143\u6d4b\u8bd5","text":"Python<pre><code>def test_tensor_creation():\n    \"\"\"Test basic tensor creation.\"\"\"\n    x = genesis.randn(3, 4)\n    assert x.shape == (3, 4)\n    assert x.dtype == genesis.float32\n</code></pre>"},{"location":"contributing/testing.zh/#_5","title":"\u6027\u80fd\u6d4b\u8bd5","text":"Python<pre><code>@pytest.mark.benchmark\ndef test_matmul_performance():\n    \"\"\"Benchmark matrix multiplication performance.\"\"\"\n    # WIP: \u6027\u80fd\u6d4b\u8bd5\u5b9e\u73b0\n    pass\n</code></pre>"},{"location":"contributing/testing.zh/#gpu","title":"GPU\u6d4b\u8bd5","text":"Python<pre><code>@pytest.mark.cuda\ndef test_cuda_operations():\n    \"\"\"Test CUDA-specific operations.\"\"\"\n    if not genesis.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n\n    x = genesis.randn(10, 10, device='cuda')\n    y = genesis.matmul(x, x)\n    assert y.device.type == 'cuda'\n</code></pre>"},{"location":"contributing/testing.zh/#_6","title":"\ud83d\ude80 \u8fd0\u884c\u6d4b\u8bd5","text":"Bash<pre><code># \u6240\u6709\u6d4b\u8bd5\npytest tests/ -v\n\n# \u7279\u5b9a\u6807\u8bb0\npytest tests/ -m \"not slow\" -v\n\n# \u8986\u76d6\u7387\u62a5\u544a\npytest tests/ --cov=genesis --cov-report=html\n</code></pre>"},{"location":"contributing/testing.zh/#_7","title":"\ud83d\udcca \u6d4b\u8bd5\u5de5\u5177","text":"<ul> <li>pytest: \u4e3b\u8981\u6d4b\u8bd5\u6846\u67b6</li> <li>pytest-cov: \u8986\u76d6\u7387\u7edf\u8ba1</li> <li>pytest-benchmark: \u6027\u80fd\u6d4b\u8bd5</li> <li>pytest-xdist: \u5e76\u884c\u6d4b\u8bd5</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"},{"location":"contributing/testing_en/","title":"Testing Guidelines","text":"<p>Under Development</p> <p>This document is currently being written and content will be continuously updated.</p> <p>This document defines testing standards and best practices for the Genesis project.</p>"},{"location":"contributing/testing_en/#testing-principles","title":"\ud83c\udfaf Testing Principles","text":""},{"location":"contributing/testing_en/#1-test-pyramid","title":"1. Test Pyramid","text":"<ul> <li>Unit Tests (70%): Test individual functions and classes</li> <li>Integration Tests (20%): Test component interactions</li> <li>End-to-End Tests (10%): Test complete workflows</li> </ul>"},{"location":"contributing/testing_en/#2-test-coverage","title":"2. Test Coverage","text":"<ul> <li>Target coverage: 90%+</li> <li>Critical modules requirement: 95%+</li> <li>New code requirement: 100%</li> </ul>"},{"location":"contributing/testing_en/#test-categories","title":"\ud83d\udccb Test Categories","text":""},{"location":"contributing/testing_en/#unit-tests","title":"Unit Tests","text":"Python<pre><code>def test_tensor_creation():\n    \"\"\"Test basic tensor creation.\"\"\"\n    x = genesis.randn(3, 4)\n    assert x.shape == (3, 4)\n    assert x.dtype == genesis.float32\n</code></pre>"},{"location":"contributing/testing_en/#performance-tests","title":"Performance Tests","text":"Python<pre><code>@pytest.mark.benchmark\ndef test_matmul_performance():\n    \"\"\"Benchmark matrix multiplication performance.\"\"\"\n    # WIP: Performance test implementation\n    pass\n</code></pre>"},{"location":"contributing/testing_en/#gpu-tests","title":"GPU Tests","text":"Python<pre><code>@pytest.mark.cuda\ndef test_cuda_operations():\n    \"\"\"Test CUDA-specific operations.\"\"\"\n    if not genesis.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n\n    x = genesis.randn(10, 10, device='cuda')\n    y = genesis.matmul(x, x)\n    assert y.device.type == 'cuda'\n</code></pre>"},{"location":"contributing/testing_en/#running-tests","title":"\ud83d\ude80 Running Tests","text":"Bash<pre><code># All tests\npytest tests/ -v\n\n# Specific markers\npytest tests/ -m \"not slow\" -v\n\n# Coverage report\npytest tests/ --cov=genesis --cov-report=html\n</code></pre>"},{"location":"contributing/testing_en/#testing-tools","title":"\ud83d\udcca Testing Tools","text":"<ul> <li>pytest: Main testing framework</li> <li>pytest-cov: Coverage statistics</li> <li>pytest-benchmark: Performance testing</li> <li>pytest-xdist: Parallel testing</li> </ul> <p>\ud83d\udcd8 Documentation Status: Currently being written, expected completion in v0.2.0.</p>"},{"location":"core-components/autograd.zh/","title":"\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf","text":"<p>Genesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u662f\u6846\u67b6\u7684\u6838\u5fc3\uff0c\u8d1f\u8d23\u6784\u5efa\u8ba1\u7b97\u56fe\u3001\u6267\u884c\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u3002\u7cfb\u7edf\u8bbe\u8ba1\u7b80\u6d01\u800c\u9ad8\u6548\uff0c\u652f\u6301\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002</p>"},{"location":"core-components/autograd.zh/#_2","title":"\ud83c\udfaf \u7cfb\u7edf\u6982\u8ff0","text":"<p>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u57fa\u4e8e\u52a8\u6001\u8ba1\u7b97\u56fe\u5b9e\u73b0\uff0c\u4e3b\u8981\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a</p> <ul> <li>Tensor - \u643a\u5e26\u68af\u5ea6\u4fe1\u606f\u7684\u5f20\u91cf</li> <li>Function - \u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u62bd\u8c61\u57fa\u7c7b</li> <li>Context - \u4fdd\u5b58\u524d\u5411\u4f20\u64ad\u4e2d\u95f4\u7ed3\u679c\u7684\u4e0a\u4e0b\u6587</li> </ul>"},{"location":"core-components/autograd.zh/#_3","title":"\ud83c\udfd7\ufe0f \u6838\u5fc3\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u8ba1\u7b97\u56fe\u8282\u70b9\"\n        A[Tensor] --&gt; B[data NDArray]\n        A --&gt; C[grad Tensor]\n        A --&gt; D[creator Function]\n        A --&gt; E[requires_grad bool]\n    end\n\n    subgraph \"\u64cd\u4f5c\u8282\u70b9\"\n        F[Function] --&gt; G[forward]\n        F --&gt; H[backward]\n        F --&gt; I[Context]\n        I --&gt; J[saved_tensors]\n    end\n\n    subgraph \"\u6267\u884c\u6d41\u7a0b\"\n        K[\u524d\u5411\u4f20\u64ad] --&gt; L[\u6784\u5efa\u8ba1\u7b97\u56fe]\n        L --&gt; M[\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c]\n        M --&gt; N[\u53cd\u5411\u4f20\u64ad]\n        N --&gt; O[\u68af\u5ea6\u8ba1\u7b97]\n        O --&gt; P[\u68af\u5ea6\u7d2f\u79ef]\n    end\n\n    A --&gt; F\n    F --&gt; A\n\n    style A fill:#e1f5fe\n    style F fill:#f3e5f5\n    style I fill:#e8f5e8</code></pre>"},{"location":"core-components/autograd.zh/#tensor","title":"\ud83e\uddee Tensor\u7c7b\u8be6\u89e3","text":""},{"location":"core-components/autograd.zh/#_4","title":"\u6838\u5fc3\u5c5e\u6027","text":"Python<pre><code>class Tensor:\n    grad: \"Tensor\"          # \u68af\u5ea6\u5f20\u91cf\n    creator: Function       # \u521b\u5efa\u6b64\u5f20\u91cf\u7684\u64cd\u4f5c\n    inputs: List[\"Tensor\"]  # \u8f93\u5165\u5f20\u91cf\u5217\u8868\n    data: NDArray          # \u5e95\u5c42\u6570\u636e\u5b58\u50a8\n    requires_grad: bool    # \u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\n    hooks: List[Callable]  # \u68af\u5ea6\u94a9\u5b50\u51fd\u6570\n</code></pre>"},{"location":"core-components/autograd.zh/#_5","title":"\u5173\u952e\u65b9\u6cd5","text":""},{"location":"core-components/autograd.zh/#1","title":"1. \u5f20\u91cf\u521b\u5efa","text":"Python<pre><code># \u4ece\u6570\u7ec4\u521b\u5efa\u5f20\u91cf\ndef __init__(self, array, *, device=None, dtype=None, requires_grad=True):\n    if dtype is not None:\n        dtype = get_dtype(dtype)  # \u8f6c\u6362\u4e3aDType\u5bf9\u8c61\n\n    # \u5904\u7406\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\n    if isinstance(array, Tensor):\n        # \u4ece\u73b0\u6709\u5f20\u91cf\u521b\u5efa\n        data = array.data if same_device_dtype else convert_data\n    elif isinstance(array, NDArray):\n        # \u4eceNDArray\u521b\u5efa\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n    else:\n        # \u4ecePython\u5bf9\u8c61\u521b\u5efa\n        device = device if device else default_device()\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n\n    self.init([], data=data, requires_grad=requires_grad)\n</code></pre>"},{"location":"core-components/autograd.zh/#2","title":"2. \u53cd\u5411\u4f20\u64ad","text":"Python<pre><code>def backward(self, out_grad=None):\n    # \u8bbe\u7f6e\u8f93\u51fa\u68af\u5ea6\n    out_grad = out_grad if out_grad else init.ones(*self.shape, dtype=self.dtype, device=self.device)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u7d2f\u79ef\u5b57\u5178\n    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}\n    node_to_output_grads_list[self] = [out_grad]\n\n    # \u62d3\u6251\u6392\u5e8f\u83b7\u53d6\u8ba1\u7b97\u987a\u5e8f\n    topo_order = topo_sort(self)\n\n    # \u9006\u62d3\u6251\u5e8f\u904d\u5386\u8ba1\u7b97\u68af\u5ea6\n    for node in reversed(topo_order):\n        if not node.requires_grad:\n            continue\n\n        # \u7d2f\u79ef\u5f53\u524d\u8282\u70b9\u7684\u68af\u5ea6\n        if node.grad is None:\n            node.grad = reduce(operator.add, node_to_output_grads_list[node])\n            # \u786e\u4fdd\u68af\u5ea6\u8fde\u7eed\u6027\uff08\u89e3\u51b3\u5e7f\u64ad\u5f20\u91cf\u95ee\u9898\uff09\n            if hasattr(node.grad, 'data') and hasattr(node.grad.data, 'data'):\n                cuda_tensor = node.grad.data.data\n                if hasattr(cuda_tensor, 'is_contiguous') and not cuda_tensor.is_contiguous():\n                    node.grad.data.data = cuda_tensor.contiguous()\n        else:\n            node.grad += reduce(operator.add, node_to_output_grads_list[node])\n\n        # \u5e94\u7528\u68af\u5ea6\u94a9\u5b50\n        node.apply_hooks(node.grad)\n\n        # \u8ba1\u7b97\u8f93\u5165\u8282\u70b9\u7684\u68af\u5ea6\n        if node.creator is not None:\n            # \u5904\u7406\u6df7\u5408\u7cbe\u5ea6\n            grad = node.grad.half() if check_dtype(node.creator.ctx.saved_tensors, genesis.float16) else node.grad\n\n            # \u8c03\u7528\u5bf9\u5e94\u64cd\u4f5c\u7684\u53cd\u5411\u4f20\u64ad\n            if node.creator.is_tuple_result:\n                backward_grad = node.creator.backward(node.creator.ctx, grad, node.idx)\n            else:\n                backward_grad = node.creator.backward(node.creator.ctx, grad)\n\n            # \u5206\u53d1\u68af\u5ea6\u5230\u8f93\u5165\u8282\u70b9\n            for i, input_node in enumerate(node.creator.inputs):\n                if input_node.requires_grad:\n                    if input_node not in node_to_output_grads_list:\n                        node_to_output_grads_list[input_node] = []\n                    node_to_output_grads_list[input_node].append(backward_grad[i].float())\n</code></pre>"},{"location":"core-components/autograd.zh/#3","title":"3. \u62d3\u6251\u6392\u5e8f","text":"Python<pre><code>def topo_sort(node):\n    \"\"\"\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u5b9e\u73b0\u62d3\u6251\u6392\u5e8f\"\"\"\n    visited = set()\n    topo_order = []\n\n    def dfs(n):\n        if n in visited:\n            return\n        visited.add(n)\n\n        # \u9012\u5f52\u8bbf\u95ee\u8f93\u5165\u8282\u70b9\n        if n.creator is not None:\n            for input_node in n.creator.inputs:\n                if isinstance(input_node, Tensor):\n                    dfs(input_node)\n\n        topo_order.append(n)\n\n    dfs(node)\n    return topo_order\n</code></pre>"},{"location":"core-components/autograd.zh/#function","title":"\u2699\ufe0f Function\u57fa\u7c7b","text":"<p>Function\u662f\u6240\u6709\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\uff0c\u5b9a\u4e49\u4e86\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7684\u63a5\u53e3\u3002</p>"},{"location":"core-components/autograd.zh/#_6","title":"\u57fa\u672c\u7ed3\u6784","text":"Python<pre><code>class Function:\n    @staticmethod\n    def forward(ctx: Context, *args) -&gt; Union[Tensor, Tuple[Tensor, ...]]:\n        \"\"\"\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\"\"\"\n        raise NotImplementedError\n\n    @staticmethod  \n    def backward(ctx: Context, grad_output, out_idx=None) -&gt; Tuple[Tensor, ...]:\n        \"\"\"\u53cd\u5411\u4f20\u64ad\u5b9e\u73b0\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def apply(cls, *args, **kwargs):\n        \"\"\"\u7edf\u4e00\u7684\u8c03\u7528\u63a5\u53e3\"\"\"\n        # \u5904\u7406\u6df7\u5408\u7cbe\u5ea6\n        instance = cls()\n        instance.ctx = Context()\n\n        # \u6267\u884c\u524d\u5411\u4f20\u64ad\n        if genesis.enable_autocast:\n            result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n        else:\n            result = cls.forward(instance.ctx, *args, **kwargs)\n\n        # \u8bbe\u7f6e\u8ba1\u7b97\u56fe\u8fde\u63a5\n        instance.is_tuple_result = isinstance(result, tuple)\n\n        if instance.is_tuple_result:\n            for idx, res in enumerate(result):\n                if isinstance(res, Tensor) and res.requires_grad:\n                    res.set_creator(instance, idx)\n        elif isinstance(result, Tensor) and result.requires_grad:\n            result.set_creator(instance)\n\n        # \u8bb0\u5f55\u8f93\u5165\u5f20\u91cf\n        instance.inputs = []\n        for t in args:\n            if isinstance(t, Tensor):\n                instance.inputs.append(t)\n            elif isinstance(t, list) and all(isinstance(item, Tensor) for item in t):\n                instance.inputs.extend(t)\n\n        return result\n</code></pre>"},{"location":"core-components/autograd.zh/#_7","title":"\u5b9e\u9645\u64cd\u4f5c\u793a\u4f8b","text":""},{"location":"core-components/autograd.zh/#_8","title":"\u77e9\u9635\u4e58\u6cd5","text":"Python<pre><code>class MatMul(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        # \u4fdd\u5b58\u8f93\u5165\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\n        ctx.save_for_backward(a, b)\n        return a @ b  # \u8c03\u7528\u5e95\u5c42NDArray\u7684\u77e9\u9635\u4e58\u6cd5\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        a, b = ctx.saved_tensors\n        # \u8ba1\u7b97\u8f93\u5165\u68af\u5ea6\n        grad_a = grad_output @ b.T\n        grad_b = a.T @ grad_output\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd.zh/#_9","title":"\u52a0\u6cd5\uff08\u652f\u6301\u5e7f\u64ad\uff09","text":"Python<pre><code>class Add(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        ctx.a_shape = a.shape\n        ctx.b_shape = b.shape\n        return a + b\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # \u5904\u7406\u5e7f\u64ad\u7684\u68af\u5ea6\n        grad_a = grad_output\n        grad_b = grad_output\n\n        # \u5bf9\u88ab\u5e7f\u64ad\u7684\u7ef4\u5ea6\u6c42\u548c\n        for i, (da, db) in enumerate(zip(ctx.a_shape, ctx.b_shape)):\n            if da == 1 and db &gt; 1:\n                grad_a = grad_a.sum(axis=i, keepdims=True)\n            elif db == 1 and da &gt; 1:\n                grad_b = grad_b.sum(axis=i, keepdims=True)\n\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd.zh/#context","title":"\ud83d\udcdd Context\u7c7b","text":"<p>Context\u7c7b\u7528\u4e8e\u5728\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u4e4b\u95f4\u4f20\u9012\u4fe1\u606f\u3002</p> Python<pre><code>class Context:\n    def __init__(self):\n        self.saved_tensors = []\n\n    def save_for_backward(self, *tensors):\n        \"\"\"\u4fdd\u5b58\u5f20\u91cf\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\"\"\"\n        self.saved_tensors.extend(tensors)\n\n    @property\n    def saved_tensors(self):\n        return self._saved_tensors\n\n    @saved_tensors.setter  \n    def saved_tensors(self, tensors):\n        self._saved_tensors = tensors\n</code></pre>"},{"location":"core-components/autograd.zh/#_10","title":"\ud83d\udd04 \u6df7\u5408\u7cbe\u5ea6\u652f\u6301","text":"<p>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u5185\u7f6e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u652f\u6301\uff1a</p> Python<pre><code># \u5168\u5c40\u5f00\u5173\ngenesis.enable_autocast = True\n\n# \u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\ndef _cast(value, dtype):\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# \u5728Function.apply\u4e2d\u5e94\u7528\nif genesis.enable_autocast:\n    result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/autograd.zh/#_11","title":"\ud83e\ude9d \u68af\u5ea6\u94a9\u5b50\u7cfb\u7edf","text":"<p>\u652f\u6301\u5728\u68af\u5ea6\u8ba1\u7b97\u65f6\u6267\u884c\u81ea\u5b9a\u4e49\u51fd\u6570\uff1a</p> Python<pre><code>class Tensor:\n    def register_hook(self, hook):\n        \"\"\"\u6ce8\u518c\u68af\u5ea6\u94a9\u5b50\"\"\"\n        self.hooks.append(hook)\n\n    def apply_hooks(self, grad):\n        \"\"\"\u5e94\u7528\u6240\u6709\u94a9\u5b50\"\"\"\n        for hook in self.hooks:\n            hook(grad)\n\n# \u4f7f\u7528\u793a\u4f8b\ndef grad_clipping_hook(grad):\n    \"\"\"\u68af\u5ea6\u88c1\u526a\u94a9\u5b50\"\"\"\n    grad.clamp_(-1.0, 1.0)\n\ntensor.register_hook(grad_clipping_hook)\n</code></pre>"},{"location":"core-components/autograd.zh/#_12","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/autograd.zh/#1_1","title":"1. \u5185\u5b58\u7ba1\u7406\u4f18\u5316","text":"<ul> <li>\u89c6\u56fe\u64cd\u4f5c\uff1areshape\u3001transpose\u7b49\u64cd\u4f5c\u521b\u5efa\u89c6\u56fe\u800c\u975e\u62f7\u8d1d\u6570\u636e</li> <li>\u5c31\u5730\u64cd\u4f5c\uff1a\u652f\u6301<code>+=</code>\u3001<code>*=</code>\u7b49\u5c31\u5730\u66f4\u65b0\u64cd\u4f5c</li> <li>\u68af\u5ea6\u7d2f\u79ef\u4f18\u5316\uff1a\u667a\u80fd\u7684\u68af\u5ea6\u7d2f\u79ef\u7b56\u7565</li> </ul>"},{"location":"core-components/autograd.zh/#2_1","title":"2. \u8ba1\u7b97\u56fe\u4f18\u5316","text":"<ul> <li>\u60f0\u6027\u6784\u5efa\uff1a\u53ea\u6709\u5728\u9700\u8981\u68af\u5ea6\u65f6\u624d\u6784\u5efa\u8ba1\u7b97\u56fe</li> <li>\u667a\u80fd\u91ca\u653e\uff1a\u81ea\u52a8\u91ca\u653e\u4e0d\u518d\u9700\u8981\u7684\u4e2d\u95f4\u7ed3\u679c</li> <li>\u62d3\u6251\u6392\u5e8f\u7f13\u5b58\uff1a\u7f13\u5b58\u62d3\u6251\u6392\u5e8f\u7ed3\u679c</li> </ul>"},{"location":"core-components/autograd.zh/#3_1","title":"3. \u8bbe\u5907\u95f4\u4f18\u5316","text":"<ul> <li>\u81ea\u52a8\u8bbe\u5907\u63a8\u65ad\uff1a\u81ea\u52a8\u9009\u62e9\u5408\u9002\u7684\u8ba1\u7b97\u8bbe\u5907</li> <li>\u5f02\u6b65\u6267\u884c\uff1a\u652f\u6301GPU\u5f02\u6b65\u8ba1\u7b97</li> <li>\u5185\u5b58\u9884\u5206\u914d\uff1a\u51cf\u5c11\u52a8\u6001\u5185\u5b58\u5206\u914d</li> </ul>"},{"location":"core-components/autograd.zh/#_13","title":"\ud83c\udfaf \u4f7f\u7528\u793a\u4f8b","text":""},{"location":"core-components/autograd.zh/#_14","title":"\u57fa\u7840\u7528\u6cd5","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u9700\u8981\u68af\u5ea6\u7684\u5f20\u91cf\nx = genesis.randn(3, 4, requires_grad=True)\ny = genesis.randn(4, 2, requires_grad=True)\n\n# \u524d\u5411\u4f20\u64ad\uff08\u81ea\u52a8\u6784\u5efa\u8ba1\u7b97\u56fe\uff09\nz = x @ y\nloss = z.sum()\n\n# \u53cd\u5411\u4f20\u64ad\uff08\u8ba1\u7b97\u6240\u6709\u68af\u5ea6\uff09\nloss.backward()\n\nprint(f\"x\u7684\u68af\u5ea6: {x.grad}\")  # \u8f93\u51fax\u7684\u68af\u5ea6\nprint(f\"y\u7684\u68af\u5ea6: {y.grad}\")  # \u8f93\u51fay\u7684\u68af\u5ea6\n</code></pre>"},{"location":"core-components/autograd.zh/#_15","title":"\u81ea\u5b9a\u4e49\u64cd\u4f5c","text":"Python<pre><code>class CustomFunction(genesis.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_tensor):\n        # \u81ea\u5b9a\u4e49\u524d\u5411\u8ba1\u7b97\n        ctx.save_for_backward(input_tensor)\n        result = input_tensor ** 2 + 2 * input_tensor + 1\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_tensor, = ctx.saved_tensors\n        # \u81ea\u5b9a\u4e49\u68af\u5ea6\u8ba1\u7b97\uff1ad/dx(x^2 + 2x + 1) = 2x + 2\n        grad_input = grad_output * (2 * input_tensor + 2)\n        return grad_input\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u64cd\u4f5c\nx = genesis.randn(3, 4, requires_grad=True)\ny = CustomFunction.apply(x)\ny.sum().backward()\n</code></pre>"},{"location":"core-components/autograd.zh/#_16","title":"\u68af\u5ea6\u94a9\u5b50","text":"Python<pre><code># \u68af\u5ea6\u76d1\u63a7\u94a9\u5b50\ndef monitor_grad(grad):\n    print(f\"\u68af\u5ea6\u7edf\u8ba1: \u5747\u503c={grad.mean():.4f}, \u6807\u51c6\u5dee={grad.std():.4f}\")\n\n# \u68af\u5ea6\u88c1\u526a\u94a9\u5b50\ndef clip_grad(grad):\n    grad.data.clamp_(-1.0, 1.0)\n\nx = genesis.randn(10, requires_grad=True)\nx.register_hook(monitor_grad)\nx.register_hook(clip_grad)\n\n# \u6267\u884c\u4e00\u4e9b\u8ba1\u7b97\ny = (x ** 3).sum()\ny.backward()  # \u4f1a\u89e6\u53d1\u94a9\u5b50\u51fd\u6570\n</code></pre> <p>Genesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u8bbe\u8ba1\u7b80\u6d01\u800c\u5f3a\u5927\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u68af\u5ea6\u8ba1\u7b97\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002</p>"},{"location":"core-components/autograd_en/","title":"Automatic Differentiation System","text":"<p>Genesis's automatic differentiation system is the core of the framework, responsible for building computation graphs, executing forward propagation and backpropagation. The system is designed to be simple yet efficient, supporting complex neural network training.</p>"},{"location":"core-components/autograd_en/#system-overview","title":"\ud83c\udfaf System Overview","text":"<p>The automatic differentiation system is based on dynamic computation graph implementation, mainly including three core components:</p> <ul> <li>Tensor - Tensors carrying gradient information</li> <li>Function - Abstract base class for differentiable operations</li> <li>Context - Context that saves intermediate results during forward propagation</li> </ul>"},{"location":"core-components/autograd_en/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":"<pre><code>graph TB\n    subgraph \"Computation Graph Nodes\"\n        A[Tensor] --&gt; B[data NDArray]\n        A --&gt; C[grad Tensor]\n        A --&gt; D[creator Function]\n        A --&gt; E[requires_grad bool]\n    end\n\n    subgraph \"Operation Nodes\"\n        F[Function] --&gt; G[forward]\n        F --&gt; H[backward]\n        F --&gt; I[Context]\n        I --&gt; J[saved_tensors]\n    end\n\n    subgraph \"Execution Flow\"\n        K[Forward Pass] --&gt; L[Build Computation Graph]\n        L --&gt; M[Save Intermediate Results]\n        M --&gt; N[Backward Pass]\n        N --&gt; O[Gradient Calculation]\n        O --&gt; P[Gradient Accumulation]\n    end\n\n    A --&gt; F\n    F --&gt; A\n\n    style A fill:#e1f5fe\n    style F fill:#f3e5f5\n    style I fill:#e8f5e8</code></pre>"},{"location":"core-components/autograd_en/#tensor-class-details","title":"\ud83e\uddee Tensor Class Details","text":""},{"location":"core-components/autograd_en/#core-attributes","title":"Core Attributes","text":"Python<pre><code>class Tensor:\n    grad: \"Tensor\"          # Gradient tensor\n    creator: Function       # Operation that created this tensor\n    inputs: List[\"Tensor\"]  # Input tensor list\n    data: NDArray          # Underlying data storage\n    requires_grad: bool    # Whether gradients are required\n    hooks: List[Callable]  # Gradient hook functions\n</code></pre>"},{"location":"core-components/autograd_en/#key-methods","title":"Key Methods","text":""},{"location":"core-components/autograd_en/#1-tensor-creation","title":"1. Tensor Creation","text":"Python<pre><code># Create tensor from array\ndef __init__(self, array, *, device=None, dtype=None, requires_grad=True):\n    if dtype is not None:\n        dtype = get_dtype(dtype)  # Convert to DType object\n\n    # Handle different input types\n    if isinstance(array, Tensor):\n        # Create from existing tensor\n        data = array.data if same_device_dtype else convert_data\n    elif isinstance(array, NDArray):\n        # Create from NDArray\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n    else:\n        # Create from Python object\n        device = device if device else default_device()\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n\n    self.init([], data=data, requires_grad=requires_grad)\n</code></pre>"},{"location":"core-components/autograd_en/#2-backward-propagation","title":"2. Backward Propagation","text":"Python<pre><code>def backward(self, out_grad=None):\n    # Set output gradient\n    out_grad = out_grad if out_grad else init.ones(*self.shape, dtype=self.dtype, device=self.device)\n\n    # Initialize gradient accumulation dictionary\n    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}\n    node_to_output_grads_list[self] = [out_grad]\n\n    # Get computation order through topological sorting\n    topo_order = topo_sort(self)\n\n    # Traverse in reverse topological order to compute gradients\n    for node in reversed(topo_order):\n        if not node.requires_grad:\n            continue\n\n        # Accumulate gradients for current node\n        if node.grad is None:\n            node.grad = reduce(operator.add, node_to_output_grads_list[node])\n            # Ensure gradient contiguity (solve broadcast tensor issues)\n            if hasattr(node.grad, 'data') and hasattr(node.grad.data, 'data'):\n                cuda_tensor = node.grad.data.data\n                if hasattr(cuda_tensor, 'is_contiguous') and not cuda_tensor.is_contiguous():\n                    node.grad.data.data = cuda_tensor.contiguous()\n        else:\n            node.grad += reduce(operator.add, node_to_output_grads_list[node])\n\n        # Apply gradient hooks\n        node.apply_hooks(node.grad)\n\n        # Compute gradients for input nodes\n        if node.creator is not None:\n            # Handle mixed precision\n            grad = node.grad.half() if check_dtype(node.creator.ctx.saved_tensors, genesis.float16) else node.grad\n\n            # Call backward propagation of corresponding operation\n            if node.creator.is_tuple_result:\n                backward_grad = node.creator.backward(node.creator.ctx, grad, node.idx)\n            else:\n                backward_grad = node.creator.backward(node.creator.ctx, grad)\n\n            # Distribute gradients to input nodes\n            for i, input_node in enumerate(node.creator.inputs):\n                if input_node.requires_grad:\n                    if input_node not in node_to_output_grads_list:\n                        node_to_output_grads_list[input_node] = []\n                    node_to_output_grads_list[input_node].append(backward_grad[i].float())\n</code></pre>"},{"location":"core-components/autograd_en/#3-topological-sort","title":"3. Topological Sort","text":"Python<pre><code>def topo_sort(node):\n    \"\"\"Depth-first search implementation for topological sorting\"\"\"\n    visited = set()\n    topo_order = []\n\n    def dfs(n):\n        if n in visited:\n            return\n        visited.add(n)\n\n        # Recursively visit input nodes\n        if n.creator is not None:\n            for input_node in n.creator.inputs:\n                if isinstance(input_node, Tensor):\n                    dfs(input_node)\n\n        topo_order.append(n)\n\n    dfs(node)\n    return topo_order\n</code></pre>"},{"location":"core-components/autograd_en/#function-base-class","title":"\u2699\ufe0f Function Base Class","text":"<p>Function is the base class for all differentiable operations, defining the interface for forward and backward propagation.</p>"},{"location":"core-components/autograd_en/#basic-structure","title":"Basic Structure","text":"Python<pre><code>class Function:\n    @staticmethod\n    def forward(ctx: Context, *args) -&gt; Union[Tensor, Tuple[Tensor, ...]]:\n        \"\"\"Forward propagation implementation\"\"\"\n        raise NotImplementedError\n\n    @staticmethod  \n    def backward(ctx: Context, grad_output, out_idx=None) -&gt; Tuple[Tensor, ...]:\n        \"\"\"Backward propagation implementation\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def apply(cls, *args, **kwargs):\n        \"\"\"Unified call interface\"\"\"\n        # Handle mixed precision\n        instance = cls()\n        instance.ctx = Context()\n\n        # Execute forward propagation\n        if genesis.enable_autocast:\n            result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n        else:\n            result = cls.forward(instance.ctx, *args, **kwargs)\n\n        # Set computation graph connections\n        instance.is_tuple_result = isinstance(result, tuple)\n\n        if instance.is_tuple_result:\n            for idx, res in enumerate(result):\n                if isinstance(res, Tensor) and res.requires_grad:\n                    res.set_creator(instance, idx)\n        elif isinstance(result, Tensor) and result.requires_grad:\n            result.set_creator(instance)\n\n        # Record input tensors\n        instance.inputs = []\n        for t in args:\n            if isinstance(t, Tensor):\n                instance.inputs.append(t)\n            elif isinstance(t, list) and all(isinstance(item, Tensor) for item in t):\n                instance.inputs.extend(t)\n\n        return result\n</code></pre>"},{"location":"core-components/autograd_en/#practical-operation-examples","title":"Practical Operation Examples","text":""},{"location":"core-components/autograd_en/#matrix-multiplication","title":"Matrix Multiplication","text":"Python<pre><code>class MatMul(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        # Save inputs for backward propagation\n        ctx.save_for_backward(a, b)\n        return a @ b  # Call underlying NDArray matrix multiplication\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        a, b = ctx.saved_tensors\n        # Compute input gradients\n        grad_a = grad_output @ b.T\n        grad_b = a.T @ grad_output\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd_en/#addition-with-broadcasting-support","title":"Addition (with Broadcasting Support)","text":"Python<pre><code>class Add(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        ctx.a_shape = a.shape\n        ctx.b_shape = b.shape\n        return a + b\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Handle broadcasting gradients\n        grad_a = grad_output\n        grad_b = grad_output\n\n        # Sum over broadcasted dimensions\n        for i, (da, db) in enumerate(zip(ctx.a_shape, ctx.b_shape)):\n            if da == 1 and db &gt; 1:\n                grad_a = grad_a.sum(axis=i, keepdims=True)\n            elif db == 1 and da &gt; 1:\n                grad_b = grad_b.sum(axis=i, keepdims=True)\n\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd_en/#context-class","title":"\ud83d\udcdd Context Class","text":"<p>Context class is used to pass information between forward and backward propagation.</p> Python<pre><code>class Context:\n    def __init__(self):\n        self.saved_tensors = []\n\n    def save_for_backward(self, *tensors):\n        \"\"\"Save tensors for backward propagation\"\"\"\n        self.saved_tensors.extend(tensors)\n\n    @property\n    def saved_tensors(self):\n        return self._saved_tensors\n\n    @saved_tensors.setter  \n    def saved_tensors(self, tensors):\n        self._saved_tensors = tensors\n</code></pre>"},{"location":"core-components/autograd_en/#mixed-precision-support","title":"\ud83d\udd04 Mixed Precision Support","text":"<p>The automatic differentiation system has built-in mixed precision training support:</p> Python<pre><code># Global switch\ngenesis.enable_autocast = True\n\n# Automatic type conversion\ndef _cast(value, dtype):\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# Applied in Function.apply\nif genesis.enable_autocast:\n    result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/autograd_en/#gradient-hook-system","title":"\ud83e\ude9d Gradient Hook System","text":"<p>Support for executing custom functions during gradient computation:</p> Python<pre><code>class Tensor:\n    def register_hook(self, hook):\n        \"\"\"Register gradient hook\"\"\"\n        self.hooks.append(hook)\n\n    def apply_hooks(self, grad):\n        \"\"\"Apply all hooks\"\"\"\n        for hook in self.hooks:\n            hook(grad)\n\n# Usage example\ndef grad_clipping_hook(grad):\n    \"\"\"Gradient clipping hook\"\"\"\n    grad.clamp_(-1.0, 1.0)\n\ntensor.register_hook(grad_clipping_hook)\n</code></pre>"},{"location":"core-components/autograd_en/#performance-optimization","title":"\ud83d\ude80 Performance Optimization","text":""},{"location":"core-components/autograd_en/#1-memory-management-optimization","title":"1. Memory Management Optimization","text":"<ul> <li>View Operations: reshape, transpose and other operations create views instead of copying data</li> <li>In-place Operations: Support for in-place update operations like <code>+=</code>, <code>*=</code></li> <li>Gradient Accumulation Optimization: Smart gradient accumulation strategy</li> </ul>"},{"location":"core-components/autograd_en/#2-computation-graph-optimization","title":"2. Computation Graph Optimization","text":"<ul> <li>Lazy Construction: Build computation graph only when gradients are needed</li> <li>Smart Release: Automatically release intermediate results that are no longer needed</li> <li>Topological Sort Caching: Cache topological sort results</li> </ul>"},{"location":"core-components/autograd_en/#3-cross-device-optimization","title":"3. Cross-device Optimization","text":"<ul> <li>Automatic Device Inference: Automatically select appropriate computing device</li> <li>Asynchronous Execution: Support for GPU asynchronous computation</li> <li>Memory Pre-allocation: Reduce dynamic memory allocation</li> </ul>"},{"location":"core-components/autograd_en/#usage-examples","title":"\ud83c\udfaf Usage Examples","text":""},{"location":"core-components/autograd_en/#basic-usage","title":"Basic Usage","text":"Python<pre><code>import genesis\n\n# Create tensors that require gradients\nx = genesis.randn(3, 4, requires_grad=True)\ny = genesis.randn(4, 2, requires_grad=True)\n\n# Forward propagation (automatically build computation graph)\nz = x @ y\nloss = z.sum()\n\n# Backward propagation (compute all gradients)\nloss.backward()\n\nprint(f\"Gradient of x: {x.grad}\")  # Output gradient of x\nprint(f\"Gradient of y: {y.grad}\")  # Output gradient of y\n</code></pre>"},{"location":"core-components/autograd_en/#custom-operations","title":"Custom Operations","text":"Python<pre><code>class CustomFunction(genesis.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_tensor):\n        # Custom forward computation\n        ctx.save_for_backward(input_tensor)\n        result = input_tensor ** 2 + 2 * input_tensor + 1\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_tensor, = ctx.saved_tensors\n        # Custom gradient computation: d/dx(x^2 + 2x + 1) = 2x + 2\n        grad_input = grad_output * (2 * input_tensor + 2)\n        return grad_input\n\n# Use custom operation\nx = genesis.randn(3, 4, requires_grad=True)\ny = CustomFunction.apply(x)\ny.sum().backward()\n</code></pre>"},{"location":"core-components/autograd_en/#gradient-hooks","title":"Gradient Hooks","text":"Python<pre><code># Gradient monitoring hook\ndef monitor_grad(grad):\n    print(f\"Gradient stats: mean={grad.mean():.4f}, std={grad.std():.4f}\")\n\n# Gradient clipping hook\ndef clip_grad(grad):\n    grad.data.clamp_(-1.0, 1.0)\n\nx = genesis.randn(10, requires_grad=True)\nx.register_hook(monitor_grad)\nx.register_hook(clip_grad)\n\n# Perform some computation\ny = (x ** 3).sum()\ny.backward()  # Will trigger hook functions\n</code></pre> <p>Genesis's automatic differentiation system is designed to be simple yet powerful, providing a reliable foundation for gradient computation in deep learning while maintaining good performance and scalability.</p>"},{"location":"core-components/cuda-memory.zh/","title":"CUDA\u5185\u5b58\u7ba1\u7406","text":"<p>Genesis\u5305\u542b\u4e86\u4e00\u4e2a\u5148\u8fdb\u7684\u9ad8\u6027\u80fdCUDA\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6bb5-\u5757\u5206\u914d\u5668\u67b6\u6784\u548c\u5148\u8fdb\u7684\u7f13\u5b58\u7b56\u7565\u63d0\u4f9b\u9ad8\u6548\u7684GPU\u5185\u5b58\u5206\u914d\u3002</p>"},{"location":"core-components/cuda-memory.zh/#_1","title":"\u6982\u8ff0","text":"<p>CUDA\u5185\u5b58\u7ba1\u7406\u5668\u662f\u4e00\u4e2a\u751f\u4ea7\u7ea7\u5185\u5b58\u5206\u914d\u5668\uff0c\u76f8\u6bd4\u7b80\u5355\u7684\u5206\u914d\u7b56\u7565\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5b83\u5177\u6709\u4e24\u7ea7\u7f13\u5b58\u7cfb\u7edf\u3001\u6bb5-\u5757\u5206\u914d\u548c\u5168\u9762\u7684\u6027\u80fd\u76d1\u63a7\u529f\u80fd\u3002</p>"},{"location":"core-components/cuda-memory.zh/#_2","title":"\u67b6\u6784","text":""},{"location":"core-components/cuda-memory.zh/#_3","title":"\u6838\u5fc3\u7ec4\u4ef6","text":""},{"location":"core-components/cuda-memory.zh/#cuda_1","title":"CUDA\u5185\u5b58\u7ba1\u7406\u5668","text":"<p>\u5177\u6709\u4f01\u4e1a\u7ea7\u7279\u6027\u7684\u4e3b\u8981\u5185\u5b58\u7ba1\u7406\u5668\u7c7b\uff1a - \u6bb5-\u5757\u5206\u914d\u5668\uff1a\u5206\u5c42\u5185\u5b58\u7ec4\u7ec7\u4ee5\u5b9e\u73b0\u9ad8\u6548\u5206\u914d - \u4e24\u7ea7\u7f13\u5b58\uff1a\u6d41\u672c\u5730\u7f13\u5b58 + \u5168\u5c40\u7f13\u5b58\u4ee5\u5b9e\u73b0\u6700\u5927\u6027\u80fd - \u9884\u70ed\u7f13\u5b58\uff1a\u5e38\u89c1\u5206\u914d\u6a21\u5f0f\u7684\u9884\u5206\u914d\u7b56\u7565 - \u6027\u80fd\u76d1\u63a7\uff1a\u8be6\u7ec6\u7684\u7edf\u8ba1\u548c\u57fa\u51c6\u6d4b\u8bd5\u529f\u80fd - \u6df7\u5408\u5206\u914d\u7b56\u7565\uff1a\u9488\u5bf9\u5c0f\u578b\u4e0e\u5927\u578b\u5206\u914d\u7684\u4f18\u5316\u8def\u5f84</p>"},{"location":"core-components/cuda-memory.zh/#-","title":"\u6bb5-\u5757\u67b6\u6784","text":"Python<pre><code>@dataclass\nclass Block:\n    \"\"\"\n    \u6bb5\u5185\u7684\u5355\u4e2a\u5185\u5b58\u5757\n    \"\"\"\n    ptr: int          # GPU\u6307\u9488\n    size: int         # \u5757\u5927\u5c0f  \n    is_free: bool     # \u53ef\u7528\u72b6\u6001\n    segment_id: int   # \u7236\u6bb5ID\n\nclass Segment:\n    \"\"\"\n    \u5305\u542b\u591a\u4e2a\u5757\u7684\u5927\u578b\u8fde\u7eed\u5185\u5b58\u533a\u57df\n    \"\"\"\n    def __init__(self, segment_id: int, size: int):\n        # \u4eceCUDA\u5206\u914d\u6574\u4e2a\u6bb5\n        self.base_ptr = _ok(cuda.cuMemAlloc(size))\n\n        # \u5c06\u5185\u5b58\u521d\u59cb\u5316\u4e3a\u96f6\uff08\u9632\u6b62\u810f\u6570\u636e\u7cbe\u5ea6\u95ee\u9898\uff09\n        _ok(cuda.cuMemsetD8(self.base_ptr, 0, size))\n\n        # \u5f00\u59cb\u4f5c\u4e3a\u5355\u4e2a\u5927\u7684\u7a7a\u95f2\u5757\n        self.blocks: List[Block] = [...]\n        self.free_blocks_by_size: Dict[int, List[Block]] = {...}\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_4","title":"\u5173\u952e\u7279\u6027","text":""},{"location":"core-components/cuda-memory.zh/#1-","title":"1. \u9ad8\u6027\u80fd\u6bb5-\u5757\u5206\u914d","text":"<ul> <li>\u6700\u4f73\u9002\u914d\u7b97\u6cd5\uff1a\u627e\u5230\u6700\u4f18\u5757\u5927\u5c0f\u4ee5\u6700\u5c0f\u5316\u788e\u7247</li> <li>\u5757\u5206\u5272\uff1a\u5927\u5757\u81ea\u52a8\u5206\u5272\u4ee5\u6ee1\u8db3\u7cbe\u786e\u5927\u5c0f\u8bf7\u6c42</li> <li>\u5757\u5408\u5e76\uff1a\u76f8\u90bb\u7a7a\u95f2\u5757\u5408\u5e76\u4ee5\u9632\u6b62\u788e\u7247</li> <li>\u57fa\u4e8e\u5927\u5c0f\u7684\u7d22\u5f15\uff1a\u6309\u5927\u5c0fO(1)\u67e5\u627e\u7a7a\u95f2\u5757</li> </ul>"},{"location":"core-components/cuda-memory.zh/#2","title":"2. \u4e24\u7ea7\u7f13\u5b58\u7cfb\u7edf","text":"Python<pre><code>class TwoLevelCache:\n    \"\"\"\n    \u5177\u6709\u6d41\u672c\u5730\u548c\u5168\u5c40\u7ea7\u522b\u7684\u5148\u8fdb\u7f13\u5b58\n    \"\"\"\n    def __init__(self):\n        self.stream_cache: Dict[int, Dict[int, List[int]]] = {}  # stream_id -&gt; size -&gt; [ptrs]\n        self.global_cache: Dict[int, List[int]] = {}             # size -&gt; [ptrs]\n        self.cache_stats = CacheStatistics()\n</code></pre> <p>\u6d41\u672c\u5730\u7f13\u5b58\uff1a - \u9488\u5bf9CUDA\u6d41\u6548\u7387\u7684\u6bcf\u6d41\u5757\u7f13\u5b58 - \u907f\u514d\u8de8\u6d41\u540c\u6b65\u5f00\u9500 - \u5bf9\u91cd\u590d\u5206\u914d\u6a21\u5f0f\u6700\u4f18</p> <p>\u5168\u5c40\u7f13\u5b58\uff1a - \u6240\u6709\u6d41\u4e4b\u95f4\u7684\u5171\u4eab\u7f13\u5b58 - \u6d41\u672c\u5730\u7f13\u5b58\u672a\u547d\u4e2d\u65f6\u7684\u56de\u9000 - \u6700\u5927\u5316\u8de8\u64cd\u4f5c\u7684\u5185\u5b58\u91cd\u7528</p>"},{"location":"core-components/cuda-memory.zh/#3","title":"3. \u9884\u70ed\u7f13\u5b58\u9884\u5206\u914d","text":"Python<pre><code>def warmup_cache(self, sizes: List[int], counts: List[int]):\n    \"\"\"\n    \u7528\u5e38\u89c1\u5206\u914d\u5927\u5c0f\u9884\u586b\u5145\u7f13\u5b58\n\n    \u9488\u5bf9\u5df2\u77e5\u5206\u914d\u6a21\u5f0f\u7684\u6027\u80fd\u4f18\u5316\uff1a\n    - Transformer\u6ce8\u610f\u529b\u77e9\u9635\n    - \u5d4c\u5165\u67e5\u627e  \n    - \u68af\u5ea6\u7f13\u51b2\u533a\n    \"\"\"\n    for size, count in zip(sizes, counts):\n        for _ in range(count):\n            ptr = self.allocate_segment_block(size)\n            self.add_to_cache(ptr, size)\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#4","title":"4. \u81ea\u9002\u5e94\u5206\u914d\u7b56\u7565","text":"Python<pre><code>def allocate_memory(self, size: int) -&gt; int:\n    \"\"\"\n    \u9488\u5bf9\u4e0d\u540c\u5927\u5c0f\u8303\u56f4\u4f18\u5316\u7684\u6df7\u5408\u5206\u914d\u7b56\u7565\n    \"\"\"\n    if size &lt; self.SMALL_BLOCK_THRESHOLD:\n        # \u5c0f\u5206\u914d\uff1a\u4f18\u5148\u7f13\u5b58\u547d\u4e2d\n        return self.allocate_from_cache(size) or self.allocate_segment_block(size)\n    else:\n        # \u5927\u5206\u914d\uff1a\u76f4\u63a5\u6bb5\u5206\u914d\n        return self.allocate_large_block(size)\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_5","title":"\u6027\u80fd\u7279\u5f81","text":""},{"location":"core-components/cuda-memory.zh/#vs-pytorch","title":"\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff08vs PyTorch\uff09","text":"\u573a\u666f Genesis\u6027\u80fd \u72b6\u6001 \u76f8\u540c\u5927\u5c0f\u5206\u914d 1.43\u500d\u66f4\u5feb \u2705 \u4f18\u79c0 \u5927\u5185\u5b58(&gt;1MB) 3.92\u500d\u66f4\u5feb \u2705\u6770\u51fa Transformer\u8bad\u7ec3 1.89\u500d\u66f4\u5feb \u2705 \u4f18\u79c0 \u5185\u5b58\u538b\u529b 4.83\u500d\u66f4\u5feb \u2705 \u6770\u51fa \u53d8\u5316\u5927\u5c0f 0.83\u500d\uff08\u66f4\u6162\uff09 \ud83d\udd04 \u4f18\u5316\u76ee\u6807"},{"location":"core-components/cuda-memory.zh/#_6","title":"\u5185\u5b58\u6548\u7387\u6539\u8fdb","text":"<ol> <li> <p>\u6d88\u9664cudaMalloc/cudaFree\u5f00\u9500\uff1a    Python<pre><code># \u4e4b\u524d\uff1a\u76f4\u63a5CUDA\u8c03\u7528\uff08\u6162\uff09\nptr = cuda.cuMemAlloc(size)  # ~100\u03bcs \u5f00\u9500\n\n# \u4e4b\u540e\uff1a\u57fa\u4e8e\u7f13\u5b58\u7684\u5206\u914d\uff08\u5feb\uff09\nptr = cache.get(size) or segment.allocate(size)  # ~1\u03bcs \u5f00\u9500\n</code></pre></p> </li> <li> <p>\u51cf\u5c11\u5185\u5b58\u788e\u7247\uff1a</p> </li> <li>\u5757\u5408\u5e76\u9632\u6b62\u788e\u7247</li> <li>\u6700\u4f73\u9002\u914d\u5206\u914d\u6700\u5c0f\u5316\u6d6a\u8d39</li> <li> <p>\u6bb5\u7ec4\u7ec7\u6539\u5584\u5c40\u90e8\u6027</p> </li> <li> <p>\u9488\u5bf9ML\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\uff1a</p> </li> <li>\u5e38\u89c1\u5f20\u91cf\u5927\u5c0f\u7684\u9884\u70ed\u7f13\u5b58</li> <li>\u5e76\u884c\u64cd\u4f5c\u7684\u6d41\u611f\u77e5\u5206\u914d</li> <li>\u591a\u5f20\u91cf\u64cd\u4f5c\u7684\u6279\u91cf\u5206\u914d\u652f\u6301</li> </ol>"},{"location":"core-components/cuda-memory.zh/#_7","title":"\u9ad8\u7ea7\u7279\u6027","text":""},{"location":"core-components/cuda-memory.zh/#1","title":"1. \u6027\u80fd\u76d1\u63a7","text":"Python<pre><code>@dataclass\nclass AllocationStatistics:\n    \"\"\"\u5168\u9762\u7684\u5206\u914d\u8ddf\u8e2a\"\"\"\n    total_allocations: int = 0\n    total_freed: int = 0\n    peak_memory_usage: int = 0\n    cache_hits: int = 0\n    cache_misses: int = 0\n    fragmentation_ratio: float = 0.0\n\n    def efficiency_score(self) -&gt; float:\n        \"\"\"\u8ba1\u7b97\u5185\u5b58\u7ba1\u7406\u5668\u6548\u7387\uff080-1\uff09\"\"\"\n        if self.total_allocations == 0:\n            return 1.0\n        return self.cache_hits / self.total_allocations\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#2_1","title":"2. \u5185\u5b58\u6c60\u4f18\u5316","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"\n    \u9ad8\u541e\u5410\u91cf\u573a\u666f\u7684\u5f02\u6b65\u5185\u5b58\u6c60\n    \"\"\"\n    def __init__(self, pool_size: int = 1024 * 1024 * 1024):  # \u9ed8\u8ba41GB\n        self.pool = MemoryPool(pool_size)\n        self.allocation_queue = AsyncQueue()\n        self.background_worker = Thread(target=self._allocation_worker)\n\n    def allocate_async(self, size: int) -&gt; Future[int]:\n        \"\"\"\u7ba1\u9053\u5e76\u884c\u7684\u975e\u963b\u585e\u5206\u914d\"\"\"\n        return self.allocation_queue.submit(self._allocate, size)\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#3_1","title":"3. \u6279\u91cf\u5206\u914d\u652f\u6301","text":"Python<pre><code>def allocate_batch(self, sizes: List[int]) -&gt; List[int]:\n    \"\"\"\n    \u591a\u5f20\u91cf\u64cd\u4f5c\u7684\u4f18\u5316\u6279\u91cf\u5206\u914d\n\n    \u4f18\u52bf\uff1a\n    - \u51cf\u5c11\u5206\u914d\u5f00\u9500\n    - \u66f4\u597d\u7684\u5185\u5b58\u5c40\u90e8\u6027  \n    - \u81ea\u52a8\u5927\u5c0f\u4f18\u5316\n    \"\"\"\n    # \u6309\u76f8\u4f3c\u5927\u5c0f\u5206\u7ec4\u4ee5\u9ad8\u6548\u4f7f\u7528\u6bb5\n    size_groups = self._group_by_size(sizes)\n\n    ptrs = []\n    for size_group in size_groups:\n        segment = self._find_or_create_segment(size_group.total_size)\n        group_ptrs = segment.allocate_batch(size_group.sizes)\n        ptrs.extend(group_ptrs)\n\n    return ptrs\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_8","title":"\u5185\u5b58\u7ba1\u7406\u6a21\u5f0f","text":""},{"location":"core-components/cuda-memory.zh/#1-transformer","title":"1. Transformer\u8bad\u7ec3\u4f18\u5316","text":"Python<pre><code># Transformer\u8bad\u7ec3\u7684\u4f18\u5316\u5185\u5b58\u5206\u914d\ndef allocate_transformer_tensors(batch_size: int, seq_len: int, hidden_size: int):\n    \"\"\"\n    \u9884\u5206\u914d\u5e38\u89c1\u7684transformer\u5f20\u91cf\u5927\u5c0f\n    \"\"\"\n    common_sizes = [\n        batch_size * seq_len * hidden_size,      # \u6ce8\u610f\u529b\u6743\u91cd\n        batch_size * seq_len * hidden_size * 4,  # \u524d\u9988\n        batch_size * seq_len * seq_len,          # \u6ce8\u610f\u529b\u5206\u6570\n    ]\n\n    # \u7528\u9884\u671f\u5206\u914d\u6a21\u5f0f\u9884\u70ed\u7f13\u5b58\n    memory_manager.warmup_cache(common_sizes, counts=[10, 5, 10])\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#2_2","title":"2. \u52a8\u6001\u5185\u5b58\u7f29\u653e","text":"Python<pre><code>def adaptive_memory_management(memory_pressure: float):\n    \"\"\"\n    \u6839\u636e\u5185\u5b58\u538b\u529b\u81ea\u52a8\u8c03\u6574\u7f13\u5b58\u5927\u5c0f\n    \"\"\"\n    if memory_pressure &gt; 0.8:\n        # \u9ad8\u538b\u529b\uff1a\u6fc0\u8fdb\u7684\u7f13\u5b58\u6e05\u7406\n        memory_manager.cleanup_cache(threshold=0.9)\n        memory_manager.enable_aggressive_coalescing()\n    elif memory_pressure &lt; 0.3:\n        # \u4f4e\u538b\u529b\uff1a\u6269\u5c55\u7f13\u5b58\u4ee5\u83b7\u5f97\u66f4\u597d\u6027\u80fd\n        memory_manager.expand_cache_size(factor=1.5)\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"core-components/cuda-memory.zh/#_10","title":"\u57fa\u7840\u5206\u914d","text":"Python<pre><code>from genesis.ndarray.cuda_memory_manager import get_memory_manager\n\n# \u83b7\u53d6\u5168\u5c40\u5185\u5b58\u7ba1\u7406\u5668\u5b9e\u4f8b\nmm = get_memory_manager()\n\n# \u5206\u914dGPU\u5185\u5b58\nptr = mm.allocate_memory(1024 * 1024)  # 1MB\n\n# \u91ca\u653e\u5185\u5b58\uff08\u81ea\u52a8\u7f13\u5b58\uff09\nmm.free_memory(ptr, 1024 * 1024)\n\n# \u68c0\u67e5\u7edf\u8ba1\nstats = mm.get_statistics()\nprint(f\"\u7f13\u5b58\u547d\u4e2d\u7387: {stats.cache_hit_rate:.2%}\")\nprint(f\"\u5185\u5b58\u6548\u7387: {stats.efficiency_score():.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_11","title":"\u9ad8\u7ea7\u914d\u7f6e","text":"Python<pre><code># \u4e3a\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u5185\u5b58\u7ba1\u7406\u5668\nmm.configure(\n    segment_size=512 * 1024 * 1024,    # 512MB\u6bb5\n    cache_sizes={\n        'stream_local': 100,            # \u6bcf\u6d41100\u4e2a\u5757\n        'global': 500,                  # \u5168\u5c40\u7f13\u5b58500\u4e2a\u5757\n    },\n    warmup_sizes=[\n        (4096, 50),    # 50\u4e2a4KB\u5757\n        (65536, 20),   # 20\u4e2a64KB\u5757  \n        (1048576, 10), # 10\u4e2a1MB\u5757\n    ]\n)\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_12","title":"\u6027\u80fd\u76d1\u63a7","text":"Python<pre><code># \u542f\u7528\u8be6\u7ec6\u6027\u80fd\u8ddf\u8e2a\nwith mm.performance_context() as perf:\n    # \u8fd0\u884c\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\n    tensors = [genesis.randn(1000, 1000) for _ in range(100)]\n\n# \u5206\u6790\u6027\u80fd\nprint(f\"\u603b\u5206\u914d\u6570: {perf.stats.total_allocations}\")\nprint(f\"\u5cf0\u503c\u5185\u5b58: {perf.stats.peak_memory_usage / 1024**3:.2f} GB\")\nprint(f\"\u788e\u7247\u5316: {perf.stats.fragmentation_ratio:.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_13","title":"\u914d\u7f6e\u548c\u8c03\u4f18","text":""},{"location":"core-components/cuda-memory.zh/#_14","title":"\u73af\u5883\u53d8\u91cf","text":"Bash<pre><code># \u5185\u5b58\u7ba1\u7406\u5668\u914d\u7f6e\nexport GENESIS_CUDA_SEGMENT_SIZE=1073741824     # 1GB\u6bb5\nexport GENESIS_CUDA_CACHE_SIZE=1000             # \u7f13\u5b581000\u4e2a\u5757\nexport GENESIS_CUDA_WARMUP_ENABLED=true         # \u542f\u7528\u9884\u70ed\nexport GENESIS_CUDA_STATS_ENABLED=true          # \u542f\u7528\u7edf\u8ba1\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_15","title":"\u8fd0\u884c\u65f6\u914d\u7f6e","text":"Python<pre><code># \u8fd0\u884c\u65f6\u914d\u7f6e\ngenesis.cuda.configure_memory_manager({\n    'segment_size': 1024 * 1024 * 1024,  # 1GB\n    'enable_warmup': True,\n    'enable_stats': True,\n    'allocation_strategy': 'best_fit',\n    'coalescing_threshold': 0.1,\n})\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_16","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4f7f\u7528\u9884\u70ed\u7f13\u5b58\uff1a\u9884\u5206\u914d\u5e38\u89c1\u5927\u5c0f\u4ee5\u83b7\u5f9738\u500d\u6027\u80fd\u63d0\u5347</li> <li>\u76d1\u63a7\u7edf\u8ba1\uff1a\u8ddf\u8e2a\u7f13\u5b58\u547d\u4e2d\u7387\u548c\u5185\u5b58\u6548\u7387</li> <li>\u6279\u91cf\u5206\u914d\uff1a\u5c06\u76f8\u4f3c\u64cd\u4f5c\u5206\u7ec4\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u5c40\u90e8\u6027</li> <li>\u907f\u514d\u9891\u7e41\u7684\u5c0f\u5206\u914d\uff1a\u5bf9\u4e8e\u5fae\u5c0f\u5757\uff0c\u7f13\u5b58\u5f00\u9500\u5360\u4e3b\u5bfc</li> <li>\u4f7f\u7528\u9002\u5f53\u7684\u6bb5\u5927\u5c0f\uff1a\u5c06\u6bb5\u5927\u5c0f\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u5185\u5b58\u6a21\u5f0f\u5339\u914d</li> </ol>"},{"location":"core-components/cuda-memory.zh/#_17","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"core-components/cuda-memory.zh/#_18","title":"\u5185\u5b58\u6cc4\u6f0f","text":"Python<pre><code># \u8c03\u8bd5\u5185\u5b58\u6cc4\u6f0f\nstats = mm.get_statistics()\nif stats.total_allocations &gt; stats.total_freed + 1000:\n    print(\"\u8b66\u544a\uff1a\u68c0\u6d4b\u5230\u6f5c\u5728\u5185\u5b58\u6cc4\u6f0f\")\n    mm.dump_allocation_trace()\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_19","title":"\u6027\u80fd\u95ee\u9898","text":"Python<pre><code># \u8bca\u65ad\u6027\u80fd\u95ee\u9898\nif stats.cache_hit_rate &lt; 0.5:\n    print(\"\u7f13\u5b58\u547d\u4e2d\u7387\u4f4e - \u8003\u8651\u9884\u70ed\u7f13\u5b58\")\n    mm.analyze_allocation_patterns()\n\nif stats.fragmentation_ratio &gt; 0.3:\n    print(\"\u9ad8\u788e\u7247\u5316 - \u542f\u7528\u6fc0\u8fdb\u5408\u5e76\")\n    mm.enable_aggressive_coalescing()\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#_20","title":"\u5185\u5b58\u538b\u529b","text":"Python<pre><code># \u5904\u7406\u5185\u5b58\u538b\u529b\ndef handle_oom():\n    \"\"\"\u5185\u5b58\u4e0d\u8db3\u5904\u7406\u7a0b\u5e8f\"\"\"\n    mm.cleanup_cache(force=True)\n    mm.coalesce_free_blocks()\n    mm.garbage_collect()\n</code></pre>"},{"location":"core-components/cuda-memory.zh/#genesis","title":"\u4e0eGenesis\u7684\u96c6\u6210","text":"<p>\u5185\u5b58\u7ba1\u7406\u5668\u4e0eGenesis\u5f20\u91cf\u548c\u64cd\u4f5c\u65e0\u7f1d\u96c6\u6210\uff1a</p> Python<pre><code># \u4e0e\u5f20\u91cf\u64cd\u4f5c\u7684\u81ea\u52a8\u96c6\u6210\nx = genesis.randn(1000, 1000)  # \u81ea\u52a8\u4f7f\u7528\u5185\u5b58\u7ba1\u7406\u5668\ny = genesis.matmul(x, x)       # \u9ad8\u6548\u5185\u5b58\u91cd\u7528\nz = x + y                      # \u7f13\u5b58\u4f18\u5316\u5206\u914d\n</code></pre> <p>\u8fd9\u4e2a\u5148\u8fdb\u7684\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\u662fGenesis\u5728\u4fdd\u6301\u4ece\u96f6\u5f00\u59cb\u5b9e\u73b0\u7684\u6559\u80b2\u6e05\u6670\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1PyTorch\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002</p>"},{"location":"core-components/cuda-memory_en/","title":"CUDA Memory Management","text":"<p>Genesis includes a sophisticated high-performance CUDA memory management system that provides efficient GPU memory allocation through a segment-block allocator architecture with advanced caching strategies.</p>"},{"location":"core-components/cuda-memory_en/#overview","title":"Overview","text":"<p>The CUDA Memory Manager is a production-grade memory allocator that achieves significant performance improvements over naive allocation strategies. It features a two-level caching system, segment-block allocation, and comprehensive performance monitoring.</p>"},{"location":"core-components/cuda-memory_en/#architecture","title":"Architecture","text":""},{"location":"core-components/cuda-memory_en/#core-components","title":"Core Components","text":""},{"location":"core-components/cuda-memory_en/#cudamemorymanager","title":"CUDAMemoryManager","text":"<p>The main memory manager class with enterprise-grade features: - Segment-Block Allocator: Hierarchical memory organization for efficient allocation - Two-Level Caching: Stream-local cache + global cache for maximum performance - Warmup Cache: Pre-allocation strategy for common allocation patterns - Performance Monitoring: Detailed statistics and benchmarking capabilities - Hybrid Allocation Strategy: Optimized paths for small vs large allocations</p>"},{"location":"core-components/cuda-memory_en/#segment-block-architecture","title":"Segment-Block Architecture","text":"Python<pre><code>@dataclass\nclass Block:\n    \"\"\"\n    Individual memory block within a segment\n    \"\"\"\n    ptr: int          # GPU pointer\n    size: int         # Block size  \n    is_free: bool     # Availability status\n    segment_id: int   # Parent segment ID\n\nclass Segment:\n    \"\"\"\n    Large contiguous memory region containing multiple blocks\n    \"\"\"\n    def __init__(self, segment_id: int, size: int):\n        # Allocate entire segment from CUDA\n        self.base_ptr = _ok(cuda.cuMemAlloc(size))\n\n        # Initialize memory to zero (prevents dirty data precision issues)\n        _ok(cuda.cuMemsetD8(self.base_ptr, 0, size))\n\n        # Start as single large free block\n        self.blocks: List[Block] = [...]\n        self.free_blocks_by_size: Dict[int, List[Block]] = {...}\n</code></pre>"},{"location":"core-components/cuda-memory_en/#key-features","title":"Key Features","text":""},{"location":"core-components/cuda-memory_en/#1-high-performance-segment-block-allocation","title":"1. High-Performance Segment-Block Allocation","text":"<ul> <li>Best-Fit Algorithm: Finds optimal block size to minimize fragmentation</li> <li>Block Splitting: Large blocks automatically split for exact size requests</li> <li>Block Coalescing: Adjacent free blocks merged to prevent fragmentation</li> <li>Size-Based Indexing: O(1) lookup for free blocks by size</li> </ul>"},{"location":"core-components/cuda-memory_en/#2-two-level-caching-system","title":"2. Two-Level Caching System","text":"Python<pre><code>class TwoLevelCache:\n    \"\"\"\n    Sophisticated caching with stream-local and global levels\n    \"\"\"\n    def __init__(self):\n        self.stream_cache: Dict[int, Dict[int, List[int]]] = {}  # stream_id -&gt; size -&gt; [ptrs]\n        self.global_cache: Dict[int, List[int]] = {}             # size -&gt; [ptrs]\n        self.cache_stats = CacheStatistics()\n</code></pre> <p>Stream-Local Cache: - Per-stream block caching for CUDA stream efficiency - Avoids cross-stream synchronization overhead - Optimal for repetitive allocation patterns</p> <p>Global Cache: - Shared cache across all streams - Fallback when stream-local cache misses - Maximizes memory reuse across operations</p>"},{"location":"core-components/cuda-memory_en/#3-warmup-cache-preallocation","title":"3. Warmup Cache Preallocation","text":"Python<pre><code>def warmup_cache(self, sizes: List[int], counts: List[int]):\n    \"\"\"\n    Pre-populate cache with common allocation sizes\n\n    Performance optimization for known allocation patterns:\n    - Transformer attention matrices\n    - Embedding lookups  \n    - Gradient buffers\n    \"\"\"\n    for size, count in zip(sizes, counts):\n        for _ in range(count):\n            ptr = self.allocate_segment_block(size)\n            self.add_to_cache(ptr, size)\n</code></pre>"},{"location":"core-components/cuda-memory_en/#4-adaptive-allocation-strategy","title":"4. Adaptive Allocation Strategy","text":"Python<pre><code>def allocate_memory(self, size: int) -&gt; int:\n    \"\"\"\n    Hybrid allocation strategy optimized for different size ranges\n    \"\"\"\n    if size &lt; self.SMALL_BLOCK_THRESHOLD:\n        # Small allocations: prioritize cache hits\n        return self.allocate_from_cache(size) or self.allocate_segment_block(size)\n    else:\n        # Large allocations: direct segment allocation\n        return self.allocate_large_block(size)\n</code></pre>"},{"location":"core-components/cuda-memory_en/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"core-components/cuda-memory_en/#benchmark-results-vs-pytorch","title":"Benchmark Results (vs PyTorch)","text":"Scenario Genesis Performance Status Same-size allocations 1.43x faster \u2705 Excellent Large memory (&gt;1MB) 3.92x faster \u2705 Outstanding Transformer training 1.89x faster \u2705 Excellent Memory pressure 4.83x faster \u2705 Outstanding Variable sizes 0.83x (slower) \ud83d\udd04 Optimization target"},{"location":"core-components/cuda-memory_en/#memory-efficiency-improvements","title":"Memory Efficiency Improvements","text":"<ol> <li> <p>Elimination of cudaMalloc/cudaFree overhead:    Python<pre><code># Before: Direct CUDA calls (slow)\nptr = cuda.cuMemAlloc(size)  # ~100\u03bcs overhead\n\n# After: Cache-based allocation (fast)\nptr = cache.get(size) or segment.allocate(size)  # ~1\u03bcs overhead\n</code></pre></p> </li> <li> <p>Reduced memory fragmentation:</p> </li> <li>Block coalescing prevents fragmentation</li> <li>Best-fit allocation minimizes waste</li> <li> <p>Segment organization improves locality</p> </li> <li> <p>Optimized for ML workloads:</p> </li> <li>Warmup cache for common tensor sizes</li> <li>Stream-aware allocation for parallel operations</li> <li>Batch allocation support for multi-tensor operations</li> </ol>"},{"location":"core-components/cuda-memory_en/#advanced-features","title":"Advanced Features","text":""},{"location":"core-components/cuda-memory_en/#1-performance-monitoring","title":"1. Performance Monitoring","text":"Python<pre><code>@dataclass\nclass AllocationStatistics:\n    \"\"\"Comprehensive allocation tracking\"\"\"\n    total_allocations: int = 0\n    total_freed: int = 0\n    peak_memory_usage: int = 0\n    cache_hits: int = 0\n    cache_misses: int = 0\n    fragmentation_ratio: float = 0.0\n\n    def efficiency_score(self) -&gt; float:\n        \"\"\"Calculate memory manager efficiency (0-1)\"\"\"\n        if self.total_allocations == 0:\n            return 1.0\n        return self.cache_hits / self.total_allocations\n</code></pre>"},{"location":"core-components/cuda-memory_en/#2-memory-pool-optimization","title":"2. Memory Pool Optimization","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"\n    Asynchronous memory pool for high-throughput scenarios\n    \"\"\"\n    def __init__(self, pool_size: int = 1024 * 1024 * 1024):  # 1GB default\n        self.pool = MemoryPool(pool_size)\n        self.allocation_queue = AsyncQueue()\n        self.background_worker = Thread(target=self._allocation_worker)\n\n    def allocate_async(self, size: int) -&gt; Future[int]:\n        \"\"\"Non-blocking allocation for pipeline parallelism\"\"\"\n        return self.allocation_queue.submit(self._allocate, size)\n</code></pre>"},{"location":"core-components/cuda-memory_en/#3-batch-allocation-support","title":"3. Batch Allocation Support","text":"Python<pre><code>def allocate_batch(self, sizes: List[int]) -&gt; List[int]:\n    \"\"\"\n    Optimized batch allocation for multi-tensor operations\n\n    Advantages:\n    - Reduced allocation overhead\n    - Better memory locality  \n    - Automatic size optimization\n    \"\"\"\n    # Group similar sizes for efficient segment usage\n    size_groups = self._group_by_size(sizes)\n\n    ptrs = []\n    for size_group in size_groups:\n        segment = self._find_or_create_segment(size_group.total_size)\n        group_ptrs = segment.allocate_batch(size_group.sizes)\n        ptrs.extend(group_ptrs)\n\n    return ptrs\n</code></pre>"},{"location":"core-components/cuda-memory_en/#memory-management-patterns","title":"Memory Management Patterns","text":""},{"location":"core-components/cuda-memory_en/#1-transformer-training-optimization","title":"1. Transformer Training Optimization","text":"Python<pre><code># Optimized memory allocation for transformer training\ndef allocate_transformer_tensors(batch_size: int, seq_len: int, hidden_size: int):\n    \"\"\"\n    Pre-allocate common transformer tensor sizes\n    \"\"\"\n    common_sizes = [\n        batch_size * seq_len * hidden_size,      # Attention weights\n        batch_size * seq_len * hidden_size * 4,  # Feed-forward\n        batch_size * seq_len * seq_len,          # Attention scores\n    ]\n\n    # Warmup cache with expected allocation pattern\n    memory_manager.warmup_cache(common_sizes, counts=[10, 5, 10])\n</code></pre>"},{"location":"core-components/cuda-memory_en/#2-dynamic-memory-scaling","title":"2. Dynamic Memory Scaling","text":"Python<pre><code>def adaptive_memory_management(memory_pressure: float):\n    \"\"\"\n    Automatically adjust cache sizes based on memory pressure\n    \"\"\"\n    if memory_pressure &gt; 0.8:\n        # High pressure: aggressive cache cleanup\n        memory_manager.cleanup_cache(threshold=0.9)\n        memory_manager.enable_aggressive_coalescing()\n    elif memory_pressure &lt; 0.3:\n        # Low pressure: expand cache for better performance\n        memory_manager.expand_cache_size(factor=1.5)\n</code></pre>"},{"location":"core-components/cuda-memory_en/#usage-examples","title":"Usage Examples","text":""},{"location":"core-components/cuda-memory_en/#basic-allocation","title":"Basic Allocation","text":"Python<pre><code>from genesis.ndarray.cuda_memory_manager import get_memory_manager\n\n# Get global memory manager instance\nmm = get_memory_manager()\n\n# Allocate GPU memory\nptr = mm.allocate_memory(1024 * 1024)  # 1MB\n\n# Free memory (automatic caching)\nmm.free_memory(ptr, 1024 * 1024)\n\n# Check statistics\nstats = mm.get_statistics()\nprint(f\"Cache hit rate: {stats.cache_hit_rate:.2%}\")\nprint(f\"Memory efficiency: {stats.efficiency_score():.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory_en/#advanced-configuration","title":"Advanced Configuration","text":"Python<pre><code># Configure memory manager for specific workload\nmm.configure(\n    segment_size=512 * 1024 * 1024,    # 512MB segments\n    cache_sizes={\n        'stream_local': 100,            # 100 blocks per stream\n        'global': 500,                  # 500 blocks global cache\n    },\n    warmup_sizes=[\n        (4096, 50),    # 50 blocks of 4KB\n        (65536, 20),   # 20 blocks of 64KB  \n        (1048576, 10), # 10 blocks of 1MB\n    ]\n)\n</code></pre>"},{"location":"core-components/cuda-memory_en/#performance-monitoring","title":"Performance Monitoring","text":"Python<pre><code># Enable detailed performance tracking\nwith mm.performance_context() as perf:\n    # Run memory-intensive operations\n    tensors = [genesis.randn(1000, 1000) for _ in range(100)]\n\n# Analyze performance\nprint(f\"Total allocations: {perf.stats.total_allocations}\")\nprint(f\"Peak memory: {perf.stats.peak_memory_usage / 1024**3:.2f} GB\")\nprint(f\"Fragmentation: {perf.stats.fragmentation_ratio:.2%}\")\n</code></pre>"},{"location":"core-components/cuda-memory_en/#configuration-and-tuning","title":"Configuration and Tuning","text":""},{"location":"core-components/cuda-memory_en/#environment-variables","title":"Environment Variables","text":"Bash<pre><code># Memory manager configuration\nexport GENESIS_CUDA_SEGMENT_SIZE=1073741824     # 1GB segments\nexport GENESIS_CUDA_CACHE_SIZE=1000             # Cache 1000 blocks\nexport GENESIS_CUDA_WARMUP_ENABLED=true         # Enable warmup\nexport GENESIS_CUDA_STATS_ENABLED=true          # Enable statistics\n</code></pre>"},{"location":"core-components/cuda-memory_en/#runtime-configuration","title":"Runtime Configuration","text":"Python<pre><code># Configure at runtime\ngenesis.cuda.configure_memory_manager({\n    'segment_size': 1024 * 1024 * 1024,  # 1GB\n    'enable_warmup': True,\n    'enable_stats': True,\n    'allocation_strategy': 'best_fit',\n    'coalescing_threshold': 0.1,\n})\n</code></pre>"},{"location":"core-components/cuda-memory_en/#best-practices","title":"Best Practices","text":"<ol> <li>Use Warmup Cache: Pre-allocate common sizes for 38x performance boost</li> <li>Monitor Statistics: Track cache hit rates and memory efficiency</li> <li>Batch Allocations: Group similar operations for better locality</li> <li>Avoid Frequent Small Allocations: Cache overhead dominates for tiny blocks</li> <li>Use Appropriate Segment Sizes: Match segment size to workload memory patterns</li> </ol>"},{"location":"core-components/cuda-memory_en/#troubleshooting","title":"Troubleshooting","text":""},{"location":"core-components/cuda-memory_en/#memory-leaks","title":"Memory Leaks","text":"Python<pre><code># Debug memory leaks\nstats = mm.get_statistics()\nif stats.total_allocations &gt; stats.total_freed + 1000:\n    print(\"Warning: Potential memory leak detected\")\n    mm.dump_allocation_trace()\n</code></pre>"},{"location":"core-components/cuda-memory_en/#performance-issues","title":"Performance Issues","text":"Python<pre><code># Diagnose performance problems\nif stats.cache_hit_rate &lt; 0.5:\n    print(\"Low cache hit rate - consider warmup cache\")\n    mm.analyze_allocation_patterns()\n\nif stats.fragmentation_ratio &gt; 0.3:\n    print(\"High fragmentation - enable aggressive coalescing\")\n    mm.enable_aggressive_coalescing()\n</code></pre>"},{"location":"core-components/cuda-memory_en/#memory-pressure","title":"Memory Pressure","text":"Python<pre><code># Handle memory pressure\ndef handle_oom():\n    \"\"\"Out of memory handler\"\"\"\n    mm.cleanup_cache(force=True)\n    mm.coalesce_free_blocks()\n    mm.garbage_collect()\n</code></pre>"},{"location":"core-components/cuda-memory_en/#integration-with-genesis","title":"Integration with Genesis","text":"<p>The memory manager integrates seamlessly with Genesis tensors and operations:</p> Python<pre><code># Automatic integration with tensor operations\nx = genesis.randn(1000, 1000)  # Uses memory manager automatically\ny = genesis.matmul(x, x)       # Efficient memory reuse\nz = x + y                      # Cache-optimized allocation\n</code></pre> <p>This sophisticated memory management system is a key factor in Genesis achieving near-PyTorch performance while maintaining the educational clarity of a from-scratch implementation.</p>"},{"location":"core-components/cuda-storage.zh/","title":"CUDA\u5b58\u50a8\u7cfb\u7edf","text":"<p>Genesis\u7684CUDA\u5b58\u50a8\uff08CUDAStorage\uff09\u662f\u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u7eafCUDA\u5b9e\u73b0\u7684GPU\u5185\u5b58\u7ba1\u7406\u548c\u64cd\u4f5c\uff0c\u5b8c\u5168\u72ec\u7acb\u4e8ePyTorch\uff0c\u76f4\u63a5\u4f7f\u7528CUDA Python API\u3002</p>"},{"location":"core-components/cuda-storage.zh/#_1","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":""},{"location":"core-components/cuda-storage.zh/#_2","title":"\u72ec\u7acb\u6027","text":"<ul> <li>\u7eafCUDA\u5b9e\u73b0\uff1a\u4e0d\u4f9d\u8d56PyTorch\u7684GPU\u540e\u7aef</li> <li>\u76f4\u63a5\u5185\u5b58\u7ba1\u7406\uff1a\u4f7f\u7528CUDA Python API\u76f4\u63a5\u7ba1\u7406GPU\u5185\u5b58</li> <li>\u9ad8\u6027\u80fd\uff1a\u9488\u5bf9GPU\u4f18\u5316\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f</li> </ul>"},{"location":"core-components/cuda-storage.zh/#_3","title":"\u517c\u5bb9\u6027","text":"<ul> <li>PyTorch\u98ce\u683cAPI\uff1a\u4fdd\u6301\u4e0ePyTorch\u5f20\u91cf\u7684\u63a5\u53e3\u517c\u5bb9\u6027</li> <li>\u81ea\u52a8\u5fae\u5206\u652f\u6301\uff1a\u4e0eGenesis\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u5b8c\u6574\u7684\u7c7b\u578b\u6ce8\u89e3\u548c\u8fd0\u884c\u65f6\u68c0\u67e5</li> </ul>"},{"location":"core-components/cuda-storage.zh/#_4","title":"\ud83c\udfd7\ufe0f \u67b6\u6784\u8bbe\u8ba1","text":""},{"location":"core-components/cuda-storage.zh/#indexplan","title":"IndexPlan\u67b6\u6784","text":"<p>CUDATensor\u4f7f\u7528\u5148\u8fdb\u7684IndexPlan\u67b6\u6784\u6765\u5904\u7406\u590d\u6742\u7684\u5f20\u91cf\u7d22\u5f15\u64cd\u4f5c\uff1a</p> Python<pre><code>class IndexKind(Enum):\n    VIEW = \"view\"           # \u7eaf\u89c6\u56fe\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\n    GATHER = \"gather\"       # \u6536\u96c6\u64cd\u4f5c\uff0c\u7528\u4e8e\u9ad8\u7ea7\u7d22\u5f15  \n    SCATTER = \"scatter\"     # \u6563\u5e03\u64cd\u4f5c\uff0c\u7528\u4e8e\u8d4b\u503c\n    COPY = \"copy\"          # \u6b65\u957f\u62f7\u8d1d\n    FILL = \"fill\"          # \u586b\u5145\u64cd\u4f5c\n\n@dataclass\nclass IndexPlan:\n    \"\"\"\u7edf\u4e00\u7684\u7d22\u5f15\u8ba1\u5212\"\"\"\n    kind: IndexKind\n    result_shape: Optional[Tuple[int, ...]] = None\n    result_strides: Optional[Tuple[int, ...]] = None\n    ptr_offset_bytes: int = 0\n    index_tensor: Optional['CUDATensor'] = None\n    needs_mask_compaction: bool = False\n    temp_memory_bytes: int = 0\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_5","title":"\u5185\u5b58\u7ba1\u7406","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"\u5f02\u6b65\u5185\u5b58\u6c60\uff0c\u4f18\u5316GPU\u5185\u5b58\u5206\u914d\u6027\u80fd\"\"\"\n\n    def __init__(self):\n        self.free_blocks = {}  # \u6309\u5927\u5c0f\u7ec4\u7ec7\u7684\u7a7a\u95f2\u5757\n        self.allocated_blocks = {}  # \u5df2\u5206\u914d\u7684\u5757\n        self.alignment = 512  # \u5185\u5b58\u5bf9\u9f50\uff0c\u4e0ePyTorch\u4e00\u81f4\n\n    def allocate(self, size_bytes: int) -&gt; int:\n        \"\"\"\u5206\u914d\u5bf9\u9f50\u7684GPU\u5185\u5b58\"\"\"\n\n    def deallocate(self, ptr: int):\n        \"\"\"\u91ca\u653eGPU\u5185\u5b58\u5230\u6c60\u4e2d\u91cd\u7528\"\"\"\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_6","title":"\ud83d\udca1 \u6838\u5fc3\u7279\u6027","text":""},{"location":"core-components/cuda-storage.zh/#1","title":"1. \u9ad8\u6548\u7684\u7d22\u5f15\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efaCUDA\u5f20\u91cf\nx = genesis.randn(1000, 1000, device='cuda')\n\n# \u57fa\u7840\u7d22\u5f15 - \u4f7f\u7528VIEW\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\ny = x[10:20, 50:100]  # IndexPlan.kind = VIEW\n\n# \u9ad8\u7ea7\u7d22\u5f15 - \u4f7f\u7528GATHER\u64cd\u4f5c  \nindices = genesis.tensor([1, 3, 5, 7], device='cuda')\nz = x[indices]  # IndexPlan.kind = GATHER\n\n# \u5e03\u5c14\u7d22\u5f15 - \u81ea\u52a8\u4f18\u5316\nmask = x &gt; 0.5\nw = x[mask]  # \u6839\u636e\u7a20\u5bc6\u5ea6\u9009\u62e9\u6700\u4f18\u7b56\u7565\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#2","title":"2. \u5185\u5b58\u9ad8\u6548\u7684\u64cd\u4f5c","text":"Python<pre><code># \u5c31\u5730\u64cd\u4f5c\uff0c\u907f\u514d\u5185\u5b58\u5206\u914d\nx = genesis.randn(1000, 1000, device='cuda')\nx += 1.0  # \u5c31\u5730\u52a0\u6cd5\n\n# \u89c6\u56fe\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\ny = x.view(100, 10000)  # \u6539\u53d8\u5f62\u72b6\u4f46\u4e0d\u590d\u5236\u6570\u636e\nz = x.transpose(0, 1)   # \u8f6c\u7f6e\u89c6\u56fe\n\n# \u6b65\u957f\u64cd\u4f5c\uff0c\u9ad8\u6548\u5b9e\u73b0\nw = x[::2, ::3]  # \u6b65\u957f\u7d22\u5f15\uff0c\u4f7f\u7528\u4f18\u5316\u7684COPY\u64cd\u4f5c\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#3-triton","title":"3. Triton\u5185\u6838\u96c6\u6210","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u4f18\u5316\u7684Triton\u52a0\u6cd5\u5185\u6838\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# CUDATensor\u81ea\u52a8\u8c03\u7528\u4f18\u5316\u7684Triton\u5185\u6838\ndef add_cuda_tensor(x: CUDATensor, y: CUDATensor) -&gt; CUDATensor:\n    \"\"\"CUDA\u5f20\u91cf\u52a0\u6cd5\uff0c\u4f7f\u7528Triton\u4f18\u5316\"\"\"\n    output = CUDATensor(x.shape, x.dtype)\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](x.data_ptr(), y.data_ptr(), output.data_ptr(), \n                     n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_7","title":"\ud83d\ude80 \u57fa\u7840\u4f7f\u7528","text":""},{"location":"core-components/cuda-storage.zh/#_8","title":"\u521b\u5efa\u5f20\u91cf","text":"Python<pre><code>import genesis\n\n# \u4ece\u6570\u636e\u521b\u5efa\ndata = [[1.0, 2.0], [3.0, 4.0]]\ntensor = genesis.tensor(data, device='cuda')\n\n# \u76f4\u63a5\u521b\u5efa\u7279\u5b9a\u5f62\u72b6\nzeros = genesis.zeros(100, 100, device='cuda')\nones = genesis.ones(50, 50, device='cuda')  \nrandom = genesis.randn(200, 200, device='cuda')\n\n# \u6307\u5b9a\u6570\u636e\u7c7b\u578b\nfloat16_tensor = genesis.randn(100, 100, dtype=genesis.float16, device='cuda')\nint_tensor = genesis.randint(0, 10, (50, 50), device='cuda')\n\nprint(f\"\u5f20\u91cf\u5f62\u72b6: {tensor.shape}\")\nprint(f\"\u6570\u636e\u7c7b\u578b: {tensor.dtype}\")\nprint(f\"\u8bbe\u5907: {tensor.device}\")\nprint(f\"\u6b65\u957f: {tensor.strides}\")\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_9","title":"\u57fa\u7840\u64cd\u4f5c","text":"Python<pre><code># \u6570\u5b66\u8fd0\u7b97\nx = genesis.randn(100, 100, device='cuda')\ny = genesis.randn(100, 100, device='cuda')\n\n# \u9010\u5143\u7d20\u8fd0\u7b97\nz = x + y      # \u52a0\u6cd5\nw = x * y      # \u4e58\u6cd5  \nu = x.pow(2)   # \u5e42\u8fd0\u7b97\nv = x.exp()    # \u6307\u6570\u51fd\u6570\n\n# \u5f52\u7ea6\u64cd\u4f5c\nsum_all = x.sum()           # \u5168\u5c40\u6c42\u548c\nsum_dim = x.sum(dim=0)      # \u6309\u7ef4\u5ea6\u6c42\u548c\nmean_val = x.mean()         # \u5e73\u5747\u503c\nmax_val, indices = x.max(dim=1)  # \u6700\u5927\u503c\u548c\u7d22\u5f15\n\n# \u7ebf\u6027\u4ee3\u6570\na = genesis.randn(100, 50, device='cuda')\nb = genesis.randn(50, 200, device='cuda') \nc = genesis.matmul(a, b)    # \u77e9\u9635\u4e58\u6cd5\n\n# \u5f62\u72b6\u64cd\u4f5c\nreshaped = x.view(10, 1000)        # \u6539\u53d8\u5f62\u72b6\ntransposed = x.transpose(0, 1)     # \u8f6c\u7f6e  \nflattened = x.flatten()            # \u5c55\u5e73\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_10","title":"\u9ad8\u7ea7\u7d22\u5f15","text":"Python<pre><code># \u521b\u5efa\u6d4b\u8bd5\u5f20\u91cf\ndata = genesis.arange(0, 100, device='cuda').view(10, 10)\nprint(\"\u539f\u59cb\u6570\u636e:\")\nprint(data)\n\n# \u57fa\u7840\u5207\u7247\nslice_basic = data[2:5, 3:7]  # \u884c2-4\uff0c\u52173-6\nprint(\"\u57fa\u7840\u5207\u7247:\", slice_basic.shape)\n\n# \u6b65\u957f\u7d22\u5f15\nslice_stride = data[::2, 1::2]  # \u6bcf\u9694\u4e00\u884c\uff0c\u4ece\u7b2c1\u5217\u5f00\u59cb\u6bcf\u9694\u4e00\u5217\nprint(\"\u6b65\u957f\u7d22\u5f15:\", slice_stride.shape)\n\n# \u9ad8\u7ea7\u7d22\u5f15 - \u6574\u6570\u6570\u7ec4\nrow_indices = genesis.tensor([0, 2, 4, 6], device='cuda')\ncol_indices = genesis.tensor([1, 3, 5, 7], device='cuda')\nadvanced = data[row_indices, col_indices]  # \u9009\u62e9\u7279\u5b9a\u4f4d\u7f6e\nprint(\"\u9ad8\u7ea7\u7d22\u5f15\u7ed3\u679c:\", advanced)\n\n# \u5e03\u5c14\u7d22\u5f15\nmask = data &gt; 50\nmasked_data = data[mask]  # \u9009\u62e9\u5927\u4e8e50\u7684\u5143\u7d20\nprint(\"\u5e03\u5c14\u7d22\u5f15\u7ed3\u679c:\", masked_data)\n\n# \u6df7\u5408\u7d22\u5f15\nmixed = data[row_indices, 2:8]  # \u7279\u5b9a\u884c\u7684\u5217\u8303\u56f4\nprint(\"\u6df7\u5408\u7d22\u5f15:\", mixed.shape)\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_11","title":"\ud83d\udd27 \u5185\u5b58\u7ba1\u7406","text":""},{"location":"core-components/cuda-storage.zh/#_12","title":"\u5185\u5b58\u6c60\u4f18\u5316","text":"Python<pre><code># \u67e5\u770b\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\nprint(f\"\u5df2\u5206\u914d\u5185\u5b58: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"\u7f13\u5b58\u5185\u5b58: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# \u624b\u52a8\u5185\u5b58\u7ba1\u7406\nx = genesis.randn(1000, 1000, device='cuda')\nprint(f\"\u521b\u5efa\u5f20\u91cf\u540e: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\ndel x  # \u5220\u9664\u5f15\u7528\ngenesis.cuda.empty_cache()  # \u6e05\u7a7a\u7f13\u5b58\nprint(f\"\u6e05\u7406\u540e: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\n# \u5185\u5b58\u5feb\u7167\uff08\u8c03\u8bd5\u7528\uff09\nsnapshot = genesis.cuda.memory_snapshot()\nfor entry in snapshot[:3]:  # \u663e\u793a\u524d3\u4e2a\u6761\u76ee\n    print(f\"\u5730\u5740: {entry['address']}, \u5927\u5c0f: {entry['size']} bytes\")\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_13","title":"\u5f02\u6b65\u64cd\u4f5c","text":"Python<pre><code># \u5f02\u6b65\u5185\u5b58\u64cd\u4f5c\nwith genesis.cuda.stream():\n    x = genesis.randn(1000, 1000, device='cuda')\n    y = genesis.randn(1000, 1000, device='cuda')\n    z = genesis.matmul(x, y)  # \u5f02\u6b65\u6267\u884c\n\n    # \u5176\u4ed6CPU\u5de5\u4f5c\u53ef\u4ee5\u5e76\u884c\u8fdb\u884c\n    print(\"\u77e9\u9635\u4e58\u6cd5\u6b63\u5728GPU\u4e0a\u5f02\u6b65\u6267\u884c...\")\n\n    # \u540c\u6b65\u7b49\u5f85\u7ed3\u679c  \n    genesis.cuda.synchronize()\n    print(\"\u8ba1\u7b97\u5b8c\u6210:\", z.shape)\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_14","title":"\u26a1 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/cuda-storage.zh/#1_1","title":"1. \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4f18\u5316","text":"Python<pre><code>def inefficient_access():\n    \"\"\"\u4f4e\u6548\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n    result = genesis.zeros(1000, device='cuda')\n\n    # \u975e\u8fde\u7eed\u8bbf\u95ee\uff0c\u7f13\u5b58\u672a\u547d\u4e2d\n    for i in range(1000):\n        result[i] = x[i, ::10].sum()  # \u6b65\u957f\u8bbf\u95ee\n\n    return result\n\ndef efficient_access():  \n    \"\"\"\u9ad8\u6548\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u8fde\u7eed\u8bbf\u95ee\uff0c\u5145\u5206\u5229\u7528\u7f13\u5b58\n    indices = genesis.arange(0, 1000, 10, device='cuda')\n    selected = x[:, indices]  # \u6279\u91cf\u9009\u62e9\n    result = selected.sum(dim=1)  # \u5411\u91cf\u5316\u6c42\u548c\n\n    return result\n\n# \u6027\u80fd\u5bf9\u6bd4\nimport time\n\nstart = time.time()\nresult1 = inefficient_access()\ntime1 = time.time() - start\n\nstart = time.time()  \nresult2 = efficient_access()\ntime2 = time.time() - start\n\nprint(f\"\u4f4e\u6548\u65b9\u6cd5: {time1:.4f}s\")\nprint(f\"\u9ad8\u6548\u65b9\u6cd5: {time2:.4f}s\")  \nprint(f\"\u52a0\u901f\u6bd4: {time1/time2:.2f}x\")\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#2_1","title":"2. \u6279\u91cf\u64cd\u4f5c\u4f18\u5316","text":"Python<pre><code>def batch_operations_demo():\n    \"\"\"\u5c55\u793a\u6279\u91cf\u64cd\u4f5c\u7684\u6027\u80fd\u4f18\u52bf\"\"\"\n\n    # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\n    matrices = [genesis.randn(100, 100, device='cuda') for _ in range(10)]\n\n    # \u65b9\u6cd51: \u9010\u4e2a\u5904\u7406\uff08\u4f4e\u6548\uff09\n    start = time.time()\n    results1 = []\n    for matrix in matrices:\n        result = matrix.exp().sum()\n        results1.append(result)\n    time1 = time.time() - start\n\n    # \u65b9\u6cd52: \u6279\u91cf\u5904\u7406\uff08\u9ad8\u6548\uff09\n    start = time.time()\n    batched = genesis.stack(matrices, dim=0)  # [10, 100, 100]\n    results2 = batched.exp().sum(dim=(1, 2))  # [10]\n    time2 = time.time() - start\n\n    print(f\"\u9010\u4e2a\u5904\u7406: {time1:.4f}s\")\n    print(f\"\u6279\u91cf\u5904\u7406: {time2:.4f}s\")\n    print(f\"\u52a0\u901f\u6bd4: {time1/time2:.2f}x\")\n\nbatch_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#3","title":"3. \u5c31\u5730\u64cd\u4f5c","text":"Python<pre><code>def inplace_operations_demo():\n    \"\"\"\u5c55\u793a\u5c31\u5730\u64cd\u4f5c\u7684\u5185\u5b58\u6548\u7387\"\"\"\n\n    # \u975e\u5c31\u5730\u64cd\u4f5c\uff08\u521b\u5efa\u65b0\u5f20\u91cf\uff09\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    y = x + 1.0      # \u521b\u5efa\u65b0\u5f20\u91cf\n    z = y * 2.0      # \u518d\u521b\u5efa\u65b0\u5f20\u91cf\n    w = z.exp()      # \u53c8\u521b\u5efa\u65b0\u5f20\u91cf\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"\u975e\u5c31\u5730\u64cd\u4f5c\u5185\u5b58\u589e\u957f: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\n    # \u5c31\u5730\u64cd\u4f5c\uff08\u4fee\u6539\u539f\u5f20\u91cf\uff09\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    x += 1.0         # \u5c31\u5730\u52a0\u6cd5\n    x *= 2.0         # \u5c31\u5730\u4e58\u6cd5  \n    x.exp_()         # \u5c31\u5730\u6307\u6570\u51fd\u6570\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"\u5c31\u5730\u64cd\u4f5c\u5185\u5b58\u589e\u957f: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\ninplace_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_15","title":"\ud83d\udc1b \u8c03\u8bd5\u548c\u8bca\u65ad","text":""},{"location":"core-components/cuda-storage.zh/#_16","title":"\u5185\u5b58\u6cc4\u6f0f\u68c0\u6d4b","text":"Python<pre><code>def detect_memory_leaks():\n    \"\"\"\u68c0\u6d4b\u5185\u5b58\u6cc4\u6f0f\"\"\"\n    genesis.cuda.empty_cache()\n    initial_memory = genesis.cuda.memory_allocated()\n\n    # \u6267\u884c\u4e00\u4e9b\u64cd\u4f5c\n    for i in range(100):\n        x = genesis.randn(100, 100, device='cuda')\n        y = x.matmul(x)\n        del x, y\n\n    genesis.cuda.empty_cache()\n    final_memory = genesis.cuda.memory_allocated()\n\n    if final_memory &gt; initial_memory:\n        print(f\"\u53ef\u80fd\u5b58\u5728\u5185\u5b58\u6cc4\u6f0f: {(final_memory - initial_memory) / 1024**2:.1f} MB\")\n    else:\n        print(\"\u672a\u68c0\u6d4b\u5230\u5185\u5b58\u6cc4\u6f0f\")\n\ndetect_memory_leaks()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_17","title":"\u9519\u8bef\u8bca\u65ad","text":"Python<pre><code>def diagnose_cuda_errors():\n    \"\"\"CUDA\u9519\u8bef\u8bca\u65ad\"\"\"\n    try:\n        # \u53ef\u80fd\u51fa\u9519\u7684\u64cd\u4f5c\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = genesis.randn(500, 500, device='cuda')  # \u5f62\u72b6\u4e0d\u5339\u914d\n        z = genesis.matmul(x, y)\n\n    except RuntimeError as e:\n        print(f\"CUDA\u9519\u8bef: {e}\")\n\n        # \u68c0\u67e5CUDA\u72b6\u6001\n        if genesis.cuda.is_available():\n            print(f\"CUDA\u8bbe\u5907: {genesis.cuda.get_device_name()}\")\n            print(f\"CUDA\u80fd\u529b: {genesis.cuda.get_device_capability()}\")\n            print(f\"\u53ef\u7528\u5185\u5b58: {genesis.cuda.get_device_properties().total_memory / 1024**3:.1f} GB\")\n        else:\n            print(\"CUDA\u4e0d\u53ef\u7528\")\n\ndiagnose_cuda_errors()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#pytorch","title":"\ud83d\udd04 \u4e0ePyTorch\u4e92\u64cd\u4f5c","text":"Python<pre><code>import torch\n\ndef pytorch_interop_demo():\n    \"\"\"\u5c55\u793a\u4e0ePyTorch\u7684\u4e92\u64cd\u4f5c\u6027\"\"\"\n\n    # Genesis\u5f20\u91cf\u8f6cPyTorch\n    genesis_tensor = genesis.randn(100, 100, device='cuda')\n\n    # \u8f6c\u6362\u4e3aPyTorch\uff08\u5171\u4eab\u5185\u5b58\uff09\n    pytorch_tensor = torch.as_tensor(genesis_tensor.detach().cpu().numpy()).cuda()\n\n    print(f\"Genesis\u5f62\u72b6: {genesis_tensor.shape}\")\n    print(f\"PyTorch\u5f62\u72b6: {pytorch_tensor.shape}\")\n\n    # PyTorch\u5f20\u91cf\u8f6cGenesis  \n    torch_data = torch.randn(50, 50, device='cuda')\n    genesis_from_torch = genesis.tensor(torch_data.cpu().numpy(), device='cuda')\n\n    print(f\"\u8f6c\u6362\u6210\u529f\uff0cGenesis\u5f20\u91cf: {genesis_from_torch.shape}\")\n\npytorch_interop_demo()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_18","title":"\ud83d\udcca \u6027\u80fd\u57fa\u51c6","text":"Python<pre><code>def benchmark_cuda_tensor():\n    \"\"\"CUDA\u5f20\u91cf\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\"\"\"\n\n    sizes = [100, 500, 1000, 2000]\n\n    print(\"\u77e9\u9635\u4e58\u6cd5\u6027\u80fd\u5bf9\u6bd4 (Genesis vs PyTorch):\")\n    print(\"-\" * 50)\n\n    for size in sizes:\n        # Genesis\u6d4b\u8bd5\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        genesis.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis.cuda.synchronize()\n        genesis_time = (time.time() - start) / 10\n\n        # PyTorch\u6d4b\u8bd5\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch.cuda.synchronize() \n        pytorch_time = (time.time() - start) / 10\n\n        ratio = genesis_time / pytorch_time\n        print(f\"{size}x{size}: Genesis {genesis_time:.4f}s, PyTorch {pytorch_time:.4f}s, \u6bd4\u7387 {ratio:.2f}\")\n\nbenchmark_cuda_tensor()\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_19","title":"\ud83c\udfaf \u6700\u4f73\u5b9e\u8df5","text":""},{"location":"core-components/cuda-storage.zh/#1_2","title":"1. \u5185\u5b58\u7ba1\u7406\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u2705 \u597d\u7684\u505a\u6cd5\ndef good_memory_practice():\n    with genesis.cuda.device(0):  # \u660e\u786e\u6307\u5b9a\u8bbe\u5907\n        x = genesis.randn(1000, 1000, device='cuda')\n\n        # \u4f7f\u7528\u5c31\u5730\u64cd\u4f5c\n        x += 1.0\n        x *= 0.5\n\n        # \u53ca\u65f6\u91ca\u653e\u5927\u5f20\u91cf\n        del x\n        genesis.cuda.empty_cache()\n\n# \u274c \u907f\u514d\u7684\u505a\u6cd5  \ndef bad_memory_practice():\n    tensors = []\n    for i in range(100):\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = x + 1.0  # \u521b\u5efa\u989d\u5916\u526f\u672c\n        tensors.append(y)  # \u4fdd\u6301\u6240\u6709\u5f15\u7528\uff0c\u5185\u5b58\u65e0\u6cd5\u91ca\u653e\n    # \u5185\u5b58\u4f1a\u5feb\u901f\u8017\u5c3d\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#2_2","title":"2. \u6027\u80fd\u4f18\u5316\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u2705 \u5411\u91cf\u5316\u64cd\u4f5c\ndef vectorized_operations():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u4f7f\u7528\u5411\u91cf\u5316\u51fd\u6570\n    result = genesis.relu(x).sum(dim=1).mean()\n\n# \u274c \u907f\u514d\u5faa\u73af\ndef avoid_loops():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u907f\u514dPython\u5faa\u73af\n    result = 0\n    for i in range(1000):\n        result += x[i].sum()  # \u6bcf\u6b21\u90fd\u542f\u52a8CUDA kernel\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#3_1","title":"3. \u8c03\u8bd5\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u542f\u7528CUDA\u9519\u8bef\u68c0\u67e5\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# \u4f7f\u7528\u65ad\u8a00\u68c0\u67e5\u5f20\u91cf\u5c5e\u6027\ndef safe_tensor_operation(x, y):\n    assert x.device == y.device, \"\u5f20\u91cf\u5fc5\u987b\u5728\u540c\u4e00\u8bbe\u5907\u4e0a\"\n    assert x.shape == y.shape, f\"\u5f62\u72b6\u4e0d\u5339\u914d: {x.shape} vs {y.shape}\"\n\n    return x + y\n</code></pre>"},{"location":"core-components/cuda-storage.zh/#_20","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"core-components/cuda-storage.zh/#q-cuda","title":"Q: CUDA\u5185\u5b58\u4e0d\u8db3\u600e\u4e48\u529e\uff1f","text":"<p>A:  Python<pre><code># \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\nbatch_size = 32  # \u6539\u4e3a16\u62168\n\n# \u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef\naccumulation_steps = 4\neffective_batch_size = batch_size * accumulation_steps\n\n# \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')\n\n# \u5b9a\u671f\u6e05\u7406\u5185\u5b58\ngenesis.cuda.empty_cache()\n</code></pre></p>"},{"location":"core-components/cuda-storage.zh/#q-cuda_1","title":"Q: \u4e3a\u4ec0\u4e48CUDA\u64cd\u4f5c\u5f88\u6162\uff1f","text":"<p>A: \u68c0\u67e5\u4ee5\u4e0b\u51e0\u70b9\uff1a Python<pre><code># 1. \u786e\u4fdd\u5f20\u91cf\u5728GPU\u4e0a\nassert x.device.type == 'cuda'\n\n# 2. \u907f\u514d\u9891\u7e41\u7684CPU-GPU\u4f20\u8f93\n# \u9519\u8bef\u505a\u6cd5\nfor i in range(1000):\n    cpu_data = x.cpu().numpy()  # \u6bcf\u6b21\u90fd\u4f20\u8f93\n\n# \u6b63\u786e\u505a\u6cd5\ncpu_data = x.cpu().numpy()  # \u53ea\u4f20\u8f93\u4e00\u6b21\n\n# 3. \u4f7f\u7528\u9002\u5f53\u7684\u6570\u636e\u7c7b\u578b\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')  # \u66f4\u5feb\n</code></pre></p>"},{"location":"core-components/cuda-storage.zh/#q-cuda-kernel","title":"Q: \u5982\u4f55\u8c03\u8bd5CUDA kernel\u9519\u8bef\uff1f","text":"<p>A: Python<pre><code># 1. \u542f\u7528\u540c\u6b65\u9519\u8bef\u68c0\u67e5\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# 2. \u68c0\u67e5tensor\u6709\u6548\u6027\ndef check_tensor(tensor, name):\n    assert not torch.isnan(tensor).any(), f\"{name}\u5305\u542bNaN\"\n    assert not torch.isinf(tensor).any(), f\"{name}\u5305\u542bInf\"\n    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n\n# 3. \u4f7f\u7528CUDA\u8c03\u8bd5\u5de5\u5177\n# cuda-memcheck python your_script.py\n# compute-sanitizer python your_script.py\n</code></pre></p> <p>\u6027\u80fd\u63d0\u793a</p> <p>CUDA\u5f20\u91cf\u7684\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u548c\u6279\u91cf\u64cd\u4f5c\u7684\u4f7f\u7528\u3002\u4f18\u5148\u8003\u8651\u5411\u91cf\u5316\u64cd\u4f5c\u548c\u5408\u7406\u7684\u5185\u5b58\u5e03\u5c40\u3002</p> <p>\u51c6\u5907\u6df1\u5165\u4e86\u89e3\u66f4\u591a\u5417\uff1f</p> <p>\u4e0b\u4e00\u6b65\uff1a\u5f20\u91cf\u64cd\u4f5c\u4f18\u5316 \u8fd4\u56de\u6838\u5fc3\u7ec4\u4ef6</p>"},{"location":"core-components/cuda-storage_en/","title":"CUDA Storage System","text":"<p>Genesis's CUDA Storage (CUDAStorage) is a core component of the framework, providing pure CUDA implementation for GPU memory management and operations, completely independent of PyTorch, using CUDA Python API directly.</p>"},{"location":"core-components/cuda-storage_en/#design-goals","title":"\ud83c\udfaf Design Goals","text":""},{"location":"core-components/cuda-storage_en/#independence","title":"Independence","text":"<ul> <li>Pure CUDA Implementation: No dependency on PyTorch's GPU backend</li> <li>Direct Memory Management: Direct GPU memory management using CUDA Python API</li> <li>High Performance: Memory access patterns optimized for GPU</li> </ul>"},{"location":"core-components/cuda-storage_en/#compatibility","title":"Compatibility","text":"<ul> <li>PyTorch-style API: Maintains interface compatibility with PyTorch tensors</li> <li>Automatic Differentiation Support: Seamless integration with Genesis's autograd system</li> <li>Type Safety: Complete type annotations and runtime checking</li> </ul>"},{"location":"core-components/cuda-storage_en/#architecture-design","title":"\ud83c\udfd7\ufe0f Architecture Design","text":""},{"location":"core-components/cuda-storage_en/#indexplan-architecture","title":"IndexPlan Architecture","text":"<p>CUDATensor uses an advanced IndexPlan architecture to handle complex tensor indexing operations:</p> Python<pre><code>class IndexKind(Enum):\n    VIEW = \"view\"           # Pure view operation, zero-copy\n    GATHER = \"gather\"       # Gather operation for advanced indexing  \n    SCATTER = \"scatter\"     # Scatter operation for assignment\n    COPY = \"copy\"          # Strided copy\n    FILL = \"fill\"          # Fill operation\n\n@dataclass\nclass IndexPlan:\n    \"\"\"Unified index plan\"\"\"\n    kind: IndexKind\n    result_shape: Optional[Tuple[int, ...]] = None\n    result_strides: Optional[Tuple[int, ...]] = None\n    ptr_offset_bytes: int = 0\n    index_tensor: Optional['CUDATensor'] = None\n    needs_mask_compaction: bool = False\n    temp_memory_bytes: int = 0\n</code></pre>"},{"location":"core-components/cuda-storage_en/#memory-management","title":"Memory Management","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"Asynchronous memory pool to optimize GPU memory allocation performance\"\"\"\n\n    def __init__(self):\n        self.free_blocks = {}  # Free blocks organized by size\n        self.allocated_blocks = {}  # Allocated blocks\n        self.alignment = 512  # Memory alignment, consistent with PyTorch\n\n    def allocate(self, size_bytes: int) -&gt; int:\n        \"\"\"Allocate aligned GPU memory\"\"\"\n\n    def deallocate(self, ptr: int):\n        \"\"\"Release GPU memory to pool for reuse\"\"\"\n</code></pre>"},{"location":"core-components/cuda-storage_en/#core-features","title":"\ud83d\udca1 Core Features","text":""},{"location":"core-components/cuda-storage_en/#1-efficient-indexing-operations","title":"1. Efficient Indexing Operations","text":"Python<pre><code>import genesis\n\n# Create CUDA tensor\nx = genesis.randn(1000, 1000, device='cuda')\n\n# Basic indexing - uses VIEW operation, zero-copy\ny = x[10:20, 50:100]  # IndexPlan.kind = VIEW\n\n# Advanced indexing - uses GATHER operation  \nindices = genesis.tensor([1, 3, 5, 7], device='cuda')\nz = x[indices]  # IndexPlan.kind = GATHER\n\n# Boolean indexing - automatic optimization\nmask = x &gt; 0.5\nw = x[mask]  # Choose optimal strategy based on sparsity\n</code></pre>"},{"location":"core-components/cuda-storage_en/#2-memory-efficient-operations","title":"2. Memory-Efficient Operations","text":"Python<pre><code># In-place operations, avoid memory allocation\nx = genesis.randn(1000, 1000, device='cuda')\nx += 1.0  # In-place addition\n\n# View operations, zero-copy\ny = x.view(100, 10000)  # Change shape without copying data\nz = x.transpose(0, 1)   # Transpose view\n\n# Strided operations, efficient implementation\nw = x[::2, ::3]  # Strided indexing using optimized COPY operation\n</code></pre>"},{"location":"core-components/cuda-storage_en/#3-triton-kernel-integration","title":"3. Triton Kernel Integration","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized Triton addition kernel\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# CUDATensor automatically calls optimized Triton kernel\ndef add_cuda_tensor(x: CUDATensor, y: CUDATensor) -&gt; CUDATensor:\n    \"\"\"CUDA tensor addition using Triton optimization\"\"\"\n    output = CUDATensor(x.shape, x.dtype)\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](x.data_ptr(), y.data_ptr(), output.data_ptr(), \n                     n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output\n</code></pre>"},{"location":"core-components/cuda-storage_en/#basic-usage","title":"\ud83d\ude80 Basic Usage","text":""},{"location":"core-components/cuda-storage_en/#creating-tensors","title":"Creating Tensors","text":"Python<pre><code>import genesis\n\n# Create from data\ndata = [[1.0, 2.0], [3.0, 4.0]]\ntensor = genesis.tensor(data, device='cuda')\n\n# Create specific shapes directly\nzeros = genesis.zeros(100, 100, device='cuda')\nones = genesis.ones(50, 50, device='cuda')  \nrandom = genesis.randn(200, 200, device='cuda')\n\n# Specify data type\nfloat16_tensor = genesis.randn(100, 100, dtype=genesis.float16, device='cuda')\nint_tensor = genesis.randint(0, 10, (50, 50), device='cuda')\n\nprint(f\"Tensor shape: {tensor.shape}\")\nprint(f\"Data type: {tensor.dtype}\")\nprint(f\"Device: {tensor.device}\")\nprint(f\"Strides: {tensor.strides}\")\n</code></pre>"},{"location":"core-components/cuda-storage_en/#basic-operations","title":"Basic Operations","text":"Python<pre><code># Mathematical operations\nx = genesis.randn(100, 100, device='cuda')\ny = genesis.randn(100, 100, device='cuda')\n\n# Element-wise operations\nz = x + y      # Addition\nw = x * y      # Multiplication  \nu = x.pow(2)   # Power operation\nv = x.exp()    # Exponential function\n\n# Reduction operations\nsum_all = x.sum()           # Global sum\nsum_dim = x.sum(dim=0)      # Sum along dimension\nmean_val = x.mean()         # Mean value\nmax_val, indices = x.max(dim=1)  # Maximum value and indices\n\n# Linear algebra\na = genesis.randn(100, 50, device='cuda')\nb = genesis.randn(50, 200, device='cuda') \nc = genesis.matmul(a, b)    # Matrix multiplication\n\n# Shape operations\nreshaped = x.view(10, 1000)        # Reshape\ntransposed = x.transpose(0, 1)     # Transpose  \nflattened = x.flatten()            # Flatten\n</code></pre>"},{"location":"core-components/cuda-storage_en/#advanced-indexing","title":"Advanced Indexing","text":"Python<pre><code># Create test tensor\ndata = genesis.arange(0, 100, device='cuda').view(10, 10)\nprint(\"Original data:\")\nprint(data)\n\n# Basic slicing\nslice_basic = data[2:5, 3:7]  # Rows 2-4, columns 3-6\nprint(\"Basic slicing:\", slice_basic.shape)\n\n# Strided indexing\nslice_stride = data[::2, 1::2]  # Every other row, every other column starting from column 1\nprint(\"Strided indexing:\", slice_stride.shape)\n\n# Advanced indexing - integer arrays\nrow_indices = genesis.tensor([0, 2, 4, 6], device='cuda')\ncol_indices = genesis.tensor([1, 3, 5, 7], device='cuda')\nadvanced = data[row_indices, col_indices]  # Select specific positions\nprint(\"Advanced indexing result:\", advanced)\n\n# Boolean indexing\nmask = data &gt; 50\nmasked_data = data[mask]  # Select elements greater than 50\nprint(\"Boolean indexing result:\", masked_data)\n\n# Mixed indexing\nmixed = data[row_indices, 2:8]  # Column range for specific rows\nprint(\"Mixed indexing:\", mixed.shape)\n</code></pre>"},{"location":"core-components/cuda-storage_en/#memory-management_1","title":"\ud83d\udd27 Memory Management","text":""},{"location":"core-components/cuda-storage_en/#memory-pool-optimization","title":"Memory Pool Optimization","text":"Python<pre><code># Check memory usage\nprint(f\"Allocated memory: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"Cached memory: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# Manual memory management\nx = genesis.randn(1000, 1000, device='cuda')\nprint(f\"After tensor creation: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\ndel x  # Delete reference\ngenesis.cuda.empty_cache()  # Empty cache\nprint(f\"After cleanup: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\n# Memory snapshot (for debugging)\nsnapshot = genesis.cuda.memory_snapshot()\nfor entry in snapshot[:3]:  # Show first 3 entries\n    print(f\"Address: {entry['address']}, Size: {entry['size']} bytes\")\n</code></pre>"},{"location":"core-components/cuda-storage_en/#asynchronous-operations","title":"Asynchronous Operations","text":"Python<pre><code># Asynchronous memory operations\nwith genesis.cuda.stream():\n    x = genesis.randn(1000, 1000, device='cuda')\n    y = genesis.randn(1000, 1000, device='cuda')\n    z = genesis.matmul(x, y)  # Asynchronous execution\n\n    # Other CPU work can proceed in parallel\n    print(\"Matrix multiplication running asynchronously on GPU...\")\n\n    # Synchronize and wait for results  \n    genesis.cuda.synchronize()\n    print(\"Computation complete:\", z.shape)\n</code></pre>"},{"location":"core-components/cuda-storage_en/#performance-optimization","title":"\u26a1 Performance Optimization","text":""},{"location":"core-components/cuda-storage_en/#1-memory-access-pattern-optimization","title":"1. Memory Access Pattern Optimization","text":"Python<pre><code>def inefficient_access():\n    \"\"\"Inefficient memory access pattern\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n    result = genesis.zeros(1000, device='cuda')\n\n    # Non-contiguous access, cache misses\n    for i in range(1000):\n        result[i] = x[i, ::10].sum()  # Strided access\n\n    return result\n\ndef efficient_access():  \n    \"\"\"Efficient memory access pattern\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Contiguous access, full cache utilization\n    indices = genesis.arange(0, 1000, 10, device='cuda')\n    selected = x[:, indices]  # Batch selection\n    result = selected.sum(dim=1)  # Vectorized summation\n\n    return result\n\n# Performance comparison\nimport time\n\nstart = time.time()\nresult1 = inefficient_access()\ntime1 = time.time() - start\n\nstart = time.time()  \nresult2 = efficient_access()\ntime2 = time.time() - start\n\nprint(f\"Inefficient method: {time1:.4f}s\")\nprint(f\"Efficient method: {time2:.4f}s\")  \nprint(f\"Speedup: {time1/time2:.2f}x\")\n</code></pre>"},{"location":"core-components/cuda-storage_en/#2-batch-operations-optimization","title":"2. Batch Operations Optimization","text":"Python<pre><code>def batch_operations_demo():\n    \"\"\"Demonstrate performance advantages of batch operations\"\"\"\n\n    # Create test data\n    matrices = [genesis.randn(100, 100, device='cuda') for _ in range(10)]\n\n    # Method 1: Individual processing (inefficient)\n    start = time.time()\n    results1 = []\n    for matrix in matrices:\n        result = matrix.exp().sum()\n        results1.append(result)\n    time1 = time.time() - start\n\n    # Method 2: Batch processing (efficient)\n    start = time.time()\n    batched = genesis.stack(matrices, dim=0)  # [10, 100, 100]\n    results2 = batched.exp().sum(dim=(1, 2))  # [10]\n    time2 = time.time() - start\n\n    print(f\"Individual processing: {time1:.4f}s\")\n    print(f\"Batch processing: {time2:.4f}s\")\n    print(f\"Speedup: {time1/time2:.2f}x\")\n\nbatch_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#3-in-place-operations","title":"3. In-place Operations","text":"Python<pre><code>def inplace_operations_demo():\n    \"\"\"Demonstrate memory efficiency of in-place operations\"\"\"\n\n    # Non-in-place operations (create new tensors)\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    y = x + 1.0      # Create new tensor\n    z = y * 2.0      # Create another new tensor\n    w = z.exp()      # Create yet another new tensor\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"Non-in-place operations memory growth: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\n    # In-place operations (modify original tensor)\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    x += 1.0         # In-place addition\n    x *= 2.0         # In-place multiplication  \n    x.exp_()         # In-place exponential function\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"In-place operations memory growth: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\ninplace_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#debugging-and-diagnostics","title":"\ud83d\udc1b Debugging and Diagnostics","text":""},{"location":"core-components/cuda-storage_en/#memory-leak-detection","title":"Memory Leak Detection","text":"Python<pre><code>def detect_memory_leaks():\n    \"\"\"Detect memory leaks\"\"\"\n    genesis.cuda.empty_cache()\n    initial_memory = genesis.cuda.memory_allocated()\n\n    # Perform some operations\n    for i in range(100):\n        x = genesis.randn(100, 100, device='cuda')\n        y = x.matmul(x)\n        del x, y\n\n    genesis.cuda.empty_cache()\n    final_memory = genesis.cuda.memory_allocated()\n\n    if final_memory &gt; initial_memory:\n        print(f\"Possible memory leak: {(final_memory - initial_memory) / 1024**2:.1f} MB\")\n    else:\n        print(\"No memory leak detected\")\n\ndetect_memory_leaks()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#error-diagnostics","title":"Error Diagnostics","text":"Python<pre><code>def diagnose_cuda_errors():\n    \"\"\"CUDA error diagnostics\"\"\"\n    try:\n        # Operations that might cause errors\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = genesis.randn(500, 500, device='cuda')  # Shape mismatch\n        z = genesis.matmul(x, y)\n\n    except RuntimeError as e:\n        print(f\"CUDA error: {e}\")\n\n        # Check CUDA status\n        if genesis.cuda.is_available():\n            print(f\"CUDA device: {genesis.cuda.get_device_name()}\")\n            print(f\"CUDA capability: {genesis.cuda.get_device_capability()}\")\n            print(f\"Available memory: {genesis.cuda.get_device_properties().total_memory / 1024**3:.1f} GB\")\n        else:\n            print(\"CUDA unavailable\")\n\ndiagnose_cuda_errors()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#pytorch-interoperability","title":"\ud83d\udd04 PyTorch Interoperability","text":"Python<pre><code>import torch\n\ndef pytorch_interop_demo():\n    \"\"\"Demonstrate interoperability with PyTorch\"\"\"\n\n    # Convert Genesis tensor to PyTorch\n    genesis_tensor = genesis.randn(100, 100, device='cuda')\n\n    # Convert to PyTorch (shared memory)\n    pytorch_tensor = torch.as_tensor(genesis_tensor.detach().cpu().numpy()).cuda()\n\n    print(f\"Genesis shape: {genesis_tensor.shape}\")\n    print(f\"PyTorch shape: {pytorch_tensor.shape}\")\n\n    # PyTorch tensor to Genesis  \n    torch_data = torch.randn(50, 50, device='cuda')\n    genesis_from_torch = genesis.tensor(torch_data.cpu().numpy(), device='cuda')\n\n    print(f\"Conversion successful, Genesis tensor: {genesis_from_torch.shape}\")\n\npytorch_interop_demo()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"Python<pre><code>def benchmark_cuda_tensor():\n    \"\"\"CUDA tensor performance benchmark tests\"\"\"\n\n    sizes = [100, 500, 1000, 2000]\n\n    print(\"Matrix multiplication performance comparison (Genesis vs PyTorch):\")\n    print(\"-\" * 50)\n\n    for size in sizes:\n        # Genesis test\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        genesis.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis.cuda.synchronize()\n        genesis_time = (time.time() - start) / 10\n\n        # PyTorch test\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch.cuda.synchronize() \n        pytorch_time = (time.time() - start) / 10\n\n        ratio = genesis_time / pytorch_time\n        print(f\"{size}x{size}: Genesis {genesis_time:.4f}s, PyTorch {pytorch_time:.4f}s, ratio {ratio:.2f}\")\n\nbenchmark_cuda_tensor()\n</code></pre>"},{"location":"core-components/cuda-storage_en/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"core-components/cuda-storage_en/#1-memory-management-best-practices","title":"1. Memory Management Best Practices","text":"Python<pre><code># \u2705 Good practices\ndef good_memory_practice():\n    with genesis.cuda.device(0):  # Explicitly specify device\n        x = genesis.randn(1000, 1000, device='cuda')\n\n        # Use in-place operations\n        x += 1.0\n        x *= 0.5\n\n        # Release large tensors promptly\n        del x\n        genesis.cuda.empty_cache()\n\n# \u274c Practices to avoid  \ndef bad_memory_practice():\n    tensors = []\n    for i in range(100):\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = x + 1.0  # Create additional copy\n        tensors.append(y)  # Keep all references, memory cannot be freed\n    # Memory will be exhausted quickly\n</code></pre>"},{"location":"core-components/cuda-storage_en/#2-performance-optimization-best-practices","title":"2. Performance Optimization Best Practices","text":"Python<pre><code># \u2705 Vectorized operations\ndef vectorized_operations():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Use vectorized functions\n    result = genesis.relu(x).sum(dim=1).mean()\n\n# \u274c Avoid loops\ndef avoid_loops():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # Avoid Python loops\n    result = 0\n    for i in range(1000):\n        result += x[i].sum()  # Launches CUDA kernel each time\n</code></pre>"},{"location":"core-components/cuda-storage_en/#3-debugging-best-practices","title":"3. Debugging Best Practices","text":"Python<pre><code># Enable CUDA error checking\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# Use assertions to check tensor properties\ndef safe_tensor_operation(x, y):\n    assert x.device == y.device, \"Tensors must be on the same device\"\n    assert x.shape == y.shape, f\"Shape mismatch: {x.shape} vs {y.shape}\"\n\n    return x + y\n</code></pre>"},{"location":"core-components/cuda-storage_en/#common-issues","title":"\u2753 Common Issues","text":""},{"location":"core-components/cuda-storage_en/#q-what-to-do-when-cuda-memory-is-insufficient","title":"Q: What to do when CUDA memory is insufficient?","text":"<p>A:  Python<pre><code># Reduce batch size\nbatch_size = 32  # Change to 16 or 8\n\n# Use gradient accumulation\naccumulation_steps = 4\neffective_batch_size = batch_size * accumulation_steps\n\n# Enable mixed precision\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')\n\n# Regularly clean memory\ngenesis.cuda.empty_cache()\n</code></pre></p>"},{"location":"core-components/cuda-storage_en/#q-why-are-cuda-operations-slow","title":"Q: Why are CUDA operations slow?","text":"<p>A: Check the following points: Python<pre><code># 1. Ensure tensors are on GPU\nassert x.device.type == 'cuda'\n\n# 2. Avoid frequent CPU-GPU transfers\n# Wrong approach\nfor i in range(1000):\n    cpu_data = x.cpu().numpy()  # Transfer each time\n\n# Correct approach\ncpu_data = x.cpu().numpy()  # Transfer only once\n\n# 3. Use appropriate data types\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')  # Faster\n</code></pre></p>"},{"location":"core-components/cuda-storage_en/#q-how-to-debug-cuda-kernel-errors","title":"Q: How to debug CUDA kernel errors?","text":"<p>A: Python<pre><code># 1. Enable synchronous error checking\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# 2. Check tensor validity\ndef check_tensor(tensor, name):\n    assert not torch.isnan(tensor).any(), f\"{name} contains NaN\"\n    assert not torch.isinf(tensor).any(), f\"{name} contains Inf\"\n    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n\n# 3. Use CUDA debugging tools\n# cuda-memcheck python your_script.py\n# compute-sanitizer python your_script.py\n</code></pre></p> <p>Performance Tip</p> <p>CUDA tensor performance largely depends on memory access patterns and the use of batch operations. Prioritize vectorized operations and reasonable memory layout.</p> <p>Ready to learn more?</p> <p>Next: Tensor Operation Optimization Back to Core Components</p>"},{"location":"core-components/dtypes.zh/","title":"\u6570\u636e\u7c7b\u578b\u7cfb\u7edf","text":"<p>Genesis\u5b9e\u73b0\u4e86\u4e00\u5957\u7edf\u4e00\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e0ePyTorch\u5bf9\u9f50\u7684\u7c7b\u578b\u7ba1\u7406\uff0c\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u8de8\u8bbe\u5907\u7c7b\u578b\u8f6c\u6362\u3002</p>"},{"location":"core-components/dtypes.zh/#_2","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":"<ul> <li>\u7edf\u4e00\u63a5\u53e3\uff1aCPU\u548cGPU\u540e\u7aef\u4f7f\u7528\u76f8\u540c\u7684\u7c7b\u578b\u5b9a\u4e49</li> <li>PyTorch\u517c\u5bb9\uff1a\u4e0ePyTorch\u7684dtype\u7cfb\u7edf\u4fdd\u6301\u4e00\u81f4\u6027</li> <li>\u6df7\u5408\u7cbe\u5ea6\uff1a\u65e0\u7f1d\u652f\u6301FP16\u3001BF16\u7b49\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u7684\u7c7b\u578b\u68c0\u67e5</li> </ul>"},{"location":"core-components/dtypes.zh/#_3","title":"\ud83c\udfd7\ufe0f \u6838\u5fc3\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"DType\u6838\u5fc3\u7c7b\"\n        A[DType] --&gt; B[name str]\n        A --&gt; C[itemsize int]\n        A --&gt; D[numpy_dtype]\n        A --&gt; E[triton_name str]\n        A --&gt; F[is_floating_point bool]\n    end\n\n    subgraph \"\u9884\u5b9a\u4e49\u7c7b\u578b\"\n        G[\u6d6e\u70b9\u7c7b\u578b] --&gt; H[float32]\n        G --&gt; I[float16] \n        G --&gt; J[bfloat16]\n        G --&gt; K[float64]\n\n        L[\u6574\u6570\u7c7b\u578b] --&gt; M[int32]\n        L --&gt; N[int64]\n        L --&gt; O[int16]\n        L --&gt; P[int8]\n        L --&gt; Q[uint8]\n\n        R[\u5e03\u5c14\u7c7b\u578b] --&gt; S[bool]\n    end\n\n    subgraph \"\u7c7b\u578b\u8f6c\u6362\"\n        T[get_dtype] --&gt; U[\u5b57\u7b26\u4e32\u8f6c\u6362]\n        T --&gt; V[NumPy\u517c\u5bb9]\n        T --&gt; W[\u7c7b\u578b\u63a8\u65ad]\n    end\n\n    A --&gt; G\n    A --&gt; L  \n    A --&gt; R\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style L fill:#fff3e0\n    style T fill:#fce4ec</code></pre>"},{"location":"core-components/dtypes.zh/#dtype","title":"\ud83d\udcca DType\u7c7b\u8be6\u89e3","text":""},{"location":"core-components/dtypes.zh/#_4","title":"\u7c7b\u5b9a\u4e49","text":"Python<pre><code>class DType:\n    \"\"\"Genesis\u6570\u636e\u7c7b\u578b\uff0c\u7c7b\u4f3ctorch.dtype\"\"\"\n\n    def __init__(self, name, itemsize, numpy_dtype, triton_name=None, is_floating_point=None):\n        self.name = name                    # \u7c7b\u578b\u540d\u79f0\uff0c\u5982\"float32\"\n        self.itemsize = itemsize           # \u5b57\u8282\u5927\u5c0f\n        self.numpy_dtype = numpy_dtype     # \u5bf9\u5e94\u7684NumPy\u7c7b\u578b\n        self.triton_name = triton_name or name  # Triton\u4e2d\u7684\u7c7b\u578b\u540d\n\n        # \u81ea\u52a8\u68c0\u6d4b\u662f\u5426\u4e3a\u6d6e\u70b9\u7c7b\u578b\n        if is_floating_point is None:\n            self.is_floating_point = np.issubdtype(numpy_dtype, np.floating)\n        else:\n            self.is_floating_point = is_floating_point\n</code></pre>"},{"location":"core-components/dtypes.zh/#_5","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"core-components/dtypes.zh/#_6","title":"\u5b57\u7b26\u4e32\u8868\u793a","text":"Python<pre><code>def __str__(self):\n    return f\"genesis.{self.name}\"\n\ndef __repr__(self):\n    return f\"genesis.{self.name}\"\n\n# \u4f7f\u7528\u793a\u4f8b\nprint(genesis.float32)  # \u8f93\u51fa: genesis.float32\n</code></pre>"},{"location":"core-components/dtypes.zh/#_7","title":"\u76f8\u7b49\u6027\u6bd4\u8f83","text":"Python<pre><code>def __eq__(self, other):\n    if isinstance(other, DType):\n        return self.name == other.name\n    elif isinstance(other, str):\n        return self.name == other  # \u5411\u540e\u517c\u5bb9\u5b57\u7b26\u4e32\u6bd4\u8f83\n    return False\n\n# \u4f7f\u7528\u793a\u4f8b\ngenesis.float32 == genesis.float32  # True\ngenesis.float32 == \"float32\"        # True (\u5411\u540e\u517c\u5bb9)\ngenesis.float32 == genesis.float16  # False\n</code></pre>"},{"location":"core-components/dtypes.zh/#_8","title":"\ud83d\udd22 \u9884\u5b9a\u4e49\u6570\u636e\u7c7b\u578b","text":""},{"location":"core-components/dtypes.zh/#_9","title":"\u6d6e\u70b9\u7c7b\u578b","text":"\u7c7b\u578b \u5b57\u8282\u6570 \u7cbe\u5ea6 \u7528\u9014 <code>float32</code> 4 \u5355\u7cbe\u5ea6 \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\uff0c\u5e73\u8861\u7cbe\u5ea6\u548c\u6027\u80fd <code>float16</code> 2 \u534a\u7cbe\u5ea6 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u8282\u7701\u5185\u5b58 <code>float64</code> 8 \u53cc\u7cbe\u5ea6 \u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u9700\u6c42 <code>bfloat16</code> 2 \u8111\u6d6e\u70b9 Google TPU\u4f18\u5316\uff0c\u52a8\u6001\u8303\u56f4\u5927 Python<pre><code># \u6d6e\u70b9\u7c7b\u578b\u5b9a\u4e49\nfloat32 = DType(\"float32\", 4, np.float32)\nfloat16 = DType(\"float16\", 2, np.float16)\nfloat64 = DType(\"float64\", 8, np.float64)\n\n# bfloat16\u7279\u6b8a\u5904\u7406 - Triton\u652f\u6301\u4f46NumPy\u4e0d\u539f\u751f\u652f\u6301\nbfloat16 = DType(\"bfloat16\", 2, np.float32, \"bfloat16\", is_floating_point=True)\n</code></pre>"},{"location":"core-components/dtypes.zh/#_10","title":"\u6574\u6570\u7c7b\u578b","text":"\u7c7b\u578b \u5b57\u8282\u6570 \u8303\u56f4 \u7528\u9014 <code>int64</code> 8 -2^63 ~ 2^63-1 \u9ed8\u8ba4\u6574\u6570\u7c7b\u578b <code>int32</code> 4 -2^31 ~ 2^31-1 \u5185\u5b58\u4f18\u5316\u7684\u6574\u6570 <code>int16</code> 2 -32,768 ~ 32,767 \u5c0f\u6574\u6570\u5b58\u50a8 <code>int8</code> 1 -128 ~ 127 \u91cf\u5316\u8ba1\u7b97 <code>uint8</code> 1 0 ~ 255 \u56fe\u50cf\u6570\u636e Python<pre><code># \u6574\u6570\u7c7b\u578b\u5b9a\u4e49\nint32 = DType(\"int32\", 4, np.int32)\nint64 = DType(\"int64\", 8, np.int64)\nint16 = DType(\"int16\", 2, np.int16)\nint8 = DType(\"int8\", 1, np.int8)\nuint8 = DType(\"uint8\", 1, np.uint8)\n</code></pre>"},{"location":"core-components/dtypes.zh/#_11","title":"\u5e03\u5c14\u7c7b\u578b","text":"Python<pre><code># \u5e03\u5c14\u7c7b\u578b\nbool = DType(\"bool\", 1, np.bool_, is_floating_point=False)\n</code></pre>"},{"location":"core-components/dtypes.zh/#_12","title":"\ud83d\udd04 \u7c7b\u578b\u8f6c\u6362\u7cfb\u7edf","text":""},{"location":"core-components/dtypes.zh/#_13","title":"\u6838\u5fc3\u8f6c\u6362\u51fd\u6570","text":"Python<pre><code>def get_dtype(obj):\n    \"\"\"\n    \u5c06\u5404\u79cd\u7c7b\u578b\u8868\u793a\u8f6c\u6362\u4e3aGenesis DType\u5bf9\u8c61\n\n    \u652f\u6301\u7684\u8f93\u5165\u7c7b\u578b:\n    - DType\u5bf9\u8c61: \u76f4\u63a5\u8fd4\u56de\n    - \u5b57\u7b26\u4e32: \"float32\", \"int64\"\u7b49\n    - NumPy dtype: np.float32, np.int64\u7b49\n    - NumPy\u7c7b\u578b: np.float32, np.int64\u7c7b\u7b49\n    - None: \u8fd4\u56de\u9ed8\u8ba4float32\n    \"\"\"\n    if obj is None:\n        return float32  # \u9ed8\u8ba4\u7c7b\u578b\n    elif isinstance(obj, DType):\n        return obj\n    elif isinstance(obj, str):\n        return _name_to_dtype[obj]\n    elif isinstance(obj, np.dtype):\n        return _numpy_to_dtype[obj.type]\n    elif isinstance(obj, type) and issubclass(obj, np.generic):\n        return _numpy_to_dtype[obj]\n    else:\n        raise ValueError(f\"Cannot convert {type(obj)} to Genesis DType: {obj}\")\n</code></pre>"},{"location":"core-components/dtypes.zh/#_14","title":"\u7c7b\u578b\u6620\u5c04\u8868","text":"Python<pre><code># \u540d\u79f0\u5230\u7c7b\u578b\u7684\u6620\u5c04\n_name_to_dtype = {\n    \"float32\": float32,\n    \"float16\": float16,\n    \"float64\": float64,\n    \"bfloat16\": bfloat16,\n    \"int32\": int32,\n    \"int64\": int64,\n    \"int16\": int16,\n    \"int8\": int8,\n    \"uint8\": uint8,\n    \"bool\": bool,\n}\n\n# NumPy\u7c7b\u578b\u5230Genesis\u7c7b\u578b\u7684\u6620\u5c04\n_numpy_to_dtype = {\n    np.float32: float32,\n    np.float16: float16,\n    np.float64: float64,\n    np.int32: int32,\n    np.int64: int64,\n    np.int16: int16,\n    np.int8: int8,\n    np.uint8: uint8,\n    np.bool_: bool,\n}\n</code></pre>"},{"location":"core-components/dtypes.zh/#_15","title":"\ud83e\uddee \u7c7b\u578b\u68c0\u67e5\u5de5\u5177","text":""},{"location":"core-components/dtypes.zh/#_16","title":"\u6d6e\u70b9\u7c7b\u578b\u68c0\u67e5","text":"Python<pre><code>def is_floating_point(dtype):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u4e3a\u6d6e\u70b9\u7c7b\u578b\"\"\"\n    dtype = get_dtype(dtype)\n    return dtype.is_floating_point\n\n# \u4f7f\u7528\u793a\u4f8b\nis_floating_point(genesis.float32)  # True\nis_floating_point(genesis.int32)    # False\nis_floating_point(\"float16\")        # True\n</code></pre>"},{"location":"core-components/dtypes.zh/#_17","title":"\u6574\u6570\u7c7b\u578b\u68c0\u67e5","text":"Python<pre><code>def is_integer(dtype):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u4e3a\u6574\u6570\u7c7b\u578b\"\"\"\n    dtype = get_dtype(dtype)\n    return not dtype.is_floating_point and dtype != bool\n\n# \u4f7f\u7528\u793a\u4f8b\nis_integer(genesis.int32)   # True\nis_integer(genesis.float32) # False\nis_integer(genesis.bool)    # False\n</code></pre>"},{"location":"core-components/dtypes.zh/#_18","title":"\u7c7b\u578b\u5206\u7c7b","text":"Python<pre><code># \u6240\u6709\u652f\u6301\u7684\u7c7b\u578b\nall_dtypes = [float32, float16, float64, bfloat16, int32, int64, int16, int8, uint8, bool]\n\n# \u6d6e\u70b9\u7c7b\u578b\u5217\u8868\nfloating_dtypes = [dt for dt in all_dtypes if dt.is_floating_point]\n# [float32, float16, float64, bfloat16]\n\n# \u6574\u6570\u7c7b\u578b\u5217\u8868\ninteger_dtypes = [dt for dt in all_dtypes if is_integer(dt)]\n# [int32, int64, int16, int8, uint8]\n</code></pre>"},{"location":"core-components/dtypes.zh/#_19","title":"\ud83d\udd00 \u6df7\u5408\u7cbe\u5ea6\u652f\u6301","text":""},{"location":"core-components/dtypes.zh/#_20","title":"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362","text":"Python<pre><code>def _cast(value, dtype):\n    \"\"\"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\uff0c\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# \u5728autograd\u4e2d\u7684\u5e94\u7528\nif genesis.enable_autocast:\n    result = cls.forward(ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/dtypes.zh/#_21","title":"\u7c7b\u578b\u63a8\u65ad","text":"Python<pre><code>def check_dtype(value, dtype):\n    \"\"\"\u9012\u5f52\u68c0\u67e5\u6570\u636e\u7ed3\u6784\u4e2d\u662f\u5426\u5305\u542b\u6307\u5b9a\u7c7b\u578b\"\"\"\n    if isinstance(value, Tensor):\n        return value.dtype == dtype\n    elif isinstance(value, dict):\n        return any(check_dtype(k, dtype) or check_dtype(v, dtype) for k, v in value.items())\n    elif isinstance(value, (list, tuple)):\n        return any(check_dtype(v, dtype) for v in value)\n    else:\n        return False\n</code></pre>"},{"location":"core-components/dtypes.zh/#_22","title":"\ud83c\udfaf \u4f7f\u7528\u793a\u4f8b","text":""},{"location":"core-components/dtypes.zh/#_23","title":"\u57fa\u7840\u7c7b\u578b\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u4e0d\u540c\u7c7b\u578b\u7684\u5f20\u91cf\nx_f32 = genesis.randn(3, 4, dtype=genesis.float32)\nx_f16 = genesis.randn(3, 4, dtype=genesis.float16)\nx_int = genesis.randint(0, 10, (3, 4), dtype=genesis.int32)\n\n# \u68c0\u67e5\u7c7b\u578b\nprint(f\"x_f32\u7c7b\u578b: {x_f32.dtype}\")          # genesis.float32\nprint(f\"\u662f\u5426\u6d6e\u70b9: {x_f32.dtype.is_floating_point}\")  # True\nprint(f\"\u5b57\u8282\u5927\u5c0f: {x_f32.dtype.itemsize}\")          # 4\n</code></pre>"},{"location":"core-components/dtypes.zh/#_24","title":"\u7c7b\u578b\u8f6c\u6362","text":"Python<pre><code># \u5b57\u7b26\u4e32\u5230\u7c7b\u578b\ndtype1 = genesis.get_dtype(\"float16\")    # genesis.float16\ndtype2 = genesis.get_dtype(np.float32)   # genesis.float32\ndtype3 = genesis.get_dtype(None)         # genesis.float32 (\u9ed8\u8ba4)\n\n# \u5f20\u91cf\u7c7b\u578b\u8f6c\u6362\nx = genesis.randn(3, 4, dtype=\"float32\")\nx_half = x.half()      # \u8f6c\u6362\u4e3afloat16\nx_float = x.float()    # \u8f6c\u6362\u4e3afloat32\n</code></pre>"},{"location":"core-components/dtypes.zh/#_25","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\n# \u6a21\u578b\u4f1a\u81ea\u52a8\u5728fp16\u548cfp32\u95f4\u8f6c\u6362\nimport genesis.nn as nn\n\nmodel = nn.Linear(784, 128)\nx = genesis.randn(32, 784, dtype=genesis.float16)\n\n# \u524d\u5411\u4f20\u64ad\u65f6\u81ea\u52a8\u5904\u7406\u7c7b\u578b\u8f6c\u6362\noutput = model(x)\n</code></pre>"},{"location":"core-components/dtypes.zh/#_26","title":"\u8bbe\u5907\u95f4\u7c7b\u578b\u4e00\u81f4\u6027","text":"Python<pre><code># CPU\u548cGPU\u4f7f\u7528\u76f8\u540c\u7684\u7c7b\u578b\u7cfb\u7edf\ncpu_tensor = genesis.randn(3, 4, device=\"cpu\", dtype=genesis.float32)\ngpu_tensor = genesis.randn(3, 4, device=\"cuda\", dtype=genesis.float32)\n\nprint(cpu_tensor.dtype == gpu_tensor.dtype)  # True\nprint(cpu_tensor.dtype.name)                 # \"float32\"\nprint(gpu_tensor.dtype.name)                 # \"float32\"\n</code></pre>"},{"location":"core-components/dtypes.zh/#bfloat16","title":"bfloat16\u7279\u6b8a\u5904\u7406","text":"Python<pre><code># bfloat16\u5728\u4e0d\u540c\u540e\u7aef\u7684\u5904\u7406\nx_bf16 = genesis.randn(3, 4, dtype=genesis.bfloat16)\n\n# CPU\u540e\u7aef: \u4f7f\u7528float32\u5b58\u50a8\u4f46\u6807\u8bb0\u4e3abfloat16\n# GPU\u540e\u7aef: \u539f\u751fbfloat16\u652f\u6301\uff08\u5982\u679c\u786c\u4ef6\u652f\u6301\uff09\nprint(f\"\u7c7b\u578b\u540d: {x_bf16.dtype.name}\")           # \"bfloat16\"\nprint(f\"Triton\u540d: {x_bf16.dtype.triton_name}\")  # \"bfloat16\"\nprint(f\"NumPy\u7c7b\u578b: {x_bf16.dtype.numpy_dtype}\") # &lt;class 'numpy.float32'&gt;\n</code></pre>"},{"location":"core-components/dtypes.zh/#_27","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/dtypes.zh/#_28","title":"\u7c7b\u578b\u8f6c\u6362\u4f18\u5316","text":"<ul> <li>\u60f0\u6027\u8f6c\u6362\uff1a\u53ea\u6709\u5728\u771f\u6b63\u9700\u8981\u65f6\u624d\u8fdb\u884c\u7c7b\u578b\u8f6c\u6362</li> <li>\u7f13\u5b58\u673a\u5236\uff1a\u5e38\u7528\u7684\u7c7b\u578b\u8f6c\u6362\u7ed3\u679c\u4f1a\u88ab\u7f13\u5b58</li> <li>\u96f6\u62f7\u8d1d\uff1a\u540c\u7c7b\u578b\u4e0d\u540c\u8bbe\u5907\u95f4\u7684\u8f6c\u6362\u5c3d\u53ef\u80fd\u96f6\u62f7\u8d1d</li> </ul>"},{"location":"core-components/dtypes.zh/#_29","title":"\u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u7d27\u51d1\u5b58\u50a8\uff1a\u4f7f\u7528\u5408\u9002\u7684\u6570\u636e\u7c7b\u578b\u51cf\u5c11\u5185\u5b58\u5360\u7528</li> <li>\u5bf9\u9f50\u4f18\u5316\uff1a\u6570\u636e\u7c7b\u578b\u5bf9\u9f50\u4ee5\u63d0\u9ad8\u8bbf\u95ee\u6548\u7387</li> <li>\u6279\u91cf\u8f6c\u6362\uff1a\u6279\u91cf\u5904\u7406\u7c7b\u578b\u8f6c\u6362\u4ee5\u63d0\u9ad8\u6548\u7387</li> </ul> <p>Genesis\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\u4e3a\u6574\u4e2a\u6846\u67b6\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u9ad8\u6548\u3001\u7c7b\u578b\u5b89\u5168\u7684\u6570\u636e\u8868\u793a\uff0c\u662f\u5b9e\u73b0\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u8de8\u8bbe\u5907\u8ba1\u7b97\u7684\u57fa\u7840\u3002</p>"},{"location":"core-components/dtypes_en/","title":"Data Type System","text":"<p>Genesis implements a unified data type system that provides PyTorch-aligned type management, supporting mixed precision training and cross-device type conversion.</p>"},{"location":"core-components/dtypes_en/#design-goals","title":"\ud83c\udfaf Design Goals","text":"<ul> <li>Unified Interface: CPU and GPU backends use the same type definitions</li> <li>PyTorch Compatibility: Maintain consistency with PyTorch's dtype system</li> <li>Mixed Precision: Seamless support for FP16, BF16 and other mixed precision training</li> <li>Type Safety: Compile-time and runtime type checking</li> </ul>"},{"location":"core-components/dtypes_en/#core-architecture","title":"\ud83c\udfd7\ufe0f Core Architecture","text":"<pre><code>graph TB\n    subgraph \"DType Core Class\"\n        A[DType] --&gt; B[name str]\n        A --&gt; C[itemsize int]\n        A --&gt; D[numpy_dtype]\n        A --&gt; E[triton_name str]\n        A --&gt; F[is_floating_point bool]\n    end\n\n    subgraph \"Predefined Types\"\n        G[Floating Types] --&gt; H[float32]\n        G --&gt; I[float16] \n        G --&gt; J[bfloat16]\n        G --&gt; K[float64]\n\n        L[Integer Types] --&gt; M[int32]\n        L --&gt; N[int64]\n        L --&gt; O[int16]\n        L --&gt; P[int8]\n        L --&gt; Q[uint8]\n\n        R[Boolean Type] --&gt; S[bool]\n    end\n\n    subgraph \"Type Conversion\"\n        T[get_dtype] --&gt; U[String Conversion]\n        T --&gt; V[NumPy Compatibility]\n        T --&gt; W[Type Inference]\n    end\n\n    A --&gt; G\n    A --&gt; L  \n    A --&gt; R\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style L fill:#fff3e0\n    style T fill:#fce4ec</code></pre>"},{"location":"core-components/dtypes_en/#dtype-class-details","title":"\ud83d\udcca DType Class Details","text":""},{"location":"core-components/dtypes_en/#class-definition","title":"Class Definition","text":"Python<pre><code>class DType:\n    \"\"\"Genesis data type, similar to torch.dtype\"\"\"\n\n    def __init__(self, name, itemsize, numpy_dtype, triton_name=None, is_floating_point=None):\n        self.name = name                    # Type name, e.g. \"float32\"\n        self.itemsize = itemsize           # Size in bytes\n        self.numpy_dtype = numpy_dtype     # Corresponding NumPy type\n        self.triton_name = triton_name or name  # Type name in Triton\n\n        # Auto-detect if floating point type\n        if is_floating_point is None:\n            self.is_floating_point = np.issubdtype(numpy_dtype, np.floating)\n        else:\n            self.is_floating_point = is_floating_point\n</code></pre>"},{"location":"core-components/dtypes_en/#core-methods","title":"Core Methods","text":""},{"location":"core-components/dtypes_en/#string-representation","title":"String Representation","text":"Python<pre><code>def __str__(self):\n    return f\"genesis.{self.name}\"\n\ndef __repr__(self):\n    return f\"genesis.{self.name}\"\n\n# Usage example\nprint(genesis.float32)  # Output: genesis.float32\n</code></pre>"},{"location":"core-components/dtypes_en/#equality-comparison","title":"Equality Comparison","text":"Python<pre><code>def __eq__(self, other):\n    if isinstance(other, DType):\n        return self.name == other.name\n    elif isinstance(other, str):\n        return self.name == other  # Backward compatibility for string comparison\n    return False\n\n# Usage examples\ngenesis.float32 == genesis.float32  # True\ngenesis.float32 == \"float32\"        # True (backward compatibility)\ngenesis.float32 == genesis.float16  # False\n</code></pre>"},{"location":"core-components/dtypes_en/#predefined-data-types","title":"\ud83d\udd22 Predefined Data Types","text":""},{"location":"core-components/dtypes_en/#floating-point-types","title":"Floating Point Types","text":"Type Bytes Precision Usage <code>float32</code> 4 Single Default floating type, balanced precision and performance <code>float16</code> 2 Half Mixed precision training, memory saving <code>float64</code> 8 Double High precision computation requirements <code>bfloat16</code> 2 Brain float Google TPU optimized, large dynamic range Python<pre><code># Floating point type definitions\nfloat32 = DType(\"float32\", 4, np.float32)\nfloat16 = DType(\"float16\", 2, np.float16)\nfloat64 = DType(\"float64\", 8, np.float64)\n\n# bfloat16 special handling - Triton supports but NumPy doesn't natively\nbfloat16 = DType(\"bfloat16\", 2, np.float32, \"bfloat16\", is_floating_point=True)\n</code></pre>"},{"location":"core-components/dtypes_en/#integer-types","title":"Integer Types","text":"Type Bytes Range Usage <code>int64</code> 8 -2^63 ~ 2^63-1 Default integer type <code>int32</code> 4 -2^31 ~ 2^31-1 Memory-optimized integer <code>int16</code> 2 -32,768 ~ 32,767 Small integer storage <code>int8</code> 1 -128 ~ 127 Quantized computation <code>uint8</code> 1 0 ~ 255 Image data Python<pre><code># Integer type definitions\nint32 = DType(\"int32\", 4, np.int32)\nint64 = DType(\"int64\", 8, np.int64)\nint16 = DType(\"int16\", 2, np.int16)\nint8 = DType(\"int8\", 1, np.int8)\nuint8 = DType(\"uint8\", 1, np.uint8)\n</code></pre>"},{"location":"core-components/dtypes_en/#boolean-type","title":"Boolean Type","text":"Python<pre><code># Boolean type\nbool = DType(\"bool\", 1, np.bool_, is_floating_point=False)\n</code></pre>"},{"location":"core-components/dtypes_en/#type-conversion-system","title":"\ud83d\udd04 Type Conversion System","text":""},{"location":"core-components/dtypes_en/#core-conversion-function","title":"Core Conversion Function","text":"Python<pre><code>def get_dtype(obj):\n    \"\"\"\n    Convert various type representations to Genesis DType objects\n\n    Supported input types:\n    - DType objects: Return directly\n    - Strings: \"float32\", \"int64\", etc.\n    - NumPy dtype: np.float32, np.int64, etc.\n    - NumPy types: np.float32, np.int64 classes, etc.\n    - None: Return default float32\n    \"\"\"\n    if obj is None:\n        return float32  # Default type\n    elif isinstance(obj, DType):\n        return obj\n    elif isinstance(obj, str):\n        return _name_to_dtype[obj]\n    elif isinstance(obj, np.dtype):\n        return _numpy_to_dtype[obj.type]\n    elif isinstance(obj, type) and issubclass(obj, np.generic):\n        return _numpy_to_dtype[obj]\n    else:\n        raise ValueError(f\"Cannot convert {type(obj)} to Genesis DType: {obj}\")\n</code></pre>"},{"location":"core-components/dtypes_en/#type-mapping-tables","title":"Type Mapping Tables","text":"Python<pre><code># Name to type mapping\n_name_to_dtype = {\n    \"float32\": float32,\n    \"float16\": float16,\n    \"float64\": float64,\n    \"bfloat16\": bfloat16,\n    \"int32\": int32,\n    \"int64\": int64,\n    \"int16\": int16,\n    \"int8\": int8,\n    \"uint8\": uint8,\n    \"bool\": bool,\n}\n\n# NumPy type to Genesis type mapping\n_numpy_to_dtype = {\n    np.float32: float32,\n    np.float16: float16,\n    np.float64: float64,\n    np.int32: int32,\n    np.int64: int64,\n    np.int16: int16,\n    np.int8: int8,\n    np.uint8: uint8,\n    np.bool_: bool,\n}\n</code></pre>"},{"location":"core-components/dtypes_en/#type-checking-tools","title":"\ud83e\uddee Type Checking Tools","text":""},{"location":"core-components/dtypes_en/#floating-point-type-check","title":"Floating Point Type Check","text":"Python<pre><code>def is_floating_point(dtype):\n    \"\"\"Check if it's a floating point type\"\"\"\n    dtype = get_dtype(dtype)\n    return dtype.is_floating_point\n\n# Usage examples\nis_floating_point(genesis.float32)  # True\nis_floating_point(genesis.int32)    # False\nis_floating_point(\"float16\")        # True\n</code></pre>"},{"location":"core-components/dtypes_en/#integer-type-check","title":"Integer Type Check","text":"Python<pre><code>def is_integer(dtype):\n    \"\"\"Check if it's an integer type\"\"\"\n    dtype = get_dtype(dtype)\n    return not dtype.is_floating_point and dtype != bool\n\n# Usage examples\nis_integer(genesis.int32)   # True\nis_integer(genesis.float32) # False\nis_integer(genesis.bool)    # False\n</code></pre>"},{"location":"core-components/dtypes_en/#type-classification","title":"Type Classification","text":"Python<pre><code># All supported types\nall_dtypes = [float32, float16, float64, bfloat16, int32, int64, int16, int8, uint8, bool]\n\n# Floating point type list\nfloating_dtypes = [dt for dt in all_dtypes if dt.is_floating_point]\n# [float32, float16, float64, bfloat16]\n\n# Integer type list\ninteger_dtypes = [dt for dt in all_dtypes if is_integer(dt)]\n# [int32, int64, int16, int8, uint8]\n</code></pre>"},{"location":"core-components/dtypes_en/#mixed-precision-support","title":"\ud83d\udd00 Mixed Precision Support","text":""},{"location":"core-components/dtypes_en/#automatic-type-conversion","title":"Automatic Type Conversion","text":"Python<pre><code>def _cast(value, dtype):\n    \"\"\"Automatic type conversion for mixed precision training\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# Application in autograd\nif genesis.enable_autocast:\n    result = cls.forward(ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/dtypes_en/#type-inference","title":"Type Inference","text":"Python<pre><code>def check_dtype(value, dtype):\n    \"\"\"Recursively check if data structure contains specified type\"\"\"\n    if isinstance(value, Tensor):\n        return value.dtype == dtype\n    elif isinstance(value, dict):\n        return any(check_dtype(k, dtype) or check_dtype(v, dtype) for k, v in value.items())\n    elif isinstance(value, (list, tuple)):\n        return any(check_dtype(v, dtype) for v in value)\n    else:\n        return False\n</code></pre>"},{"location":"core-components/dtypes_en/#usage-examples","title":"\ud83c\udfaf Usage Examples","text":""},{"location":"core-components/dtypes_en/#basic-type-operations","title":"Basic Type Operations","text":"Python<pre><code>import genesis\n\n# Create tensors with different types\nx_f32 = genesis.randn(3, 4, dtype=genesis.float32)\nx_f16 = genesis.randn(3, 4, dtype=genesis.float16)\nx_int = genesis.randint(0, 10, (3, 4), dtype=genesis.int32)\n\n# Check types\nprint(f\"x_f32 type: {x_f32.dtype}\")          # genesis.float32\nprint(f\"Is floating: {x_f32.dtype.is_floating_point}\")  # True\nprint(f\"Byte size: {x_f32.dtype.itemsize}\")          # 4\n</code></pre>"},{"location":"core-components/dtypes_en/#type-conversion","title":"Type Conversion","text":"Python<pre><code># String to type\ndtype1 = genesis.get_dtype(\"float16\")    # genesis.float16\ndtype2 = genesis.get_dtype(np.float32)   # genesis.float32\ndtype3 = genesis.get_dtype(None)         # genesis.float32 (default)\n\n# Tensor type conversion\nx = genesis.randn(3, 4, dtype=\"float32\")\nx_half = x.half()      # Convert to float16\nx_float = x.float()    # Convert to float32\n</code></pre>"},{"location":"core-components/dtypes_en/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable mixed precision\ngenesis.enable_autocast = True\n\n# Model will automatically convert between fp16 and fp32\nimport genesis.nn as nn\n\nmodel = nn.Linear(784, 128)\nx = genesis.randn(32, 784, dtype=genesis.float16)\n\n# Automatic type conversion handling during forward pass\noutput = model(x)\n</code></pre>"},{"location":"core-components/dtypes_en/#cross-device-type-consistency","title":"Cross-Device Type Consistency","text":"Python<pre><code># CPU and GPU use the same type system\ncpu_tensor = genesis.randn(3, 4, device=\"cpu\", dtype=genesis.float32)\ngpu_tensor = genesis.randn(3, 4, device=\"cuda\", dtype=genesis.float32)\n\nprint(cpu_tensor.dtype == gpu_tensor.dtype)  # True\nprint(cpu_tensor.dtype.name)                 # \"float32\"\nprint(gpu_tensor.dtype.name)                 # \"float32\"\n</code></pre>"},{"location":"core-components/dtypes_en/#bfloat16-special-handling","title":"bfloat16 Special Handling","text":"Python<pre><code># bfloat16 handling across different backends\nx_bf16 = genesis.randn(3, 4, dtype=genesis.bfloat16)\n\n# CPU backend: Uses float32 storage but marked as bfloat16\n# GPU backend: Native bfloat16 support (if hardware supports)\nprint(f\"Type name: {x_bf16.dtype.name}\")           # \"bfloat16\"\nprint(f\"Triton name: {x_bf16.dtype.triton_name}\")  # \"bfloat16\"\nprint(f\"NumPy type: {x_bf16.dtype.numpy_dtype}\") # &lt;class 'numpy.float32'&gt;\n</code></pre>"},{"location":"core-components/dtypes_en/#performance-optimization","title":"\ud83d\ude80 Performance Optimization","text":""},{"location":"core-components/dtypes_en/#type-conversion-optimization","title":"Type Conversion Optimization","text":"<ul> <li>Lazy Conversion: Type conversion only occurs when truly needed</li> <li>Caching Mechanism: Common type conversion results are cached</li> <li>Zero-Copy: Cross-device conversions of the same type attempt zero-copy when possible</li> </ul>"},{"location":"core-components/dtypes_en/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Compact Storage: Use appropriate data types to reduce memory usage</li> <li>Alignment Optimization: Data type alignment to improve access efficiency</li> <li>Batch Conversion: Batch processing of type conversions to improve efficiency</li> </ul> <p>Genesis's data type system provides unified, efficient, type-safe data representation for the entire framework, serving as the foundation for mixed precision training and cross-device computation.</p>"},{"location":"core-components/index.zh/","title":"\u6838\u5fc3\u7ec4\u4ef6\u6982\u8ff0","text":"<p>Genesis\u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u5f20\u91cf\u7cfb\u7edf\u3001\u81ea\u52a8\u5fae\u5206\u5f15\u64ce\u3001\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\u548c\u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3\u3002</p>"},{"location":"core-components/index.zh/#_2","title":"\ud83e\udde9 \u7ec4\u4ef6\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u6838\u5fc3\u7ec4\u4ef6\"\n        A[Tensor\u5f20\u91cf] --&gt; B[\u81ea\u52a8\u5fae\u5206\u5f15\u64ce]\n        C[\u6570\u636e\u7c7b\u578b\u7cfb\u7edf] --&gt; A\n        D[\u51fd\u6570\u5f0f\u64cd\u4f5c] --&gt; A\n        E[\u521d\u59cb\u5316\u51fd\u6570] --&gt; A\n    end\n\n    subgraph \"\u81ea\u52a8\u5fae\u5206\u8be6\u7ec6\"\n        B --&gt; F[Function\u57fa\u7c7b]\n        B --&gt; G[Context\u4e0a\u4e0b\u6587]\n        B --&gt; H[\u8ba1\u7b97\u56fe\u6784\u5efa]\n        B --&gt; I[\u53cd\u5411\u4f20\u64ad]\n    end\n\n    subgraph \"\u7c7b\u578b\u7cfb\u7edf\"\n        C --&gt; J[DType\u7c7b]\n        C --&gt; K[\u7c7b\u578b\u8f6c\u6362]\n        C --&gt; L[\u7cbe\u5ea6\u7ba1\u7406]\n    end\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0</code></pre>"},{"location":"core-components/index.zh/#_3","title":"\ud83c\udfaf \u6838\u5fc3\u7ec4\u4ef6\u6e05\u5355","text":"\u7ec4\u4ef6 \u6587\u4ef6 \u4e3b\u8981\u529f\u80fd \u5f20\u91cf\u7cfb\u7edf <code>autograd.py</code> \u57fa\u7840\u6570\u636e\u7ed3\u6784\u3001\u81ea\u52a8\u5fae\u5206 \u6570\u636e\u7c7b\u578b <code>dtypes.py</code> \u7edf\u4e00\u7c7b\u578b\u7cfb\u7edf\u3001\u7cbe\u5ea6\u7ba1\u7406 \u51fd\u6570\u5f0f\u64cd\u4f5c <code>functional.py</code> \u5f20\u91cf\u64cd\u4f5c\u7684\u51fd\u6570\u5f0f\u63a5\u53e3 \u521d\u59cb\u5316 <code>init.py</code> \u5f20\u91cf\u521b\u5efa\u548c\u521d\u59cb\u5316 \u540e\u7aef\u62bd\u8c61 <code>backend.py</code> \u8bbe\u5907\u548c\u540e\u7aef\u7ba1\u7406"},{"location":"core-components/index.zh/#_4","title":"\ud83d\ude80 \u8bbe\u8ba1\u7279\u8272","text":""},{"location":"core-components/index.zh/#1","title":"1. \u7edf\u4e00\u7684\u5f20\u91cf\u63a5\u53e3","text":"<ul> <li>\u4e00\u81f4\u7684API\uff1a\u65e0\u8bbaCPU\u8fd8\u662fGPU\uff0c\u7528\u6237\u4f7f\u7528\u76f8\u540c\u7684\u63a5\u53e3</li> <li>\u900f\u660e\u7684\u8bbe\u5907\u5207\u6362\uff1a\u81ea\u52a8\u5904\u7406\u4e0d\u540c\u8bbe\u5907\u95f4\u7684\u6570\u636e\u8f6c\u6362</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u7684\u7c7b\u578b\u68c0\u67e5</li> </ul>"},{"location":"core-components/index.zh/#2","title":"2. \u9ad8\u6548\u7684\u81ea\u52a8\u5fae\u5206","text":"<ul> <li>\u60f0\u6027\u8ba1\u7b97\u56fe\uff1a\u6309\u9700\u6784\u5efa\u8ba1\u7b97\u56fe\uff0c\u8282\u7701\u5185\u5b58</li> <li>\u667a\u80fd\u68af\u5ea6\u4f20\u64ad\uff1a\u4f18\u5316\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5</li> <li>\u5185\u5b58\u4f18\u5316\uff1a\u81ea\u52a8\u91ca\u653e\u4e0d\u518d\u9700\u8981\u7684\u4e2d\u95f4\u7ed3\u679c</li> </ul>"},{"location":"core-components/index.zh/#3","title":"3. \u7075\u6d3b\u7684\u7c7b\u578b\u7cfb\u7edf","text":"<ul> <li>\u6df7\u5408\u7cbe\u5ea6\u652f\u6301\uff1a\u81ea\u52a8\u5728FP32\u548cFP16\u95f4\u8f6c\u6362</li> <li>\u8bbe\u5907\u65e0\u5173\uff1a\u7c7b\u578b\u5b9a\u4e49\u72ec\u7acb\u4e8e\u5177\u4f53\u8bbe\u5907</li> <li>NumPy\u517c\u5bb9\uff1a\u65e0\u7f1d\u5bf9\u63a5NumPy\u751f\u6001</li> </ul>"},{"location":"core-components/index.zh/#_5","title":"\ud83d\udcca \u6027\u80fd\u7279\u6027","text":""},{"location":"core-components/index.zh/#_6","title":"\u5185\u5b58\u6548\u7387","text":"<ul> <li>\u89c6\u56fe\u64cd\u4f5c\u96f6\u62f7\u8d1d\uff1areshape\u3001transpose\u7b49\u64cd\u4f5c\u4e0d\u590d\u5236\u6570\u636e</li> <li>\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff1a\u57fa\u4e8e\u5f15\u7528\u8ba1\u6570\u7684\u81ea\u52a8\u5185\u5b58\u91ca\u653e</li> <li>\u68af\u5ea6\u7d2f\u79ef\u4f18\u5316\uff1a\u51cf\u5c11\u4e34\u65f6\u5f20\u91cf\u521b\u5efa</li> </ul>"},{"location":"core-components/index.zh/#_7","title":"\u8ba1\u7b97\u4f18\u5316","text":"<ul> <li>\u5ef6\u8fdf\u6267\u884c\uff1a\u64cd\u4f5c\u5728\u9700\u8981\u65f6\u624d\u771f\u6b63\u6267\u884c</li> <li>\u878d\u5408\u4f18\u5316\uff1a\u76f8\u90bb\u64cd\u4f5c\u81ea\u52a8\u878d\u5408\u4ee5\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee</li> <li>\u5e76\u884c\u8ba1\u7b97\uff1a\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u80fd\u529b</li> </ul>"},{"location":"core-components/index.zh/#_8","title":"\ud83d\udd17 \u7ec4\u4ef6\u95f4\u534f\u4f5c","text":""},{"location":"core-components/index.zh/#_9","title":"\u5f20\u91cf\u521b\u5efa\u6d41\u7a0b","text":"Python<pre><code># \u7528\u6237\u8c03\u7528\nx = genesis.randn(3, 4)\n\n# \u5185\u90e8\u6d41\u7a0b\ninit.randn() -&gt; \nNDArray.randn() -&gt; \nDevice.randn() -&gt; \nTensor.__init__() -&gt;\n\u8bbe\u7f6erequires_grad\u7b49\u5c5e\u6027\n</code></pre>"},{"location":"core-components/index.zh/#_10","title":"\u81ea\u52a8\u5fae\u5206\u6d41\u7a0b","text":"Python<pre><code># \u524d\u5411\u4f20\u64ad\nz = x * y + x.sum()\n\n# \u6784\u5efa\u8ba1\u7b97\u56fe\nMulFunction.apply(x, y) -&gt; \nSumFunction.apply(x) -&gt;\nAddFunction.apply(mul_result, sum_result) -&gt;\n\u8bbe\u7f6ecreator\u5173\u7cfb\n\n# \u53cd\u5411\u4f20\u64ad\nz.backward()\n\n# \u8ba1\u7b97\u68af\u5ea6\ntopo_sort(z) -&gt;\n\u9006\u62d3\u6251\u5e8f\u904d\u5386 -&gt;\n\u8c03\u7528\u5404Function\u7684backward() -&gt;\n\u68af\u5ea6\u7d2f\u79ef\u5230\u53f6\u5b50\u8282\u70b9\n</code></pre>"},{"location":"core-components/index.zh/#_11","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84\u5efa\u8bae","text":""},{"location":"core-components/index.zh/#_12","title":"\u521d\u7ea7\u7528\u6237","text":"<ol> <li>\u5f20\u91cf\u57fa\u7840 - \u4e86\u89e3Tensor\u7684\u521b\u5efa\u548c\u57fa\u672c\u64cd\u4f5c</li> <li>\u81ea\u52a8\u5fae\u5206 - \u7406\u89e3requires_grad\u548cbackward()</li> <li>\u8bbe\u5907\u7ba1\u7406 - \u5b66\u4e60CPU/GPU\u5207\u6362</li> </ol>"},{"location":"core-components/index.zh/#_13","title":"\u4e2d\u7ea7\u7528\u6237","text":"<ol> <li>\u6570\u636e\u7c7b\u578b - \u638c\u63e1\u4e0d\u540c\u7cbe\u5ea6\u7684\u4f7f\u7528\u573a\u666f</li> <li>\u51fd\u6570\u5f0f\u63a5\u53e3 - \u4f7f\u7528functional\u6a21\u5757</li> <li>\u5185\u5b58\u4f18\u5316 - \u7406\u89e3\u89c6\u56fe\u64cd\u4f5c\u548c\u5185\u5b58\u7ba1\u7406</li> </ol>"},{"location":"core-components/index.zh/#_14","title":"\u9ad8\u7ea7\u7528\u6237","text":"<ol> <li>\u81ea\u5b9a\u4e49Function - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u7684\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6548\u7387</li> <li>\u6e90\u7801\u7406\u89e3 - \u6df1\u5165\u7406\u89e3\u5404\u7ec4\u4ef6\u7684\u5b9e\u73b0\u7ec6\u8282</li> </ol> <p>\u5404\u7ec4\u4ef6\u7684\u8be6\u7ec6\u6587\u6863\u8bf7\u67e5\u770b\u5bf9\u5e94\u7684\u4e13\u95e8\u9875\u9762\uff1a</p> <ul> <li>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf - \u6df1\u5165\u7406\u89e3\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u8ba1\u7b97</li> <li>\u5f20\u91cf\u64cd\u4f5c - \u5168\u9762\u7684\u5f20\u91cf\u64cd\u4f5c\u6307\u5357  </li> <li>\u6570\u636e\u7c7b\u578b - \u7c7b\u578b\u7cfb\u7edf\u548c\u7cbe\u5ea6\u7ba1\u7406</li> <li>\u51fd\u6570\u5f0f\u63a5\u53e3 - \u51fd\u6570\u5f0f\u7f16\u7a0b\u98ce\u683c\u7684\u64cd\u4f5c</li> </ul>"},{"location":"core-components/index_en/","title":"Core Components Overview","text":"<p>Genesis framework's core components provide the infrastructure for deep learning computation, including tensor systems, automatic differentiation engine, data type system, and functional operation interfaces.</p>"},{"location":"core-components/index_en/#component-architecture","title":"\ud83e\udde9 Component Architecture","text":"<pre><code>graph TB\n    subgraph \"Core Components\"\n        A[Tensor] --&gt; B[Autograd Engine]\n        C[Data Type System] --&gt; A\n        D[Functional Operations] --&gt; A\n        E[Initialization Functions] --&gt; A\n    end\n\n    subgraph \"Autograd Details\"\n        B --&gt; F[Function Base Class]\n        B --&gt; G[Context]\n        B --&gt; H[Computation Graph Building]\n        B --&gt; I[Backpropagation]\n    end\n\n    subgraph \"Type System\"\n        C --&gt; J[DType Class]\n        C --&gt; K[Type Conversion]\n        C --&gt; L[Precision Management]\n    end\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0</code></pre>"},{"location":"core-components/index_en/#core-component-list","title":"\ud83c\udfaf Core Component List","text":"Component File Main Functions Tensor System <code>autograd.py</code> Basic data structures, automatic differentiation Data Types <code>dtypes.py</code> Unified type system, precision management Functional Operations <code>functional.py</code> Functional interface for tensor operations Initialization <code>init.py</code> Tensor creation and initialization Backend Abstraction <code>backend.py</code> Device and backend management"},{"location":"core-components/index_en/#design-features","title":"\ud83d\ude80 Design Features","text":""},{"location":"core-components/index_en/#1-unified-tensor-interface","title":"1. Unified Tensor Interface","text":"<ul> <li>Consistent API: Users use the same interface whether on CPU or GPU</li> <li>Transparent Device Switching: Automatic handling of data conversion between different devices</li> <li>Type Safety: Compile-time and runtime type checking</li> </ul>"},{"location":"core-components/index_en/#2-efficient-automatic-differentiation","title":"2. Efficient Automatic Differentiation","text":"<ul> <li>Lazy Computation Graph: Build computation graph on demand to save memory</li> <li>Smart Gradient Propagation: Optimized backpropagation algorithm</li> <li>Memory Optimization: Automatic release of intermediate results no longer needed</li> </ul>"},{"location":"core-components/index_en/#3-flexible-type-system","title":"3. Flexible Type System","text":"<ul> <li>Mixed Precision Support: Automatic conversion between FP32 and FP16</li> <li>Device Agnostic: Type definitions independent of specific devices</li> <li>NumPy Compatible: Seamless integration with NumPy ecosystem</li> </ul>"},{"location":"core-components/index_en/#performance-characteristics","title":"\ud83d\udcca Performance Characteristics","text":""},{"location":"core-components/index_en/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Zero-copy View Operations: Operations like reshape, transpose don't copy data</li> <li>Smart Memory Management: Automatic memory release based on reference counting</li> <li>Gradient Accumulation Optimization: Reduce temporary tensor creation</li> </ul>"},{"location":"core-components/index_en/#compute-optimization","title":"Compute Optimization","text":"<ul> <li>Lazy Execution: Operations execute only when needed</li> <li>Fusion Optimization: Adjacent operations automatically fused to reduce memory access</li> <li>Parallel Computing: Full utilization of GPU parallel capabilities</li> </ul>"},{"location":"core-components/index_en/#component-collaboration","title":"\ud83d\udd17 Component Collaboration","text":""},{"location":"core-components/index_en/#tensor-creation-process","title":"Tensor Creation Process","text":"Python<pre><code># User call\nx = genesis.randn(3, 4)\n\n# Internal flow\ninit.randn() -&gt; \nNDArray.randn() -&gt; \nDevice.randn() -&gt; \nTensor.__init__() -&gt;\nSet attributes like requires_grad\n</code></pre>"},{"location":"core-components/index_en/#automatic-differentiation-process","title":"Automatic Differentiation Process","text":"Python<pre><code># Forward pass\nz = x * y + x.sum()\n\n# Build computation graph\nMulFunction.apply(x, y) -&gt; \nSumFunction.apply(x) -&gt;\nAddFunction.apply(mul_result, sum_result) -&gt;\nSet creator relationships\n\n# Backward pass\nz.backward()\n\n# Compute gradients\ntopo_sort(z) -&gt;\nReverse topological traversal -&gt;\nCall backward() of each Function -&gt;\nGradient accumulation to leaf nodes\n</code></pre>"},{"location":"core-components/index_en/#learning-path-recommendations","title":"\ud83c\udf93 Learning Path Recommendations","text":""},{"location":"core-components/index_en/#beginner-users","title":"Beginner Users","text":"<ol> <li>Tensor Basics - Understand Tensor creation and basic operations</li> <li>Automatic Differentiation - Understand requires_grad and backward()</li> <li>Device Management - Learn CPU/GPU switching</li> </ol>"},{"location":"core-components/index_en/#intermediate-users","title":"Intermediate Users","text":"<ol> <li>Data Types - Master usage scenarios for different precisions</li> <li>Functional Interface - Use the functional module</li> <li>Memory Optimization - Understand view operations and memory management</li> </ol>"},{"location":"core-components/index_en/#advanced-users","title":"Advanced Users","text":"<ol> <li>Custom Functions - Implement custom forward and backward propagation</li> <li>Performance Tuning - Optimize memory usage and computational efficiency</li> <li>Source Code Understanding - Deep understanding of component implementation details</li> </ol> <p>For detailed documentation of each component, please check the corresponding dedicated pages:</p> <ul> <li>Automatic Differentiation System - Deep understanding of computation graphs and gradient computation</li> <li>Tensor Operations - Comprehensive tensor operation guide  </li> <li>Data Types - Type system and precision management</li> <li>Functional Interface - Functional programming style operations</li> </ul>"},{"location":"getting-started/first-steps.zh/","title":"\u7b2c\u4e00\u4e2a\u5b8c\u6574\u7a0b\u5e8f","text":"<p>\u5b8c\u6210\u5b89\u88c5\u540e\uff0c\u8ba9\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u5b8c\u6574\u7684\u4f8b\u5b50\u6765\u5b66\u4e60Genesis\u7684\u57fa\u672c\u7528\u6cd5\u3002\u6211\u4eec\u5c06\u5b9e\u73b0\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668\u6765\u6f14\u793a\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u3002</p>"},{"location":"getting-started/first-steps.zh/#_2","title":"\ud83c\udfaf \u9879\u76ee\u76ee\u6807","text":"<p>\u6784\u5efa\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u8bc6\u522b\u5668\uff08\u7c7b\u4f3cMNIST\uff09\uff0c\u5b66\u4e60Genesis\u7684\u6838\u5fc3\u6982\u5ff5\uff1a</p> <ul> <li>\u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406</li> <li>\u6a21\u578b\u5b9a\u4e49\u548c\u521d\u59cb\u5316</li> <li>\u8bad\u7ec3\u5faa\u73af\u548c\u9a8c\u8bc1</li> <li>\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d</li> </ul>"},{"location":"getting-started/first-steps.zh/#_3","title":"\ud83d\udcca \u9879\u76ee\u7ed3\u6784","text":"<p>\u521b\u5efa\u9879\u76ee\u76ee\u5f55\u7ed3\u6784\uff1a</p> Text Only<pre><code>first_project/\n\u251c\u2500\u2500 data/                # \u6570\u636e\u76ee\u5f55\n\u251c\u2500\u2500 models/              # \u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\n\u251c\u2500\u2500 train.py            # \u8bad\u7ec3\u811a\u672c\n\u251c\u2500\u2500 model.py            # \u6a21\u578b\u5b9a\u4e49\n\u251c\u2500\u2500 dataset.py          # \u6570\u636e\u52a0\u8f7d\n\u2514\u2500\u2500 utils.py            # \u5de5\u5177\u51fd\u6570\n</code></pre>"},{"location":"getting-started/first-steps.zh/#1-datasetpy","title":"\ud83d\udcc1 1. \u6570\u636e\u5904\u7406 (<code>dataset.py</code>)","text":"Python<pre><code>\"\"\"\u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406\u6a21\u5757\"\"\"\nimport genesis\nimport numpy as np\nfrom typing import Tuple, List\nimport pickle\nimport os\n\nclass SimpleDataset:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(self, data: np.ndarray, labels: np.ndarray, transform=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u6570\u636e\u96c6\n\n        Args:\n            data: \u8f93\u5165\u6570\u636e (N, H, W) \u6216 (N, D)\n            labels: \u6807\u7b7e (N,)\n            transform: \u6570\u636e\u53d8\u6362\u51fd\u6570\n        \"\"\"\n        self.data = data.astype(np.float32)\n        self.labels = labels.astype(np.int64)\n        self.transform = transform\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[genesis.Tensor, genesis.Tensor]:\n        \"\"\"\u83b7\u53d6\u5355\u4e2a\u6837\u672c\"\"\"\n        x = self.data[idx]\n        y = self.labels[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return genesis.tensor(x), genesis.tensor(y)\n\nclass DataLoader:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u52a0\u8f7d\u5668\"\"\"\n\n    def __init__(self, dataset: SimpleDataset, batch_size: int = 32, shuffle: bool = True):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self._reset_indices()\n\n    def _reset_indices(self):\n        \"\"\"\u91cd\u7f6e\u7d22\u5f15\"\"\"\n        self.indices = np.arange(len(self.dataset))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.current = 0\n\n    def __iter__(self):\n        self._reset_indices()\n        return self\n\n    def __next__(self):\n        if self.current &gt;= len(self.dataset):\n            raise StopIteration\n\n        # \u83b7\u53d6\u5f53\u524d\u6279\u6b21\u7684\u7d22\u5f15\n        end_idx = min(self.current + self.batch_size, len(self.dataset))\n        batch_indices = self.indices[self.current:end_idx]\n\n        # \u6536\u96c6\u6279\u6b21\u6570\u636e\n        batch_data = []\n        batch_labels = []\n\n        for idx in batch_indices:\n            x, y = self.dataset[idx]\n            batch_data.append(x)\n            batch_labels.append(y)\n\n        self.current = end_idx\n\n        # \u5806\u53e0\u6210\u6279\u6b21\n        batch_x = genesis.stack(batch_data, dim=0)\n        batch_y = genesis.stack(batch_labels, dim=0)\n\n        return batch_x, batch_y\n\ndef create_synthetic_data(n_samples: int = 1000, n_features: int = 784, n_classes: int = 10) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\u521b\u5efa\u5408\u6210\u6570\u636e\u7528\u4e8e\u6f14\u793a\"\"\"\n    np.random.seed(42)\n\n    # \u751f\u6210\u968f\u673a\u6570\u636e\n    data = np.random.randn(n_samples, n_features).astype(np.float32)\n\n    # \u4e3a\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u4e00\u4e9b\u6a21\u5f0f\n    labels = np.random.randint(0, n_classes, n_samples)\n    for i in range(n_classes):\n        mask = labels == i\n        # \u7ed9\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u7279\u5b9a\u7684\u504f\u7f6e\n        data[mask] += np.random.randn(n_features) * 0.5\n\n    return data, labels\n\ndef load_data() -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\u52a0\u8f7d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\"\"\"\n    print(\"\ud83d\udd04 \u52a0\u8f7d\u6570\u636e...\")\n\n    # \u521b\u5efa\u5408\u6210\u6570\u636e (\u5b9e\u9645\u9879\u76ee\u4e2d\u66ff\u6362\u4e3a\u771f\u5b9e\u6570\u636e)\n    train_data, train_labels = create_synthetic_data(800, 784, 10)\n    val_data, val_labels = create_synthetic_data(200, 784, 10)\n\n    # \u6570\u636e\u6807\u51c6\u5316\n    def normalize(x):\n        return (x - x.mean()) / (x.std() + 1e-8)\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    train_dataset = SimpleDataset(train_data, train_labels, transform=normalize)\n    val_dataset = SimpleDataset(val_data, val_labels, transform=normalize)\n\n    # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n    print(f\"\u2705 \u6570\u636e\u52a0\u8f7d\u5b8c\u6210 - \u8bad\u7ec3\u96c6: {len(train_dataset)}, \u9a8c\u8bc1\u96c6: {len(val_dataset)}\")\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"getting-started/first-steps.zh/#2-modelpy","title":"\ud83e\udde0 2. \u6a21\u578b\u5b9a\u4e49 (<code>model.py</code>)","text":"Python<pre><code>\"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9a\u4e49\"\"\"\nimport genesis\nimport genesis.nn as nn\nimport genesis.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\"\"\"\n\n    def __init__(self, input_dim: int = 784, hidden_dims: list = None, num_classes: int = 10, dropout_rate: float = 0.2):\n        \"\"\"\n        \u521d\u59cb\u5316MLP\u6a21\u578b\n\n        Args:\n            input_dim: \u8f93\u5165\u7ef4\u5ea6\n            hidden_dims: \u9690\u85cf\u5c42\u7ef4\u5ea6\u5217\u8868\n            num_classes: \u5206\u7c7b\u6570\u91cf\n            dropout_rate: Dropout\u6bd4\u7387\n        \"\"\"\n        super().__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [512, 256, 128]\n\n        # \u6784\u5efa\u7f51\u7edc\u5c42\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = hidden_dim\n\n        # \u8f93\u51fa\u5c42\n        layers.append(nn.Linear(prev_dim, num_classes))\n\n        self.network = nn.Sequential(*layers)\n\n        # \u521d\u59cb\u5316\u6743\u91cd\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"\u6743\u91cd\u521d\u59cb\u5316\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                # Xavier\u521d\u59cb\u5316\n                std = (2.0 / (module.in_features + module.out_features)) ** 0.5\n                module.weight.data.normal_(0, std)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        \"\"\"\u524d\u5411\u4f20\u64ad\"\"\"\n        # \u5c55\u5e73\u8f93\u5165 (\u5982\u679c\u662f\u56fe\u50cf\u6570\u636e)\n        if x.dim() &gt; 2:\n            x = x.view(x.size(0), -1)\n\n        return self.network(x)\n\nclass CNN(nn.Module):\n    \"\"\"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668 (\u5982\u679c\u9700\u8981\u5904\u7406\u56fe\u50cf)\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n\n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # \u6c60\u5316\u5c42\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # \u5047\u8bbe\u8f93\u5165\u662f28x28\n        self.fc2 = nn.Linear(512, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        # \u5377\u79ef + \u6c60\u5316\n        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -&gt; 7x7  \n        x = self.pool(F.relu(self.conv3(x)))  # 7x7 -&gt; 3x3\n\n        # \u5c55\u5e73\n        x = x.view(x.size(0), -1)\n\n        # \u5168\u8fde\u63a5\u5c42\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\ndef create_model(model_type: str = \"mlp\", **kwargs) -&gt; nn.Module:\n    \"\"\"\u5de5\u5382\u51fd\u6570\uff1a\u521b\u5efa\u6a21\u578b\"\"\"\n    if model_type.lower() == \"mlp\":\n        return MLP(**kwargs)\n    elif model_type.lower() == \"cnn\":\n        return CNN(**kwargs)\n    else:\n        raise ValueError(f\"\u672a\u77e5\u7684\u6a21\u578b\u7c7b\u578b: {model_type}\")\n</code></pre>"},{"location":"getting-started/first-steps.zh/#3-utilspy","title":"\ud83d\udee0\ufe0f 3. \u5de5\u5177\u51fd\u6570 (<code>utils.py</code>)","text":"Python<pre><code>\"\"\"\u5de5\u5177\u51fd\u6570\u6a21\u5757\"\"\"\nimport genesis\nimport time\nimport os\nfrom typing import Dict, Any\nimport json\n\nclass AverageMeter:\n    \"\"\"\u5e73\u5747\u503c\u8ba1\u7b97\u5668\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Timer:\n    \"\"\"\u8ba1\u65f6\u5668\"\"\"\n\n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self):\n        self.end_time = time.time()\n        return self.end_time - self.start_time\n\n    def elapsed(self):\n        if self.start_time is None:\n            return 0\n        return time.time() - self.start_time\n\ndef accuracy(output: genesis.Tensor, target: genesis.Tensor, topk: tuple = (1,)) -&gt; list:\n    \"\"\"\u8ba1\u7b97\u51c6\u786e\u7387\"\"\"\n    with genesis.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n\n        return res\n\ndef save_checkpoint(model: genesis.nn.Module, optimizer: genesis.optim.Optimizer, \n                   epoch: int, loss: float, accuracy: float, filepath: str):\n    \"\"\"\u4fdd\u5b58\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'accuracy': accuracy\n    }\n\n    # \u786e\u4fdd\u76ee\u5f55\u5b58\u5728\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    genesis.save(checkpoint, filepath)\n    print(f\"\ud83d\udcbe \u68c0\u67e5\u70b9\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef load_checkpoint(filepath: str, model: genesis.nn.Module, optimizer: genesis.optim.Optimizer = None) -&gt; Dict[str, Any]:\n    \"\"\"\u52a0\u8f7d\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = genesis.load(filepath)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if optimizer and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    print(f\"\ud83d\udcc1 \u68c0\u67e5\u70b9\u5df2\u52a0\u8f7d: {filepath}\")\n    print(f\"   \u8f6e\u6b21: {checkpoint['epoch']}, \u635f\u5931: {checkpoint['loss']:.4f}, \u51c6\u786e\u7387: {checkpoint['accuracy']:.2f}%\")\n\n    return checkpoint\n\ndef save_training_history(history: Dict[str, list], filepath: str):\n    \"\"\"\u4fdd\u5b58\u8bad\u7ec3\u5386\u53f2\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    with open(filepath, 'w') as f:\n        json.dump(history, f, indent=2)\n\n    print(f\"\ud83d\udcca \u8bad\u7ec3\u5386\u53f2\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef print_model_summary(model: genesis.nn.Module, input_shape: tuple):\n    \"\"\"\u6253\u5370\u6a21\u578b\u6458\u8981\"\"\"\n    print(\"\ud83c\udfd7\ufe0f  \u6a21\u578b\u67b6\u6784:\")\n    print(\"=\" * 50)\n\n    # \u8ba1\u7b97\u53c2\u6570\u6570\u91cf\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"\u603b\u53c2\u6570\u91cf: {total_params:,}\")\n    print(f\"\u53ef\u8bad\u7ec3\u53c2\u6570: {trainable_params:,}\")\n    print(f\"\u8f93\u5165\u5f62\u72b6: {input_shape}\")\n\n    # \u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\n    dummy_input = genesis.randn(*input_shape)\n    try:\n        with genesis.no_grad():\n            output = model(dummy_input)\n        print(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")\n    except Exception as e:\n        print(f\"\u524d\u5411\u4f20\u64ad\u6d4b\u8bd5\u5931\u8d25: {e}\")\n\n    print(\"=\" * 50)\n</code></pre>"},{"location":"getting-started/first-steps.zh/#4-trainpy","title":"\ud83d\ude82 4. \u8bad\u7ec3\u811a\u672c (<code>train.py</code>)","text":"<p> [{\"id\": \"1\", \"content\": \"\\u521b\\u5efa\\u6587\\u6863\\u76ee\\u5f55\\u7ed3\\u6784\\u548c\\u914d\\u7f6e\\u6587\\u4ef6\", \"status\": \"completed\"}, {\"id\": \"2\", \"content\": \"\\u7f16\\u5199\\u9996\\u9875\\u548c\\u5feb\\u901f\\u5f00\\u59cb\\u6587\\u6863\", \"status\": \"completed\"}, {\"id\": \"3\", \"content\": \"\\u7f16\\u5199\\u67b6\\u6784\\u8bbe\\u8ba1\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"4\", \"content\": \"\\u7f16\\u5199\\u6838\\u5fc3\\u7ec4\\u4ef6\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"5\", \"content\": \"\\u7f16\\u5199\\u795e\\u7ecf\\u7f51\\u7edc\\u6a21\\u5757\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"6\", \"content\": \"\\u7f16\\u5199API\\u53c2\\u8003\\u6587\\u6863\", \"status\": \"pending\"}]"},{"location":"getting-started/first-steps_en/","title":"Your First Complete Program","text":"<p>After completing the installation, let's learn Genesis basics through a complete example. We will implement an image classifier to demonstrate the complete deep learning workflow.</p>"},{"location":"getting-started/first-steps_en/#project-goals","title":"\ud83c\udfaf Project Goals","text":"<p>Build a handwritten digit recognizer (MNIST-like) to learn Genesis core concepts:</p> <ul> <li>Data loading and preprocessing</li> <li>Model definition and initialization</li> <li>Training loop and validation</li> <li>Model saving and loading</li> </ul>"},{"location":"getting-started/first-steps_en/#project-structure","title":"\ud83d\udcca Project Structure","text":"<p>Create the project directory structure:</p> Text Only<pre><code>first_project/\n\u251c\u2500\u2500 data/                # Data directory\n\u251c\u2500\u2500 models/              # Model save directory\n\u251c\u2500\u2500 train.py            # Training script\n\u251c\u2500\u2500 model.py            # Model definition\n\u251c\u2500\u2500 dataset.py          # Data loading\n\u2514\u2500\u2500 utils.py            # Utility functions\n</code></pre>"},{"location":"getting-started/first-steps_en/#1-data-processing-datasetpy","title":"\ud83d\udcc1 1. Data Processing (<code>dataset.py</code>)","text":"Python<pre><code>\"\"\"\u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406\u6a21\u5757\"\"\"\nimport genesis\nimport numpy as np\nfrom typing import Tuple, List\nimport pickle\nimport os\n\nclass SimpleDataset:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(self, data: np.ndarray, labels: np.ndarray, transform=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u6570\u636e\u96c6\n\n        Args:\n            data: \u8f93\u5165\u6570\u636e (N, H, W) \u6216 (N, D)\n            labels: \u6807\u7b7e (N,)\n            transform: \u6570\u636e\u53d8\u6362\u51fd\u6570\n        \"\"\"\n        self.data = data.astype(np.float32)\n        self.labels = labels.astype(np.int64)\n        self.transform = transform\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[genesis.Tensor, genesis.Tensor]:\n        \"\"\"\u83b7\u53d6\u5355\u4e2a\u6837\u672c\"\"\"\n        x = self.data[idx]\n        y = self.labels[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return genesis.tensor(x), genesis.tensor(y)\n\nclass DataLoader:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u52a0\u8f7d\u5668\"\"\"\n\n    def __init__(self, dataset: SimpleDataset, batch_size: int = 32, shuffle: bool = True):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self._reset_indices()\n\n    def _reset_indices(self):\n        \"\"\"\u91cd\u7f6e\u7d22\u5f15\"\"\"\n        self.indices = np.arange(len(self.dataset))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.current = 0\n\n    def __iter__(self):\n        self._reset_indices()\n        return self\n\n    def __next__(self):\n        if self.current &gt;= len(self.dataset):\n            raise StopIteration\n\n        # \u83b7\u53d6\u5f53\u524d\u6279\u6b21\u7684\u7d22\u5f15\n        end_idx = min(self.current + self.batch_size, len(self.dataset))\n        batch_indices = self.indices[self.current:end_idx]\n\n        # \u6536\u96c6\u6279\u6b21\u6570\u636e\n        batch_data = []\n        batch_labels = []\n\n        for idx in batch_indices:\n            x, y = self.dataset[idx]\n            batch_data.append(x)\n            batch_labels.append(y)\n\n        self.current = end_idx\n\n        # \u5806\u53e0\u6210\u6279\u6b21\n        batch_x = genesis.stack(batch_data, dim=0)\n        batch_y = genesis.stack(batch_labels, dim=0)\n\n        return batch_x, batch_y\n\ndef create_synthetic_data(n_samples: int = 1000, n_features: int = 784, n_classes: int = 10) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\u521b\u5efa\u5408\u6210\u6570\u636e\u7528\u4e8e\u6f14\u793a\"\"\"\n    np.random.seed(42)\n\n    # \u751f\u6210\u968f\u673a\u6570\u636e\n    data = np.random.randn(n_samples, n_features).astype(np.float32)\n\n    # \u4e3a\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u4e00\u4e9b\u6a21\u5f0f\n    labels = np.random.randint(0, n_classes, n_samples)\n    for i in range(n_classes):\n        mask = labels == i\n        # \u7ed9\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u7279\u5b9a\u7684\u504f\u7f6e\n        data[mask] += np.random.randn(n_features) * 0.5\n\n    return data, labels\n\ndef load_data() -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\u52a0\u8f7d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\"\"\"\n    print(\"\ud83d\udd04 \u52a0\u8f7d\u6570\u636e...\")\n\n    # \u521b\u5efa\u5408\u6210\u6570\u636e (\u5b9e\u9645\u9879\u76ee\u4e2d\u66ff\u6362\u4e3a\u771f\u5b9e\u6570\u636e)\n    train_data, train_labels = create_synthetic_data(800, 784, 10)\n    val_data, val_labels = create_synthetic_data(200, 784, 10)\n\n    # \u6570\u636e\u6807\u51c6\u5316\n    def normalize(x):\n        return (x - x.mean()) / (x.std() + 1e-8)\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    train_dataset = SimpleDataset(train_data, train_labels, transform=normalize)\n    val_dataset = SimpleDataset(val_data, val_labels, transform=normalize)\n\n    # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n    print(f\"\u2705 \u6570\u636e\u52a0\u8f7d\u5b8c\u6210 - \u8bad\u7ec3\u96c6: {len(train_dataset)}, \u9a8c\u8bc1\u96c6: {len(val_dataset)}\")\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"getting-started/first-steps_en/#2-modelpy","title":"\ud83e\udde0 2. \u6a21\u578b\u5b9a\u4e49 (<code>model.py</code>)","text":"Python<pre><code>\"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9a\u4e49\"\"\"\nimport genesis\nimport genesis.nn as nn\nimport genesis.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\"\"\"\n\n    def __init__(self, input_dim: int = 784, hidden_dims: list = None, num_classes: int = 10, dropout_rate: float = 0.2):\n        \"\"\"\n        \u521d\u59cb\u5316MLP\u6a21\u578b\n\n        Args:\n            input_dim: \u8f93\u5165\u7ef4\u5ea6\n            hidden_dims: \u9690\u85cf\u5c42\u7ef4\u5ea6\u5217\u8868\n            num_classes: \u5206\u7c7b\u6570\u91cf\n            dropout_rate: Dropout\u6bd4\u7387\n        \"\"\"\n        super().__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [512, 256, 128]\n\n        # \u6784\u5efa\u7f51\u7edc\u5c42\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = hidden_dim\n\n        # \u8f93\u51fa\u5c42\n        layers.append(nn.Linear(prev_dim, num_classes))\n\n        self.network = nn.Sequential(*layers)\n\n        # \u521d\u59cb\u5316\u6743\u91cd\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"\u6743\u91cd\u521d\u59cb\u5316\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                # Xavier\u521d\u59cb\u5316\n                std = (2.0 / (module.in_features + module.out_features)) ** 0.5\n                module.weight.data.normal_(0, std)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        \"\"\"\u524d\u5411\u4f20\u64ad\"\"\"\n        # \u5c55\u5e73\u8f93\u5165 (\u5982\u679c\u662f\u56fe\u50cf\u6570\u636e)\n        if x.dim() &gt; 2:\n            x = x.view(x.size(0), -1)\n\n        return self.network(x)\n\nclass CNN(nn.Module):\n    \"\"\"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668 (\u5982\u679c\u9700\u8981\u5904\u7406\u56fe\u50cf)\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n\n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # \u6c60\u5316\u5c42\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # \u5047\u8bbe\u8f93\u5165\u662f28x28\n        self.fc2 = nn.Linear(512, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        # \u5377\u79ef + \u6c60\u5316\n        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -&gt; 7x7  \n        x = self.pool(F.relu(self.conv3(x)))  # 7x7 -&gt; 3x3\n\n        # \u5c55\u5e73\n        x = x.view(x.size(0), -1)\n\n        # \u5168\u8fde\u63a5\u5c42\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\ndef create_model(model_type: str = \"mlp\", **kwargs) -&gt; nn.Module:\n    \"\"\"\u5de5\u5382\u51fd\u6570\uff1a\u521b\u5efa\u6a21\u578b\"\"\"\n    if model_type.lower() == \"mlp\":\n        return MLP(**kwargs)\n    elif model_type.lower() == \"cnn\":\n        return CNN(**kwargs)\n    else:\n        raise ValueError(f\"\u672a\u77e5\u7684\u6a21\u578b\u7c7b\u578b: {model_type}\")\n</code></pre>"},{"location":"getting-started/first-steps_en/#3-utilspy","title":"\ud83d\udee0\ufe0f 3. \u5de5\u5177\u51fd\u6570 (<code>utils.py</code>)","text":"Python<pre><code>\"\"\"\u5de5\u5177\u51fd\u6570\u6a21\u5757\"\"\"\nimport genesis\nimport time\nimport os\nfrom typing import Dict, Any\nimport json\n\nclass AverageMeter:\n    \"\"\"\u5e73\u5747\u503c\u8ba1\u7b97\u5668\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Timer:\n    \"\"\"\u8ba1\u65f6\u5668\"\"\"\n\n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self):\n        self.end_time = time.time()\n        return self.end_time - self.start_time\n\n    def elapsed(self):\n        if self.start_time is None:\n            return 0\n        return time.time() - self.start_time\n\ndef accuracy(output: genesis.Tensor, target: genesis.Tensor, topk: tuple = (1,)) -&gt; list:\n    \"\"\"\u8ba1\u7b97\u51c6\u786e\u7387\"\"\"\n    with genesis.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n\n        return res\n\ndef save_checkpoint(model: genesis.nn.Module, optimizer: genesis.optim.Optimizer, \n                   epoch: int, loss: float, accuracy: float, filepath: str):\n    \"\"\"\u4fdd\u5b58\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'accuracy': accuracy\n    }\n\n    # \u786e\u4fdd\u76ee\u5f55\u5b58\u5728\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    genesis.save(checkpoint, filepath)\n    print(f\"\ud83d\udcbe \u68c0\u67e5\u70b9\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef load_checkpoint(filepath: str, model: genesis.nn.Module, optimizer: genesis.optim.Optimizer = None) -&gt; Dict[str, Any]:\n    \"\"\"\u52a0\u8f7d\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = genesis.load(filepath)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if optimizer and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    print(f\"\ud83d\udcc1 \u68c0\u67e5\u70b9\u5df2\u52a0\u8f7d: {filepath}\")\n    print(f\"   \u8f6e\u6b21: {checkpoint['epoch']}, \u635f\u5931: {checkpoint['loss']:.4f}, \u51c6\u786e\u7387: {checkpoint['accuracy']:.2f}%\")\n\n    return checkpoint\n\ndef save_training_history(history: Dict[str, list], filepath: str):\n    \"\"\"\u4fdd\u5b58\u8bad\u7ec3\u5386\u53f2\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    with open(filepath, 'w') as f:\n        json.dump(history, f, indent=2)\n\n    print(f\"\ud83d\udcca \u8bad\u7ec3\u5386\u53f2\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef print_model_summary(model: genesis.nn.Module, input_shape: tuple):\n    \"\"\"\u6253\u5370\u6a21\u578b\u6458\u8981\"\"\"\n    print(\"\ud83c\udfd7\ufe0f  \u6a21\u578b\u67b6\u6784:\")\n    print(\"=\" * 50)\n\n    # \u8ba1\u7b97\u53c2\u6570\u6570\u91cf\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"\u603b\u53c2\u6570\u91cf: {total_params:,}\")\n    print(f\"\u53ef\u8bad\u7ec3\u53c2\u6570: {trainable_params:,}\")\n    print(f\"\u8f93\u5165\u5f62\u72b6: {input_shape}\")\n\n    # \u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\n    dummy_input = genesis.randn(*input_shape)\n    try:\n        with genesis.no_grad():\n            output = model(dummy_input)\n        print(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")\n    except Exception as e:\n        print(f\"\u524d\u5411\u4f20\u64ad\u6d4b\u8bd5\u5931\u8d25: {e}\")\n\n    print(\"=\" * 50)\n</code></pre>"},{"location":"getting-started/first-steps_en/#4-trainpy","title":"\ud83d\ude82 4. \u8bad\u7ec3\u811a\u672c (<code>train.py</code>)","text":"<p> [{\"id\": \"1\", \"content\": \"\\u521b\\u5efa\\u6587\\u6863\\u76ee\\u5f55\\u7ed3\\u6784\\u548c\\u914d\\u7f6e\\u6587\\u4ef6\", \"status\": \"completed\"}, {\"id\": \"2\", \"content\": \"\\u7f16\\u5199\\u9996\\u9875\\u548c\\u5feb\\u901f\\u5f00\\u59cb\\u6587\\u6863\", \"status\": \"completed\"}, {\"id\": \"3\", \"content\": \"\\u7f16\\u5199\\u67b6\\u6784\\u8bbe\\u8ba1\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"4\", \"content\": \"\\u7f16\\u5199\\u6838\\u5fc3\\u7ec4\\u4ef6\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"5\", \"content\": \"\\u7f16\\u5199\\u795e\\u7ecf\\u7f51\\u7edc\\u6a21\\u5757\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"6\", \"content\": \"\\u7f16\\u5199API\\u53c2\\u8003\\u6587\\u6863\", \"status\": \"pending\"}]"},{"location":"getting-started/index.zh/","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u6b22\u8fce\u4f7f\u7528 Genesis \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff01\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u60a8\u5728\u51e0\u5206\u949f\u5185\u5f00\u59cb\u4f7f\u7528 Genesis\u3002</p>"},{"location":"getting-started/index.zh/#_2","title":"\ud83c\udfaf \u6982\u8ff0","text":"<p>Genesis \u662f\u4e00\u4e2a\u4e13\u4e3a\u5b66\u4e60\u548c\u7814\u7a76\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\uff1a</p> <ul> <li>\u7b80\u6d01\u76f4\u89c2\u7684API\u8bbe\u8ba1</li> <li>\u9ad8\u6027\u80fdGPU\u52a0\u901f\u8ba1\u7b97</li> <li>\u5b8c\u6574\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u529f\u80fd</li> <li>\u4e0ePyTorch\u751f\u6001\u826f\u597d\u7684\u517c\u5bb9\u6027</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u652f\u6301\uff08FP16/BF16\uff09</li> <li>\u5185\u7f6eLLM\u6a21\u578b\u5982Qwen</li> </ul>"},{"location":"getting-started/index.zh/#5","title":"\u26a1 5\u5206\u949f\u5feb\u901f\u4f53\u9a8c","text":""},{"location":"getting-started/index.zh/#1-genesis","title":"1. \u5b89\u88c5 Genesis","text":"Bash<pre><code># \u5b89\u88c5\u6838\u5fc3\u4f9d\u8d56\npip install torch triton numpy cuda-python\n\n# \u514b\u9686\u4ee3\u7801\u4ed3\u5e93\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u5b89\u88c5 Genesis\npip install -e .\n</code></pre>"},{"location":"getting-started/index.zh/#2","title":"2. \u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# \u5b9a\u4e49\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        return self.layer2(x)\n\n# \u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\nmodel = MLP(784, 128, 10)\nx = genesis.randn(32, 784)  # \u6279\u6b21\u5927\u5c0f32\uff0c\u8f93\u5165\u7ef4\u5ea6784\n\n# \u524d\u5411\u4f20\u64ad\noutput = model(x)\nprint(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")  # torch.Size([32, 10])\n</code></pre>"},{"location":"getting-started/index.zh/#3","title":"3. \u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis.optim as optim\n\n# \u521b\u5efa\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u6a21\u62df\u8bad\u7ec3\u6570\u636e\ntargets = genesis.randint(0, 10, (32,))\n\n# \u8bad\u7ec3\u4e00\u4e2a\u6279\u6b21\noptimizer.zero_grad()        # \u6e05\u96f6\u68af\u5ea6\noutput = model(x)           # \u524d\u5411\u4f20\u64ad\nloss = criterion(output, targets)  # \u8ba1\u7b97\u635f\u5931\nloss.backward()             # \u53cd\u5411\u4f20\u64ad\noptimizer.step()            # \u66f4\u65b0\u53c2\u6570\n\nprint(f\"\u635f\u5931: {loss.item():.4f}\")\n</code></pre>"},{"location":"getting-started/index.zh/#_3","title":"\ud83d\udcda \u6838\u5fc3\u6982\u5ff5","text":""},{"location":"getting-started/index.zh/#tensor","title":"\u5f20\u91cf (Tensor)","text":"<p>Genesis\u4e2d\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u81ea\u52a8\u5fae\u5206\uff1a</p> Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = genesis.tensor([4.0, 5.0, 6.0], requires_grad=True)\n\n# \u6267\u884c\u8fd0\u7b97\nz = x * y + x.sum()\nz.backward(genesis.ones_like(z))\n\nprint(f\"x\u7684\u68af\u5ea6: {x.grad}\")  # [5., 6., 7.]\nprint(f\"y\u7684\u68af\u5ea6: {y.grad}\")  # [1., 2., 3.]\n</code></pre>"},{"location":"getting-started/index.zh/#module","title":"\u6a21\u5757 (Module)","text":"<p>\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u7684\u57fa\u7c7b\uff1a</p> Python<pre><code>import genesis.nn as nn\n\nclass CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = genesis.randn(out_features, in_features, requires_grad=True)\n        self.bias = genesis.zeros(out_features, requires_grad=True)\n\n    def forward(self, x):\n        return genesis.functional.linear(x, self.weight, self.bias)\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u5c42\nlayer = CustomLayer(10, 5)\ninput_tensor = genesis.randn(3, 10)\noutput = layer(input_tensor)\n</code></pre>"},{"location":"getting-started/index.zh/#optimizer","title":"\u4f18\u5316\u5668 (Optimizer)","text":"<p>\u53c2\u6570\u66f4\u65b0\u7b97\u6cd5\uff1a</p> Python<pre><code>import genesis.optim as optim\n\n# \u4e0d\u540c\u7684\u4f18\u5316\u5668\u9009\u62e9\nsgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nadam_optimizer = optim.Adam(model.parameters(), lr=0.001)\nadamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n</code></pre>"},{"location":"getting-started/index.zh/#_4","title":"\ud83d\udee0\ufe0f \u73af\u5883\u914d\u7f6e","text":""},{"location":"getting-started/index.zh/#_5","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: \u73b0\u4ee3\u591a\u6838\u5904\u7406\u5668</li> <li>\u5185\u5b58: \u6700\u5c118GB RAM\uff0c\u63a8\u835016GB+</li> <li>GPU: NVIDIA GPU with CUDA\u652f\u6301 (\u63a8\u8350)</li> <li>\u5b58\u50a8: \u81f3\u5c112GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"getting-started/index.zh/#_6","title":"\u8f6f\u4ef6\u4f9d\u8d56","text":"Bash<pre><code># Python\u73af\u5883\nPython &gt;= 3.8\n\n# \u6838\u5fc3\u4f9d\u8d56\ntorch &gt;= 2.0.0\ntriton &gt;= 2.0.0\nnumpy &gt;= 1.21.0\ncuda-python &gt;= 11.8.0  # GPU\u652f\u6301\n\n# \u53ef\u9009\u4f9d\u8d56\nmatplotlib &gt;= 3.5.0  # \u7528\u4e8e\u53ef\u89c6\u5316\ntqdm &gt;= 4.64.0      # \u8fdb\u5ea6\u6761\nwandb &gt;= 0.13.0     # \u5b9e\u9a8c\u8ddf\u8e2a\n</code></pre>"},{"location":"getting-started/index.zh/#_7","title":"\ud83d\udcd6 \u4e0b\u4e00\u6b65","text":"<p>\u73b0\u5728\u4f60\u5df2\u7ecf\u4e86\u89e3\u4e86Genesis\u7684\u57fa\u7840\u7528\u6cd5\uff0c\u53ef\u4ee5\u7ee7\u7eed\u63a2\u7d22\uff1a</p>"},{"location":"getting-started/index.zh/#_8","title":"\ud83c\udf93 \u6df1\u5165\u5b66\u4e60","text":"<ul> <li>\u5b8c\u6574\u5b89\u88c5\u6307\u5357 - \u8be6\u7ec6\u7684\u5b89\u88c5\u548c\u914d\u7f6e\u6b65\u9aa4</li> <li>\u7b2c\u4e00\u4e2a\u5b8c\u6574\u7a0b\u5e8f - \u6784\u5efa\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b</li> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u7cfb\u7edf\u6027\u7684\u8bad\u7ec3\u6559\u7a0b</li> </ul>"},{"location":"getting-started/index.zh/#_9","title":"\ud83d\udd0d \u67b6\u6784\u7406\u89e3","text":"<ul> <li>\u67b6\u6784\u6982\u8ff0 - \u4e86\u89e3Genesis\u7684\u6574\u4f53\u8bbe\u8ba1</li> <li>\u6838\u5fc3\u7ec4\u4ef6 - \u6df1\u5165\u7406\u89e3\u5185\u90e8\u5b9e\u73b0</li> <li>API\u53c2\u8003 - \u5b8c\u6574\u7684API\u6587\u6863</li> </ul>"},{"location":"getting-started/index.zh/#_10","title":"\ud83d\ude80 \u9ad8\u7ea7\u7279\u6027","text":"<ul> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u64cd\u4f5c</li> <li>\u6027\u80fd\u4f18\u5316 - \u8bad\u7ec3\u6027\u80fd\u8c03\u4f18</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u8bad\u7ec3</li> </ul>"},{"location":"getting-started/index.zh/#_11","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"getting-started/index.zh/#q-genesispytorch","title":"Q: Genesis\u4e0ePyTorch\u6709\u4ec0\u4e48\u533a\u522b\uff1f","text":"<p>A: Genesis\u662f\u6559\u80b2\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u4ee3\u7801\u66f4\u7b80\u6d01\u6613\u61c2\uff0c\u9002\u5408\u5b66\u4e60\u6df1\u5ea6\u5b66\u4e60\u7684\u5185\u90e8\u5b9e\u73b0\u3002PyTorch\u66f4\u9002\u5408\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u3002</p>"},{"location":"getting-started/index.zh/#q-genesis","title":"Q: \u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528Genesis\u5417\uff1f","text":"<p>A: Genesis\u4e3b\u8981\u7528\u4e8e\u6559\u80b2\u548c\u7814\u7a76\uff0c\u867d\u7136\u529f\u80fd\u5b8c\u6574\uff0c\u4f46\u5efa\u8bae\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u66f4\u6210\u719f\u7684\u6846\u67b6\u5982PyTorch\u3002</p>"},{"location":"getting-started/index.zh/#q","title":"Q: \u5982\u4f55\u83b7\u5f97\u5e2e\u52a9\uff1f","text":"<p>A: \u53ef\u4ee5\u901a\u8fc7GitHub Issues\u3001Discussions\u6216\u67e5\u770b\u8be6\u7ec6\u6587\u6863\u83b7\u5f97\u5e2e\u52a9\u3002</p>"},{"location":"getting-started/index.zh/#_12","title":"\ud83c\udf89 \u51c6\u5907\u597d\u4e86\u5417\uff1f","text":"<p>\u8ba9\u6211\u4eec\u5f00\u59cb\u6df1\u5165\u4e86\u89e3Genesis\u5427\uff01</p> <p>\u8be6\u7ec6\u5b89\u88c5\u6307\u5357 \u5b8c\u6574\u6559\u7a0b</p>"},{"location":"getting-started/index_en/","title":"Getting Started","text":"<p>Welcome to the Genesis deep learning framework! This guide will help you start using Genesis in just a few minutes.</p>"},{"location":"getting-started/index_en/#overview","title":"\ud83c\udfaf Overview","text":"<p>Genesis is a lightweight deep learning framework designed specifically for learning and research. It provides:</p> <ul> <li>Simple and intuitive API design</li> <li>High-performance GPU-accelerated computing</li> <li>Complete neural network training capabilities</li> <li>Good compatibility with PyTorch ecosystem</li> </ul>"},{"location":"getting-started/index_en/#5-minute-quick-start","title":"\u26a1 5-Minute Quick Start","text":""},{"location":"getting-started/index_en/#1-install-genesis","title":"1. Install Genesis","text":"Bash<pre><code># Install core dependencies\npip install torch triton\n\n# Clone source code\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# Install Genesis\npip install -e .\n</code></pre>"},{"location":"getting-started/index_en/#2-your-first-neural-network","title":"2. Your First Neural Network","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Define a simple multi-layer perceptron\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        return self.layer2(x)\n\n# Create model and data\nmodel = MLP(784, 128, 10)\nx = genesis.randn(32, 784)  # batch size 32, input dimension 784\n\n# Forward pass\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")  # torch.Size([32, 10])\n</code></pre>"},{"location":"getting-started/index_en/#3-training-loop","title":"3. Training Loop","text":"Python<pre><code>import genesis.optim as optim\n\n# Create optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Simulate training data\ntargets = genesis.randint(0, 10, (32,))\n\n# Train one batch\noptimizer.zero_grad()        # Zero gradients\noutput = model(x)           # Forward pass\nloss = criterion(output, targets)  # Compute loss\nloss.backward()             # Backward pass\noptimizer.step()            # Update parameters\n\nprint(f\"Loss value: {loss.item():.4f}\")\n</code></pre>"},{"location":"getting-started/index_en/#core-concepts","title":"\ud83d\udcda Core Concepts","text":""},{"location":"getting-started/index_en/#tensor","title":"Tensor","text":"<p>The fundamental data structure in Genesis, supporting automatic differentiation:</p> Python<pre><code>import genesis\n\n# Create tensors\nx = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = genesis.tensor([4.0, 5.0, 6.0], requires_grad=True)\n\n# Compute operations\nz = x * y + x.sum()\nz.backward(genesis.ones_like(z))\n\nprint(f\"x gradients: {x.grad}\")  # [5., 6., 7.]\nprint(f\"y gradients: {y.grad}\")  # [1., 2., 3.]\n</code></pre>"},{"location":"getting-started/index_en/#module","title":"Module","text":"<p>Base class for neural network components:</p> Python<pre><code>import genesis.nn as nn\n\nclass CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = genesis.randn(out_features, in_features, requires_grad=True)\n        self.bias = genesis.zeros(out_features, requires_grad=True)\n\n    def forward(self, x):\n        return genesis.functional.linear(x, self.weight, self.bias)\n\n# Use custom layer\nlayer = CustomLayer(10, 5)\ninput_tensor = genesis.randn(3, 10)\noutput = layer(input_tensor)\n</code></pre>"},{"location":"getting-started/index_en/#optimizer","title":"Optimizer","text":"<p>Parameter update algorithms:</p> Python<pre><code>import genesis.optim as optim\n\n# Different optimizer choices\nsgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nadam_optimizer = optim.Adam(model.parameters(), lr=0.001)\nadamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n</code></pre>"},{"location":"getting-started/index_en/#environment-setup","title":"\ud83d\udee0\ufe0f Environment Setup","text":""},{"location":"getting-started/index_en/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: Modern multi-core processor</li> <li>Memory: Minimum 8GB RAM, 16GB+ recommended</li> <li>GPU: NVIDIA GPU with CUDA support (recommended)</li> <li>Storage: At least 2GB available space</li> </ul>"},{"location":"getting-started/index_en/#software-dependencies","title":"Software Dependencies","text":"Bash<pre><code># Python environment\nPython &gt;= 3.8\n\n# Core dependencies\ntorch &gt;= 2.0.0\ntriton &gt;= 2.0.0\nnumpy &gt;= 1.21.0\ncuda-python &gt;= 11.8.0  # GPU support\n\n# Optional dependencies\nmatplotlib &gt;= 3.5.0  # For visualization\ntqdm &gt;= 4.64.0      # Progress bars\nwandb &gt;= 0.13.0     # Experiment tracking\n</code></pre>"},{"location":"getting-started/index_en/#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<p>Now that you understand the basics of Genesis, you can continue exploring:</p>"},{"location":"getting-started/index_en/#deep-learning","title":"\ud83c\udf93 Deep Learning","text":"<ul> <li>Complete Installation Guide - Detailed installation and configuration steps</li> <li>First Complete Program - Build a complete training workflow</li> <li>Basic Training Tutorial - Systematic training tutorials</li> </ul>"},{"location":"getting-started/index_en/#architecture-understanding","title":"\ud83d\udd0d Architecture Understanding","text":"<ul> <li>Architecture Overview - Understand Genesis's overall design</li> <li>Core Components - Deep dive into internal implementation</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/index_en/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":"<ul> <li>Custom Operators - Implement custom operations</li> <li>Performance Optimization - Training performance tuning</li> <li>Distributed Training - Multi-GPU training</li> </ul>"},{"location":"getting-started/index_en/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":""},{"location":"getting-started/index_en/#q-whats-the-difference-between-genesis-and-pytorch","title":"Q: What's the difference between Genesis and PyTorch?","text":"<p>A: Genesis is education-oriented with cleaner, more understandable code, suitable for learning deep learning internals. PyTorch is better suited for production environments.</p>"},{"location":"getting-started/index_en/#q-can-genesis-be-used-in-production","title":"Q: Can Genesis be used in production?","text":"<p>A: Genesis is primarily for education and research. While fully functional, we recommend more mature frameworks like PyTorch for production use.</p>"},{"location":"getting-started/index_en/#q-how-to-get-help","title":"Q: How to get help?","text":"<p>A: You can get help through GitHub Issues, Discussions, or by consulting the detailed documentation.</p>"},{"location":"getting-started/index_en/#ready","title":"\ud83c\udf89 Ready?","text":"<p>Let's start diving deep into Genesis!</p> <p>Detailed Installation Guide Complete Tutorials</p>"},{"location":"getting-started/installation.zh/","title":"\u5b89\u88c5\u6307\u5357","text":"<p>\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u5b89\u88c5Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002</p>"},{"location":"getting-started/installation.zh/#_2","title":"\ud83d\udccb \u7cfb\u7edf\u8981\u6c42","text":""},{"location":"getting-started/installation.zh/#_3","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: x86_64\u67b6\u6784\uff0c\u652f\u6301AVX\u6307\u4ee4\u96c6</li> <li>\u5185\u5b58: \u6700\u5c118GB\uff0c\u63a8\u835016GB+</li> <li>GPU: NVIDIA GPU with Compute Capability \u2265 6.0 (\u53ef\u9009\u4f46\u63a8\u8350)</li> <li>\u5b58\u50a8: 2GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"getting-started/installation.zh/#_4","title":"\u8f6f\u4ef6\u8981\u6c42","text":"<ul> <li>\u64cd\u4f5c\u7cfb\u7edf: Linux (Ubuntu 20.04+), macOS (10.15+), Windows 10+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>CUDA: 11.0+ (GPU\u52a0\u901f\u9700\u8981)</li> </ul>"},{"location":"getting-started/installation.zh/#_5","title":"\ud83d\ude80 \u5feb\u901f\u5b89\u88c5","text":""},{"location":"getting-started/installation.zh/#_6","title":"\u65b9\u5f0f\u4e00\uff1a\u4ece\u6e90\u7801\u5b89\u88c5 (\u63a8\u8350)","text":"Bash<pre><code># 1. \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# 2. \u521b\u5efa\u865a\u62df\u73af\u5883 (\u63a8\u8350)\npython -m venv genesis-env\nsource genesis-env/bin/activate  # Linux/macOS\n# genesis-env\\\\Scripts\\\\activate  # Windows\n\n# 3. \u5b89\u88c5\u4f9d\u8d56\npip install -r genesis/requirements.txt\n\n# 4. \u5b89\u88c5Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation.zh/#pip","title":"\u65b9\u5f0f\u4e8c\uff1a\u4f7f\u7528pip\u5b89\u88c5","text":"Bash<pre><code># \u5b89\u88c5\u53d1\u5e03\u7248\u672c\npip install genesis-dl\n\n# \u5b89\u88c5\u9884\u53d1\u5e03\u7248\u672c\npip install --pre genesis-dl\n</code></pre>"},{"location":"getting-started/installation.zh/#_7","title":"\ud83d\udd27 \u8be6\u7ec6\u5b89\u88c5\u6b65\u9aa4","text":""},{"location":"getting-started/installation.zh/#python","title":"\u7b2c\u4e00\u6b65\uff1a\u51c6\u5907Python\u73af\u5883","text":"Ubuntu/DebianCentOS/RHELmacOSWindows Bash<pre><code># \u5b89\u88c5Python\u548cpip\nsudo apt update\nsudo apt install python3.9 python3.9-pip python3.9-venv\n\n# \u521b\u5efa\u8f6f\u94fe\u63a5 (\u53ef\u9009)\nsudo ln -sf /usr/bin/python3.9 /usr/bin/python\n</code></pre> Bash<pre><code># \u5b89\u88c5EPEL\u4ed3\u5e93\nsudo yum install epel-release\n\n# \u5b89\u88c5Python\nsudo yum install python39 python39-pip\n</code></pre> Bash<pre><code># \u4f7f\u7528Homebrew\u5b89\u88c5\nbrew install python@3.9\n\n# \u6216\u4f7f\u7528\u5b98\u65b9\u5b89\u88c5\u5305\n# \u4ece https://python.org \u4e0b\u8f7d\u5b89\u88c5\n</code></pre> PowerShell<pre><code># \u4e0b\u8f7dPython\u5b89\u88c5\u5305\n# https://python.org/downloads/windows/\n\n# \u6216\u4f7f\u7528Chocolatey\nchoco install python39\n</code></pre>"},{"location":"getting-started/installation.zh/#cuda-gpu","title":"\u7b2c\u4e8c\u6b65\uff1a\u5b89\u88c5CUDA (GPU\u52a0\u901f)","text":"<p>GPU\u652f\u6301\u8bf4\u660e</p> <p>\u5982\u679c\u4f60\u53ea\u9700\u8981CPU\u7248\u672c\uff0c\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002\u4f46\u5f3a\u70c8\u63a8\u8350\u5b89\u88c5CUDA\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002</p> Ubuntu/DebianCentOS/RHELWindows Bash<pre><code># \u4e0b\u8f7dCUDA Toolkit\nwget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\nsudo sh cuda_11.8.0_520.61.05_linux.run\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> Bash<pre><code># \u5b89\u88c5NVIDIA\u9a71\u52a8\u4ed3\u5e93\nsudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\n\n# \u5b89\u88c5CUDA\nsudo yum install cuda-11-8\n</code></pre> PowerShell<pre><code># \u4e0b\u8f7dCUDA\u5b89\u88c5\u5305\n# https://developer.nvidia.com/cuda-downloads\n\n# \u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f\u5e76\u6309\u7167\u63d0\u793a\u64cd\u4f5c\n</code></pre>"},{"location":"getting-started/installation.zh/#_8","title":"\u7b2c\u4e09\u6b65\uff1a\u5b89\u88c5\u6838\u5fc3\u4f9d\u8d56","text":"Bash<pre><code># \u521b\u5efa\u5e76\u6fc0\u6d3b\u865a\u62df\u73af\u5883\npython -m venv genesis-env\nsource genesis-env/bin/activate\n\n# \u5347\u7ea7pip\npip install --upgrade pip setuptools wheel\n\n# \u5b89\u88c5PyTorch (\u6839\u636e\u4f60\u7684CUDA\u7248\u672c\u9009\u62e9)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU\u7248\u672c\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# \u5b89\u88c5Triton\npip install triton\n\n# \u5b89\u88c5\u5176\u4ed6\u4f9d\u8d56\npip install numpy matplotlib tqdm\n</code></pre>"},{"location":"getting-started/installation.zh/#genesis","title":"\u7b2c\u56db\u6b65\uff1a\u5b89\u88c5Genesis","text":"Bash<pre><code># \u514b\u9686\u6e90\u7801\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u67e5\u770b\u53ef\u7528\u7248\u672c\ngit tag\n\n# \u5207\u6362\u5230\u7a33\u5b9a\u7248\u672c (\u53ef\u9009)\ngit checkout v0.1.0\n\n# \u5b89\u88c5Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation.zh/#_9","title":"\u2705 \u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f\uff1a</p> Python<pre><code>#!/usr/bin/env python3\n\"\"\"Genesis\u5b89\u88c5\u9a8c\u8bc1\u811a\u672c\"\"\"\n\ndef test_basic_import():\n    \"\"\"\u6d4b\u8bd5\u57fa\u7840\u5bfc\u5165\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n        print(\"\u2705 Genesis\u5bfc\u5165\u6210\u529f\")\n        print(f\"   \u6838\u5fc3\u6a21\u5757: genesis, nn, optim\")\n        print(f\"   \u53ef\u7528\u51fd\u6570: {len([x for x in dir(genesis) if not x.startswith('_')])}\")\n    except ImportError as e:\n        print(f\"\u274c Genesis\u5bfc\u5165\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_tensor_operations():\n    \"\"\"\u6d4b\u8bd5\u5f20\u91cf\u64cd\u4f5c\"\"\"\n    try:\n        import genesis\n\n        # \u521b\u5efa\u5f20\u91cf\n        x = genesis.randn(3, 4)\n        y = genesis.randn(3, 4)\n\n        # \u57fa\u7840\u8fd0\u7b97\n        z = x + y\n        w = genesis.matmul(x, y.T)  # \u4f7f\u7528\u5b9e\u9645\u7684Genesis API\n\n        print(\"\u2705 \u5f20\u91cf\u8fd0\u7b97\u6b63\u5e38\")\n        print(f\"   \u52a0\u6cd5\u7ed3\u679c\u5f62\u72b6: {z.shape}\")\n        print(f\"   \u77e9\u9635\u4e58\u6cd5\u5f62\u72b6: {w.shape}\")\n    except Exception as e:\n        print(f\"\u274c \u5f20\u91cf\u8fd0\u7b97\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_neural_networks():\n    \"\"\"\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # \u4f7f\u7528\u5b9e\u9645\u7684Genesis\u6a21\u5757\u521b\u5efa\u7b80\u5355\u6a21\u578b\n        model = nn.Sequential([\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 1)\n        ])\n\n        # \u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\n        x = genesis.randn(2, 10)\n        y = model(x)\n        print(\"\u2705 \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u6b63\u5e38\")\n        print(f\"   \u6a21\u578b\u5c42\u6570: {len(list(model.parameters()))} \u4e2a\u53c2\u6570\u5f20\u91cf\")\n        print(f\"   \u8f93\u51fa\u5f62\u72b6: {y.shape}\")\n    except Exception as e:\n        print(f\"\u274c \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_backend_support():\n    \"\"\"\u6d4b\u8bd5\u540e\u7aef\u652f\u6301\"\"\"\n    try:\n        import genesis\n        from genesis.backend import default_device\n\n        # \u6d4b\u8bd5\u57fa\u7840\u540e\u7aef\u529f\u80fd\n        device = default_device()\n        x = genesis.randn(5, 5)\n\n        print(\"\u2705 \u540e\u7aef\u652f\u6301\u6b63\u5e38\")\n        print(f\"   \u9ed8\u8ba4\u8bbe\u5907: {device}\")\n        print(f\"   \u5f20\u91cf\u8bbe\u5907: {x.device}\")\n\n        # \u5c1d\u8bd5\u68c0\u6d4bCUDA\u662f\u5426\u53ef\u7528\n        try:\n            # \u6d4b\u8bd5\u662f\u5426\u53ef\u4ee5\u521b\u5efaCUDA\u5f20\u91cf\n            import torch\n            if torch.cuda.is_available():\n                print(\"   \u68c0\u6d4b\u5230CUDA\uff08\u901a\u8fc7PyTorch\u540e\u7aef\uff09\")\n            else:\n                print(\"   CUDA\u4e0d\u53ef\u7528\uff08\u4ec5CPU\uff09\")\n        except:\n            print(\"   \u540e\u7aef: Genesis\u539f\u751f\")\n\n    except Exception as e:\n        print(f\"\u274c \u540e\u7aef\u6d4b\u8bd5\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_autograd():\n    \"\"\"\u6d4b\u8bd5\u81ea\u52a8\u5fae\u5206\"\"\"\n    try:\n        import genesis\n\n        # \u6d4b\u8bd5\u57fa\u7840\u81ea\u52a8\u5fae\u5206\n        x = genesis.randn(5, requires_grad=True)\n        y = genesis.functional.sum(x * x)  # \u4f7f\u7528\u5b9e\u9645\u7684Genesis API\n        y.backward()\n\n        print(\"\u2705 \u81ea\u52a8\u5fae\u5206\u6b63\u5e38\")\n        print(f\"   \u8f93\u5165\u5f62\u72b6: {x.shape}\")\n        print(f\"   \u68af\u5ea6\u5df2\u8ba1\u7b97: {x.grad is not None}\")\n        print(f\"   \u68af\u5ea6\u5f62\u72b6: {x.grad.shape if x.grad is not None else 'None'}\")\n    except Exception as e:\n        print(f\"\u274c \u81ea\u52a8\u5fae\u5206\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_optimizers():\n    \"\"\"\u6d4b\u8bd5\u4f18\u5316\u5668\u529f\u80fd\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n\n        # \u521b\u5efa\u7b80\u5355\u6a21\u578b\u548c\u4f18\u5316\u5668\n        model = nn.Linear(5, 1)\n        optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n        # \u6d4b\u8bd5\u57fa\u7840\u4f18\u5316\u6b65\u9aa4\n        x = genesis.randn(3, 5)\n        y_pred = model(x)\n        loss = genesis.functional.sum(y_pred * y_pred)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print(\"\u2705 \u4f18\u5316\u5668\u529f\u80fd\u6b63\u5e38\")\n        print(f\"   \u4f18\u5316\u5668\u7c7b\u578b: {type(optimizer).__name__}\")\n        print(f\"   \u5b66\u4e60\u7387: 0.01\")\n        print(f\"   \u53c2\u6570\u5df2\u66f4\u65b0: {len(list(model.parameters()))}\")\n    except Exception as e:\n        print(f\"\u274c \u4f18\u5316\u5668\u6d4b\u8bd5\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_serialization():\n    \"\"\"\u6d4b\u8bd5\u6a21\u578b\u4fdd\u5b58/\u52a0\u8f7d\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # \u521b\u5efa\u5e76\u4fdd\u5b58\u6a21\u578b\n        model = nn.Linear(3, 2)\n        state_dict = model.state_dict()\n\n        # \u6d4b\u8bd5\u5e8f\u5217\u5316\u529f\u80fd\n        genesis.save(state_dict, 'test_model.pkl')\n        loaded_state = genesis.load('test_model.pkl')\n\n        print(\"\u2705 \u5e8f\u5217\u5316\u529f\u80fd\u6b63\u5e38\")\n        print(f\"   \u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d\u6210\u529f\")\n        print(f\"   \u72b6\u6001\u5b57\u5178\u952e\u6570\u91cf: {len(state_dict)}\")\n\n        # \u6e05\u7406\n        import os\n        if os.path.exists('test_model.pkl'):\n            os.remove('test_model.pkl')\n\n    except Exception as e:\n        print(f\"\u274c \u5e8f\u5217\u5316\u6d4b\u8bd5\u5931\u8d25: {e}\")\n        return False\n    return True\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd0d Genesis\u5b89\u88c5\u9a8c\u8bc1\\n\")\n\n    tests = [\n        test_basic_import,\n        test_tensor_operations,\n        test_neural_networks,\n        test_backend_support,\n        test_autograd,\n        test_optimizers,\n        test_serialization\n    ]\n\n    passed = 0\n    total = len(tests)\n\n    for test in tests:\n        try:\n            if test():\n                passed += 1\n        except Exception as e:\n            print(f\"\u274c \u6d4b\u8bd5\u5f02\u5e38\u5931\u8d25: {e}\")\n        print()\n\n    print(f\"\ud83d\udcca \u6d4b\u8bd5\u7ed3\u679c: {passed}/{total} \u901a\u8fc7\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 \u606d\u559c\uff01Genesis\u5b89\u88c5\u6210\u529f\uff0c\u6240\u6709\u529f\u80fd\u6b63\u5e38\uff01\")\n    elif passed &gt;= total * 0.8:  # 80%\u901a\u8fc7\u7387\n        print(\"\u2705 Genesis\u5b89\u88c5\u57fa\u672c\u6210\u529f\uff01\u68c0\u6d4b\u5230\u5c11\u91cf\u95ee\u9898\u3002\")\n        print(\"   \u5927\u90e8\u5206\u529f\u80fd\u6b63\u5e38\u8fd0\u884c\u3002\u8bf7\u68c0\u67e5\u4e0a\u9762\u7684\u5931\u8d25\u6d4b\u8bd5\u3002\")\n    else:\n        print(\"\u26a0\ufe0f  Genesis\u5b89\u88c5\u5b58\u5728\u95ee\u9898\u3002\u8bf7\u68c0\u67e5\uff1a\")\n        print(\"   1. Genesis\u6b63\u786e\u5b89\u88c5: pip install -e .\")\n        print(\"   2. \u4f9d\u8d56\u5df2\u5b89\u88c5: pip install torch triton\")\n        print(\"   3. Python\u7248\u672c\u4e3a3.8+\")\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u4ee3\u7801\u4fdd\u5b58\u4e3a <code>test_installation.py</code> \u5e76\u8fd0\u884c\uff1a</p> Bash<pre><code>python test_installation.py\n</code></pre>"},{"location":"getting-started/installation.zh/#_10","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u89e3\u51b3","text":""},{"location":"getting-started/installation.zh/#1cuda","title":"\u95ee\u98981\uff1aCUDA\u7248\u672c\u4e0d\u5339\u914d","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>RuntimeError: CUDA version mismatch\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u68c0\u67e5\u7cfb\u7edfCUDA\u7248\u672c\nnvidia-smi\n\n# \u68c0\u67e5PyTorch CUDA\u7248\u672c\npython -c \"import torch; print(torch.version.cuda)\"\n\n# \u91cd\u65b0\u5b89\u88c5\u5339\u914d\u7248\u672c\u7684PyTorch\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"getting-started/installation.zh/#2triton","title":"\u95ee\u98982\uff1aTriton\u7f16\u8bd1\u5931\u8d25","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>Failed to compile Triton kernel\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u5347\u7ea7Triton\npip install --upgrade triton\n\n# \u6216\u5b89\u88c5\u5f00\u53d1\u7248\u672c\npip install --pre triton\n</code></pre></p>"},{"location":"getting-started/installation.zh/#3","title":"\u95ee\u98983\uff1a\u5185\u5b58\u4e0d\u8db3","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>CUDA out of memory\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Python<pre><code>import genesis\n\n# \u542f\u7528\u5185\u5b58\u4f18\u5316\ngenesis.cuda.empty_cache()\n\n# \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\nbatch_size = 16  # \u66ff\u4ee3\u539f\u6765\u768432\n\n# \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9 (\u5982\u679c\u652f\u6301)\nmodel.gradient_checkpointing = True\n</code></pre></p>"},{"location":"getting-started/installation.zh/#4","title":"\u95ee\u98984\uff1a\u5bfc\u5165\u9519\u8bef","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>ModuleNotFoundError: No module named 'genesis'\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u68c0\u67e5\u865a\u62df\u73af\u5883\nwhich python\npip list | grep genesis\n\n# \u91cd\u65b0\u5b89\u88c5\npip uninstall genesis-dl\npip install -e genesis/\n</code></pre></p>"},{"location":"getting-started/installation.zh/#docker","title":"\ud83d\udc33 Docker\u5b89\u88c5","text":"<p>\u5982\u679c\u4f60\u9047\u5230\u73af\u5883\u95ee\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528Docker\uff1a</p> Bash<pre><code># \u4e0b\u8f7d\u9884\u6784\u5efa\u955c\u50cf\ndocker pull genesis/genesis:latest\n\n# \u6216\u6784\u5efa\u81ea\u5df1\u7684\u955c\u50cf\ngit clone https://github.com/phonism/genesis.git\ncd genesis\ndocker build -t genesis:local .\n\n# \u8fd0\u884c\u5bb9\u5668\ndocker run -it --gpus all genesis:local bash\n</code></pre> <p>Dockerfile\u5185\u5bb9\uff1a Docker<pre><code>FROM nvidia/cuda:11.8-devel-ubuntu22.04\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nENV DEBIAN_FRONTEND=noninteractive\nENV PATH=\"/opt/conda/bin:$PATH\"\n\n# \u5b89\u88c5\u7cfb\u7edf\u4f9d\u8d56\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    wget git build-essential &amp;&amp; \\\\\n    rm -rf /var/lib/apt/lists/*\n\n# \u5b89\u88c5Miniconda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda &amp;&amp; \\\\\n    rm Miniconda3-latest-Linux-x86_64.sh\n\n# \u521b\u5efa\u73af\u5883\u5e76\u5b89\u88c5\u4f9d\u8d56\nRUN conda create -n genesis python=3.9 -y\nSHELL [\"conda\", \"run\", \"-n\", \"genesis\", \"/bin/bash\", \"-c\"]\n\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 &amp;&amp; \\\\\n    pip install triton numpy matplotlib tqdm\n\n# \u590d\u5236\u5e76\u5b89\u88c5Genesis\nCOPY . /workspace/genesis\nWORKDIR /workspace/genesis\nRUN pip install -e genesis/\n\n# \u8bbe\u7f6e\u542f\u52a8\u547d\u4ee4\nENTRYPOINT [\"conda\", \"run\", \"-n\", \"genesis\"]\nCMD [\"bash\"]\n</code></pre></p>"},{"location":"getting-started/installation.zh/#_11","title":"\ud83d\udcca \u6027\u80fd\u4f18\u5316\u5efa\u8bae","text":"<p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4f18\u5316\u6027\u80fd\uff1a</p> Bash<pre><code># \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nexport CUDA_VISIBLE_DEVICES=0  # \u6307\u5b9aGPU\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n# \u542f\u7528\u4f18\u5316\u9009\u9879\nexport GENESIS_OPTIMIZE=1\nexport TRITON_CACHE_DIR=/tmp/triton_cache\n</code></pre>"},{"location":"getting-started/installation.zh/#_12","title":"\ud83c\udfaf \u4e0b\u4e00\u6b65","text":"<p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5efa\u8bae\uff1a</p> <ol> <li>\u8fd0\u884c\u7b2c\u4e00\u4e2a\u7a0b\u5e8f - \u9a8c\u8bc1\u5b89\u88c5\u5e76\u5b66\u4e60\u57fa\u7840\u7528\u6cd5</li> <li>\u67e5\u770b\u6559\u7a0b - \u7cfb\u7edf\u5b66\u4e60Genesis\u7684\u4f7f\u7528</li> <li>\u9605\u8bfb\u67b6\u6784\u6587\u6863 - \u7406\u89e3\u6846\u67b6\u8bbe\u8ba1\u7406\u5ff5</li> </ol> <p>\u5982\u679c\u5728\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\uff0c\u8bf7\u67e5\u770bFAQ\u6216\u5728GitHub\u4e0a\u63d0\u4ea4issue\u3002</p>"},{"location":"getting-started/installation_en/","title":"Installation Guide","text":"<p>This guide will help you install the Genesis deep learning framework in different environments.</p>"},{"location":"getting-started/installation_en/#system-requirements","title":"\ud83d\udccb System Requirements","text":""},{"location":"getting-started/installation_en/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: x86_64 architecture with AVX instruction set support</li> <li>Memory: Minimum 8GB, recommended 16GB+</li> <li>GPU: NVIDIA GPU with Compute Capability \u2265 6.0 (optional but recommended)</li> <li>Storage: 2GB available space</li> </ul>"},{"location":"getting-started/installation_en/#software-requirements","title":"Software Requirements","text":"<ul> <li>Operating System: Linux (Ubuntu 20.04+), macOS (10.15+), Windows 10+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>CUDA: 11.0+ (required for GPU acceleration)</li> </ul>"},{"location":"getting-started/installation_en/#quick-installation","title":"\ud83d\ude80 Quick Installation","text":""},{"location":"getting-started/installation_en/#method-1-install-from-source-recommended","title":"Method 1: Install from Source (Recommended)","text":"Bash<pre><code># 1. Clone the repository\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# 2. Create virtual environment (recommended)\npython -m venv genesis-env\nsource genesis-env/bin/activate  # Linux/macOS\n# genesis-env\\\\Scripts\\\\activate  # Windows\n\n# 3. Install dependencies\npip install -r genesis/requirements.txt\n\n# 4. Install Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation_en/#method-2-install-via-pip","title":"Method 2: Install via pip","text":"Bash<pre><code># Install release version\npip install genesis-dl\n\n# Install pre-release version\npip install --pre genesis-dl\n</code></pre>"},{"location":"getting-started/installation_en/#detailed-installation-steps","title":"\ud83d\udd27 Detailed Installation Steps","text":""},{"location":"getting-started/installation_en/#step-1-prepare-python-environment","title":"Step 1: Prepare Python Environment","text":"Ubuntu/DebianCentOS/RHELmacOSWindows Bash<pre><code># Install Python and pip\nsudo apt update\nsudo apt install python3.9 python3.9-pip python3.9-venv\n\n# Create symbolic link (optional)\nsudo ln -sf /usr/bin/python3.9 /usr/bin/python\n</code></pre> Bash<pre><code># Install EPEL repository\nsudo yum install epel-release\n\n# Install Python\nsudo yum install python39 python39-pip\n</code></pre> Bash<pre><code># Install using Homebrew\nbrew install python@3.9\n\n# Or use official installer\n# Download from https://python.org\n</code></pre> PowerShell<pre><code># Download Python installer\n# https://python.org/downloads/windows/\n\n# Or use Chocolatey\nchoco install python39\n</code></pre>"},{"location":"getting-started/installation_en/#step-2-install-cuda-gpu-acceleration","title":"Step 2: Install CUDA (GPU Acceleration)","text":"<p>GPU Support Note</p> <p>You can skip this step if you only need CPU version. However, installing CUDA is strongly recommended for optimal performance.</p> Ubuntu/DebianCentOS/RHELWindows Bash<pre><code># Download CUDA Toolkit\nwget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\nsudo sh cuda_11.8.0_520.61.05_linux.run\n\n# Set environment variables\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> Bash<pre><code># Install NVIDIA driver repository\nsudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\n\n# Install CUDA\nsudo yum install cuda-11-8\n</code></pre> PowerShell<pre><code># Download CUDA installer\n# https://developer.nvidia.com/cuda-downloads\n\n# Run the installer and follow the prompts\n</code></pre>"},{"location":"getting-started/installation_en/#step-3-install-core-dependencies","title":"Step 3: Install Core Dependencies","text":"Bash<pre><code># Create and activate virtual environment\npython -m venv genesis-env\nsource genesis-env/bin/activate\n\n# Upgrade pip\npip install --upgrade pip setuptools wheel\n\n# Install PyTorch (choose based on your CUDA version)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU version\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# Install Triton\npip install triton\n\n# Install other dependencies\npip install numpy matplotlib tqdm\n</code></pre>"},{"location":"getting-started/installation_en/#step-4-install-genesis","title":"Step 4: Install Genesis","text":"Bash<pre><code># Clone source code\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# View available versions\ngit tag\n\n# Switch to stable version (optional)\ngit checkout v0.1.0\n\n# Install Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation_en/#verify-installation","title":"\u2705 Verify Installation","text":"<p>Run the following code to verify that the installation was successful:</p> Python<pre><code>#!/usr/bin/env python3\n\"\"\"Genesis installation verification script\"\"\"\n\ndef test_basic_import():\n    \"\"\"Test basic import\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n        print(\"\u2705 Genesis import successful\")\n        print(f\"   Core modules: genesis, nn, optim\")\n        print(f\"   Available functions: {len([x for x in dir(genesis) if not x.startswith('_')])}\")\n    except ImportError as e:\n        print(f\"\u274c Genesis import failed: {e}\")\n        return False\n    return True\n\ndef test_tensor_operations():\n    \"\"\"Test tensor operations\"\"\"\n    try:\n        import genesis\n\n        # Create tensors\n        x = genesis.randn(3, 4)\n        y = genesis.randn(3, 4)\n\n        # Basic operations\n        z = x + y\n        w = genesis.matmul(x, y.T)  # Use actual Genesis API\n\n        print(\"\u2705 Tensor operations working\")\n        print(f\"   Addition result shape: {z.shape}\")\n        print(f\"   Matrix multiplication shape: {w.shape}\")\n    except Exception as e:\n        print(f\"\u274c Tensor operations failed: {e}\")\n        return False\n    return True\n\ndef test_neural_networks():\n    \"\"\"Test neural network modules\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # Create simple model using actual Genesis modules\n        model = nn.Sequential([\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 1)\n        ])\n\n        # Test forward pass\n        x = genesis.randn(2, 10)\n        y = model(x)\n        print(\"\u2705 Neural network modules working\")\n        print(f\"   Model layers: {len(list(model.parameters()))} parameter tensors\")\n        print(f\"   Output shape: {y.shape}\")\n    except Exception as e:\n        print(f\"\u274c Neural network modules failed: {e}\")\n        return False\n    return True\n\ndef test_backend_support():\n    \"\"\"Test backend support\"\"\"\n    try:\n        import genesis\n        from genesis.backend import default_device\n\n        # Test basic backend functionality\n        device = default_device()\n        x = genesis.randn(5, 5)\n\n        print(\"\u2705 Backend support working\")\n        print(f\"   Default device: {device}\")\n        print(f\"   Tensor device: {x.device}\")\n\n        # Try to detect CUDA if available\n        try:\n            # Test if we can create CUDA tensors\n            import torch\n            if torch.cuda.is_available():\n                print(\"   CUDA detected (via PyTorch backend)\")\n            else:\n                print(\"   CUDA not available (CPU only)\")\n        except:\n            print(\"   Backend: Genesis native\")\n\n    except Exception as e:\n        print(f\"\u274c Backend test failed: {e}\")\n        return False\n    return True\n\ndef test_autograd():\n    \"\"\"Test automatic differentiation\"\"\"\n    try:\n        import genesis\n\n        # Test basic autograd\n        x = genesis.randn(5, requires_grad=True)\n        y = genesis.functional.sum(x * x)  # Use actual Genesis API\n        y.backward()\n\n        print(\"\u2705 Automatic differentiation working\")\n        print(f\"   Input shape: {x.shape}\")\n        print(f\"   Gradient computed: {x.grad is not None}\")\n        print(f\"   Gradient shape: {x.grad.shape if x.grad is not None else 'None'}\")\n    except Exception as e:\n        print(f\"\u274c Automatic differentiation failed: {e}\")\n        return False\n    return True\n\ndef test_optimizers():\n    \"\"\"Test optimizer functionality\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n        import genesis.optim as optim\n\n        # Create a simple model and optimizer\n        model = nn.Linear(5, 1)\n        optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n        # Test basic optimization step\n        x = genesis.randn(3, 5)\n        y_pred = model(x)\n        loss = genesis.functional.sum(y_pred * y_pred)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        print(\"\u2705 Optimizer functionality working\")\n        print(f\"   Optimizer type: {type(optimizer).__name__}\")\n        print(f\"   Learning rate: 0.01\")\n        print(f\"   Parameters updated: {len(list(model.parameters()))}\")\n    except Exception as e:\n        print(f\"\u274c Optimizer test failed: {e}\")\n        return False\n    return True\n\ndef test_serialization():\n    \"\"\"Test model saving/loading\"\"\"\n    try:\n        import genesis\n        import genesis.nn as nn\n\n        # Create and save a model\n        model = nn.Linear(3, 2)\n        state_dict = model.state_dict()\n\n        # Test serialization functionality\n        genesis.save(state_dict, 'test_model.pkl')\n        loaded_state = genesis.load('test_model.pkl')\n\n        print(\"\u2705 Serialization working\")\n        print(f\"   Model saved and loaded successfully\")\n        print(f\"   State dict keys: {len(state_dict)}\")\n\n        # Cleanup\n        import os\n        if os.path.exists('test_model.pkl'):\n            os.remove('test_model.pkl')\n\n    except Exception as e:\n        print(f\"\u274c Serialization test failed: {e}\")\n        return False\n    return True\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd0d Genesis Installation Verification\\n\")\n\n    tests = [\n        test_basic_import,\n        test_tensor_operations,\n        test_neural_networks,\n        test_backend_support,\n        test_autograd,\n        test_optimizers,\n        test_serialization\n    ]\n\n    passed = 0\n    total = len(tests)\n\n    for test in tests:\n        try:\n            if test():\n                passed += 1\n        except Exception as e:\n            print(f\"\u274c Test failed with exception: {e}\")\n        print()\n\n    print(f\"\ud83d\udcca Test Results: {passed}/{total} passed\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 Congratulations! Genesis installation successful, all features working!\")\n    elif passed &gt;= total * 0.8:  # 80% pass rate\n        print(\"\u2705 Genesis installation mostly successful! Minor issues detected.\")\n        print(\"   Most functionality is working. Check failed tests above.\")\n    else:\n        print(\"\u26a0\ufe0f  Genesis installation has issues. Please check:\")\n        print(\"   1. Genesis is properly installed: pip install -e .\")\n        print(\"   2. Dependencies are installed: pip install torch triton\")\n        print(\"   3. Python version is 3.8+\")\n</code></pre> <p>Save the above code as <code>test_installation.py</code> and run:</p> Bash<pre><code>python test_installation.py\n</code></pre>"},{"location":"getting-started/installation_en/#common-issues-and-solutions","title":"\ud83d\udd27 Common Issues and Solutions","text":""},{"location":"getting-started/installation_en/#issue-1-cuda-version-mismatch","title":"Issue 1: CUDA Version Mismatch","text":"<p>Error Message: Text Only<pre><code>RuntimeError: CUDA version mismatch\n</code></pre></p> <p>Solution: Bash<pre><code># Check system CUDA version\nnvidia-smi\n\n# Check PyTorch CUDA version\npython -c \"import torch; print(torch.version.cuda)\"\n\n# Reinstall matching PyTorch version\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"getting-started/installation_en/#issue-2-triton-compilation-failure","title":"Issue 2: Triton Compilation Failure","text":"<p>Error Message: Text Only<pre><code>Failed to compile Triton kernel\n</code></pre></p> <p>Solution: Bash<pre><code># Upgrade Triton\npip install --upgrade triton\n\n# Or install development version\npip install --pre triton\n</code></pre></p>"},{"location":"getting-started/installation_en/#issue-3-out-of-memory","title":"Issue 3: Out of Memory","text":"<p>Error Message: Text Only<pre><code>CUDA out of memory\n</code></pre></p> <p>Solution: Python<pre><code>import genesis\n\n# Enable memory optimization\ngenesis.cuda.empty_cache()\n\n# Reduce batch size\nbatch_size = 16  # Instead of 32\n\n# Enable gradient checkpointing (if supported)\nmodel.gradient_checkpointing = True\n</code></pre></p>"},{"location":"getting-started/installation_en/#issue-4-import-error","title":"Issue 4: Import Error","text":"<p>Error Message: Text Only<pre><code>ModuleNotFoundError: No module named 'genesis'\n</code></pre></p> <p>Solution: Bash<pre><code># Check virtual environment\nwhich python\npip list | grep genesis\n\n# Reinstall\npip uninstall genesis-dl\npip install -e genesis/\n</code></pre></p>"},{"location":"getting-started/installation_en/#docker-installation","title":"\ud83d\udc33 Docker Installation","text":"<p>If you encounter environment issues, you can use Docker:</p> Bash<pre><code># Download pre-built image\ndocker pull genesis/genesis:latest\n\n# Or build your own image\ngit clone https://github.com/phonism/genesis.git\ncd genesis\ndocker build -t genesis:local .\n\n# Run container\ndocker run -it --gpus all genesis:local bash\n</code></pre> <p>Dockerfile contents: Docker<pre><code>FROM nvidia/cuda:11.8-devel-ubuntu22.04\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV PATH=\"/opt/conda/bin:$PATH\"\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    wget git build-essential &amp;&amp; \\\\\n    rm -rf /var/lib/apt/lists/*\n\n# Install Miniconda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda &amp;&amp; \\\\\n    rm Miniconda3-latest-Linux-x86_64.sh\n\n# Create environment and install dependencies\nRUN conda create -n genesis python=3.9 -y\nSHELL [\"conda\", \"run\", \"-n\", \"genesis\", \"/bin/bash\", \"-c\"]\n\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 &amp;&amp; \\\\\n    pip install triton numpy matplotlib tqdm\n\n# Copy and install Genesis\nCOPY . /workspace/genesis\nWORKDIR /workspace/genesis\nRUN pip install -e genesis/\n\n# Set startup command\nENTRYPOINT [\"conda\", \"run\", \"-n\", \"genesis\"]\nCMD [\"bash\"]\n</code></pre></p>"},{"location":"getting-started/installation_en/#performance-optimization-tips","title":"\ud83d\udcca Performance Optimization Tips","text":"<p>After installation, you can optimize performance with:</p> Bash<pre><code># Set environment variables\nexport CUDA_VISIBLE_DEVICES=0  # Specify GPU\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n# Enable optimization options\nexport GENESIS_OPTIMIZE=1\nexport TRITON_CACHE_DIR=/tmp/triton_cache\n</code></pre>"},{"location":"getting-started/installation_en/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<p>After installation, it's recommended to:</p> <ol> <li>Run your first program - Verify installation and learn basic usage</li> <li>Check tutorials - Systematically learn Genesis usage</li> <li>Read architecture documentation - Understand framework design principles</li> </ol> <p>If you encounter problems during installation, please check the FAQ or submit an issue on GitHub.</p>"},{"location":"models/qwen.zh/","title":"Qwen\u6a21\u578b\u5b9e\u73b0","text":""},{"location":"models/qwen.zh/#_1","title":"\u6982\u8ff0","text":"<p>Genesis\u5305\u542b\u4e86Qwen\uff08\u901a\u4e49\u5343\u95ee\uff09\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b8c\u6574\u5b9e\u73b0\uff0c\u652f\u6301\u5b8c\u6574\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"models/qwen.zh/#_2","title":"\u6a21\u578b\u67b6\u6784","text":"<p>Qwen\u6a21\u578b\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a</p> <ul> <li>\u6ce8\u610f\u529b\u673a\u5236: \u591a\u5934\u6ce8\u610f\u529b\u4e0eRoPE\uff08\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff09</li> <li>\u6fc0\u6d3b\u51fd\u6570: SwiGLU\u6fc0\u6d3b\u51fd\u6570</li> <li>\u5c42\u5f52\u4e00\u5316: RMSNorm</li> <li>\u4f4d\u7f6e\u7f16\u7801: \u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09</li> </ul>"},{"location":"models/qwen.zh/#_3","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"models/qwen.zh/#_4","title":"\u57fa\u7840\u63a8\u7406","text":"Python<pre><code>import genesis\nfrom genesis.models.qwen import QwenModel, QwenConfig\n\n# \u521b\u5efa\u6a21\u578b\u914d\u7f6e\nconfig = QwenConfig(\n    vocab_size=32000,\n    n_layer=24,\n    n_head=16,\n    n_embd=2048,\n    max_seq_len=2048\n)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = QwenModel(config)\n\n# \u63a8\u7406\ninput_ids = genesis.tensor([[1, 2, 3, 4, 5]])  # [batch_size, seq_len]\noutput = model(input_ids)\nprint(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")  # [1, 5, 32000]\n</code></pre>"},{"location":"models/qwen.zh/#_5","title":"\u8bad\u7ec3\u793a\u4f8b","text":"Python<pre><code>import genesis.optim as optim\nimport genesis.nn as nn\n\n# \u521b\u5efa\u4f18\u5316\u5668\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor batch in dataloader:\n    input_ids, labels = batch\n\n    # \u524d\u5411\u4f20\u64ad\n    logits = model(input_ids)\n\n    # \u8ba1\u7b97\u635f\u5931\n    loss = nn.functional.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        labels.view(-1)\n    )\n\n    # \u53cd\u5411\u4f20\u64ad\n    optimizer.zero_grad()\n    loss.backward()\n\n    # \u68af\u5ea6\u88c1\u526a\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # \u53c2\u6570\u66f4\u65b0\n    optimizer.step()\n</code></pre>"},{"location":"models/qwen.zh/#_6","title":"\u914d\u7f6e\u53c2\u6570","text":""},{"location":"models/qwen.zh/#qwenconfig","title":"QwenConfig","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>vocab_size</code> int 32000 \u8bcd\u6c47\u8868\u5927\u5c0f <code>n_layer</code> int 24 Transformer\u5c42\u6570 <code>n_head</code> int 16 \u6ce8\u610f\u529b\u5934\u6570 <code>n_embd</code> int 2048 \u9690\u85cf\u5c42\u7ef4\u5ea6 <code>max_seq_len</code> int 2048 \u6700\u5927\u5e8f\u5217\u957f\u5ea6 <code>dropout</code> float 0.1 Dropout\u6982\u7387 <code>bias</code> bool False \u662f\u5426\u4f7f\u7528\u504f\u7f6e\u9879"},{"location":"models/qwen.zh/#_7","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"models/qwen.zh/#_8","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\nfrom genesis.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    logits = model(input_ids)\n    loss = criterion(logits, labels)\n\n# \u68af\u5ea6\u7f29\u653e\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"models/qwen.zh/#_9","title":"\u68af\u5ea6\u68c0\u67e5\u70b9","text":"Python<pre><code># \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\u4ee5\u8282\u7701\u5185\u5b58\nmodel.enable_gradient_checkpointing()\n</code></pre>"},{"location":"models/qwen.zh/#_10","title":"\u5e94\u7528\u793a\u4f8b","text":""},{"location":"models/qwen.zh/#_11","title":"\u6587\u672c\u751f\u6210","text":"Python<pre><code>def generate_text(model, tokenizer, prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = genesis.tensor([input_ids])\n\n    with genesis.no_grad():\n        for _ in range(max_length):\n            logits = model(input_tensor)\n            next_token = logits[0, -1].argmax()\n            input_tensor = genesis.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(input_tensor[0].tolist())\n\n# \u4f7f\u7528\u793a\u4f8b\ngenerated = generate_text(model, tokenizer, \"\u4eca\u5929\u7684\u5929\u6c14\")\nprint(generated)\n</code></pre>"},{"location":"models/qwen.zh/#_12","title":"\u5fae\u8c03","text":"<p>\u53c2\u8003 <code>apps/llm/train_sft_qwen.py</code> \u83b7\u5f97\u5b8c\u6574\u7684SFT\uff08\u76d1\u7763\u5fae\u8c03\uff09\u5b9e\u73b0\u3002</p> Bash<pre><code># \u8fd0\u884cSFT\u8bad\u7ec3\ncd apps/llm\npython train_sft_qwen.py \\\n    --model_size 0.5b \\\n    --data_path /path/to/data \\\n    --batch_size 4 \\\n    --learning_rate 5e-5 \\\n    --num_epochs 3\n</code></pre>"},{"location":"models/qwen.zh/#_13","title":"\u6587\u4ef6\u7ed3\u6784","text":"<ul> <li><code>genesis/models/qwen.py</code> - \u6a21\u578b\u5b9e\u73b0</li> <li><code>apps/llm/qwen_model.py</code> - \u8bad\u7ec3\u914d\u7f6e\u548c\u5de5\u5177</li> <li><code>apps/llm/train_sft_qwen.py</code> - SFT\u8bad\u7ec3\u811a\u672c</li> <li><code>apps/llm/chat_qwen.py</code> - \u63a8\u7406\u804a\u5929\u811a\u672c</li> </ul>"},{"location":"models/qwen.zh/#2025-01","title":"\u6700\u65b0\u66f4\u65b0 (2025-01)","text":"<ul> <li>\u2705 \u5b8c\u6574\u7684Qwen\u6a21\u578b\u5b9e\u73b0\uff0c\u652f\u6301RoPE\u548cRMSNorm</li> <li>\u2705 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u652f\u6301 (FP16/BF16)</li> <li>\u2705 \u68af\u5ea6\u88c1\u526a\u548c\u5b66\u4e60\u7387\u8c03\u5ea6</li> <li>\u2705 SFT\u8bad\u7ec3\u811a\u672c\u548c\u804a\u5929\u754c\u9762</li> <li>\u2705 \u6a21\u578b\u68c0\u67e5\u70b9\u548c\u72b6\u6001\u7ba1\u7406</li> <li>\ud83d\udea7 \u6027\u80fd\u4f18\u5316\u6301\u7eed\u8fdb\u884c\u4e2d</li> </ul>"},{"location":"models/qwen.zh/#_14","title":"\u76f8\u5173\u8d44\u6e90","text":"<ul> <li>Qwen\u5b98\u65b9\u8bba\u6587</li> <li>\u9ad8\u7ea7\u8bad\u7ec3\u7279\u6027</li> <li>\u6027\u80fd\u8c03\u4f18\u6307\u5357</li> <li>Genesis API\u53c2\u8003</li> </ul>"},{"location":"models/qwen_en/","title":"Qwen Model Implementation","text":""},{"location":"models/qwen_en/#overview","title":"Overview","text":"<p>The Genesis framework includes a built-in implementation of the Qwen (Tongyi Qianwen) large language model, supporting complete training and inference workflows.</p>"},{"location":"models/qwen_en/#model-architecture","title":"Model Architecture","text":"<p>The Qwen model is based on the Transformer architecture with the following features:</p> <ul> <li>Attention Mechanism: Multi-Head Attention with RoPE (Rotary Position Embedding)</li> <li>Activation Function: SwiGLU activation</li> <li>Layer Normalization: RMSNorm</li> <li>Position Encoding: Rotary Position Embedding (RoPE)</li> </ul>"},{"location":"models/qwen_en/#quick-start","title":"Quick Start","text":""},{"location":"models/qwen_en/#basic-inference","title":"Basic Inference","text":"Python<pre><code>import genesis\nfrom genesis.models.qwen import QwenModel, QwenConfig\n\n# Create model configuration\nconfig = QwenConfig(\n    vocab_size=32000,\n    n_layer=24,\n    n_head=16,\n    n_embd=2048,\n    max_seq_len=2048\n)\n\n# Create model\nmodel = QwenModel(config)\n\n# Inference\ninput_ids = genesis.tensor([[1, 2, 3, 4, 5]])  # [batch_size, seq_len]\noutput = model(input_ids)\nprint(f\"Output shape: {output.shape}\")  # [1, 5, 32000]\n</code></pre>"},{"location":"models/qwen_en/#training-example","title":"Training Example","text":"Python<pre><code>import genesis.optim as optim\nimport genesis.nn as nn\n\n# Create optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# Training loop\nfor batch in dataloader:\n    input_ids, labels = batch\n\n    # Forward pass\n    logits = model(input_ids)\n\n    # Calculate loss\n    loss = nn.functional.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        labels.view(-1)\n    )\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Gradient clipping\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # Parameter update\n    optimizer.step()\n</code></pre>"},{"location":"models/qwen_en/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"models/qwen_en/#qwenconfig","title":"QwenConfig","text":"Parameter Type Default Description <code>vocab_size</code> int 32000 Vocabulary size <code>n_layer</code> int 24 Number of Transformer layers <code>n_head</code> int 16 Number of attention heads <code>n_embd</code> int 2048 Hidden dimension <code>max_seq_len</code> int 2048 Maximum sequence length <code>dropout</code> float 0.1 Dropout probability <code>bias</code> bool False Whether to use bias"},{"location":"models/qwen_en/#performance-optimization","title":"Performance Optimization","text":""},{"location":"models/qwen_en/#mixed-precision-training","title":"Mixed Precision Training","text":"Python<pre><code># Enable mixed precision\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    logits = model(input_ids)\n    loss = criterion(logits, labels)\n\n# Gradient scaling\nscaler = genesis.GradScaler()\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"models/qwen_en/#gradient-checkpointing","title":"Gradient Checkpointing","text":"Python<pre><code># Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing = True\n</code></pre>"},{"location":"models/qwen_en/#application-examples","title":"Application Examples","text":""},{"location":"models/qwen_en/#text-generation","title":"Text Generation","text":"Python<pre><code>def generate_text(model, tokenizer, prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = genesis.tensor([input_ids])\n\n    with genesis.no_grad():\n        for _ in range(max_length):\n            logits = model(input_tensor)\n            next_token = logits[0, -1].argmax()\n            input_tensor = genesis.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(input_tensor[0].tolist())\n\n# Usage example\ngenerated = generate_text(model, tokenizer, \"Today's weather\")\nprint(generated)\n</code></pre>"},{"location":"models/qwen_en/#fine-tuning-training","title":"Fine-tuning Training","text":"<p>Refer to <code>apps/llm/train_sft_qwen.py</code> for complete SFT (Supervised Fine-tuning) implementation.</p>"},{"location":"models/qwen_en/#file-structure","title":"File Structure","text":"<ul> <li><code>genesis/models/qwen.py</code> - Model implementation</li> <li><code>apps/llm/qwen_model.py</code> - Training configuration and utilities</li> <li><code>apps/llm/train_sft_qwen.py</code> - SFT training script</li> <li><code>apps/llm/chat_qwen.py</code> - Inference chat script</li> </ul>"},{"location":"models/qwen_en/#related-resources","title":"Related Resources","text":"<ul> <li>Qwen Official Paper</li> <li>RoPE Position Encoding Explained</li> <li>LLM Training Best Practices</li> </ul>"},{"location":"performance/gpu-operations.zh/","title":"GPU\u64cd\u4f5c\u6027\u80fd\u6307\u5357","text":"<p>\u672c\u6307\u5357\u6db5\u76d6Genesis\u4e2dGPU\u64cd\u4f5c\u7684\u4f18\u5316\uff0c\u91cd\u70b9\u4ecb\u7ecd\u6a21\u5757\u5316GPU\u64cd\u4f5c\u7ed3\u6784\u3001Triton\u5185\u6838\u5b9e\u73b0\u548c\u6027\u80fd\u8c03\u4f18\u7b56\u7565\u3002</p>"},{"location":"performance/gpu-operations.zh/#_1","title":"\u6982\u8ff0","text":"<p>Genesis\u5b9e\u73b0\u4e86\u590d\u6742\u7684GPU\u540e\u7aef\uff0c\u5305\u62ec\uff1a - \u4f7f\u7528Triton\u7684\u6a21\u5757\u5316GPU\u64cd\u4f5c - \u81ea\u5b9a\u4e49CUDA\u5185\u5b58\u7ba1\u7406 - \u81ea\u9002\u5e94\u5757\u5927\u5c0f\u4f18\u5316 - \u6027\u80fd\u76d1\u63a7\u548c\u5206\u6790\u5de5\u5177</p>"},{"location":"performance/gpu-operations.zh/#_2","title":"\u67b6\u6784\u6982\u8ff0","text":""},{"location":"performance/gpu-operations.zh/#gpu_1","title":"\u6a21\u5757\u5316GPU\u64cd\u4f5c\u7ed3\u6784","text":"<p>Genesis\u5c06GPU\u64cd\u4f5c\u5206\u79bb\u4e3a\u4e13\u95e8\u7684\u6a21\u5757\uff1a</p> Text Only<pre><code>genesis/ndarray/gpu_ops/\n\u251c\u2500\u2500 __init__.py          # \u64cd\u4f5c\u6ce8\u518c\u548c\u5206\u6d3e\n\u251c\u2500\u2500 basic_ops.py         # \u9010\u5143\u7d20\u64cd\u4f5c\uff08add\u3001mul\u7b49\uff09\n\u251c\u2500\u2500 tensor_ops.py        # \u5f20\u91cf\u64cd\u4f5c\uff08matmul\u3001conv\u7b49\uff09  \n\u251c\u2500\u2500 random_ops.py        # \u968f\u673a\u6570\u751f\u6210\n\u2514\u2500\u2500 reduction_ops.py     # \u7ea6\u7b80\u64cd\u4f5c\uff08sum\u3001mean\u7b49\uff09\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_3","title":"\u64cd\u4f5c\u5206\u6d3e\u7cfb\u7edf","text":"Python<pre><code># genesis/ndarray/gpu_ops/__init__.py\nfrom .basic_ops import add_triton, mul_triton, div_triton\nfrom .tensor_ops import matmul_triton, conv2d_triton  \nfrom .reduction_ops import sum_triton, mean_triton\n\n# \u52a8\u6001\u5206\u6d3e\u7684\u64cd\u4f5c\u6ce8\u518c\u8868\nGPU_OPS_REGISTRY = {\n    'add': add_triton,\n    'mul': mul_triton,\n    'div': div_triton,\n    'matmul': matmul_triton,\n    'sum': sum_triton,\n    'mean': mean_triton,\n}\n\ndef dispatch_gpu_op(op_name, *args, **kwargs):\n    \"\"\"\u5c06\u64cd\u4f5c\u5206\u6d3e\u5230\u76f8\u5e94\u7684GPU\u5185\u6838\u3002\"\"\"\n    if op_name not in GPU_OPS_REGISTRY:\n        raise NotImplementedError(f\"GPU\u64cd\u4f5c {op_name} \u672a\u5b9e\u73b0\")\n\n    return GPU_OPS_REGISTRY[op_name](*args, **kwargs)\n</code></pre>"},{"location":"performance/gpu-operations.zh/#triton","title":"Triton\u5185\u6838\u5b9e\u73b0","text":""},{"location":"performance/gpu-operations.zh/#_4","title":"\u57fa\u672c\u9010\u5143\u7d20\u64cd\u4f5c","text":"Python<pre><code># genesis/ndarray/gpu_ops/basic_ops.py\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u4f18\u5316\u7684\u9010\u5143\u7d20\u52a0\u6cd5\u5185\u6838\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # \u4f7f\u7528\u5411\u91cf\u5316\u52a0\u8f7d\u6570\u636e\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    # \u8ba1\u7b97\n    output = x + y\n\n    # \u5b58\u50a8\u7ed3\u679c\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add_triton(x, y):\n    \"\"\"\u57fa\u4e8eTriton\u7684\u5f20\u91cf\u52a0\u6cd5\u3002\"\"\"\n    output = genesis.empty_like(x)\n    n_elements = x.numel()\n\n    # \u57fa\u4e8e\u5f20\u91cf\u5927\u5c0f\u7684\u81ea\u9002\u5e94\u5757\u5927\u5c0f\n    if n_elements &lt; 262144:  # &lt; 256K\u5143\u7d20\n        BLOCK_SIZE = 256\n    elif n_elements &lt; 4194304:  # &lt; 4M\u5143\u7d20  \n        BLOCK_SIZE = 512\n    else:\n        BLOCK_SIZE = 1024\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](\n        x.data_ptr(), y.data_ptr(), output.data_ptr(),\n        n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_5","title":"\u9ad8\u7ea7\u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code># genesis/ndarray/gpu_ops/tensor_ops.py\n@triton.jit  \ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"\u9ad8\u6027\u80fd\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] &lt; K - k * BLOCK_SIZE_K))\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] &lt; K - k * BLOCK_SIZE_K))\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul_triton(a, b):\n    \"\"\"\u4f7f\u7528Triton\u4f18\u5316\u7684\u77e9\u9635\u4e58\u6cd5\u3002\"\"\"\n    assert a.shape[-1] == b.shape[-2], f\"\u5f62\u72b6\u4e0d\u5339\u914d: {a.shape} @ {b.shape}\"\n\n    M, K = a.shape[-2:]\n    K2, N = b.shape[-2:]\n    assert K == K2\n\n    c = genesis.empty((*a.shape[:-2], M, N), dtype=a.dtype, device=a.device)\n\n    # \u57fa\u4e8e\u95ee\u9898\u5927\u5c0f\u4f18\u5316\u5757\u5927\u5c0f\n    if M &gt;= 2048 and N &gt;= 2048:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 128, 32\n    elif M &gt;= 512 and N &gt;= 512:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 64, 64, 32\n    else:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 32, 32, 32\n\n    grid = lambda meta: (\n        triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),\n    )\n\n    matmul_kernel[grid](\n        a.data_ptr(), b.data_ptr(), c.data_ptr(),\n        M, N, K,\n        a.stride(-2), a.stride(-1),\n        b.stride(-2), b.stride(-1),\n        c.stride(-2), c.stride(-1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N, \n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_6","title":"\u5185\u5b58\u4f18\u5316\u7684\u7ea6\u7b80\u64cd\u4f5c","text":"Python<pre><code># genesis/ndarray/gpu_ops/reduction_ops.py\n@triton.jit\ndef sum_kernel(\n    input_ptr, output_ptr, \n    n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\u5185\u5b58\u9ad8\u6548\u7684\u6c42\u548c\u5185\u6838\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # \u6309\u5757\u52a0\u8f7d\u548c\u6c42\u548c\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    block_sum = tl.sum(x)\n\n    # \u4f7f\u7528\u539f\u5b50\u52a0\u6cd5\u8fdb\u884c\u6700\u7ec8\u7ea6\u7b80\n    tl.atomic_add(output_ptr, block_sum)\n\n@triton.jit\ndef reduce_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE_X: tl.constexpr,\n    BLOCK_SIZE_Y: tl.constexpr  \n):\n    \"\"\"\u5177\u6709\u6700\u4f18\u5185\u5b58\u8bbf\u95ee\u76842D\u7ea6\u7b80\u5185\u6838\u3002\"\"\"\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n\n    offs_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offs_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n\n    mask_x = offs_x &lt; n_rows\n    mask_y = offs_y &lt; n_cols\n\n    # \u52a0\u8f7d\u5757\n    ptrs = input_ptr + offs_x[:, None] * n_cols + offs_y[None, :]\n    mask = mask_x[:, None] &amp; mask_y[None, :]\n    x = tl.load(ptrs, mask=mask, other=0.0)\n\n    # \u5757\u5185\u7ea6\u7b80\n    result = tl.sum(x, axis=1)  # \u8de8\u5217\u6c42\u548c\n\n    # \u5b58\u50a8\u7ed3\u679c\n    out_ptrs = output_ptr + offs_x\n    tl.store(out_ptrs, result, mask=mask_x)\n\ndef sum_triton(x, dim=None, keepdim=False):\n    \"\"\"\u4f18\u5316\u7684\u5f20\u91cf\u6c42\u548c\u3002\"\"\"\n    if dim is None:\n        # \u5168\u5c40\u6c42\u548c\n        result = genesis.zeros((), dtype=x.dtype, device=x.device)\n        n_elements = x.numel()\n\n        BLOCK_SIZE = min(1024, triton.next_power_of_2(n_elements))\n        grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n\n        sum_kernel[grid](\n            x.data_ptr(), result.data_ptr(),\n            n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n\n        return result\n\n    else:\n        # \u7279\u5b9a\u7ef4\u5ea6\u7ea6\u7b80\n        # \u5b9e\u73b0\u7279\u5b9a\u7ef4\u5ea6\u7ea6\u7b80\n        return reduce_along_dim(x, dim, keepdim)\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_7","title":"\u6027\u80fd\u4f18\u5316\u7b56\u7565","text":""},{"location":"performance/gpu-operations.zh/#1","title":"1. \u81ea\u9002\u5e94\u5757\u5927\u5c0f\u4f18\u5316","text":"Python<pre><code>class AdaptiveBlockSize:\n    \"\"\"\u57fa\u4e8e\u5f20\u91cf\u7279\u6027\u52a8\u6001\u4f18\u5316\u5757\u5927\u5c0f\u3002\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n        self.performance_history = {}\n\n    def get_optimal_block_size(self, operation, tensor_size, dtype):\n        \"\"\"\u83b7\u53d6\u7ed9\u5b9a\u64cd\u4f5c\u548c\u5f20\u91cf\u7684\u6700\u4f18\u5757\u5927\u5c0f\u3002\"\"\"\n        cache_key = (operation, tensor_size, dtype.name)\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # \u57fa\u4e8e\u5f20\u91cf\u5927\u5c0f\u548c\u64cd\u4f5c\u786e\u5b9a\u5757\u5927\u5c0f\n        if operation == 'elementwise':\n            if tensor_size &lt; 262144:  # &lt; 256K\u5143\u7d20\n                block_size = 256\n            elif tensor_size &lt; 4194304:  # &lt; 4M\u5143\u7d20\n                block_size = 512  \n            else:\n                block_size = 1024\n\n        elif operation == 'matmul':\n            # \u77e9\u9635\u4e58\u6cd5\u7279\u5b9a\u4f18\u5316\n            sqrt_size = int(tensor_size ** 0.5)\n            if sqrt_size &lt; 512:\n                block_size = (32, 32, 32)\n            elif sqrt_size &lt; 2048:\n                block_size = (64, 64, 32)\n            else:\n                block_size = (128, 128, 32)\n\n        elif operation == 'reduction':\n            # \u7ea6\u7b80\u64cd\u4f5c\u4f18\u5316\n            block_size = min(1024, triton.next_power_of_2(tensor_size))\n\n        else:\n            # \u9ed8\u8ba4\u56de\u9000\n            block_size = 512\n\n        self.cache[cache_key] = block_size\n        return block_size\n\n    def update_performance(self, operation, tensor_size, dtype, block_size, elapsed_time):\n        \"\"\"\u66f4\u65b0\u6027\u80fd\u5386\u53f2\u8bb0\u5f55\u7528\u4e8e\u672a\u6765\u4f18\u5316\u3002\"\"\"\n        key = (operation, tensor_size, dtype.name, block_size)\n        if key not in self.performance_history:\n            self.performance_history[key] = []\n\n        self.performance_history[key].append(elapsed_time)\n\n        # \u53ea\u4fdd\u7559\u6700\u8fd1\u7684\u6d4b\u91cf\n        if len(self.performance_history[key]) &gt; 10:\n            self.performance_history[key] = self.performance_history[key][-10:]\n\n# \u5168\u5c40\u4f18\u5316\u5668\u5b9e\u4f8b\nblock_optimizer = AdaptiveBlockSize()\n</code></pre>"},{"location":"performance/gpu-operations.zh/#2","title":"2. \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4f18\u5316","text":"Python<pre><code>@triton.jit\ndef coalesced_copy_kernel(\n    src_ptr, dst_ptr,\n    n_elements, stride_src, stride_dst,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\u5185\u5b58\u5408\u5e76\u7684\u5f20\u91cf\u62f7\u8d1d\u5185\u6838\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # \u786e\u4fdd\u5408\u5e76\u7684\u5185\u5b58\u8bbf\u95ee\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # \u4f7f\u7528\u9002\u5f53\u7684\u6b65\u957f\u52a0\u8f7d\n    src_offsets = offsets * stride_src\n    dst_offsets = offsets * stride_dst\n\n    data = tl.load(src_ptr + src_offsets, mask=mask)\n    tl.store(dst_ptr + dst_offsets, data, mask=mask)\n\n@triton.jit  \ndef transpose_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\u7f13\u5b58\u53cb\u597d\u7684\u77e9\u9635\u8f6c\u7f6e\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # \u57fa\u4e8etile\u7684\u8f6c\u7f6e\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u7f13\u5b58\u4f7f\u7528\n    row_start = (pid // (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n    col_start = (pid % (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n\n    rows = row_start + tl.arange(0, BLOCK_SIZE)\n    cols = col_start + tl.arange(0, BLOCK_SIZE)\n\n    row_mask = rows &lt; n_rows\n    col_mask = cols &lt; n_cols\n\n    # \u52a0\u8f7dtile\n    input_offsets = rows[:, None] * n_cols + cols[None, :]\n    mask = row_mask[:, None] &amp; col_mask[None, :]\n\n    tile = tl.load(input_ptr + input_offsets, mask=mask)\n\n    # \u5b58\u50a8\u8f6c\u7f6e\u7684tile\n    output_offsets = cols[:, None] * n_rows + rows[None, :]\n    tl.store(output_ptr + output_offsets, tl.trans(tile), mask=tl.trans(mask))\n</code></pre>"},{"location":"performance/gpu-operations.zh/#3","title":"3. \u5185\u6838\u878d\u5408\u4f18\u5316","text":"Python<pre><code>@triton.jit\ndef fused_linear_relu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    n_batch, n_input, n_output,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    \"\"\"\u878d\u5408\u7684\u7ebf\u6027\u5c42 + ReLU\u5185\u6838\u4ee5\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u3002\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # \u77e9\u9635\u4e58\u6cd5\u903b\u8f91\uff08\u7b80\u5316\uff09\n    # ... matmul computation ...\n\n    # \u878d\u5408\u7684ReLU\u6fc0\u6d3b\n    result = tl.maximum(matmul_result + bias, 0.0)\n\n    # \u5355\u6b21\u5185\u5b58\u5199\u5165\n    tl.store(output_ptr + offsets, result, mask=mask)\n\n@triton.jit\ndef fused_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    seq_len, head_dim,\n    scale: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\u878d\u5408\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u5185\u6838\u3002\"\"\"\n    # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n    scores = compute_qk_scores(q_ptr, k_ptr, seq_len, head_dim)\n\n    # \u5e94\u7528\u7f29\u653e\u548csoftmax\n    scores = scores * scale\n    attention_weights = tl_softmax(scores, axis=-1)\n\n    # \u5c06\u6ce8\u610f\u529b\u5e94\u7528\u5230\u503c\n    output = compute_attention_output(attention_weights, v_ptr, seq_len, head_dim)\n\n    # \u5355\u6b21\u8f93\u51fa\u5199\u5165\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef fused_linear_relu(x, weight, bias):\n    \"\"\"\u878d\u5408\u7684\u7ebf\u6027\u5c42\u4e0eReLU\u6fc0\u6d3b\u3002\"\"\"\n    batch_size, input_size = x.shape\n    output_size = weight.shape[0]\n\n    output = genesis.empty(batch_size, output_size, dtype=x.dtype, device=x.device)\n\n    # \u878d\u5408\u7684\u6700\u4f18\u5757\u5927\u5c0f\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64  \n    BLOCK_SIZE_K = 32\n\n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']) * \n        triton.cdiv(output_size, meta['BLOCK_SIZE_N']),\n    )\n\n    fused_linear_relu_kernel[grid](\n        x.data_ptr(), weight.data_ptr(), bias.data_ptr(), output.data_ptr(),\n        batch_size, input_size, output_size,\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_8","title":"\u6027\u80fd\u76d1\u63a7\u548c\u5206\u6790","text":""},{"location":"performance/gpu-operations.zh/#1_1","title":"1. \u5185\u7f6e\u6027\u80fd\u6307\u6807","text":"Python<pre><code>import time\nimport contextlib\n\nclass GPUProfiler:\n    \"\"\"\u5206\u6790GPU\u64cd\u4f5c\u6027\u80fd\u3002\"\"\"\n\n    def __init__(self):\n        self.metrics = {}\n        self.current_operation = None\n\n    @contextlib.contextmanager\n    def profile_operation(self, operation_name):\n        \"\"\"\u7528\u4e8e\u5206\u6790\u64cd\u4f5c\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\u3002\"\"\"\n        self.current_operation = operation_name\n\n        # \u8ba1\u65f6\u524d\u540c\u6b65\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        try:\n            yield\n        finally:\n            genesis.cuda.synchronize()\n            end_time = time.perf_counter()\n\n            elapsed = end_time - start_time\n            if operation_name not in self.metrics:\n                self.metrics[operation_name] = []\n\n            self.metrics[operation_name].append(elapsed)\n\n    def get_stats(self, operation_name=None):\n        \"\"\"\u83b7\u53d6\u6027\u80fd\u7edf\u8ba1\u3002\"\"\"\n        if operation_name:\n            times = self.metrics.get(operation_name, [])\n            if not times:\n                return None\n\n            return {\n                'mean': sum(times) / len(times),\n                'min': min(times),\n                'max': max(times),\n                'count': len(times)\n            }\n        else:\n            stats = {}\n            for op_name in self.metrics:\n                stats[op_name] = self.get_stats(op_name)\n            return stats\n\n    def print_summary(self):\n        \"\"\"\u6253\u5370\u6027\u80fd\u6458\u8981\u3002\"\"\"\n        print(\"GPU\u64cd\u4f5c\u6027\u80fd\u6458\u8981:\")\n        print(\"-\" * 50)\n\n        for op_name, stats in self.get_stats().items():\n            print(f\"{op_name}:\")\n            print(f\"  \u5e73\u5747: {stats['mean']*1000:.3f}ms\")\n            print(f\"  \u6700\u5c0f:  {stats['min']*1000:.3f}ms\")\n            print(f\"  \u6700\u5927:  {stats['max']*1000:.3f}ms\")\n            print(f\"  \u6b21\u6570: {stats['count']}\")\n            print()\n\n# \u5168\u5c40\u5206\u6790\u5668\u5b9e\u4f8b\ngpu_profiler = GPUProfiler()\n\n# \u5728\u64cd\u4f5c\u4e2d\u4f7f\u7528\u5206\u6790\u5668\u7684\u793a\u4f8b\ndef add_with_profiling(x, y):\n    with gpu_profiler.profile_operation('add'):\n        return add_triton(x, y)\n</code></pre>"},{"location":"performance/gpu-operations.zh/#2_1","title":"2. \u5185\u5b58\u5e26\u5bbd\u5206\u6790","text":"Python<pre><code>def analyze_memory_bandwidth(operation_func, tensor_sizes, dtype=genesis.float32):\n    \"\"\"\u5206\u6790\u64cd\u4f5c\u7684\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\u3002\"\"\"\n\n    results = []\n    theoretical_bandwidth = 1555e9  # A800 HBM2e\u5e26\u5bbd\uff0c\u5b57\u8282/\u79d2\n\n    for size in tensor_sizes:\n        # \u521b\u5efa\u6d4b\u8bd5\u5f20\u91cf\n        if isinstance(size, tuple):\n            x = genesis.randn(*size, dtype=dtype, device='cuda')\n            y = genesis.randn(*size, dtype=dtype, device='cuda')\n        else:\n            x = genesis.randn(size, dtype=dtype, device='cuda')\n            y = genesis.randn(size, dtype=dtype, device='cuda')\n\n        # \u8ba1\u7b97\u7406\u8bba\u4f20\u8f93\u7684\u5b57\u8282\u6570\n        bytes_per_element = dtype.itemsize\n        total_elements = x.numel()\n\n        # \u5bf9\u4e8e\u4e8c\u5143\u64cd\u4f5c\uff1a\u8bfb\u53d62\u4e2a\u5f20\u91cf + \u5199\u51651\u4e2a\u5f20\u91cf\n        total_bytes = total_elements * bytes_per_element * 3\n\n        # \u9884\u70ed\n        for _ in range(5):\n            _ = operation_func(x, y)\n\n        # \u8ba1\u65f6\u64cd\u4f5c\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        num_iterations = 10\n        for _ in range(num_iterations):\n            result = operation_func(x, y)\n\n        genesis.cuda.synchronize()\n        end_time = time.perf_counter()\n\n        # \u8ba1\u7b97\u6307\u6807\n        elapsed_time = (end_time - start_time) / num_iterations\n        achieved_bandwidth = total_bytes / elapsed_time\n        bandwidth_efficiency = achieved_bandwidth / theoretical_bandwidth\n\n        results.append({\n            'size': size,\n            'elements': total_elements,\n            'elapsed_ms': elapsed_time * 1000,\n            'bandwidth_gb_s': achieved_bandwidth / 1e9,\n            'efficiency_percent': bandwidth_efficiency * 100,\n            'theoretical_gb_s': theoretical_bandwidth / 1e9\n        })\n\n        print(f\"\u5927\u5c0f {size}: {achieved_bandwidth/1e9:.1f} GB/s ({bandwidth_efficiency:.1%})\")\n\n    return results\n\n# \u5206\u6790\u52a0\u6cd5\u64cd\u4f5c\u6027\u80fd\nsizes = [(256, 256), (1024, 1024), (2048, 2048), (4096, 4096)]\nbandwidth_results = analyze_memory_bandwidth(add_triton, sizes)\n</code></pre>"},{"location":"performance/gpu-operations.zh/#3_1","title":"3. \u81ea\u52a8\u6027\u80fd\u8c03\u4f18","text":"Python<pre><code>class AutoTuner:\n    \"\"\"\u81ea\u52a8\u8c03\u4f18\u5185\u6838\u53c2\u6570\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002\"\"\"\n\n    def __init__(self):\n        self.best_configs = {}\n\n    def tune_kernel(self, kernel_func, test_inputs, param_space):\n        \"\"\"\u81ea\u52a8\u8c03\u4f18\u5185\u6838\u53c2\u6570\u3002\"\"\"\n        best_time = float('inf')\n        best_config = None\n\n        print(f\"\u6b63\u5728\u8c03\u4f18\u5185\u6838\uff0c\u5171\u6709 {len(param_space)} \u4e2a\u914d\u7f6e...\")\n\n        for i, config in enumerate(param_space):\n            try:\n                # \u9884\u70ed\n                for _ in range(3):\n                    _ = kernel_func(*test_inputs, **config)\n\n                # \u8ba1\u65f6\u6267\u884c\n                genesis.cuda.synchronize()\n                start_time = time.perf_counter()\n\n                num_runs = 10\n                for _ in range(num_runs):\n                    result = kernel_func(*test_inputs, **config)\n\n                genesis.cuda.synchronize()\n                end_time = time.perf_counter()\n\n                elapsed = (end_time - start_time) / num_runs\n\n                if elapsed &lt; best_time:\n                    best_time = elapsed\n                    best_config = config\n\n                print(f\"\u914d\u7f6e {i+1}: {elapsed*1000:.3f}ms - {config}\")\n\n            except Exception as e:\n                print(f\"\u914d\u7f6e {i+1} \u5931\u8d25: {e}\")\n\n        print(f\"\u6700\u4f73\u914d\u7f6e: {best_config} ({best_time*1000:.3f}ms)\")\n        return best_config, best_time\n\n# \u77e9\u9635\u4e58\u6cd5\u81ea\u52a8\u8c03\u4f18\u793a\u4f8b\ndef tune_matmul():\n    # \u5b9a\u4e49\u53c2\u6570\u7a7a\u95f4\n    block_sizes = [\n        {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64},\n    ]\n\n    # \u6d4b\u8bd5\u8f93\u5165\n    a = genesis.randn(1024, 1024, device='cuda')\n    b = genesis.randn(1024, 1024, device='cuda')\n\n    # \u8fd0\u884c\u81ea\u52a8\u8c03\u4f18\u5668\n    tuner = AutoTuner()\n    best_config, best_time = tuner.tune_kernel(\n        matmul_triton, [a, b], block_sizes\n    )\n\n    return best_config\n</code></pre>"},{"location":"performance/gpu-operations.zh/#_9","title":"\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"performance/gpu-operations.zh/#1_2","title":"1. \u5185\u6838\u5f00\u53d1\u6307\u5357","text":"<ul> <li>\u5185\u5b58\u5408\u5e76: \u786e\u4fdd\u8fde\u7eed\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f</li> <li>\u5757\u5927\u5c0f\u4f18\u5316: \u4f7f\u75282\u7684\u5e42\uff0c\u8003\u8651\u5360\u7528\u7387</li> <li>\u5bc4\u5b58\u5668\u4f7f\u7528: \u76d1\u63a7\u5927\u5185\u6838\u7684\u5bc4\u5b58\u5668\u6ea2\u51fa</li> <li>\u5171\u4eab\u5185\u5b58: \u4e3a\u6570\u636e\u91cd\u7528\u4f7f\u7528\u5171\u4eab\u5185\u5b58</li> <li>\u5206\u652f\u53d1\u6563\u6700\u5c0f\u5316: \u53ef\u80fd\u65f6\u907f\u514d\u6761\u4ef6\u5206\u652f</li> </ul>"},{"location":"performance/gpu-operations.zh/#2_2","title":"2. \u6027\u80fd\u4f18\u5316\u68c0\u67e5\u6e05\u5355","text":"<ul> <li> \u5206\u6790\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387</li> <li> \u4e3a\u76ee\u6807GPU\u4f18\u5316\u5757\u5927\u5c0f</li> <li> \u6700\u5c0f\u5316\u5185\u6838\u542f\u52a8\u5f00\u9500</li> <li> \u4e3a\u76f8\u5173\u64cd\u4f5c\u4f7f\u7528\u5185\u6838\u878d\u5408</li> <li> \u76d1\u63a7GPU\u5360\u7528\u7387\u548c\u8d44\u6e90\u4f7f\u7528</li> <li> \u4f18\u5316\u540e\u9a8c\u8bc1\u6570\u503c\u7cbe\u5ea6</li> </ul>"},{"location":"performance/gpu-operations.zh/#3-gpu","title":"3. \u8c03\u8bd5GPU\u64cd\u4f5c","text":"Python<pre><code>def debug_gpu_operation(operation_func, *inputs):\n    \"\"\"\u4f7f\u7528\u8be6\u7ec6\u5206\u6790\u8c03\u8bd5GPU\u64cd\u4f5c\u3002\"\"\"\n\n    print(\"GPU\u64cd\u4f5c\u8c03\u8bd5\u4fe1\u606f:\")\n    print(\"=\" * 40)\n\n    # \u8f93\u5165\u5206\u6790\n    for i, inp in enumerate(inputs):\n        print(f\"\u8f93\u5165 {i}:\")\n        print(f\"  \u5f62\u72b6: {inp.shape}\")\n        print(f\"  \u6570\u636e\u7c7b\u578b: {inp.dtype}\")\n        print(f\"  \u8bbe\u5907: {inp.device}\")\n        print(f\"  \u5185\u5b58\u4f7f\u7528: {inp.numel() * inp.dtype.itemsize / 1e6:.1f} MB\")\n        print(f\"  \u8fde\u7eed\u6027: {inp.is_contiguous()}\")\n        print()\n\n    # GPU\u5185\u5b58\u72b6\u6001\n    print(\"GPU\u5185\u5b58\u72b6\u6001:\")\n    print(f\"  \u5df2\u5206\u914d: {genesis.cuda.memory_allocated() / 1e6:.1f} MB\")\n    print(f\"  \u7f13\u5b58: {genesis.cuda.memory_cached() / 1e6:.1f} MB\")\n    print()\n\n    # \u6267\u884c\u5e26\u5206\u6790\u7684\u64cd\u4f5c\n    genesis.cuda.synchronize()\n    start_time = time.perf_counter()\n\n    result = operation_func(*inputs)\n\n    genesis.cuda.synchronize()\n    end_time = time.perf_counter()\n\n    # \u7ed3\u679c\u5206\u6790\n    print(\"\u64cd\u4f5c\u7ed3\u679c:\")\n    print(f\"  \u6267\u884c\u65f6\u95f4: {(end_time - start_time) * 1000:.3f}ms\")\n    print(f\"  \u8f93\u51fa\u5f62\u72b6: {result.shape}\")\n    print(f\"  \u8f93\u51fa\u6570\u636e\u7c7b\u578b: {result.dtype}\")\n    print(f\"  \u8f93\u51fa\u8bbe\u5907: {result.device}\")\n    print()\n\n    # \u6570\u503c\u9a8c\u8bc1\n    print(\"\u6570\u503c\u9a8c\u8bc1:\")\n    print(f\"  \u6700\u5c0f\u503c: {result.min().item():.6f}\")\n    print(f\"  \u6700\u5927\u503c: {result.max().item():.6f}\")\n    print(f\"  \u5e73\u5747\u503c: {result.mean().item():.6f}\")\n    print(f\"  \u5b58\u5728NaN: {genesis.isnan(result).any().item()}\")\n    print(f\"  \u5b58\u5728Inf: {genesis.isinf(result).any().item()}\")\n\n    return result\n\n# \u4f7f\u7528\u793a\u4f8b\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.randn(1000, 1000, device='cuda')\nresult = debug_gpu_operation(add_triton, x, y)\n</code></pre> <p>\u8fd9\u4e2a\u5168\u9762\u7684\u6307\u5357\u6db5\u76d6\u4e86Genesis\u4e2d\u6a21\u5757\u5316GPU\u64cd\u4f5c\u67b6\u6784\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u5b9e\u73b0\u793a\u4f8b\u548c\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002</p>"},{"location":"performance/gpu-operations_en/","title":"GPU Operations Performance Guide","text":"<p>This guide covers GPU operations optimization in Genesis, focusing on the modular GPU operations structure, Triton kernel implementation, and performance tuning strategies.</p>"},{"location":"performance/gpu-operations_en/#overview","title":"Overview","text":"<p>Genesis implements a sophisticated GPU backend with: - Modular GPU operations using Triton - Custom CUDA memory management - Adaptive block size optimization - Performance monitoring and profiling tools</p>"},{"location":"performance/gpu-operations_en/#architecture-overview","title":"Architecture Overview","text":""},{"location":"performance/gpu-operations_en/#modular-gpu-operations-structure","title":"Modular GPU Operations Structure","text":"<p>Genesis separates GPU operations into specialized modules:</p> Text Only<pre><code>genesis/ndarray/gpu_ops/\n\u251c\u2500\u2500 __init__.py          # Operation registry and dispatch\n\u251c\u2500\u2500 basic_ops.py         # Element-wise operations (add, mul, etc.)\n\u251c\u2500\u2500 tensor_ops.py        # Tensor operations (matmul, conv, etc.)  \n\u251c\u2500\u2500 random_ops.py        # Random number generation\n\u2514\u2500\u2500 reduction_ops.py     # Reduction operations (sum, mean, etc.)\n</code></pre>"},{"location":"performance/gpu-operations_en/#operation-dispatch-system","title":"Operation Dispatch System","text":"Python<pre><code># genesis/ndarray/gpu_ops/__init__.py\nfrom .basic_ops import add_triton, mul_triton, div_triton\nfrom .tensor_ops import matmul_triton, conv2d_triton  \nfrom .reduction_ops import sum_triton, mean_triton\n\n# Operation registry for dynamic dispatch\nGPU_OPS_REGISTRY = {\n    'add': add_triton,\n    'mul': mul_triton,\n    'div': div_triton,\n    'matmul': matmul_triton,\n    'sum': sum_triton,\n    'mean': mean_triton,\n}\n\ndef dispatch_gpu_op(op_name, *args, **kwargs):\n    \"\"\"Dispatch operation to appropriate GPU kernel.\"\"\"\n    if op_name not in GPU_OPS_REGISTRY:\n        raise NotImplementedError(f\"GPU operation {op_name} not implemented\")\n\n    return GPU_OPS_REGISTRY[op_name](*args, **kwargs)\n</code></pre>"},{"location":"performance/gpu-operations_en/#triton-kernel-implementation","title":"Triton Kernel Implementation","text":""},{"location":"performance/gpu-operations_en/#basic-element-wise-operations","title":"Basic Element-wise Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/basic_ops.py\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized element-wise addition kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load data with vectorization\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n\n    # Compute\n    output = x + y\n\n    # Store result\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef add_triton(x, y):\n    \"\"\"Triton-based tensor addition.\"\"\"\n    output = genesis.empty_like(x)\n    n_elements = x.numel()\n\n    # Adaptive block size based on tensor size\n    if n_elements &lt; 262144:  # &lt; 256K elements\n        BLOCK_SIZE = 256\n    elif n_elements &lt; 4194304:  # &lt; 4M elements  \n        BLOCK_SIZE = 512\n    else:\n        BLOCK_SIZE = 1024\n\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](\n        x.data_ptr(), y.data_ptr(), output.data_ptr(),\n        n_elements, BLOCK_SIZE=BLOCK_SIZE\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations_en/#advanced-tensor-operations","title":"Advanced Tensor Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/tensor_ops.py\n@triton.jit  \ndef matmul_kernel(\n    a_ptr, b_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bk, stride_bn, \n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n):\n    \"\"\"High-performance matrix multiplication kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_k[None, :] &lt; K - k * BLOCK_SIZE_K))\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] &lt; K - k * BLOCK_SIZE_K))\n\n        accumulator += tl.dot(a, b)\n\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(tl.float16)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n\n    c_mask = (offs_cm[:, None] &lt; M) &amp; (offs_cn[None, :] &lt; N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\ndef matmul_triton(a, b):\n    \"\"\"Optimized matrix multiplication using Triton.\"\"\"\n    assert a.shape[-1] == b.shape[-2], f\"Shape mismatch: {a.shape} @ {b.shape}\"\n\n    M, K = a.shape[-2:]\n    K2, N = b.shape[-2:]\n    assert K == K2\n\n    c = genesis.empty((*a.shape[:-2], M, N), dtype=a.dtype, device=a.device)\n\n    # Optimize block sizes based on problem size\n    if M &gt;= 2048 and N &gt;= 2048:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 128, 128, 32\n    elif M &gt;= 512 and N &gt;= 512:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 64, 64, 32\n    else:\n        BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K = 32, 32, 32\n\n    grid = lambda meta: (\n        triton.cdiv(M, meta['BLOCK_SIZE_M']) * triton.cdiv(N, meta['BLOCK_SIZE_N']),\n    )\n\n    matmul_kernel[grid](\n        a.data_ptr(), b.data_ptr(), c.data_ptr(),\n        M, N, K,\n        a.stride(-2), a.stride(-1),\n        b.stride(-2), b.stride(-1),\n        c.stride(-2), c.stride(-1),\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N, \n        BLOCK_SIZE_K=BLOCK_SIZE_K,\n    )\n\n    return c\n</code></pre>"},{"location":"performance/gpu-operations_en/#memory-optimized-reduction-operations","title":"Memory-Optimized Reduction Operations","text":"Python<pre><code># genesis/ndarray/gpu_ops/reduction_ops.py\n@triton.jit\ndef sum_kernel(\n    input_ptr, output_ptr, \n    n_elements,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Memory-efficient summation kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load and sum in blocks\n    x = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n    block_sum = tl.sum(x)\n\n    # Use atomic add for final reduction\n    tl.atomic_add(output_ptr, block_sum)\n\n@triton.jit\ndef reduce_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE_X: tl.constexpr,\n    BLOCK_SIZE_Y: tl.constexpr  \n):\n    \"\"\"2D reduction kernel with optimal memory access.\"\"\"\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n\n    offs_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offs_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n\n    mask_x = offs_x &lt; n_rows\n    mask_y = offs_y &lt; n_cols\n\n    # Load block\n    ptrs = input_ptr + offs_x[:, None] * n_cols + offs_y[None, :]\n    mask = mask_x[:, None] &amp; mask_y[None, :]\n    x = tl.load(ptrs, mask=mask, other=0.0)\n\n    # Reduce within block\n    result = tl.sum(x, axis=1)  # Sum across columns\n\n    # Store result\n    out_ptrs = output_ptr + offs_x\n    tl.store(out_ptrs, result, mask=mask_x)\n\ndef sum_triton(x, dim=None, keepdim=False):\n    \"\"\"Optimized tensor summation.\"\"\"\n    if dim is None:\n        # Global sum\n        result = genesis.zeros((), dtype=x.dtype, device=x.device)\n        n_elements = x.numel()\n\n        BLOCK_SIZE = min(1024, triton.next_power_of_2(n_elements))\n        grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n\n        sum_kernel[grid](\n            x.data_ptr(), result.data_ptr(),\n            n_elements, BLOCK_SIZE=BLOCK_SIZE\n        )\n\n        return result\n\n    else:\n        # Dimension-specific reduction\n        # Implementation for specific dimension reduction\n        return reduce_along_dim(x, dim, keepdim)\n</code></pre>"},{"location":"performance/gpu-operations_en/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"performance/gpu-operations_en/#1-adaptive-block-size-optimization","title":"1. Adaptive Block Size Optimization","text":"Python<pre><code>class AdaptiveBlockSize:\n    \"\"\"Dynamically optimize block sizes based on tensor characteristics.\"\"\"\n\n    def __init__(self):\n        self.cache = {}\n        self.performance_history = {}\n\n    def get_optimal_block_size(self, operation, tensor_size, dtype):\n        \"\"\"Get optimal block size for given operation and tensor.\"\"\"\n        cache_key = (operation, tensor_size, dtype.name)\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # Determine block size based on tensor size and operation\n        if operation == 'elementwise':\n            if tensor_size &lt; 262144:  # &lt; 256K elements\n                block_size = 256\n            elif tensor_size &lt; 4194304:  # &lt; 4M elements\n                block_size = 512  \n            else:\n                block_size = 1024\n\n        elif operation == 'matmul':\n            # Matrix multiplication specific optimization\n            sqrt_size = int(tensor_size ** 0.5)\n            if sqrt_size &lt; 512:\n                block_size = (32, 32, 32)\n            elif sqrt_size &lt; 2048:\n                block_size = (64, 64, 32)\n            else:\n                block_size = (128, 128, 32)\n\n        elif operation == 'reduction':\n            # Reduction operations optimization\n            block_size = min(1024, triton.next_power_of_2(tensor_size))\n\n        else:\n            # Default fallback\n            block_size = 512\n\n        self.cache[cache_key] = block_size\n        return block_size\n\n    def update_performance(self, operation, tensor_size, dtype, block_size, elapsed_time):\n        \"\"\"Update performance history for future optimization.\"\"\"\n        key = (operation, tensor_size, dtype.name, block_size)\n        if key not in self.performance_history:\n            self.performance_history[key] = []\n\n        self.performance_history[key].append(elapsed_time)\n\n        # Keep only recent measurements\n        if len(self.performance_history[key]) &gt; 10:\n            self.performance_history[key] = self.performance_history[key][-10:]\n\n# Global optimizer instance\nblock_optimizer = AdaptiveBlockSize()\n</code></pre>"},{"location":"performance/gpu-operations_en/#2-memory-access-pattern-optimization","title":"2. Memory Access Pattern Optimization","text":"Python<pre><code>@triton.jit\ndef coalesced_copy_kernel(\n    src_ptr, dst_ptr,\n    n_elements, stride_src, stride_dst,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Memory-coalesced tensor copy kernel.\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    # Ensure coalesced memory access\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    # Load with proper stride\n    src_offsets = offsets * stride_src\n    dst_offsets = offsets * stride_dst\n\n    data = tl.load(src_ptr + src_offsets, mask=mask)\n    tl.store(dst_ptr + dst_offsets, data, mask=mask)\n\n@triton.jit  \ndef transpose_kernel(\n    input_ptr, output_ptr,\n    n_rows, n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Cache-friendly matrix transpose.\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # Tile-based transpose for better cache usage\n    row_start = (pid // (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n    col_start = (pid % (n_cols // BLOCK_SIZE)) * BLOCK_SIZE\n\n    rows = row_start + tl.arange(0, BLOCK_SIZE)\n    cols = col_start + tl.arange(0, BLOCK_SIZE)\n\n    row_mask = rows &lt; n_rows\n    col_mask = cols &lt; n_cols\n\n    # Load tile\n    input_offsets = rows[:, None] * n_cols + cols[None, :]\n    mask = row_mask[:, None] &amp; col_mask[None, :]\n\n    tile = tl.load(input_ptr + input_offsets, mask=mask)\n\n    # Store transposed tile\n    output_offsets = cols[:, None] * n_rows + rows[None, :]\n    tl.store(output_ptr + output_offsets, tl.trans(tile), mask=tl.trans(mask))\n</code></pre>"},{"location":"performance/gpu-operations_en/#3-kernel-fusion-optimization","title":"3. Kernel Fusion Optimization","text":"Python<pre><code>@triton.jit\ndef fused_linear_relu_kernel(\n    x_ptr, weight_ptr, bias_ptr, output_ptr,\n    n_batch, n_input, n_output,\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr\n):\n    \"\"\"Fused linear + ReLU kernel to reduce memory bandwidth.\"\"\"\n    pid = tl.program_id(axis=0)\n\n    # Matrix multiplication logic (simplified)\n    # ... matmul computation ...\n\n    # Fused ReLU activation\n    result = tl.maximum(matmul_result + bias, 0.0)\n\n    # Single memory write\n    tl.store(output_ptr + offsets, result, mask=mask)\n\n@triton.jit\ndef fused_attention_kernel(\n    q_ptr, k_ptr, v_ptr, output_ptr,\n    seq_len, head_dim,\n    scale: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"Fused attention computation kernel.\"\"\"\n    # Compute attention scores\n    scores = compute_qk_scores(q_ptr, k_ptr, seq_len, head_dim)\n\n    # Apply scaling and softmax\n    scores = scores * scale\n    attention_weights = tl_softmax(scores, axis=-1)\n\n    # Apply attention to values\n    output = compute_attention_output(attention_weights, v_ptr, seq_len, head_dim)\n\n    # Single output write\n    tl.store(output_ptr + offsets, output, mask=mask)\n\ndef fused_linear_relu(x, weight, bias):\n    \"\"\"Fused linear layer with ReLU activation.\"\"\"\n    batch_size, input_size = x.shape\n    output_size = weight.shape[0]\n\n    output = genesis.empty(batch_size, output_size, dtype=x.dtype, device=x.device)\n\n    # Optimal block sizes for fusion\n    BLOCK_SIZE_M = 64\n    BLOCK_SIZE_N = 64  \n    BLOCK_SIZE_K = 32\n\n    grid = lambda meta: (\n        triton.cdiv(batch_size, meta['BLOCK_SIZE_M']) * \n        triton.cdiv(output_size, meta['BLOCK_SIZE_N']),\n    )\n\n    fused_linear_relu_kernel[grid](\n        x.data_ptr(), weight.data_ptr(), bias.data_ptr(), output.data_ptr(),\n        batch_size, input_size, output_size,\n        BLOCK_SIZE_M=BLOCK_SIZE_M,\n        BLOCK_SIZE_N=BLOCK_SIZE_N,\n        BLOCK_SIZE_K=BLOCK_SIZE_K\n    )\n\n    return output\n</code></pre>"},{"location":"performance/gpu-operations_en/#performance-monitoring-and-profiling","title":"Performance Monitoring and Profiling","text":""},{"location":"performance/gpu-operations_en/#1-built-in-performance-metrics","title":"1. Built-in Performance Metrics","text":"Python<pre><code>import time\nimport contextlib\n\nclass GPUProfiler:\n    \"\"\"Profile GPU operations performance.\"\"\"\n\n    def __init__(self):\n        self.metrics = {}\n        self.current_operation = None\n\n    @contextlib.contextmanager\n    def profile_operation(self, operation_name):\n        \"\"\"Context manager for profiling operations.\"\"\"\n        self.current_operation = operation_name\n\n        # Synchronize before timing\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        try:\n            yield\n        finally:\n            genesis.cuda.synchronize()\n            end_time = time.perf_counter()\n\n            elapsed = end_time - start_time\n            if operation_name not in self.metrics:\n                self.metrics[operation_name] = []\n\n            self.metrics[operation_name].append(elapsed)\n\n    def get_stats(self, operation_name=None):\n        \"\"\"Get performance statistics.\"\"\"\n        if operation_name:\n            times = self.metrics.get(operation_name, [])\n            if not times:\n                return None\n\n            return {\n                'mean': sum(times) / len(times),\n                'min': min(times),\n                'max': max(times),\n                'count': len(times)\n            }\n        else:\n            stats = {}\n            for op_name in self.metrics:\n                stats[op_name] = self.get_stats(op_name)\n            return stats\n\n    def print_summary(self):\n        \"\"\"Print performance summary.\"\"\"\n        print(\"GPU Operation Performance Summary:\")\n        print(\"-\" * 50)\n\n        for op_name, stats in self.get_stats().items():\n            print(f\"{op_name}:\")\n            print(f\"  Mean: {stats['mean']*1000:.3f}ms\")\n            print(f\"  Min:  {stats['min']*1000:.3f}ms\")\n            print(f\"  Max:  {stats['max']*1000:.3f}ms\")\n            print(f\"  Count: {stats['count']}\")\n            print()\n\n# Global profiler instance\ngpu_profiler = GPUProfiler()\n\n# Example usage in operations\ndef add_with_profiling(x, y):\n    with gpu_profiler.profile_operation('add'):\n        return add_triton(x, y)\n</code></pre>"},{"location":"performance/gpu-operations_en/#2-memory-bandwidth-analysis","title":"2. Memory Bandwidth Analysis","text":"Python<pre><code>def analyze_memory_bandwidth(operation_func, tensor_sizes, dtype=genesis.float32):\n    \"\"\"Analyze memory bandwidth utilization for operations.\"\"\"\n\n    results = []\n    theoretical_bandwidth = 1555e9  # A800 HBM2e bandwidth in bytes/s\n\n    for size in tensor_sizes:\n        # Create test tensors\n        if isinstance(size, tuple):\n            x = genesis.randn(*size, dtype=dtype, device='cuda')\n            y = genesis.randn(*size, dtype=dtype, device='cuda')\n        else:\n            x = genesis.randn(size, dtype=dtype, device='cuda')\n            y = genesis.randn(size, dtype=dtype, device='cuda')\n\n        # Calculate theoretical bytes transferred\n        bytes_per_element = dtype.itemsize\n        total_elements = x.numel()\n\n        # For binary operations: read 2 tensors + write 1 tensor\n        total_bytes = total_elements * bytes_per_element * 3\n\n        # Warm up\n        for _ in range(5):\n            _ = operation_func(x, y)\n\n        # Time operation\n        genesis.cuda.synchronize()\n        start_time = time.perf_counter()\n\n        num_iterations = 10\n        for _ in range(num_iterations):\n            result = operation_func(x, y)\n\n        genesis.cuda.synchronize()\n        end_time = time.perf_counter()\n\n        # Calculate metrics\n        elapsed_time = (end_time - start_time) / num_iterations\n        achieved_bandwidth = total_bytes / elapsed_time\n        bandwidth_efficiency = achieved_bandwidth / theoretical_bandwidth\n\n        results.append({\n            'size': size,\n            'elements': total_elements,\n            'elapsed_ms': elapsed_time * 1000,\n            'bandwidth_gb_s': achieved_bandwidth / 1e9,\n            'efficiency_percent': bandwidth_efficiency * 100,\n            'theoretical_gb_s': theoretical_bandwidth / 1e9\n        })\n\n        print(f\"Size {size}: {achieved_bandwidth/1e9:.1f} GB/s ({bandwidth_efficiency:.1%})\")\n\n    return results\n\n# Analyze add operation performance\nsizes = [(256, 256), (1024, 1024), (2048, 2048), (4096, 4096)]\nbandwidth_results = analyze_memory_bandwidth(add_triton, sizes)\n</code></pre>"},{"location":"performance/gpu-operations_en/#3-automated-performance-tuning","title":"3. Automated Performance Tuning","text":"Python<pre><code>class AutoTuner:\n    \"\"\"Automatically tune kernel parameters for optimal performance.\"\"\"\n\n    def __init__(self):\n        self.best_configs = {}\n\n    def tune_kernel(self, kernel_func, test_inputs, param_space):\n        \"\"\"Auto-tune kernel parameters.\"\"\"\n        best_time = float('inf')\n        best_config = None\n\n        print(f\"Tuning kernel with {len(param_space)} configurations...\")\n\n        for i, config in enumerate(param_space):\n            try:\n                # Warm up\n                for _ in range(3):\n                    _ = kernel_func(*test_inputs, **config)\n\n                # Time execution\n                genesis.cuda.synchronize()\n                start_time = time.perf_counter()\n\n                num_runs = 10\n                for _ in range(num_runs):\n                    result = kernel_func(*test_inputs, **config)\n\n                genesis.cuda.synchronize()\n                end_time = time.perf_counter()\n\n                elapsed = (end_time - start_time) / num_runs\n\n                if elapsed &lt; best_time:\n                    best_time = elapsed\n                    best_config = config\n\n                print(f\"Config {i+1}: {elapsed*1000:.3f}ms - {config}\")\n\n            except Exception as e:\n                print(f\"Config {i+1} failed: {e}\")\n\n        print(f\"Best configuration: {best_config} ({best_time*1000:.3f}ms)\")\n        return best_config, best_time\n\n# Example auto-tuning for matrix multiplication\ndef tune_matmul():\n    # Define parameter space\n    block_sizes = [\n        {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n        {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64},\n    ]\n\n    # Test inputs\n    a = genesis.randn(1024, 1024, device='cuda')\n    b = genesis.randn(1024, 1024, device='cuda')\n\n    # Run auto-tuner\n    tuner = AutoTuner()\n    best_config, best_time = tuner.tune_kernel(\n        matmul_triton, [a, b], block_sizes\n    )\n\n    return best_config\n</code></pre>"},{"location":"performance/gpu-operations_en/#best-practices","title":"Best Practices","text":""},{"location":"performance/gpu-operations_en/#1-kernel-development-guidelines","title":"1. Kernel Development Guidelines","text":"<ul> <li>Memory Coalescing: Ensure contiguous memory access patterns</li> <li>Block Size Optimization: Use powers of 2, consider occupancy</li> <li>Register Usage: Monitor register spilling with large kernels</li> <li>Shared Memory: Use shared memory for data reuse</li> <li>Divergence Minimization: Avoid conditional branches when possible</li> </ul>"},{"location":"performance/gpu-operations_en/#2-performance-optimization-checklist","title":"2. Performance Optimization Checklist","text":"<ul> <li> Profile memory bandwidth utilization</li> <li> Optimize block sizes for target GPU</li> <li> Minimize kernel launch overhead</li> <li> Use kernel fusion for related operations</li> <li> Monitor GPU occupancy and resource usage</li> <li> Validate numerical accuracy after optimization</li> </ul>"},{"location":"performance/gpu-operations_en/#3-debugging-gpu-operations","title":"3. Debugging GPU Operations","text":"Python<pre><code>def debug_gpu_operation(operation_func, *inputs):\n    \"\"\"Debug GPU operation with detailed analysis.\"\"\"\n\n    print(\"GPU Operation Debug Information:\")\n    print(\"=\" * 40)\n\n    # Input analysis\n    for i, inp in enumerate(inputs):\n        print(f\"Input {i}:\")\n        print(f\"  Shape: {inp.shape}\")\n        print(f\"  Dtype: {inp.dtype}\")\n        print(f\"  Device: {inp.device}\")\n        print(f\"  Memory usage: {inp.numel() * inp.dtype.itemsize / 1e6:.1f} MB\")\n        print(f\"  Contiguous: {inp.is_contiguous()}\")\n        print()\n\n    # GPU memory status\n    print(\"GPU Memory Status:\")\n    print(f\"  Allocated: {genesis.cuda.memory_allocated() / 1e6:.1f} MB\")\n    print(f\"  Cached: {genesis.cuda.memory_cached() / 1e6:.1f} MB\")\n    print()\n\n    # Execute operation with profiling\n    genesis.cuda.synchronize()\n    start_time = time.perf_counter()\n\n    result = operation_func(*inputs)\n\n    genesis.cuda.synchronize()\n    end_time = time.perf_counter()\n\n    # Results analysis\n    print(\"Operation Results:\")\n    print(f\"  Execution time: {(end_time - start_time) * 1000:.3f}ms\")\n    print(f\"  Output shape: {result.shape}\")\n    print(f\"  Output dtype: {result.dtype}\")\n    print(f\"  Output device: {result.device}\")\n    print()\n\n    # Numerical validation\n    print(\"Numerical Validation:\")\n    print(f\"  Min value: {result.min().item():.6f}\")\n    print(f\"  Max value: {result.max().item():.6f}\")\n    print(f\"  Mean value: {result.mean().item():.6f}\")\n    print(f\"  Has NaN: {genesis.isnan(result).any().item()}\")\n    print(f\"  Has Inf: {genesis.isinf(result).any().item()}\")\n\n    return result\n\n# Example usage\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.randn(1000, 1000, device='cuda')\nresult = debug_gpu_operation(add_triton, x, y)\n</code></pre> <p>This comprehensive guide covers the modular GPU operations architecture in Genesis, providing detailed implementation examples and optimization strategies for maximum performance.</p>"},{"location":"performance/optimization-guide.zh/","title":"Genesis \u6027\u80fd\u4f18\u5316\u6307\u5357","text":""},{"location":"performance/optimization-guide.zh/#_1","title":"\u6982\u8ff0","text":"<p>\u672c\u6587\u6863\u63d0\u4f9bGenesis\u6846\u67b6\u7684\u6027\u80fd\u7279\u5f81\u3001\u5f53\u524d\u5b9e\u73b0\u72b6\u6001\u548c\u4f18\u5316\u7b56\u7565\u7684\u5168\u9762\u6307\u5357\u3002Genesis\u8bbe\u8ba1\u4e3a\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6559\u80b2\u4ef7\u503c\u7684\u540c\u65f6\u8ffd\u6c42\u7ade\u4e89\u6027\u80fd\u3002</p>"},{"location":"performance/optimization-guide.zh/#_2","title":"\u5f53\u524d\u6027\u80fd\u72b6\u6001","text":""},{"location":"performance/optimization-guide.zh/#add","title":"\u5143\u7d20\u64cd\u4f5c (ADD) \u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c","text":"<p>\u6d4b\u8bd5\u73af\u5883: - GPU: NVIDIA A800-SXM4-80GB - \u663e\u5b58: 79.3 GB - \u7406\u8bba\u5e26\u5bbd: 1555 GB/s - \u6d4b\u8bd5\u65e5\u671f: 2025\u5e748\u6708</p> <p>\u6027\u80fd\u603b\u7ed3: - \u5e73\u5747\u6548\u7387: 18.0% \u7406\u8bba\u5e26\u5bbd\u5229\u7528\u7387 - \u6700\u4f73\u6027\u80fd: 33.1% (\u6279\u5904\u7406\u5f20\u91cf) - \u6700\u5dee\u6027\u80fd: 3.1% (\u5927\u5f20\u91cf) - \u6574\u4f53\u72b6\u6001: \u5f00\u53d1\u9636\u6bb5</p>"},{"location":"performance/optimization-guide.zh/#_3","title":"\u6309\u5f20\u91cf\u5927\u5c0f\u5206\u7c7b\u7684\u6027\u80fd","text":"\u7c7b\u522b \u5e73\u5747\u6548\u7387 \u72b6\u6001 vs PyTorch \u5c0f\u5f20\u91cf (64K-262K) 18.9% \u274c \u4e25\u91cd 0.19x \u4e2d\u7b49\u5f20\u91cf (4.2M) 29.6% \ud83d\udd34 \u8f83\u5dee 0.27-0.32x \u5927\u5f20\u91cf (16.8M) 4.7% \u274c \u4e25\u91cd 0.03-0.06x \u8d85\u5927\u5f20\u91cf (67M) 5.4% \u274c \u4e25\u91cd 0.05-0.06x \u6279\u5904\u7406 31.2% \ud83d\udd34 \u8f83\u5dee 0.29-0.33x"},{"location":"performance/optimization-guide.zh/#detailed-performance-data","title":"Detailed Performance Data","text":"Shape Size PyTorch Genesis Speed Ratio Efficiency Status Primary Issue 256\u00d7256 65.5K 0.019ms 0.104ms 0.19x 18.7% \u274c Critical Launch overhead 2048\u00d72048 4.2M 0.053ms 0.166ms 0.32x 32.0% \ud83d\udd34 Poor Autograd cost 4096\u00d74096 16.8M 0.147ms 2.334ms 0.06x 6.3% \u274c Critical Bandwidth limit 8192\u00d78192 67M 0.478ms 8.208ms 0.06x 5.8% \u274c Critical Memory bound"},{"location":"performance/optimization-guide.zh/#matrix-multiplication-performance","title":"Matrix Multiplication Performance","text":"Matrix Size Genesis Time PyTorch Time Speed Ratio Status 512x512 0.087ms 0.024ms 0.28x \ud83d\udd34 Poor 1024x1024 0.243ms 0.089ms 0.37x \ud83d\udd34 Poor 2048x2048 1.456ms 0.387ms 0.27x \ud83d\udd34 Poor 4096x4096 8.932ms 2.234ms 0.25x \ud83d\udd34 Poor"},{"location":"performance/optimization-guide.zh/#_4","title":"\u67b6\u6784\u5b9e\u73b0","text":""},{"location":"performance/optimization-guide.zh/#add_1","title":"\u5f53\u524dADD\u64cd\u4f5c\u5b9e\u73b0","text":"<p>Genesis\u91c7\u7528\u53cc\u540e\u7aef\u67b6\u6784: - CPU\u540e\u7aef: PyTorch\u5f20\u91cf\u64cd\u4f5c - GPU\u540e\u7aef: \u81ea\u5b9a\u4e49CUDA + Triton\u5185\u6838</p>"},{"location":"performance/optimization-guide.zh/#gpu","title":"GPU\u5185\u6838\u5b9e\u73b0","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u4f18\u5316\u7684\u52a0\u6cd5\u5185\u6838\uff0c\u540c\u5f62\u72b6\u5f20\u91cf\uff0c\u66f4\u597d\u7684\u5185\u5b58\u8bbf\u95ee\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"performance/optimization-guide.zh/#_5","title":"\u81ea\u9002\u5e94\u5757\u5927\u5c0f\u914d\u7f6e","text":"<p>\u5f53\u524d\u4f18\u5316\u914d\u7f6e:</p> Python<pre><code>BLOCK_SIZE_CONFIGS = {\n    (0, 262144): 256,         # \u5c0f\u5f20\u91cf: \u66f4\u5c0f\u5757\u63d0\u5347\u7f13\u5b58\u5229\u7528\u7387\n    (262144, 4194304): 512,   # \u4e2d\u7b49\u5f20\u91cf: \u5e73\u8861\u5360\u7528\u7387\u4e0e\u7f13\u5b58\n    (4194304, float('inf')): 1024,  # \u5927\u5f20\u91cf: \u66f4\u5927\u5757\u63d0\u5347\u5e26\u5bbd\n}\n</code></pre>"},{"location":"performance/optimization-guide.zh/#_6","title":"\u6027\u80fd\u74f6\u9888\u5206\u6790","text":""},{"location":"performance/optimization-guide.zh/#1-triton","title":"1. \u4e3b\u8981\u74f6\u9888: Triton\u5185\u6838\u6027\u80fd","text":"<ul> <li>\u5185\u6838\u5f00\u9500: \u6bd4PyTorch\u616223.6\u500d</li> <li>\u6839\u672c\u539f\u56e0: Triton\u5185\u6838\u6548\u7387\u8fdc\u4f4e\u4e8ePyTorch\u4f18\u5316\u7684CUDA\u5185\u6838</li> <li>\u5f71\u54cd: \u5927\u5f20\u91cf(&gt;16M\u5143\u7d20)\u6700\u4e3a\u4e25\u91cd</li> </ul>"},{"location":"performance/optimization-guide.zh/#2","title":"2. \u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387","text":"<ul> <li>PyTorch: 71.4% \u5e26\u5bbd\u6548\u7387</li> <li>Genesis: 18.0% \u5e73\u5747\u6548\u7387  </li> <li>\u7406\u8bba\u6700\u5927\u503c: 1555 GB/s (A800 HBM2e)</li> </ul> <p>\u95ee\u9898: - \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u672a\u5145\u5206\u4f18\u5316 - \u5927\u5185\u6838\u53ef\u80fd\u5b58\u5728\u5bc4\u5b58\u5668\u6ea2\u51fa - \u5185\u5b58\u5408\u5e76\u8bbf\u95ee\u4e0d\u591f\u4f18\u5316</p>"},{"location":"performance/optimization-guide.zh/#3-gpu","title":"3. GPU\u5360\u7528\u7387\u95ee\u9898","text":"<ul> <li>\u5757\u5927\u5c0f\u914d\u7f6e\u672a\u8fbe\u5230\u6700\u4f18\u5360\u7528\u7387</li> <li>\u8d85\u5927\u5f20\u91cfGPU\u5229\u7528\u7387\u663e\u8457\u4e0b\u964d</li> <li>\u8d44\u6e90\u9650\u5236\u963b\u6b62\u5145\u5206\u5229\u7528SM</li> </ul>"},{"location":"performance/optimization-guide.zh/#_7","title":"\u4f18\u5316\u8def\u7ebf\u56fe","text":""},{"location":"performance/optimization-guide.zh/#1","title":"\u9636\u6bb51: \u7acb\u5373\u6539\u8fdb (\u5df2\u5b8c\u6210)","text":"<p>\u2705 \u5df2\u5b8c\u6210: - \u7b80\u5316\u81ea\u9002\u5e94\u5757\u5927\u5c0f\u914d\u7f6e - \u4e13\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\u8bbe\u65bd - \u6027\u80fd\u5206\u6790\u5de5\u5177</p> <p>\ud83d\udcca \u7ed3\u679c: - \u5e73\u5747\u6548\u7387\u4ece5.7%\u63d0\u5347\u523018.0% - \u4e2d\u7b49/\u6279\u5904\u7406\u5f20\u91cf\u8fbe\u523029-33%\u6548\u7387</p>"},{"location":"performance/optimization-guide.zh/#2_1","title":"\u9636\u6bb52: \u5185\u6838\u4f18\u5316 (\u8fdb\u884c\u4e2d)","text":"<p>\ud83c\udfaf \u76ee\u6807\u9886\u57df: - \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4f18\u5316(\u5411\u91cf\u5316\u3001\u7f13\u5b58\u53cb\u597d\u5e73\u94fa) - \u5757\u5927\u5c0f\u81ea\u52a8\u8c03\u4f18 - \u5185\u6838\u878d\u5408\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u538b\u529b</p>"},{"location":"performance/optimization-guide.zh/#3","title":"\u9636\u6bb53: \u9ad8\u7ea7\u4f18\u5316 (\u672a\u6765)","text":"<ul> <li>\u81ea\u5b9a\u4e49CUDA\u5185\u6838\u624b\u5de5\u4f18\u5316</li> <li>\u5185\u5b58\u5e03\u5c40\u4f18\u5316</li> <li>\u591aGPU\u652f\u6301</li> </ul>"},{"location":"performance/optimization-guide.zh/#_8","title":"\u4f7f\u7528\u5efa\u8bae","text":""},{"location":"performance/optimization-guide.zh/#genesis-vs-pytorch","title":"Genesis vs PyTorch\u9009\u62e9","text":"<p>\u63a8\u8350\u4f7f\u7528Genesis: - \u6559\u80b2\u5b66\u4e60\u548c\u6846\u67b6\u7406\u89e3 - \u4e2d\u7b49\u6279\u5904\u7406\u64cd\u4f5c(\u6700\u4f73\u6027\u80fd31%\u6548\u7387) - \u9700\u8981\u81ea\u5b9a\u4e49\u5185\u6838\u5f00\u53d1\u7684\u7814\u7a76</p> <p>\u63a8\u8350\u4f7f\u7528PyTorch: - \u751f\u4ea7\u73af\u5883\u6700\u5927\u6027\u80fd\u9700\u6c42 - \u5927\u5f20\u91cf\u64cd\u4f5c(&gt;16M\u5143\u7d20) - \u5bf95-25\u500d\u6027\u80fd\u5dee\u5f02\u654f\u611f\u7684\u5e94\u7528</p>"},{"location":"performance/optimization-guide.zh/#_9","title":"\u6027\u80fd\u6280\u5de7","text":"<ol> <li>\u5f20\u91cf\u5927\u5c0f\u610f\u8bc6</li> <li>\u6700\u4f73\u6027\u80fd\u8303\u56f4: 1M-4M\u5143\u7d20</li> <li>\u907f\u514d\u8d85\u5927\u5f20\u91cf(&gt;67M)</li> <li> <p>\u8003\u8651\u5927\u64cd\u4f5c\u7684\u5f20\u91cf\u5206\u5272</p> </li> <li> <p>\u5185\u5b58\u7ba1\u7406 Python<pre><code># \u4f7f\u7528\u5c31\u5730\u64cd\u4f5c\nresult = genesis.add(a, b, out=existing_tensor)\n</code></pre></p> </li> </ol>"},{"location":"performance/optimization-guide.zh/#_10","title":"\u6027\u80fd\u76d1\u63a7","text":""},{"location":"performance/optimization-guide.zh/#_11","title":"\u5185\u7f6e\u57fa\u51c6\u6d4b\u8bd5","text":"Bash<pre><code># \u5feb\u901f\u6027\u80fd\u68c0\u67e5\npython benchmark/bench_ops.py --op add --fast\n\n# \u5168\u9762\u5206\u6790\npython benchmark/bench_ops.py --op add --size large\n</code></pre>"},{"location":"performance/optimization-guide.zh/#_12","title":"\u5173\u952e\u6307\u6807","text":"<ul> <li>\u5185\u5b58\u5e26\u5bbd\u6548\u7387: \u76ee\u6807&gt;50%</li> <li>GPU\u5229\u7528\u7387: \u7528<code>nvidia-smi</code>\u76d1\u63a7</li> <li>\u5185\u6838\u542f\u52a8\u5f00\u9500: \u7528Nsight Compute\u5206\u6790</li> </ul>"},{"location":"performance/optimization-guide.zh/#_13","title":"\u6027\u80fd\u76ee\u6807","text":"\u5f20\u91cf\u7c7b\u522b \u6700\u5c0f\u6548\u7387 \u76ee\u6807\u6548\u7387 \u5c0f\u5f20\u91cf 15% 25% \u4e2d\u7b49\u5f20\u91cf 25% 40% \u5927\u5f20\u91cf 10% 30% \u8d85\u5927\u5f20\u91cf 10% 25% \u6279\u5904\u7406 25% 45% <p>\u6700\u540e\u66f4\u65b0: 2025\u5e748\u6708 \u6846\u67b6\u7248\u672c: Genesis 0.3.0-dev \u57fa\u51c6\u73af\u5883: A800-SXM4-80GB</p>"},{"location":"performance/optimization-guide_en/","title":"Genesis Performance Optimization Guide","text":""},{"location":"performance/optimization-guide_en/#overview","title":"Overview","text":"<p>This document provides a comprehensive guide to the performance characteristics, current implementation status, and optimization strategies of the Genesis framework. Genesis is designed as a lightweight deep learning framework that pursues competitive performance while maintaining educational value.</p>"},{"location":"performance/optimization-guide_en/#current-performance-status","title":"Current Performance Status","text":""},{"location":"performance/optimization-guide_en/#element-wise-operation-add-benchmark-results","title":"Element-wise Operation (ADD) Benchmark Results","text":"<p>Test Environment: - GPU: NVIDIA A800-SXM4-80GB - Memory: 79.3 GB - Theoretical Bandwidth: 1555 GB/s - Test Date: August 2025</p> <p>Performance Summary: - Average Efficiency: 18.0% theoretical bandwidth utilization - Best Performance: 33.1% (batch tensors) - Worst Performance: 3.1% (large tensors) - Overall Status: Development phase</p>"},{"location":"performance/optimization-guide_en/#performance-by-tensor-size-category","title":"Performance by Tensor Size Category","text":"Category Average Efficiency Status vs PyTorch Small Tensors (64K-262K) 18.9% \u274c Critical 0.19x Medium Tensors (4.2M) 29.6% \ud83d\udd34 Poor 0.27-0.32x Large Tensors (16.8M) 4.7% \u274c Critical 0.03-0.06x XLarge Tensors (67M) 5.4% \u274c Critical 0.05-0.06x Batch Processing 31.2% \ud83d\udd34 Poor 0.29-0.33x"},{"location":"performance/optimization-guide_en/#detailed-performance-data","title":"Detailed Performance Data","text":"Shape Size PyTorch Genesis Speed Ratio Efficiency Status 256\u00d7256 65.5K 0.019ms 0.104ms 0.19x 18.7% \u274c Critical 2048\u00d72048 4.2M 0.053ms 0.166ms 0.32x 32.0% \ud83d\udd34 Poor 4096\u00d74096 16.8M 0.147ms 2.334ms 0.06x 6.3% \u274c Critical 8192\u00d78192 67M 0.478ms 8.208ms 0.06x 5.8% \u274c Critical"},{"location":"performance/optimization-guide_en/#architecture-implementation","title":"Architecture Implementation","text":""},{"location":"performance/optimization-guide_en/#current-add-operation-implementation","title":"Current ADD Operation Implementation","text":"<p>Genesis uses a dual backend architecture: - CPU Backend: PyTorch tensor operations - GPU Backend: Custom CUDA + Triton kernels</p>"},{"location":"performance/optimization-guide_en/#gpu-kernel-implementation","title":"GPU Kernel Implementation","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"Optimized addition kernel for same-shape tensors with better memory access\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"performance/optimization-guide_en/#adaptive-block-size-configuration","title":"Adaptive Block Size Configuration","text":"<p>Current optimization configuration:</p> Python<pre><code>BLOCK_SIZE_CONFIGS = {\n    (0, 262144): 256,         # Small tensors: smaller blocks improve cache utilization\n    (262144, 4194304): 512,   # Medium tensors: balance occupancy and cache\n    (4194304, float('inf')): 1024,  # Large tensors: larger blocks improve bandwidth\n}\n</code></pre>"},{"location":"performance/optimization-guide_en/#performance-bottleneck-analysis","title":"Performance Bottleneck Analysis","text":""},{"location":"performance/optimization-guide_en/#1-primary-bottleneck-triton-kernel-performance","title":"1. Primary Bottleneck: Triton Kernel Performance","text":"<ul> <li>Kernel Overhead: 23.6x slower than PyTorch</li> <li>Root Cause: Triton kernel efficiency far below PyTorch's optimized CUDA kernels</li> <li>Impact: Most severe for large tensors (&gt;16M elements)</li> </ul>"},{"location":"performance/optimization-guide_en/#2-memory-bandwidth-utilization","title":"2. Memory Bandwidth Utilization","text":"<ul> <li>PyTorch: 71.4% bandwidth efficiency</li> <li>Genesis: 18.0% average efficiency  </li> <li>Theoretical Maximum: 1555 GB/s (A800 HBM2e)</li> </ul> <p>Issues: - Memory access patterns not sufficiently optimized - Large kernels may have register spillage - Memory coalesced access not well optimized</p>"},{"location":"performance/optimization-guide_en/#3-gpu-occupancy-issues","title":"3. GPU Occupancy Issues","text":"<ul> <li>Block size configuration not achieving optimal occupancy</li> <li>XLarge tensors show significant GPU utilization drop</li> <li>Resource limitations prevent full SM utilization</li> </ul>"},{"location":"performance/optimization-guide_en/#optimization-roadmap","title":"Optimization Roadmap","text":""},{"location":"performance/optimization-guide_en/#phase-1-immediate-improvements-completed","title":"Phase 1: Immediate Improvements (Completed)","text":"<p>\u2705 Completed: - Simplified adaptive block size configuration - Professional benchmarking infrastructure - Performance analysis tools</p> <p>\ud83d\udcca Results: - Average efficiency improved from 5.7% to 18.0% - Medium/batch tensors achieving 29-33% efficiency</p>"},{"location":"performance/optimization-guide_en/#phase-2-kernel-optimization-in-progress","title":"Phase 2: Kernel Optimization (In Progress)","text":"<p>\ud83c\udfaf Target Areas: - Memory access pattern optimization (vectorization, cache-friendly tiling) - Automatic block size tuning - Kernel fusion to reduce memory bandwidth pressure</p>"},{"location":"performance/optimization-guide_en/#phase-3-advanced-optimization-future","title":"Phase 3: Advanced Optimization (Future)","text":"<ul> <li>Custom CUDA kernel hand optimization</li> <li>Memory layout optimization</li> <li>Multi-GPU support</li> </ul>"},{"location":"performance/optimization-guide_en/#usage-recommendations","title":"Usage Recommendations","text":""},{"location":"performance/optimization-guide_en/#genesis-vs-pytorch-choice","title":"Genesis vs PyTorch Choice","text":"<p>Recommend Using Genesis: - Educational learning and framework understanding - Medium batch processing operations (best performance 31% efficiency) - Research requiring custom kernel development</p> <p>Recommend Using PyTorch: - Production environments with maximum performance requirements - Large tensor operations (&gt;16M elements) - Applications sensitive to 5-25x performance differences</p>"},{"location":"performance/optimization-guide_en/#performance-tips","title":"Performance Tips","text":"<ol> <li>Tensor Size Awareness</li> <li>Optimal performance range: 1M-4M elements</li> <li>Avoid xlarge tensors (&gt;67M)</li> <li> <p>Consider tensor splitting for large operations</p> </li> <li> <p>Memory Management Python<pre><code># Use in-place operations\nresult = genesis.add(a, b, out=existing_tensor)\n</code></pre></p> </li> </ol>"},{"location":"performance/optimization-guide_en/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"performance/optimization-guide_en/#built-in-benchmarking","title":"Built-in Benchmarking","text":"Bash<pre><code># Quick performance check\npython benchmark/bench_ops.py --op add --fast\n\n# Comprehensive analysis\npython benchmark/bench_ops.py --op add --size large\n</code></pre>"},{"location":"performance/optimization-guide_en/#key-metrics","title":"Key Metrics","text":"<ul> <li>Memory Bandwidth Efficiency: Target &gt;50%</li> <li>GPU Utilization: Monitor with <code>nvidia-smi</code></li> <li>Kernel Launch Overhead: Analyze with Nsight Compute</li> </ul>"},{"location":"performance/optimization-guide_en/#performance-targets","title":"Performance Targets","text":"Tensor Category Minimum Efficiency Target Efficiency Small Tensors 15% 25% Medium Tensors 25% 40% Large Tensors 10% 30% XLarge Tensors 10% 25% Batch Processing 25% 45% <p>Last Updated: August 2025 Framework Version: Genesis 0.3.0-dev Benchmark Environment: A800-SXM4-80GB</p>"},{"location":"training/advanced-features.zh/","title":"\u9ad8\u7ea7\u8bad\u7ec3\u7279\u6027","text":"<p>Genesis\u63d0\u4f9b\u4e86\u591a\u4e2a\u9ad8\u7ea7\u7279\u6027\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002</p>"},{"location":"training/advanced-features.zh/#amp","title":"\ud83d\ude80 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 (AMP)","text":"<p>\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u5141\u8bb8\u4f60\u5728\u9002\u5f53\u7684\u5730\u65b9\u4f7f\u7528FP16/BF16\u8ba1\u7b97\u6765\u66f4\u5feb\u5730\u8bad\u7ec3\u6a21\u578b\uff0c\u540c\u65f6\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u901a\u8fc7\u7ef4\u6301FP32\u4e3b\u6743\u91cd\u6765\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u3002</p>"},{"location":"training/advanced-features.zh/#_2","title":"\u57fa\u672c\u7528\u6cd5","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\n\n# \u521b\u5efa\u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = nn.Linear(1024, 512)\noptimizer = optim.Adam(model.parameters())\n\n# \u4e3a\u6df7\u5408\u7cbe\u5ea6\u521d\u59cb\u5316\u68af\u5ea6\u7f29\u653e\u5668\nscaler = GradScaler()\n\n# \u4f7f\u7528AMP\u7684\u8bad\u7ec3\u5faa\u73af\nfor data, target in dataloader:\n    optimizer.zero_grad()\n\n    # \u4f7f\u7528autocast\u8fdb\u884c\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\n    with autocast():\n        output = model(data)\n        loss = criterion(output, target)\n\n    # \u7f29\u653e\u635f\u5931\u5e76\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\n    scaler.scale(loss).backward()\n\n    # \u53cd\u7f29\u653e\u5e76\u6267\u884c\u4f18\u5316\u5668\u6b65\u9aa4\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"training/advanced-features.zh/#_3","title":"\u652f\u6301\u7684\u6570\u636e\u7c7b\u578b","text":"<p>Genesis\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\u683c\u5f0f\uff1a</p> <ul> <li>float16 (FP16): \u534a\u7cbe\u5ea6\uff0c\u5728\u5927\u591a\u6570GPU\u4e0a\u6700\u5feb</li> <li>bfloat16 (BF16): \u8111\u6d6e\u70b9\u6570\uff0c\u6bd4FP16\u6709\u66f4\u597d\u7684\u6570\u503c\u8303\u56f4</li> <li>float32 (FP32): \u5355\u7cbe\u5ea6\uff0c\u4e3b\u6743\u91cd\u7684\u9ed8\u8ba4\u7c7b\u578b</li> </ul>"},{"location":"training/advanced-features.zh/#_4","title":"\u4f18\u52bf","text":"<ul> <li>\u901f\u5ea6: \u5728\u73b0\u4ee3GPU\u4e0a\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe2\u500d</li> <li>\u5185\u5b58: \u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u5141\u8bb8\u66f4\u5927\u7684\u6279\u6b21\u5927\u5c0f</li> <li>\u7cbe\u5ea6: \u901a\u8fc7\u635f\u5931\u7f29\u653e\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6</li> </ul>"},{"location":"training/advanced-features.zh/#_5","title":"\u2702\ufe0f \u68af\u5ea6\u88c1\u526a","text":"<p>\u68af\u5ea6\u88c1\u526a\u6709\u52a9\u4e8e\u9632\u6b62\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u7206\u70b8\uff0c\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8eRNN\u548cTransformer\u3002</p>"},{"location":"training/advanced-features.zh/#_6","title":"\u68af\u5ea6\u8303\u6570\u88c1\u526a","text":"<p>\u5f53\u68af\u5ea6\u7684L2\u8303\u6570\u8d85\u8fc7\u9608\u503c\u65f6\u8fdb\u884c\u88c1\u526a\uff1a</p> Python<pre><code>import genesis.nn.utils as nn_utils\n\n# \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\nloss.backward()\n\n# \u6309\u8303\u6570\u88c1\u526a\u68af\u5ea6\uff08\u5927\u591a\u6570\u60c5\u51b5\u63a8\u8350\uff09\nnn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\noptimizer.step()\n</code></pre>"},{"location":"training/advanced-features.zh/#_7","title":"\u68af\u5ea6\u503c\u88c1\u526a","text":"<p>\u5c06\u68af\u5ea6\u503c\u88c1\u526a\u5230\u7279\u5b9a\u8303\u56f4\uff1a</p> Python<pre><code># \u6309\u503c\u88c1\u526a\u68af\u5ea6\nnn_utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n</code></pre>"},{"location":"training/advanced-features.zh/#_8","title":"\u4f55\u65f6\u4f7f\u7528","text":"<ul> <li>\u68af\u5ea6\u8303\u6570\u88c1\u526a: \u63a8\u8350\u7528\u4e8eRNN\u3001LSTM\u548cTransformer</li> <li>\u68af\u5ea6\u503c\u88c1\u526a: \u5f53\u9700\u8981\u5bf9\u68af\u5ea6\u503c\u8fdb\u884c\u786c\u9650\u5236\u65f6\u6709\u7528</li> <li>\u5178\u578b\u503c: \u5927\u591a\u6570\u6a21\u578b\u7684max_norm\u57280.5\u52305.0\u4e4b\u95f4</li> </ul>"},{"location":"training/advanced-features.zh/#_9","title":"\ud83d\udcc8 \u5b66\u4e60\u7387\u8c03\u5ea6\u5668","text":"<p>\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u6574\u5b66\u4e60\u7387\uff0c\u4ee5\u6539\u5584\u6536\u655b\u6027\u548c\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u3002</p>"},{"location":"training/advanced-features.zh/#steplr","title":"StepLR","text":"<p>\u6bcfstep_size\u4e2aepoch\u5c06\u5b66\u4e60\u7387\u8870\u51cfgamma\u500d\uff1a</p> Python<pre><code>from genesis.optim.lr_scheduler import StepLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # \u6bcf30\u4e2aepoch\u8870\u51cf\u5b66\u4e60\u7387\n</code></pre>"},{"location":"training/advanced-features.zh/#exponentiallr","title":"ExponentialLR","text":"<p>\u6307\u6570\u8870\u51cf\u5b66\u4e60\u7387\uff1a</p> Python<pre><code>from genesis.optim.lr_scheduler import ExponentialLR\n\nscheduler = ExponentialLR(optimizer, gamma=0.95)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # \u6bcf\u4e2aepoch\u5b66\u4e60\u7387 = \u5b66\u4e60\u7387 * 0.95\n</code></pre>"},{"location":"training/advanced-features.zh/#cosineannealinglr","title":"CosineAnnealingLR","text":"<p>\u4f7f\u7528\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\uff1a</p> Python<pre><code>from genesis.optim.lr_scheduler import CosineAnnealingLR\n\n# T_max: \u6700\u5927\u8fed\u4ee3\u6b21\u6570\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()\n</code></pre>"},{"location":"training/advanced-features.zh/#_10","title":"\u81ea\u5b9a\u4e49\u5b66\u4e60\u7387\u8c03\u5ea6","text":"<p>\u4f60\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u81ea\u5b9a\u4e49\u8c03\u5ea6\uff1a</p> Python<pre><code>def custom_lr_lambda(epoch):\n    # \u524d10\u4e2aepoch\u9884\u70ed\uff0c\u7136\u540e\u8870\u51cf\n    if epoch &lt; 10:\n        return epoch / 10\n    else:\n        return 0.95 ** (epoch - 10)\n\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n</code></pre>"},{"location":"training/advanced-features.zh/#_11","title":"\ud83d\udcbe \u68c0\u67e5\u70b9","text":"<p>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u548c\u6062\u590d\u6a21\u578b\u72b6\u6001\uff0c\u4ee5\u5b9e\u73b0\u5bb9\u9519\u548c\u6a21\u578b\u90e8\u7f72\u3002</p>"},{"location":"training/advanced-features.zh/#_12","title":"\u4fdd\u5b58\u68c0\u67e5\u70b9","text":"Python<pre><code>import genesis\n\n# \u4fdd\u5b58\u6a21\u578b\u72b6\u6001\ngenesis.save_checkpoint({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    'best_accuracy': best_acc\n}, 'checkpoint_epoch_10.pth')\n</code></pre>"},{"location":"training/advanced-features.zh/#_13","title":"\u52a0\u8f7d\u68c0\u67e5\u70b9","text":"Python<pre><code># \u52a0\u8f7d\u68c0\u67e5\u70b9\ncheckpoint = genesis.load_checkpoint('checkpoint_epoch_10.pth')\n\n# \u6062\u590d\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n</code></pre>"},{"location":"training/advanced-features.zh/#_14","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u5b9a\u671f\u4fdd\u5b58: \u6bcfN\u4e2aepoch\u4fdd\u5b58\u68c0\u67e5\u70b9</li> <li>\u6700\u4f73\u6a21\u578b\u8ddf\u8e2a: \u4fdd\u7559\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b</li> <li>\u5143\u6570\u636e\u5b58\u50a8: \u5305\u542b\u8bad\u7ec3\u914d\u7f6e\u548c\u6307\u6807</li> </ol> Python<pre><code># \u793a\u4f8b\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u6700\u4f73\u6a21\u578b\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    val_loss = validate(model, val_loader)\n\n    if val_loss &lt; best_loss:\n        best_loss = val_loss\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'best_loss': best_loss\n        }, 'best_model.pth')\n</code></pre>"},{"location":"training/advanced-features.zh/#_15","title":"\ud83d\udd27 \u5b8c\u6574\u8bad\u7ec3\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u662f\u7ed3\u5408\u6240\u6709\u9ad8\u7ea7\u7279\u6027\u7684\u5b8c\u6574\u793a\u4f8b\uff1a</p> Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\nfrom genesis.optim.lr_scheduler import CosineAnnealingLR\nimport genesis.nn.utils as nn_utils\n\n# \u6a21\u578b\u8bbe\u7f6e\nmodel = YourModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\nscaler = GradScaler()\n\n# \u8bad\u7ec3\u914d\u7f6e\nmax_grad_norm = 1.0\ncheckpoint_interval = 10\n\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        # \u6df7\u5408\u7cbe\u5ea6\u524d\u5411\u4f20\u64ad\n        with autocast():\n            output = model(data)\n            loss = criterion(output, target)\n\n        # \u7f29\u653e\u7684\u53cd\u5411\u4f20\u64ad\n        scaler.scale(loss).backward()\n\n        # \u68af\u5ea6\u88c1\u526a\n        scaler.unscale_(optimizer)\n        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        # \u5e26\u7f29\u653e\u7684\u4f18\u5316\u5668\u6b65\u9aa4\n        scaler.step(optimizer)\n        scaler.update()\n\n    # \u66f4\u65b0\u5b66\u4e60\u7387\n    scheduler.step()\n\n    # \u4fdd\u5b58\u68c0\u67e5\u70b9\n    if epoch % checkpoint_interval == 0:\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n        }, f'checkpoint_epoch_{epoch}.pth')\n</code></pre>"},{"location":"training/advanced-features.zh/#_16","title":"\ud83d\udcca \u6027\u80fd\u63d0\u793a","text":""},{"location":"training/advanced-features.zh/#_17","title":"\u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef\u83b7\u5f97\u66f4\u5927\u7684\u6709\u6548\u6279\u6b21\u5927\u5c0f</li> <li>\u4e3a\u975e\u5e38\u6df1\u7684\u6a21\u578b\u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9</li> <li>\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u51cf\u5c11\u5185\u5b58\u4f7f\u7528</li> </ul>"},{"location":"training/advanced-features.zh/#_18","title":"\u901f\u5ea6\u4f18\u5316","text":"<ul> <li>\u4f7f\u7528\u9002\u5f53\u7684\u6570\u636e\u7c7b\u578b\uff08FP16\u7528\u4e8e\u901f\u5ea6\uff0cBF16\u7528\u4e8e\u7a33\u5b9a\u6027\uff09</li> <li>\u8c03\u6574\u68af\u5ea6\u7d2f\u79ef\u6b65\u6570</li> <li>\u5206\u6790\u8bad\u7ec3\u5faa\u73af\u4ee5\u8bc6\u522b\u74f6\u9888</li> </ul>"},{"location":"training/advanced-features.zh/#_19","title":"\u6536\u655b\u6280\u5de7","text":"<ul> <li>\u4ece\u5b66\u4e60\u7387\u67e5\u627e\u5668\u5f00\u59cb\u8bc6\u522b\u6700\u4f18\u5b66\u4e60\u7387</li> <li>\u5bf9\u5927\u6279\u6b21\u8bad\u7ec3\u4f7f\u7528\u9884\u70ed</li> <li>\u76d1\u63a7\u68af\u5ea6\u8303\u6570\u4ee5\u65e9\u671f\u68c0\u6d4b\u4e0d\u7a33\u5b9a\u6027</li> </ul>"},{"location":"training/advanced-features.zh/#_20","title":"\ud83d\udd17 \u76f8\u5173\u4e3b\u9898","text":"<ul> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b</li> <li>\u6027\u80fd\u8c03\u4f18\u6307\u5357</li> <li>\u6a21\u578b\u67b6\u6784\u6307\u5357</li> <li>\u4f18\u5316\u5668\u6587\u6863</li> </ul>"},{"location":"training/advanced-features_en/","title":"Advanced Training Features","text":"<p>Genesis provides several advanced features to enhance training efficiency and model performance.</p>"},{"location":"training/advanced-features_en/#mixed-precision-training-amp","title":"\ud83d\ude80 Mixed Precision Training (AMP)","text":"<p>Automatic Mixed Precision (AMP) allows you to train models faster with lower memory usage by utilizing FP16/BF16 computations where appropriate while maintaining FP32 master weights for numerical stability.</p>"},{"location":"training/advanced-features_en/#basic-usage","title":"Basic Usage","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\n\n# Create model and optimizer\nmodel = nn.Linear(1024, 512)\noptimizer = optim.Adam(model.parameters())\n\n# Initialize gradient scaler for mixed precision\nscaler = GradScaler()\n\n# Training loop with AMP\nfor data, target in dataloader:\n    optimizer.zero_grad()\n\n    # Use autocast for automatic mixed precision\n    with autocast():\n        output = model(data)\n        loss = criterion(output, target)\n\n    # Scale the loss and backward pass\n    scaler.scale(loss).backward()\n\n    # Unscale and step optimizer\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"training/advanced-features_en/#supported-data-types","title":"Supported Data Types","text":"<p>Genesis supports multiple precision formats:</p> <ul> <li>float16 (FP16): Half precision, fastest on most GPUs</li> <li>bfloat16 (BF16): Brain floating point, better range than FP16</li> <li>float32 (FP32): Single precision, default for master weights</li> </ul>"},{"location":"training/advanced-features_en/#benefits","title":"Benefits","text":"<ul> <li>Speed: Up to 2x faster training on modern GPUs</li> <li>Memory: Reduced memory usage allows larger batch sizes</li> <li>Accuracy: Maintains model accuracy with loss scaling</li> </ul>"},{"location":"training/advanced-features_en/#gradient-clipping","title":"\u2702\ufe0f Gradient Clipping","text":"<p>Gradient clipping helps prevent gradient explosion in deep networks and improves training stability, especially for RNNs and Transformers.</p>"},{"location":"training/advanced-features_en/#gradient-norm-clipping","title":"Gradient Norm Clipping","text":"<p>Clips gradients when their L2 norm exceeds a threshold:</p> Python<pre><code>import genesis.nn.utils as nn_utils\n\n# During training\nloss.backward()\n\n# Clip gradients by norm (recommended for most cases)\nnn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\noptimizer.step()\n</code></pre>"},{"location":"training/advanced-features_en/#gradient-value-clipping","title":"Gradient Value Clipping","text":"<p>Clips gradient values to a specific range:</p> Python<pre><code># Clip gradients by value\nnn_utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n</code></pre>"},{"location":"training/advanced-features_en/#when-to-use","title":"When to Use","text":"<ul> <li>Gradient Norm Clipping: Recommended for RNNs, LSTMs, and Transformers</li> <li>Gradient Value Clipping: Useful when you need hard limits on gradient values</li> <li>Typical Values: max_norm between 0.5 and 5.0 for most models</li> </ul>"},{"location":"training/advanced-features_en/#learning-rate-schedulers","title":"\ud83d\udcc8 Learning Rate Schedulers","text":"<p>Learning rate schedulers adjust the learning rate during training to improve convergence and final model performance.</p>"},{"location":"training/advanced-features_en/#steplr","title":"StepLR","text":"<p>Decays learning rate by gamma every step_size epochs:</p> Python<pre><code>from genesis.optim.lr_scheduler import StepLR\n\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nscheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # Decay LR every 30 epochs\n</code></pre>"},{"location":"training/advanced-features_en/#exponentiallr","title":"ExponentialLR","text":"<p>Decays learning rate exponentially:</p> Python<pre><code>from genesis.optim.lr_scheduler import ExponentialLR\n\nscheduler = ExponentialLR(optimizer, gamma=0.95)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()  # LR = LR * 0.95 each epoch\n</code></pre>"},{"location":"training/advanced-features_en/#cosineannealinglr","title":"CosineAnnealingLR","text":"<p>Uses cosine annealing schedule:</p> Python<pre><code>from genesis.optim.lr_scheduler import CosineAnnealingLR\n\n# T_max: Maximum number of iterations\nscheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n\nfor epoch in range(100):\n    train(...)\n    scheduler.step()\n</code></pre>"},{"location":"training/advanced-features_en/#custom-learning-rate-schedule","title":"Custom Learning Rate Schedule","text":"<p>You can also implement custom schedules:</p> Python<pre><code>def custom_lr_lambda(epoch):\n    # Warmup for first 10 epochs, then decay\n    if epoch &lt; 10:\n        return epoch / 10\n    else:\n        return 0.95 ** (epoch - 10)\n\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n</code></pre>"},{"location":"training/advanced-features_en/#checkpointing","title":"\ud83d\udcbe Checkpointing","text":"<p>Save and restore model states during training for fault tolerance and model deployment.</p>"},{"location":"training/advanced-features_en/#saving-checkpoints","title":"Saving Checkpoints","text":"Python<pre><code>import genesis\n\n# Save model state\ngenesis.save_checkpoint({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    'best_accuracy': best_acc\n}, 'checkpoint_epoch_10.pth')\n</code></pre>"},{"location":"training/advanced-features_en/#loading-checkpoints","title":"Loading Checkpoints","text":"Python<pre><code># Load checkpoint\ncheckpoint = genesis.load_checkpoint('checkpoint_epoch_10.pth')\n\n# Restore model and optimizer states\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n</code></pre>"},{"location":"training/advanced-features_en/#best-practices","title":"Best Practices","text":"<ol> <li>Regular Saving: Save checkpoints every N epochs</li> <li>Best Model Tracking: Keep the best performing model</li> <li>Metadata Storage: Include training configuration and metrics</li> </ol> Python<pre><code># Example: Save best model during training\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    val_loss = validate(model, val_loader)\n\n    if val_loss &lt; best_loss:\n        best_loss = val_loss\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'best_loss': best_loss\n        }, 'best_model.pth')\n</code></pre>"},{"location":"training/advanced-features_en/#complete-training-example","title":"\ud83d\udd27 Complete Training Example","text":"<p>Here's a complete example combining all advanced features:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom genesis.amp import autocast, GradScaler\nfrom genesis.optim.lr_scheduler import CosineAnnealingLR\nimport genesis.nn.utils as nn_utils\n\n# Model setup\nmodel = YourModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\nscaler = GradScaler()\n\n# Training configuration\nmax_grad_norm = 1.0\ncheckpoint_interval = 10\n\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        # Mixed precision forward pass\n        with autocast():\n            output = model(data)\n            loss = criterion(output, target)\n\n        # Scaled backward pass\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        scaler.unscale_(optimizer)\n        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n        # Optimizer step with scaling\n        scaler.step(optimizer)\n        scaler.update()\n\n    # Update learning rate\n    scheduler.step()\n\n    # Save checkpoint\n    if epoch % checkpoint_interval == 0:\n        genesis.save_checkpoint({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict(),\n        }, f'checkpoint_epoch_{epoch}.pth')\n</code></pre>"},{"location":"training/advanced-features_en/#performance-tips","title":"\ud83d\udcca Performance Tips","text":""},{"location":"training/advanced-features_en/#memory-optimization","title":"Memory Optimization","text":"<ul> <li>Use gradient accumulation for larger effective batch sizes</li> <li>Enable gradient checkpointing for very deep models</li> <li>Use mixed precision training to reduce memory usage</li> </ul>"},{"location":"training/advanced-features_en/#speed-optimization","title":"Speed Optimization","text":"<ul> <li>Use the appropriate data type (FP16 for speed, BF16 for stability)</li> <li>Tune gradient accumulation steps</li> <li>Profile your training loop to identify bottlenecks</li> </ul>"},{"location":"training/advanced-features_en/#convergence-tips","title":"Convergence Tips","text":"<ul> <li>Start with a learning rate finder to identify optimal LR</li> <li>Use warmup for large batch training</li> <li>Monitor gradient norms to detect instability early</li> </ul>"},{"location":"training/advanced-features_en/#related-topics","title":"\ud83d\udd17 Related Topics","text":"<ul> <li>Basic Training Tutorial</li> <li>Performance Tuning Guide</li> <li>Model Architecture Guide</li> <li>Optimizer Documentation</li> </ul>"},{"location":"tutorials/basic-training.zh/","title":"\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u5e26\u4f60\u4ece\u96f6\u5f00\u59cb\uff0c\u4f7f\u7528Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6784\u5efa\u548c\u8bad\u7ec3\u4f60\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u5c06\u901a\u8fc7\u4e00\u4e2a\u5b8c\u6574\u7684\u56fe\u50cf\u5206\u7c7b\u9879\u76ee\u6765\u5b66\u4e60Genesis\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u7528\u6cd5\u3002</p>"},{"location":"tutorials/basic-training.zh/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u5b66\u4f1a\uff1a - Genesis\u7684\u57fa\u672cAPI\u548c\u6570\u636e\u7ed3\u6784 - \u5982\u4f55\u5b9a\u4e49\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b - \u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406 - \u8bad\u7ec3\u5faa\u73af\u7684\u6784\u5efa\u548c\u4f18\u5316 - \u6a21\u578b\u8bc4\u4f30\u548c\u4fdd\u5b58</p>"},{"location":"tutorials/basic-training.zh/#_3","title":"\ud83d\udee0\ufe0f \u73af\u5883\u51c6\u5907","text":""},{"location":"tutorials/basic-training.zh/#_4","title":"\u5b89\u88c5\u4f9d\u8d56","text":"Bash<pre><code># \u786e\u4fdd\u5df2\u5b89\u88c5Genesis\npip install torch triton\ngit clone https://github.com/phonism/genesis.git\ncd genesis\npip install -e .\n\n# \u5b89\u88c5\u989d\u5916\u4f9d\u8d56\npip install matplotlib torchvision tqdm\n</code></pre>"},{"location":"tutorials/basic-training.zh/#_5","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\nprint(f\"Genesis\u7248\u672c: {genesis.__version__}\")\nprint(f\"CUDA\u53ef\u7528: {genesis.cuda.is_available()}\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#_6","title":"\ud83d\udcca \u9879\u76ee\uff1a\u624b\u5199\u6570\u5b57\u8bc6\u522b","text":"<p>\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528\u7ecf\u5178\u7684MNIST\u6570\u636e\u96c6\u3002</p>"},{"location":"tutorials/basic-training.zh/#1","title":"1. \u6570\u636e\u51c6\u5907","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# \u6570\u636e\u9884\u5904\u7406\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# \u52a0\u8f7dMNIST\u6570\u636e\u96c6\ntrain_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('data', train=False, transform=transform)\n\n# \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\nbatch_size = 64\ntrain_loader = genesis.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = genesis.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\u8bad\u7ec3\u96c6\u5927\u5c0f: {len(train_dataset)}\")\nprint(f\"\u6d4b\u8bd5\u96c6\u5927\u5c0f: {len(test_dataset)}\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#2","title":"2. \u6a21\u578b\u5b9a\u4e49","text":"<p>\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a</p> Python<pre><code>class MNISTNet(nn.Module):\n    \"\"\"MNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b\u7f51\u7edc\"\"\"\n\n    def __init__(self, num_classes=10):\n        super(MNISTNet, self).__init__()\n\n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n        # \u6fc0\u6d3b\u51fd\u6570\u548cDropout\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # \u5377\u79ef\u57571\n        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n\n        # \u5377\u79ef\u57572  \n        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -&gt; 7x7\n\n        # \u5c55\u5e73\n        x = x.view(x.size(0), -1)  # [batch_size, 64*7*7]\n\n        # \u5168\u8fde\u63a5\u5c42\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# \u521b\u5efa\u6a21\u578b\u5b9e\u4f8b\ndevice = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\nmodel = MNISTNet().to(device)\n\nprint(\"\u6a21\u578b\u7ed3\u6784:\")\nprint(model)\nprint(f\"\\\\n\u53c2\u6570\u603b\u6570: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#3","title":"3. \u8bad\u7ec3\u914d\u7f6e","text":"Python<pre><code># \u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n\n# \u8bad\u7ec3\u53c2\u6570\nnum_epochs = 10\nprint_every = 100  # \u6bcf100\u4e2abatch\u6253\u5370\u4e00\u6b21\n\nprint(f\"\u8bbe\u5907: {device}\")\nprint(f\"\u6279\u91cf\u5927\u5c0f: {batch_size}\")\nprint(f\"\u8bad\u7ec3\u8f6e\u6570: {num_epochs}\")\nprint(f\"\u5b66\u4e60\u7387: {optimizer.param_groups[0]['lr']}\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#4","title":"4. \u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>def train_epoch(model, train_loader, criterion, optimizer, epoch):\n    \"\"\"\u8bad\u7ec3\u4e00\u4e2aepoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # \u6570\u636e\u79fb\u5230\u8bbe\u5907\n        data, target = data.to(device), target.to(device)\n\n        # \u524d\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n\n        # \u7edf\u8ba1\n        running_loss += loss.item()\n        _, predicted = genesis.max(output, dim=1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n        # \u6253\u5370\u8fdb\u5ea6\n        if batch_idx % print_every == 0:\n            print(f'Epoch {epoch+1}/{num_epochs}, '\n                  f'Batch {batch_idx}/{len(train_loader)}, '\n                  f'Loss: {loss.item():.4f}, '\n                  f'Acc: {100*correct/total:.2f}%')\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n\n    return epoch_loss, epoch_acc\n\ndef validate(model, test_loader, criterion):\n    \"\"\"\u9a8c\u8bc1\u6a21\u578b\"\"\"\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with genesis.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            test_loss += criterion(output, target).item()\n            _, predicted = genesis.max(output, dim=1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    avg_loss = test_loss / len(test_loader)\n    accuracy = 100 * correct / total\n\n    return avg_loss, accuracy\n\n# \u5f00\u59cb\u8bad\u7ec3\nprint(\"\u5f00\u59cb\u8bad\u7ec3...\")\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\n\nfor epoch in range(num_epochs):\n    # \u8bad\u7ec3\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, epoch)\n\n    # \u9a8c\u8bc1\n    val_loss, val_acc = validate(model, test_loader, criterion)\n\n    # \u5b66\u4e60\u7387\u8c03\u5ea6\n    scheduler.step()\n\n    # \u8bb0\u5f55\u7ed3\u679c\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n    print(f\"  \u8bad\u7ec3 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n    print(f\"  \u9a8c\u8bc1 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n    print(f\"  \u5b66\u4e60\u7387: {optimizer.param_groups[0]['lr']:.6f}\")\n    print(\"-\" * 50)\n\nprint(\"\u8bad\u7ec3\u5b8c\u6210\uff01\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#5","title":"5. \u7ed3\u679c\u53ef\u89c6\u5316","text":"Python<pre><code># \u7ed8\u5236\u8bad\u7ec3\u66f2\u7ebf\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# \u635f\u5931\u66f2\u7ebf\nax1.plot(train_losses, label='\u8bad\u7ec3\u635f\u5931', color='blue')\nax1.plot(val_losses, label='\u9a8c\u8bc1\u635f\u5931', color='red')\nax1.set_title('\u635f\u5931\u66f2\u7ebf')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True)\n\n# \u51c6\u786e\u7387\u66f2\u7ebf\nax2.plot(train_accs, label='\u8bad\u7ec3\u51c6\u786e\u7387', color='blue')\nax2.plot(val_accs, label='\u9a8c\u8bc1\u51c6\u786e\u7387', color='red')\nax2.set_title('\u51c6\u786e\u7387\u66f2\u7ebf')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u6700\u7ec8\u6d4b\u8bd5\u51c6\u786e\u7387: {val_accs[-1]:.2f}%\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#6","title":"6. \u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d","text":"Python<pre><code># \u4fdd\u5b58\u6a21\u578b\nmodel_path = 'mnist_model.pth'\ngenesis.save_checkpoint({\n    'epoch': num_epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'train_loss': train_losses[-1],\n    'val_loss': val_losses[-1],\n    'val_acc': val_accs[-1]\n}, model_path)\n\nprint(f\"\u6a21\u578b\u5df2\u4fdd\u5b58\u5230: {model_path}\")\n\n# \u52a0\u8f7d\u6a21\u578b\ndef load_model(model_path, model_class, num_classes=10):\n    \"\"\"\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\"\"\"\n    checkpoint = genesis.load_checkpoint(model_path)\n\n    model = model_class(num_classes)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    print(f\"\u6a21\u578b\u52a0\u8f7d\u6210\u529f\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387: {checkpoint['val_acc']:.2f}%\")\n    return model\n\n# \u6d4b\u8bd5\u52a0\u8f7d\nloaded_model = load_model(model_path, MNISTNet)\n</code></pre>"},{"location":"tutorials/basic-training.zh/#7","title":"7. \u5355\u5f20\u56fe\u7247\u9884\u6d4b","text":"Python<pre><code>def predict_single_image(model, image, class_names=None):\n    \"\"\"\u5bf9\u5355\u5f20\u56fe\u7247\u8fdb\u884c\u9884\u6d4b\"\"\"\n    model.eval()\n\n    if class_names is None:\n        class_names = [str(i) for i in range(10)]\n\n    with genesis.no_grad():\n        if image.dim() == 3:  # \u6dfb\u52a0batch\u7ef4\u5ea6\n            image = image.unsqueeze(0)\n\n        image = image.to(device)\n        output = model(image)\n        probabilities = genesis.softmax(output, dim=1)\n\n        confidence, predicted = genesis.max(probabilities, dim=1)\n\n    return predicted.item(), confidence.item()\n\n# \u6d4b\u8bd5\u9884\u6d4b\ntest_iter = iter(test_loader)\nimages, labels = next(test_iter)\n\n# \u9884\u6d4b\u524d5\u5f20\u56fe\u7247\nfig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i in range(5):\n    image = images[i]\n    true_label = labels[i].item()\n\n    predicted, confidence = predict_single_image(model, image)\n\n    # \u663e\u793a\u56fe\u7247\n    axes[i].imshow(image.squeeze(), cmap='gray')\n    axes[i].set_title(f'\u771f\u5b9e: {true_label}\\\\n\u9884\u6d4b: {predicted}\\\\n\u7f6e\u4fe1\u5ea6: {confidence:.3f}')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-training.zh/#_7","title":"\ud83d\udcc8 \u6027\u80fd\u5bf9\u6bd4","text":"<p>\u8ba9\u6211\u4eec\u6bd4\u8f83Genesis\u4e0ePyTorch\u7684\u6027\u80fd\uff1a</p> Python<pre><code>import time\n\ndef benchmark_training(model, train_loader, criterion, optimizer, device, num_batches=100):\n    \"\"\"\u8bad\u7ec3\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\"\"\"\n    model.train()\n    start_time = time.time()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if batch_idx &gt;= num_batches:\n            break\n\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n    elapsed_time = time.time() - start_time\n    return elapsed_time\n\n# \u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\nprint(\"\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 (100\u4e2abatch):\")\ngenesis_time = benchmark_training(model, train_loader, criterion, optimizer, device)\nprint(f\"Genesis\u8bad\u7ec3\u65f6\u95f4: {genesis_time:.2f} \u79d2\")\nprint(f\"\u5e73\u5747\u6bcf\u4e2abatch: {genesis_time/100*1000:.1f} ms\")\n</code></pre>"},{"location":"tutorials/basic-training.zh/#_8","title":"\ud83c\udfaf \u5173\u952e\u6982\u5ff5\u603b\u7ed3","text":""},{"location":"tutorials/basic-training.zh/#1_1","title":"1. \u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code># \u521b\u5efa\u5f20\u91cf\nx = genesis.randn(3, 4, requires_grad=True)\ny = genesis.ones(3, 4)\n\n# \u57fa\u7840\u8fd0\u7b97\nz = x + y\nw = genesis.matmul(x, y.T)\n\n# \u68af\u5ea6\u8ba1\u7b97\nz.sum().backward()\nprint(x.grad)  # x\u7684\u68af\u5ea6\n</code></pre>"},{"location":"tutorials/basic-training.zh/#2_1","title":"2. \u6a21\u578b\u5b9a\u4e49\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code>class BestPracticeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # \u4f7f\u7528nn.Sequential\u7b80\u5316\u5b9a\u4e49\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n</code></pre>"},{"location":"tutorials/basic-training.zh/#3_1","title":"3. \u8bad\u7ec3\u6280\u5de7","text":"Python<pre><code># \u68af\u5ea6\u88c1\u526a\ngenesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n# \u6743\u91cd\u521d\u59cb\u5316\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        genesis.nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            genesis.nn.init.zeros_(m.bias)\n\nmodel.apply(init_weights)\n</code></pre>"},{"location":"tutorials/basic-training.zh/#_9","title":"\ud83d\ude80 \u4e0b\u4e00\u6b65","text":"<p>\u606d\u559c\uff01\u4f60\u5df2\u7ecf\u5b8c\u6210\u4e86\u7b2c\u4e00\u4e2aGenesis\u8bad\u7ec3\u9879\u76ee\u3002\u63a5\u4e0b\u6765\u53ef\u4ee5\u63a2\u7d22\uff1a</p> <ol> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u52a0\u901f\u8bad\u7ec3\u5e76\u8282\u7701\u663e\u5b58</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u8bad\u7ec3\u6027\u80fd</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3</li> </ol>"},{"location":"tutorials/basic-training.zh/#_10","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u8bad\u7ec3\u901f\u5ea6\u6bd4\u9884\u671f\u6162\uff1f A: \u68c0\u67e5\u662f\u5426\u542f\u7528\u4e86CUDA\uff0c\u786e\u4fdd\u6570\u636e\u9884\u5904\u7406\u4e0d\u662f\u74f6\u9888\uff0c\u8003\u8651\u8c03\u6574batch_size\u3002</p> <p>Q: \u5185\u5b58\u4e0d\u8db3\u9519\u8bef\uff1f A: \u51cf\u5c0fbatch_size\uff0c\u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\uff0c\u6216\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002</p> <p>Q: \u6a21\u578b\u4e0d\u6536\u655b\uff1f A: \u68c0\u67e5\u5b66\u4e60\u7387\u8bbe\u7f6e\uff0c\u786e\u8ba4\u6570\u636e\u9884\u5904\u7406\u6b63\u786e\uff0c\u5c1d\u8bd5\u4e0d\u540c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002</p> <p>\u5b8c\u6210\u4e86\u57fa\u7840\u6559\u7a0b\uff01</p> <p>\u4f60\u73b0\u5728\u5df2\u7ecf\u638c\u63e1\u4e86Genesis\u7684\u6838\u5fc3\u6982\u5ff5\u3002\u7ee7\u7eed\u63a2\u7d22\u66f4\u9ad8\u7ea7\u7684\u7279\u6027\u5427\uff01</p> <p>\u4e0b\u4e00\u6559\u7a0b\uff1a\u81ea\u5b9a\u4e49\u7b97\u5b50 \u8fd4\u56de\u6559\u7a0b\u76ee\u5f55</p>"},{"location":"tutorials/basic-training_en/","title":"Basic Training Tutorial","text":"<p>This tutorial will take you from zero to building and training your first neural network using the Genesis deep learning framework. We will learn Genesis core concepts and usage through a complete image classification project.</p>"},{"location":"tutorials/basic-training_en/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<p>Through this tutorial, you will learn: - Genesis basic APIs and data structures - How to define and train neural network models - Data loading and preprocessing - Building and optimizing training loops - Model evaluation and saving</p>"},{"location":"tutorials/basic-training_en/#environment-setup","title":"\ud83d\udee0\ufe0f Environment Setup","text":""},{"location":"tutorials/basic-training_en/#install-dependencies","title":"Install Dependencies","text":"Bash<pre><code># Ensure Genesis is installed\npip install torch triton numpy matplotlib tqdm\ngit clone https://github.com/phonism/genesis.git\ncd genesis\npip install -e .\n</code></pre>"},{"location":"tutorials/basic-training_en/#verify-installation","title":"Verify Installation","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Test basic functionality\nx = genesis.randn(2, 3)\nprint(f\"Genesis tensor created: {x.shape}\")\nprint(f\"Genesis modules available: {dir(nn)}\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#project-handwritten-digit-recognition","title":"\ud83d\udcca Project: Handwritten Digit Recognition","text":"<p>We will build a handwritten digit recognition system using a simple fully connected neural network on synthetic data to demonstrate Genesis capabilities.</p>"},{"location":"tutorials/basic-training_en/#1-data-preparation","title":"1. Data Preparation","text":"<p>Since Genesis doesn't have built-in data loading utilities yet, we'll create synthetic data that mimics the MNIST structure:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass SimpleDataset:\n    \"\"\"Simple dataset class for demonstration\"\"\"\n\n    def __init__(self, num_samples=1000, input_dim=784, num_classes=10):\n        # Generate synthetic data similar to flattened MNIST\n        self.data = genesis.randn(num_samples, input_dim)\n\n        # Create labels based on data patterns (synthetic)\n        labels = genesis.randn(num_samples, num_classes)\n        self.labels = genesis.functional.max(labels, axis=1, keepdims=False)\n\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return self.num_samples\n\n    def get_batch(self, batch_size=32, start_idx=0):\n        \"\"\"Get a batch of data\"\"\"\n        end_idx = min(start_idx + batch_size, self.num_samples)\n        return (self.data[start_idx:end_idx], \n                self.labels[start_idx:end_idx])\n\n# Create datasets\ntrain_dataset = SimpleDataset(num_samples=800, input_dim=784, num_classes=10)\ntest_dataset = SimpleDataset(num_samples=200, input_dim=784, num_classes=10)\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\nprint(f\"Input dimension: 784 (28x28 flattened)\")\nprint(f\"Number of classes: 10\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#2-model-definition","title":"2. Model Definition","text":"<p>We'll build a simple but effective fully connected neural network using Genesis modules:</p> Python<pre><code>class MNISTNet(nn.Module):\n    \"\"\"Simple fully connected network for digit recognition\"\"\"\n\n    def __init__(self, input_dim=784, hidden_dim=128, num_classes=10):\n        super(MNISTNet, self).__init__()\n\n        # Define layers using actual Genesis modules\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, num_classes)\n\n        # Activation and regularization\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, x):\n        # Flatten input if needed\n        if len(x.shape) &gt; 2:\n            x = x.view(x.shape[0], -1)\n\n        # First hidden layer\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        # Second hidden layer\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n\n        # Output layer\n        x = self.fc3(x)\n\n        return x\n\n# Create model instance\nmodel = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)\n\nprint(\"Model structure:\")\nprint(f\"Layer 1: {model.fc1}\")\nprint(f\"Layer 2: {model.fc2}\")\nprint(f\"Layer 3: {model.fc3}\")\nprint(f\"Total parameters: {sum(p.data.size for p in model.parameters())}\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#3-loss-function-and-optimizer","title":"3. Loss Function and Optimizer","text":"Python<pre><code># Define loss function and optimizer using Genesis\ncriterion = nn.SoftmaxLoss()  # Use Genesis SoftmaxLoss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(f\"Loss function: {criterion}\")\nprint(f\"Optimizer: {optimizer}\")\nprint(f\"Learning rate: 0.001\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#4-training-loop","title":"4. Training Loop","text":"Python<pre><code>def train_epoch(model, dataset, criterion, optimizer, batch_size=32):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()  # Set to training mode\n\n    total_loss = 0.0\n    num_batches = len(dataset) // batch_size\n\n    for i in range(num_batches):\n        # Get batch data\n        start_idx = i * batch_size\n        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)\n\n        # Forward pass\n        outputs = model(batch_data)\n        loss = criterion(outputs, batch_labels)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Apply gradient clipping (optional)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Update weights\n        optimizer.step()\n\n        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n    return total_loss / num_batches\n\ndef evaluate(model, dataset, criterion, batch_size=32):\n    \"\"\"Evaluate model performance\"\"\"\n    model.eval()  # Set to evaluation mode\n\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = len(dataset) // batch_size\n\n    for i in range(num_batches):\n        start_idx = i * batch_size\n        batch_data, batch_labels = dataset.get_batch(batch_size, start_idx)\n\n        # Forward pass (no gradients needed)\n        outputs = model(batch_data)\n        loss = criterion(outputs, batch_labels)\n\n        # Calculate accuracy\n        predicted = genesis.functional.max(outputs, axis=1, keepdims=False)\n        total += batch_labels.shape[0]\n        correct += (predicted == batch_labels).sum().data\n\n        total_loss += loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n    accuracy = correct / total\n    avg_loss = total_loss / num_batches\n\n    return avg_loss, accuracy\n\n# Training configuration\nnum_epochs = 10\nbatch_size = 32\n\nprint(\"Starting training...\")\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Batch size: {batch_size}\")\nprint(\"-\" * 50)\n\n# Training loop\ntrain_losses = []\ntest_losses = []\ntest_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Train for one epoch\n    train_loss = train_epoch(model, train_dataset, criterion, optimizer, batch_size)\n\n    # Evaluate on test set\n    test_loss, test_accuracy = evaluate(model, test_dataset, criterion, batch_size)\n\n    # Record metrics\n    train_losses.append(train_loss)\n    test_losses.append(test_loss)\n    test_accuracies.append(test_accuracy)\n\n    # Print progress\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"  Train Loss: {train_loss:.4f}\")\n    print(f\"  Test Loss: {test_loss:.4f}\")\n    print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n    print(\"-\" * 30)\n\nprint(\"Training completed!\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#5-model-evaluation-and-visualization","title":"5. Model Evaluation and Visualization","text":"Python<pre><code># Plot training progress\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\n# Plot losses\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.grid(True)\n\n# Plot accuracy\nplt.subplot(1, 2, 2)\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Test Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Final evaluation\nfinal_test_loss, final_test_accuracy = evaluate(model, test_dataset, criterion, batch_size)\nprint(f\"\\nFinal Results:\")\nprint(f\"Test Loss: {final_test_loss:.4f}\")\nprint(f\"Test Accuracy: {final_test_accuracy:.4f}\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#6-model-saving-and-loading","title":"6. Model Saving and Loading","text":"Python<pre><code># Save model using Genesis serialization\nmodel_path = \"mnist_model.pkl\"\ngenesis.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")\n\n# Load model\nmodel_new = MNISTNet(input_dim=784, hidden_dim=128, num_classes=10)\nmodel_new.load_state_dict(genesis.load(model_path))\nprint(\"Model loaded successfully!\")\n\n# Verify loaded model works\ntest_loss, test_accuracy = evaluate(model_new, test_dataset, criterion, batch_size)\nprint(f\"Loaded model accuracy: {test_accuracy:.4f}\")\n</code></pre>"},{"location":"tutorials/basic-training_en/#key-concepts-learned","title":"\ud83c\udf93 Key Concepts Learned","text":""},{"location":"tutorials/basic-training_en/#1-genesis-tensor-operations","title":"1. Genesis Tensor Operations","text":"<ul> <li>Creating tensors with <code>genesis.randn()</code>, <code>genesis.tensor()</code></li> <li>Basic operations like matrix multiplication and element-wise operations</li> <li>Automatic differentiation with <code>requires_grad</code></li> </ul>"},{"location":"tutorials/basic-training_en/#2-neural-network-modules","title":"2. Neural Network Modules","text":"<ul> <li>Defining models by inheriting from <code>nn.Module</code></li> <li>Using built-in layers: <code>nn.Linear</code>, <code>nn.ReLU</code>, <code>nn.Dropout</code></li> <li>Understanding forward pass implementation</li> </ul>"},{"location":"tutorials/basic-training_en/#3-training-process","title":"3. Training Process","text":"<ul> <li>Setting up loss functions and optimizers</li> <li>Implementing training and evaluation loops</li> <li>Using gradient clipping and regularization</li> </ul>"},{"location":"tutorials/basic-training_en/#4-model-management","title":"4. Model Management","text":"<ul> <li>Saving and loading model state with Genesis serialization</li> <li>Managing model parameters and optimization state</li> </ul>"},{"location":"tutorials/basic-training_en/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>After completing this tutorial, you can:</p> <ol> <li>Explore more complex models - Try different architectures with more layers</li> <li>Learn advanced features - Explore mixed precision training and learning rate scheduling</li> <li>Work with real data - Integrate with actual datasets when data loading utilities are available</li> <li>Performance optimization - Learn about GPU acceleration and Triton kernel usage</li> </ol>"},{"location":"tutorials/basic-training_en/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Genesis API Reference - Complete API documentation</li> <li>Advanced Training Features - Mixed precision, schedulers, etc.</li> <li>Performance Optimization - Tips for faster training</li> </ul>"},{"location":"tutorials/basic-training_en/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"tutorials/basic-training_en/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Ensure Genesis is properly installed with <code>pip install -e .</code></li> <li>Shape mismatches: Check tensor dimensions in forward pass</li> <li>Memory issues: Reduce batch size if encountering out-of-memory errors</li> <li>Slow training: Enable GPU support when available</li> </ol>"},{"location":"tutorials/basic-training_en/#getting-help","title":"Getting Help","text":"<ul> <li>Check the Genesis Documentation</li> <li>Report issues on GitHub Issues</li> <li>Join discussions in the community forums</li> </ul>"},{"location":"tutorials/custom-ops.zh/","title":"\u81ea\u5b9a\u4e49\u7b97\u5b50\u5f00\u53d1","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>Genesis\u6846\u67b6\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u5b50\u5f00\u53d1\uff0c\u8ba9\u4f60\u53ef\u4ee5\u5b9e\u73b0\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u3002\u672c\u6559\u7a0b\u5c06\u6559\u4f60\u5982\u4f55\u4ece\u96f6\u5f00\u59cb\u521b\u5efa\u9ad8\u6027\u80fd\u7684\u81ea\u5b9a\u4e49\u7b97\u5b50\u3002</p>"},{"location":"tutorials/custom-ops.zh/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3Genesis\u7684\u7b97\u5b50\u7cfb\u7edf\u67b6\u6784</li> <li>\u5b66\u4f1a\u5b9e\u73b0CPU\u548cGPU\u7248\u672c\u7684\u81ea\u5b9a\u4e49\u7b97\u5b50</li> <li>\u638c\u63e1Triton kernel\u7f16\u7a0b\u6280\u5de7</li> <li>\u4e86\u89e3\u7b97\u5b50\u4f18\u5316\u548c\u6027\u80fd\u8c03\u8bd5\u65b9\u6cd5</li> </ul>"},{"location":"tutorials/custom-ops.zh/#_3","title":"\ud83d\udccb \u9884\u5907\u77e5\u8bc6","text":"<p>\u5728\u5f00\u59cb\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\uff1a - \u5b8c\u6210\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u4e86\u89e3CUDA\u7f16\u7a0b\u57fa\u7840 - \u719f\u6089Python C\u6269\u5c55\u5f00\u53d1</p>"},{"location":"tutorials/custom-ops.zh/#_4","title":"\ud83d\udee0\ufe0f \u5f00\u53d1\u73af\u5883","text":"Bash<pre><code># \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56\npip install triton pybind11 cmake ninja\n</code></pre>"},{"location":"tutorials/custom-ops.zh/#rmsnorm","title":"\ud83d\udcdd \u793a\u4f8b\uff1aRMSNorm\u7b97\u5b50","text":"<p>\u6211\u4eec\u5c06\u5b9e\u73b0RMSNorm\uff08Root Mean Square Normalization\uff09\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"tutorials/custom-ops.zh/#cpu","title":"CPU\u5b9e\u73b0","text":"Python<pre><code># WIP: CPU\u5b9e\u73b0\u4ee3\u7801\u5c06\u5728\u540e\u7eed\u7248\u672c\u4e2d\u6dfb\u52a0\n</code></pre>"},{"location":"tutorials/custom-ops.zh/#gpu-triton","title":"GPU\u5b9e\u73b0 (Triton)","text":"Python<pre><code># WIP: Triton\u5b9e\u73b0\u4ee3\u7801\u5c06\u5728\u540e\u7eed\u7248\u672c\u4e2d\u6dfb\u52a0\n</code></pre>"},{"location":"tutorials/custom-ops.zh/#_5","title":"\ud83d\ude80 \u9ad8\u7ea7\u7279\u6027","text":"<ul> <li>\u81ea\u52a8\u5fae\u5206\u652f\u6301</li> <li>\u5185\u5b58\u4f18\u5316\u6280\u5de7</li> <li>\u7b97\u5b50\u878d\u5408\u7b56\u7565</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"},{"location":"tutorials/custom-ops_en/","title":"Custom Operator Development","text":"<p>Under Development</p> <p>This document is being written and content will be continuously updated.</p> <p>The Genesis framework supports custom operator development, allowing you to implement specialized neural network operations. This tutorial will teach you how to create high-performance custom operators from scratch.</p>"},{"location":"tutorials/custom-ops_en/#learning-objectives","title":"\ud83c\udfaf Learning Objectives","text":"<ul> <li>Understand Genesis operator system architecture</li> <li>Learn to implement CPU and GPU versions of custom operators</li> <li>Master Triton kernel programming techniques</li> <li>Learn operator optimization and performance debugging methods</li> </ul>"},{"location":"tutorials/custom-ops_en/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Before starting, please ensure you have: - Completed the Basic Training Tutorial - Understanding of CUDA programming basics - Familiarity with Python C extension development</p>"},{"location":"tutorials/custom-ops_en/#development-environment","title":"\ud83d\udee0\ufe0f Development Environment","text":"Bash<pre><code># Install development dependencies\npip install triton pybind11 cmake ninja\n</code></pre>"},{"location":"tutorials/custom-ops_en/#example-rmsnorm-operator","title":"\ud83d\udcdd Example: RMSNorm Operator","text":"<p>We will implement RMSNorm (Root Mean Square Normalization) as an example.</p>"},{"location":"tutorials/custom-ops_en/#cpu-implementation","title":"CPU Implementation","text":"Python<pre><code># WIP: CPU implementation code will be added in future versions\n</code></pre>"},{"location":"tutorials/custom-ops_en/#gpu-implementation-triton","title":"GPU Implementation (Triton)","text":"Python<pre><code># WIP: Triton implementation code will be added in future versions\n</code></pre>"},{"location":"tutorials/custom-ops_en/#advanced-features","title":"\ud83d\ude80 Advanced Features","text":"<ul> <li>Automatic differentiation support</li> <li>Memory optimization techniques</li> <li>Operator fusion strategies</li> </ul> <p>\ud83d\udcd8 Documentation Status: Under development, expected to be completed in v0.2.0.</p>"},{"location":"tutorials/index.zh/","title":"\u6559\u7a0b\u603b\u89c8","text":"<p>\u6b22\u8fce\u6765\u5230Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6559\u7a0b\u7cfb\u5217\uff01\u8fd9\u4e9b\u6559\u7a0b\u5c06\u5e2e\u52a9\u4f60\u4ece\u5165\u95e8\u5230\u7cbe\u901aGenesis\u6846\u67b6\u7684\u5404\u4e2a\u65b9\u9762\u3002</p>"},{"location":"tutorials/index.zh/#_2","title":"\ud83d\udcda \u6559\u7a0b\u5206\u7c7b","text":""},{"location":"tutorials/index.zh/#_3","title":"\ud83c\udfaf \u57fa\u7840\u6559\u7a0b","text":"<p>\u9002\u5408\u521d\u5b66\u8005\uff0c\u6db5\u76d6Genesis\u7684\u57fa\u672c\u6982\u5ff5\u548c\u7528\u6cd5\u3002</p> <ul> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u5b66\u4e60\u5982\u4f55\u4f7f\u7528Genesis\u8bad\u7ec3\u4f60\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc</li> <li>\u6570\u636e\u5904\u7406\u6559\u7a0b - \u6570\u636e\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6570\u636e\u7ba1\u9053\u6784\u5efa</li> <li>\u6a21\u578b\u5b9a\u4e49\u6559\u7a0b - \u5982\u4f55\u5b9a\u4e49\u548c\u7ec4\u7ec7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b</li> </ul>"},{"location":"tutorials/index.zh/#_4","title":"\ud83d\ude80 \u8fdb\u9636\u6559\u7a0b","text":"<p>\u6df1\u5165\u4e86\u89e3Genesis\u7684\u9ad8\u7ea7\u7279\u6027\u548c\u4f18\u5316\u6280\u5de7\u3002</p> <ul> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u4f7f\u7528AMP\u52a0\u901f\u8bad\u7ec3\u5e76\u8282\u7701\u663e\u5b58</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u8bad\u7ec3\u6027\u80fd\u548c\u5185\u5b58\u4f7f\u7528</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3\u5b9e\u73b0</li> </ul>"},{"location":"tutorials/index.zh/#_5","title":"\ud83d\udee0\ufe0f \u5b9e\u6218\u9879\u76ee","text":"<p>\u901a\u8fc7\u5b8c\u6574\u7684\u9879\u76ee\u5b66\u4e60Genesis\u7684\u5b9e\u9645\u5e94\u7528\u3002</p> <ul> <li>Qwen\u5927\u6a21\u578b\u8bad\u7ec3 - \u4f7f\u7528Genesis\u8bad\u7ec3Qwen\u8bed\u8a00\u6a21\u578b</li> <li>\u56fe\u50cf\u5206\u7c7b\u9879\u76ee - \u6784\u5efa\u5b8c\u6574\u7684\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf</li> <li>\u8bed\u8a00\u6a21\u578b\u5fae\u8c03 - \u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b9e\u6218</li> </ul>"},{"location":"tutorials/index.zh/#_6","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84","text":""},{"location":"tutorials/index.zh/#1-2","title":"\u521d\u5b66\u8005\u8def\u5f84 (1-2\u5468)","text":"<ol> <li>\u5b89\u88c5\u548c\u73af\u5883\u914d\u7f6e</li> <li>\u7b2c\u4e00\u4e2a\u7a0b\u5e8f </li> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b</li> <li>\u6570\u636e\u5904\u7406\u6559\u7a0b</li> </ol>"},{"location":"tutorials/index.zh/#2-4","title":"\u8fdb\u9636\u7528\u6237\u8def\u5f84 (2-4\u5468)","text":"<ol> <li>\u5b8c\u6210\u521d\u5b66\u8005\u8def\u5f84</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 </li> <li>\u6027\u80fd\u8c03\u4f18</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> </ol>"},{"location":"tutorials/index.zh/#4-8","title":"\u7814\u7a76\u8005\u8def\u5f84 (4-8\u5468)","text":"<ol> <li>\u5b8c\u6210\u8fdb\u9636\u7528\u6237\u8def\u5f84</li> <li>\u67b6\u6784\u6df1\u5165\u7406\u89e3</li> <li>\u6838\u5fc3\u7ec4\u4ef6\u6e90\u7801\u5206\u6790</li> <li>Qwen\u5927\u6a21\u578b\u8bad\u7ec3</li> <li>\u8d21\u732e\u4ee3\u7801</li> </ol>"},{"location":"tutorials/index.zh/#_7","title":"\ud83d\udca1 \u6559\u7a0b\u7279\u8272","text":"<ul> <li>\ud83c\udfaf \u5b9e\u6218\u5bfc\u5411 - \u6bcf\u4e2a\u6559\u7a0b\u90fd\u5305\u542b\u5b8c\u6574\u7684\u53ef\u8fd0\u884c\u4ee3\u7801</li> <li>\ud83d\udcca \u6027\u80fd\u5bf9\u6bd4 - \u4e0e\u5176\u4ed6\u6846\u67b6\u7684\u6027\u80fd\u5bf9\u6bd4\u548c\u5206\u6790</li> <li>\ud83d\udd0d \u6e90\u7801\u89e3\u6790 - \u6df1\u5165\u7406\u89e3Genesis\u5185\u90e8\u5b9e\u73b0\u539f\u7406</li> <li>\u26a1 \u6700\u4f73\u5b9e\u8df5 - \u603b\u7ed3\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u7ecf\u9a8c\u548c\u6280\u5de7</li> </ul>"},{"location":"tutorials/index.zh/#_8","title":"\ud83e\udd1d \u8d21\u732e\u6559\u7a0b","text":"<p>\u6211\u4eec\u6b22\u8fce\u793e\u533a\u8d21\u732e\u66f4\u591a\u9ad8\u8d28\u91cf\u7684\u6559\u7a0b\uff01</p>"},{"location":"tutorials/index.zh/#_9","title":"\u5982\u4f55\u8d21\u732e","text":"<ol> <li>Fork\u9879\u76ee\u5230\u4f60\u7684GitHub\u8d26\u6237</li> <li>\u5728<code>docs/tutorials/</code>\u76ee\u5f55\u4e0b\u521b\u5efa\u65b0\u7684Markdown\u6587\u4ef6</li> <li>\u6309\u7167\u73b0\u6709\u6559\u7a0b\u7684\u683c\u5f0f\u7f16\u5199\u5185\u5bb9</li> <li>\u63d0\u4ea4Pull Request</li> </ol>"},{"location":"tutorials/index.zh/#_10","title":"\u6559\u7a0b\u6807\u51c6","text":"<ul> <li>\u6e05\u6670\u7684\u6807\u9898\u548c\u7ed3\u6784 - \u4f7f\u7528\u9002\u5f53\u7684\u6807\u9898\u5c42\u7ea7</li> <li>\u5b8c\u6574\u7684\u4ee3\u7801\u793a\u4f8b - \u786e\u4fdd\u4ee3\u7801\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c</li> <li>\u8be6\u7ec6\u7684\u89e3\u91ca - \u89e3\u91ca\u6bcf\u4e2a\u6b65\u9aa4\u7684\u539f\u7406\u548c\u76ee\u7684</li> <li>\u5b9e\u9645\u7684\u5e94\u7528\u573a\u666f - \u7ed3\u5408\u771f\u5b9e\u7684\u4f7f\u7528\u6848\u4f8b</li> </ul>"},{"location":"tutorials/index.zh/#_11","title":"\ud83d\udcde \u83b7\u53d6\u5e2e\u52a9","text":"<p>\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\uff1f</p> <ul> <li>GitHub Issues - \u62a5\u544a\u95ee\u9898\u6216\u5efa\u8bae\u6539\u8fdb</li> <li>Discussions - \u4e0e\u793e\u533a\u8ba8\u8bba\u6280\u672f\u95ee\u9898</li> <li>API\u6587\u6863 - \u67e5\u770b\u8be6\u7ec6\u7684API\u53c2\u8003\u6587\u6863</li> </ul> <p>\u5b66\u4e60\u5efa\u8bae</p> <p>\u5efa\u8bae\u6309\u7167\u63a8\u8350\u7684\u5b66\u4e60\u8def\u5f84\u5faa\u5e8f\u6e10\u8fdb\uff0c\u6bcf\u5b8c\u6210\u4e00\u4e2a\u6559\u7a0b\u540e\u5b9e\u9645\u52a8\u624b\u7ec3\u4e60\uff0c\u52a0\u6df1\u7406\u89e3\u3002</p> <p>\u51c6\u5907\u5f00\u59cb\u5b66\u4e60\u4e86\u5417\uff1f</p> <p>\u5f00\u59cb\u57fa\u7840\u6559\u7a0b \u67e5\u770bAPI\u6587\u6863</p>"},{"location":"tutorials/index_en/","title":"Tutorial Overview","text":"<p>Welcome to the Genesis deep learning framework tutorial series! These tutorials will help you master all aspects of the Genesis framework from beginner to expert.</p>"},{"location":"tutorials/index_en/#tutorial-categories","title":"\ud83d\udcda Tutorial Categories","text":""},{"location":"tutorials/index_en/#basic-tutorials","title":"\ud83c\udfaf Basic Tutorials","text":"<p>Suitable for beginners, covering Genesis basic concepts and usage.</p> <ul> <li>Basic Training Tutorial - Learn how to train your first neural network with Genesis</li> <li>Data Processing Tutorial - Data loading, preprocessing, and pipeline construction</li> <li>Model Definition Tutorial - How to define and organize neural network models</li> </ul>"},{"location":"tutorials/index_en/#advanced-tutorials","title":"\ud83d\ude80 Advanced Tutorials","text":"<p>Deep dive into Genesis advanced features and optimization techniques.</p> <ul> <li>Mixed Precision Training - Use AMP to accelerate training and save memory</li> <li>Custom Operators - Implement custom neural network operations</li> <li>Performance Tuning - Optimize training performance and memory usage</li> <li>Distributed Training - Multi-GPU parallel training implementation</li> </ul>"},{"location":"tutorials/index_en/#practical-projects","title":"\ud83d\udee0\ufe0f Practical Projects","text":"<p>Learn Genesis practical applications through complete projects.</p> <ul> <li>Qwen Large Model Training - Use Genesis to train Qwen language models</li> <li>Image Classification Project - Build a complete image classification system</li> <li>Language Model Fine-tuning - Large language model fine-tuning in practice</li> </ul>"},{"location":"tutorials/index_en/#learning-path","title":"\ud83c\udf93 Learning Path","text":""},{"location":"tutorials/index_en/#beginner-path-1-2-weeks","title":"Beginner Path (1-2 weeks)","text":"<ol> <li>Installation and Environment Setup</li> <li>First Program </li> <li>Basic Training Tutorial</li> <li>Data Processing Tutorial</li> </ol>"},{"location":"tutorials/index_en/#advanced-user-path-2-4-weeks","title":"Advanced User Path (2-4 weeks)","text":"<ol> <li>Complete beginner path</li> <li>Custom Operators</li> <li>Mixed Precision Training </li> <li>Performance Tuning</li> <li>Distributed Training</li> </ol>"},{"location":"tutorials/index_en/#researcher-path-4-8-weeks","title":"Researcher Path (4-8 weeks)","text":"<ol> <li>Complete advanced user path</li> <li>Deep Architecture Understanding</li> <li>Core Component Source Analysis</li> <li>Qwen Large Model Training</li> <li>Contributing Code</li> </ol>"},{"location":"tutorials/index_en/#tutorial-features","title":"\ud83d\udca1 Tutorial Features","text":"<ul> <li>\ud83c\udfaf Practical-oriented - Every tutorial includes complete runnable code</li> <li>\ud83d\udcca Performance comparison - Performance comparison and analysis with other frameworks</li> <li>\ud83d\udd0d Source code analysis - Deep understanding of Genesis internal implementation principles</li> <li>\u26a1 Best practices - Summary of experience and techniques from real projects</li> </ul>"},{"location":"tutorials/index_en/#contributing-tutorials","title":"\ud83e\udd1d Contributing Tutorials","text":"<p>We welcome the community to contribute more high-quality tutorials!</p>"},{"location":"tutorials/index_en/#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Fork the project to your GitHub account</li> <li>Create new Markdown files in the <code>docs/tutorials/</code> directory</li> <li>Write content following the format of existing tutorials</li> <li>Submit a Pull Request</li> </ol>"},{"location":"tutorials/index_en/#tutorial-standards","title":"Tutorial Standards","text":"<ul> <li>Clear titles and structure - Use appropriate heading levels</li> <li>Complete code examples - Ensure code can run directly</li> <li>Detailed explanations - Explain the principles and purpose of each step</li> <li>Practical application scenarios - Combine real use cases</li> </ul>"},{"location":"tutorials/index_en/#getting-help","title":"\ud83d\udcde Getting Help","text":"<p>Encountered problems during learning?</p> <ul> <li>GitHub Issues - Report issues or suggest improvements</li> <li>Discussions - Discuss technical issues with the community</li> <li>API Documentation - View detailed API reference documentation</li> </ul> <p>Learning Suggestions</p> <p>It's recommended to follow the recommended learning path progressively, and practice hands-on after completing each tutorial to deepen understanding.</p> <p>Ready to start learning?</p> <p>Start Basic Tutorial View API Documentation</p>"},{"location":"tutorials/llm-training.zh/","title":"\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6307\u5357","text":"<p>\u672c\u5168\u9762\u6307\u5357\u6db5\u76d6\u4e86\u4f7f\u7528Genesis\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5168\u8fc7\u7a0b\uff0c\u4ece\u57fa\u7840\u8bbe\u7f6e\u5230\u9ad8\u7ea7\u4f18\u5316\u6280\u672f\u3002\u6211\u4eec\u5c06\u4ee5Qwen\u6a21\u578b\u4f5c\u4e3a\u4e3b\u8981\u793a\u4f8b\u3002</p>"},{"location":"tutorials/llm-training.zh/#_2","title":"\u6982\u8ff0","text":"<p>Genesis\u4e3a\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6846\u67b6\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u6027\uff1a - Qwen\u6a21\u578b\u67b6\u6784\u5b9e\u73b0 - \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08FP16/BF16\uff09 - \u68af\u5ea6\u88c1\u526a\u548c\u5b66\u4e60\u7387\u8c03\u5ea6 - \u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301 - \u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u7ba1\u7406</p>"},{"location":"tutorials/llm-training.zh/#_3","title":"\u524d\u7f6e\u8981\u6c42","text":"<ul> <li>\u81f3\u5c1116GB\u663e\u5b58\u7684GPU\uff08\u63a8\u8350A100/A800\uff09</li> <li>CUDA 11.8+\u548c\u76f8\u5e94\u7684\u9a71\u52a8\u7a0b\u5e8f</li> <li>Python 3.8+\u5e76\u5df2\u5b89\u88c5Genesis</li> </ul>"},{"location":"tutorials/llm-training.zh/#_4","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"tutorials/llm-training.zh/#1-qwen","title":"1. \u57fa\u7840Qwen\u6a21\u578b\u8bbe\u7f6e","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nfrom genesis.models.qwen import QwenConfig, QwenModel\n\n# \u914d\u7f6e\u6a21\u578b\nconfig = QwenConfig(\n    vocab_size=32000,\n    hidden_size=2048,\n    num_attention_heads=16,\n    num_hidden_layers=24,\n    intermediate_size=5632,\n    max_position_embeddings=2048,\n    dtype=genesis.float16  # \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\n)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = QwenModel(config)\nprint(f\"\u6a21\u578b\u53c2\u6570\u91cf: {model.num_parameters() / 1e6:.1f}M\")\n</code></pre>"},{"location":"tutorials/llm-training.zh/#2","title":"2. \u6570\u636e\u51c6\u5907","text":"Python<pre><code>import genesis\nfrom torch.utils.data import DataLoader\n\nclass TextDataset:\n    def __init__(self, texts, tokenizer, max_length=512):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        # \u6807\u8bb0\u5316\u5e76\u586b\u5145/\u622a\u65ad\n        tokens = self.tokenizer.encode(text, max_length=self.max_length)\n\n        # \u8f6c\u6362\u4e3aGenesis\u5f20\u91cf\n        input_ids = genesis.tensor(tokens[:-1], dtype=genesis.int64)\n        labels = genesis.tensor(tokens[1:], dtype=genesis.int64)\n\n        return {\n            'input_ids': input_ids,\n            'labels': labels\n        }\n\n# \u52a0\u8f7d\u4f60\u7684\u6570\u636e\u96c6\ndataset = TextDataset(train_texts, tokenizer)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n</code></pre>"},{"location":"tutorials/llm-training.zh/#3","title":"3. \u8bad\u7ec3\u8bbe\u7f6e","text":"Python<pre><code>import genesis.optim as optim\nimport genesis.nn as nn\n\n# \u5c06\u6a21\u578b\u79fb\u52a8\u5230GPU\ndevice = genesis.cuda()\nmodel = model.to(device)\n\n# \u8bbe\u7f6e\u4f18\u5316\u5668\u548c\u6743\u91cd\u8870\u51cf\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=5e-4,\n    weight_decay=0.1,\n    beta1=0.9,\n    beta2=0.95,\n    eps=1e-8\n)\n\n# \u5e26\u9884\u70ed\u7684\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\ntotal_steps = len(dataloader) * num_epochs\nwarmup_steps = total_steps // 10\n\nscheduler = optim.get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# \u635f\u5931\u51fd\u6570\ncriterion = nn.CrossEntropyLoss()\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_5","title":"\u8bad\u7ec3\u5faa\u73af\u5b9e\u73b0","text":""},{"location":"tutorials/llm-training.zh/#_6","title":"\u57fa\u7840\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    for batch_idx, batch in enumerate(dataloader):\n        # \u5c06\u6279\u6b21\u79fb\u52a8\u5230\u8bbe\u5907\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(input_ids)\n        logits = outputs.logits\n\n        # \u8ba1\u7b97\u635f\u5931\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        loss = criterion(\n            shift_logits.view(-1, shift_logits.size(-1)),\n            shift_labels.view(-1)\n        )\n\n        # \u53cd\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u4f18\u5316\u5668\u6b65\u9aa4\n        optimizer.step()\n        scheduler.step()\n\n        # \u7d2f\u79ef\u635f\u5931\n        total_loss += loss.item()\n        num_batches += 1\n\n        # \u65e5\u5fd7\u8bb0\u5f55\n        if batch_idx % 100 == 0:\n            current_lr = scheduler.get_last_lr()\n            print(f'\u6279\u6b21 {batch_idx}: loss={loss.item():.4f}, lr={current_lr:.2e}')\n\n    return total_loss / num_batches\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    avg_loss = train_epoch(model, dataloader, optimizer, scheduler, criterion, device)\n    print(f'\u8f6e\u6b21 {epoch}: \u5e73\u5747\u635f\u5931 = {avg_loss:.4f}')\n\n    # \u4fdd\u5b58\u68c0\u67e5\u70b9\n    if epoch % 10 == 0:\n        genesis.save_checkpoint(\n            model.state_dict(),\n            optimizer.state_dict(),\n            f'qwen_checkpoint_epoch_{epoch}.pth'\n        )\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_7","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code>import genesis\n\ndef train_epoch_mixed_precision(model, dataloader, optimizer, scheduler, criterion, device):\n    model.train()\n    total_loss = 0.0\n\n    # \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\n    genesis.enable_autocast = True\n\n    for batch_idx, batch in enumerate(dataloader):\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        # \u4f7f\u7528autocast\u8fdb\u884c\u524d\u5411\u4f20\u64ad\n        with genesis.autocast():\n            outputs = model(input_ids)\n            logits = outputs.logits\n\n            # \u8ba1\u7b97\u635f\u5931\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss = criterion(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n        # \u53cd\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u4f18\u5316\u5668\u6b65\u9aa4\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'\u6279\u6b21 {batch_idx}: loss={loss.item():.4f}')\n\n    return total_loss / len(dataloader)\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_8","title":"\u9ad8\u7ea7\u8bad\u7ec3\u6280\u672f","text":""},{"location":"tutorials/llm-training.zh/#1","title":"1. \u68af\u5ea6\u7d2f\u79ef","text":"Python<pre><code>def train_with_gradient_accumulation(model, dataloader, optimizer, scheduler, \n                                   criterion, device, accumulation_steps=4):\n    model.train()\n    total_loss = 0.0\n    optimizer.zero_grad()\n\n    for batch_idx, batch in enumerate(dataloader):\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(input_ids)\n        logits = outputs.logits\n\n        # \u8ba1\u7b97\u635f\u5931\u5e76\u6309\u7d2f\u79ef\u6b65\u6570\u7f29\u653e\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        loss = criterion(\n            shift_logits.view(-1, shift_logits.size(-1)),\n            shift_labels.view(-1)\n        ) / accumulation_steps\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n\n        # \u6bcfaccumulation_steps\u66f4\u65b0\u4e00\u6b21\n        if (batch_idx + 1) % accumulation_steps == 0:\n            # \u68af\u5ea6\u88c1\u526a\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # \u4f18\u5316\u5668\u6b65\u9aa4\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        total_loss += loss.item() * accumulation_steps\n\n        if batch_idx % (100 * accumulation_steps) == 0:\n            print(f'\u6279\u6b21 {batch_idx}: loss={loss.item() * accumulation_steps:.4f}')\n\n    return total_loss / len(dataloader)\n</code></pre>"},{"location":"tutorials/llm-training.zh/#2_1","title":"2. \u52a8\u6001\u635f\u5931\u7f29\u653e","text":"Python<pre><code>class DynamicLossScaler:\n    def __init__(self, init_scale=2**16, scale_factor=2.0, scale_window=1000):\n        self.scale = init_scale\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n        self._growth_tracker = 0\n\n    def scale_loss(self, loss):\n        return loss * self.scale\n\n    def unscale_gradients(self, optimizer):\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    param.grad.data /= self.scale\n\n    def step(self, optimizer, has_overflow=False):\n        if has_overflow:\n            self.scale = max(self.scale / self.scale_factor, 1.0)\n            self._growth_tracker = 0\n        else:\n            self._growth_tracker += 1\n            if self._growth_tracker &gt;= self.scale_window:\n                self.scale *= self.scale_factor\n                self._growth_tracker = 0\n\n        return not has_overflow\n\n# \u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u4f7f\u7528\nscaler = DynamicLossScaler()\n\nfor batch in dataloader:\n    # \u524d\u5411\u4f20\u64ad\n    loss = compute_loss(model, batch)\n    scaled_loss = scaler.scale_loss(loss)\n\n    # \u53cd\u5411\u4f20\u64ad\n    optimizer.zero_grad()\n    scaled_loss.backward()\n\n    # \u68c0\u67e5\u6ea2\u51fa\n    has_overflow = check_gradient_overflow(model.parameters())\n\n    # \u53cd\u7f29\u653e\u548c\u6b65\u8fdb\n    scaler.unscale_gradients(optimizer)\n    should_step = scaler.step(optimizer, has_overflow)\n\n    if should_step:\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n</code></pre>"},{"location":"tutorials/llm-training.zh/#3_1","title":"3. \u68c0\u67e5\u70b9\u4fdd\u5b58\u548c\u6062\u590d\u8bad\u7ec3","text":"Python<pre><code>class TrainingManager:\n    def __init__(self, model, optimizer, scheduler, save_dir='checkpoints'):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.save_dir = save_dir\n        self.current_epoch = 0\n        self.best_loss = float('inf')\n\n    def save_checkpoint(self, epoch, loss, metrics=None):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'loss': loss,\n            'best_loss': self.best_loss,\n            'metrics': metrics or {}\n        }\n\n        # \u4fdd\u5b58\u5e38\u89c4\u68c0\u67e5\u70b9\n        checkpoint_path = f'{self.save_dir}/checkpoint_epoch_{epoch}.pth'\n        genesis.save(checkpoint, checkpoint_path)\n\n        # \u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n        if loss &lt; self.best_loss:\n            self.best_loss = loss\n            best_path = f'{self.save_dir}/best_model.pth'\n            genesis.save(checkpoint, best_path)\n            print(f\"\u4fdd\u5b58\u65b0\u7684\u6700\u4f73\u6a21\u578b\uff0c\u635f\u5931: {loss:.4f}\")\n\n        # \u4fdd\u5b58\u6700\u65b0\u68c0\u67e5\u70b9\n        latest_path = f'{self.save_dir}/latest_checkpoint.pth'\n        genesis.save(checkpoint, latest_path)\n\n    def load_checkpoint(self, checkpoint_path):\n        checkpoint = genesis.load(checkpoint_path)\n\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n        self.current_epoch = checkpoint['epoch']\n        self.best_loss = checkpoint['best_loss']\n\n        print(f\"\u4ece\u8f6e\u6b21 {self.current_epoch} \u6062\u590d\uff0c\u6700\u4f73\u635f\u5931: {self.best_loss:.4f}\")\n        return checkpoint\n\n# \u4f7f\u7528\u65b9\u6cd5\ntraining_manager = TrainingManager(model, optimizer, scheduler)\n\n# \u5982\u679c\u5b58\u5728\u68c0\u67e5\u70b9\u5219\u6062\u590d\ntry:\n    training_manager.load_checkpoint('checkpoints/latest_checkpoint.pth')\nexcept FileNotFoundError:\n    print(\"\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\")\n\n# \u5e26\u68c0\u67e5\u70b9\u4fdd\u5b58\u7684\u8bad\u7ec3\u5faa\u73af\nfor epoch in range(training_manager.current_epoch, num_epochs):\n    avg_loss = train_epoch(model, dataloader, optimizer, scheduler, criterion, device)\n\n    # \u4fdd\u5b58\u68c0\u67e5\u70b9\n    training_manager.save_checkpoint(epoch, avg_loss)\n\n    print(f'\u8f6e\u6b21 {epoch}: \u5e73\u5747\u635f\u5931 = {avg_loss:.4f}')\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_9","title":"\u6a21\u578b\u8bc4\u4f30\u548c\u63a8\u7406","text":""},{"location":"tutorials/llm-training.zh/#1_1","title":"1. \u6a21\u578b\u8bc4\u4f30","text":"Python<pre><code>def evaluate_model(model, eval_dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n\n    with genesis.no_grad():\n        for batch in eval_dataloader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            # \u524d\u5411\u4f20\u64ad\n            outputs = model(input_ids)\n            logits = outputs.logits\n\n            # \u8ba1\u7b97\u635f\u5931\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            loss = criterion(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1)\n            )\n\n            total_loss += loss.item() * shift_labels.numel()\n            total_tokens += shift_labels.numel()\n\n    avg_loss = total_loss / total_tokens\n    perplexity = genesis.exp(avg_loss)\n\n    return {\n        'loss': avg_loss,\n        'perplexity': perplexity.item()\n    }\n\n# \u8bc4\u4f30\u6a21\u578b\neval_metrics = evaluate_model(model, eval_dataloader, criterion, device)\nprint(f\"\u8bc4\u4f30\u635f\u5931: {eval_metrics['loss']:.4f}, \u56f0\u60d1\u5ea6: {eval_metrics['perplexity']:.2f}\")\n</code></pre>"},{"location":"tutorials/llm-training.zh/#2_2","title":"2. \u6587\u672c\u751f\u6210","text":"Python<pre><code>def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_p=0.9):\n    model.eval()\n    device = next(model.parameters()).device\n\n    # \u6807\u8bb0\u5316\u63d0\u793a\u8bcd\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = genesis.tensor([input_ids], dtype=genesis.int64).to(device)\n\n    generated_ids = input_ids.copy()\n\n    with genesis.no_grad():\n        for _ in range(max_length):\n            # \u524d\u5411\u4f20\u64ad\n            outputs = model(input_tensor)\n            logits = outputs.logits[0, -1, :]  # \u6700\u540e\u4e00\u4e2atoken\u7684logits\n\n            # \u5e94\u7528\u6e29\u5ea6\n            logits = logits / temperature\n\n            # \u5e94\u7528top-p\u8fc7\u6ee4\n            sorted_logits, sorted_indices = genesis.sort(logits, descending=True)\n            cumulative_probs = genesis.cumsum(genesis.softmax(sorted_logits, dim=-1), dim=-1)\n\n            # \u79fb\u9664\u7d2f\u79ef\u6982\u7387\u8d85\u8fc7\u9608\u503c\u7684tokens\n            sorted_indices_to_remove = cumulative_probs &gt; top_p\n            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n            sorted_indices_to_remove[0] = False\n\n            indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n            logits[indices_to_remove] = float('-inf')\n\n            # \u4ece\u8fc7\u6ee4\u540e\u7684\u5206\u5e03\u4e2d\u91c7\u6837\n            probs = genesis.softmax(logits, dim=-1)\n            next_token = genesis.multinomial(probs, 1).item()\n\n            # \u6dfb\u52a0\u5230\u751f\u6210\u5e8f\u5217\n            generated_ids.append(next_token)\n\n            # \u66f4\u65b0\u8f93\u5165\u5f20\u91cf\n            input_tensor = genesis.tensor([generated_ids], dtype=genesis.int64).to(device)\n\n            # \u68c0\u67e5\u7ed3\u675ftoken\n            if next_token == tokenizer.eos_token_id:\n                break\n\n    # \u89e3\u7801\u751f\u6210\u7684\u6587\u672c\n    generated_text = tokenizer.decode(generated_ids)\n    return generated_text\n\n# \u751f\u6210\u6587\u672c\nprompt = \"\u4eba\u5de5\u667a\u80fd\u7684\u672a\u6765\u662f\"\ngenerated = generate_text(model, tokenizer, prompt, max_length=50)\nprint(f\"\u751f\u6210\u7ed3\u679c: {generated}\")\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_10","title":"\u751f\u4ea7\u90e8\u7f72","text":""},{"location":"tutorials/llm-training.zh/#1_2","title":"1. \u63a8\u7406\u4f18\u5316","text":"Python<pre><code>def optimize_for_inference(model, save_path):\n    \"\"\"\u4e3a\u751f\u4ea7\u63a8\u7406\u4f18\u5316\u6a21\u578b\u3002\"\"\"\n    model.eval()\n\n    # \u521b\u5efa\u63a8\u7406\u4f18\u5316\u72b6\u6001\n    inference_state = {\n        'model_state_dict': model.state_dict(),\n        'model_config': model.config.__dict__,\n        'inference_optimized': True,\n        'genesis_version': genesis.__version__\n    }\n\n    genesis.save(inference_state, save_path)\n    print(f\"\u63a8\u7406\u4f18\u5316\u6a21\u578b\u5df2\u4fdd\u5b58\u5230 {save_path}\")\n\ndef load_for_inference(model_path, device=None):\n    \"\"\"\u52a0\u8f7d\u63a8\u7406\u4f18\u5316\u7684\u6a21\u578b\u3002\"\"\"\n    checkpoint = genesis.load(model_path)\n    config = QwenConfig(**checkpoint['model_config'])\n\n    # \u521b\u5efa\u6a21\u578b\n    model = QwenModel(config)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    if device:\n        model = model.to(device)\n\n    return model\n\n# \u4f18\u5316\u5e76\u4fdd\u5b58\noptimize_for_inference(model, 'qwen_inference.pth')\n\n# \u52a0\u8f7d\u7528\u4e8e\u63a8\u7406\ninference_model = load_for_inference('qwen_inference.pth', device=genesis.cuda())\n</code></pre>"},{"location":"tutorials/llm-training.zh/#2_3","title":"2. \u63a8\u7406\u670d\u52a1\u5668\u8bbe\u7f6e","text":"Python<pre><code>class LLMInferenceServer:\n    def __init__(self, model_path, tokenizer, device=None):\n        self.tokenizer = tokenizer\n        self.device = device or genesis.cuda()\n        self.model = load_for_inference(model_path, self.device)\n\n    def generate(self, prompt, max_length=100, temperature=0.8, top_p=0.9):\n        \"\"\"\u4ece\u63d0\u793a\u8bcd\u751f\u6210\u6587\u672c\u3002\"\"\"\n        return generate_text(\n            self.model, self.tokenizer, prompt,\n            max_length=max_length, temperature=temperature, top_p=top_p\n        )\n\n    def batch_generate(self, prompts, max_length=100, temperature=0.8, top_p=0.9):\n        \"\"\"\u4e3a\u591a\u4e2a\u63d0\u793a\u8bcd\u751f\u6210\u6587\u672c\u3002\"\"\"\n        results = []\n        for prompt in prompts:\n            result = self.generate(prompt, max_length, temperature, top_p)\n            results.append(result)\n        return results\n\n# \u521b\u5efa\u63a8\u7406\u670d\u52a1\u5668\nserver = LLMInferenceServer('qwen_inference.pth', tokenizer)\n\n# \u751f\u6210\u54cd\u5e94\nresponses = server.batch_generate([\n    \"\u751f\u547d\u7684\u610f\u4e49\u662f\u4ec0\u4e48\uff1f\",\n    \"\u7528\u7b80\u5355\u7684\u8bed\u8a00\u89e3\u91ca\u91cf\u5b50\u8ba1\u7b97\u3002\",\n    \"\u5199\u4e00\u4e2a\u5173\u4e8eAI\u7684\u77ed\u6545\u4e8b\u3002\"\n])\n</code></pre>"},{"location":"tutorials/llm-training.zh/#_11","title":"\u6027\u80fd\u4f18\u5316\u6280\u5de7","text":""},{"location":"tutorials/llm-training.zh/#1_3","title":"1. \u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u5bf9\u5927\u6a21\u578b\u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9</li> <li>\u542f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08FP16/BF16\uff09</li> <li>\u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef\u5b9e\u73b0\u5927\u7684\u6709\u6548\u6279\u91cf\u5927\u5c0f</li> <li>\u5b9a\u671f\u6e05\u7406GPU\u7f13\u5b58</li> </ul>"},{"location":"tutorials/llm-training.zh/#2_4","title":"2. \u8bad\u7ec3\u901f\u5ea6","text":"<ul> <li>\u4e3a\u4f60\u7684GPU\u4f7f\u7528\u9002\u5f53\u7684\u6279\u91cf\u5927\u5c0f</li> <li>\u5982\u679c\u53ef\u7528\uff0c\u542f\u7528\u7f16\u8bd1\u6a21\u5f0f</li> <li>\u4f7f\u7528\u591a\u8fdb\u7a0b\u8fdb\u884c\u9ad8\u6548\u7684\u6570\u636e\u52a0\u8f7d</li> <li>\u5bf9\u8bad\u7ec3\u8fdb\u884c\u6027\u80fd\u5206\u6790\u4ee5\u8bc6\u522b\u74f6\u9888</li> </ul>"},{"location":"tutorials/llm-training.zh/#3_2","title":"3. \u6a21\u578b\u8d28\u91cf","text":"<ul> <li>\u4f7f\u7528\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6</li> <li>\u5e94\u7528\u68af\u5ea6\u88c1\u526a\u6765\u7a33\u5b9a\u8bad\u7ec3</li> <li>\u5bc6\u5207\u76d1\u63a7\u8bad\u7ec3\u6307\u6807</li> <li>\u4f7f\u7528\u9a8c\u8bc1\u96c6\u9632\u6b62\u8fc7\u62df\u5408</li> </ul> <p>\u672c\u6307\u5357\u4e3a\u4f7f\u7528Genesis\u8bad\u7ec3LLM\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u7840\u3002\u8bf7\u6839\u636e\u4f60\u7684\u5177\u4f53\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u8d44\u6e90\u6765\u8c03\u6574\u8fd9\u4e9b\u6280\u672f\u3002</p>"},{"location":"tutorials/llm-training_en/","title":"Large Language Model Training Guide","text":"<p>This comprehensive guide covers training large language models (LLMs) using Genesis's Qwen implementation, from basic setup to advanced optimization techniques.</p>"},{"location":"tutorials/llm-training_en/#overview","title":"Overview","text":"<p>Genesis provides LLM training capabilities through two approaches: 1. Pure Genesis Implementation: Native Qwen model in <code>genesis.models.qwen</code> 2. Hybrid Approach: Integration with PyTorch/Transformers for production training</p> <p>This guide covers both approaches, starting with the pure Genesis implementation and then showing the production-ready hybrid approach used in <code>apps/llm/</code>.</p>"},{"location":"tutorials/llm-training_en/#prerequisites","title":"Prerequisites","text":"<ul> <li>GPU with at least 16GB VRAM (A100/A800 recommended for large models)</li> <li>CUDA 11.8+ and appropriate drivers</li> <li>Python 3.8+ with Genesis installed</li> <li>For hybrid approach: PyTorch and Transformers library</li> </ul> Bash<pre><code>pip install torch transformers datasets accelerate\n</code></pre>"},{"location":"tutorials/llm-training_en/#approach-1-pure-genesis-implementation","title":"Approach 1: Pure Genesis Implementation","text":""},{"location":"tutorials/llm-training_en/#1-model-configuration-and-setup","title":"1. Model Configuration and Setup","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nfrom genesis.models.qwen import ModelArgs, Transformer\nimport numpy as np\n\n# Configure Qwen model for educational purposes\nconfig = ModelArgs(\n    block_size=2048,          # Context length\n    vocab_size=32000,         # Vocabulary size\n    n_layer=12,               # Number of transformer layers\n    num_attention_heads=12,   # Number of attention heads\n    hidden_size=768,          # Hidden dimension\n    intermediate_size=3072,   # Feed-forward dimension\n    num_key_value_heads=12,   # Key-value heads (for GQA)\n    head_dim=64,              # Attention head dimension\n    rope_base=10000,          # RoPE base frequency\n    max_position_embeddings=2048\n)\n\nprint(f\"Model configuration:\")\nprint(f\"  Layers: {config.n_layer}\")\nprint(f\"  Hidden size: {config.hidden_size}\")\nprint(f\"  Attention heads: {config.num_attention_heads}\")\nprint(f\"  Vocabulary size: {config.vocab_size}\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#2-model-instantiation","title":"2. Model Instantiation","text":"Python<pre><code># Create Qwen model using Genesis\nmodel = Transformer(config)\n\n# Calculate model parameters\ntotal_params = sum(p.data.size for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.1f}M\")\n\n# Initialize weights\ndef init_weights(module):\n    \"\"\"Initialize model weights\"\"\"\n    if isinstance(module, nn.Linear):\n        # Initialize weights with small random values\n        module.weight.data = genesis.randn(*module.weight.shape) * 0.02\n        if module.bias is not None:\n            module.bias.data = genesis.zeros(*module.bias.shape)\n\n# Apply weight initialization\nmodel.apply(init_weights)\nprint(\"Model weights initialized\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#3-simple-data-preparation","title":"3. Simple Data Preparation","text":"Python<pre><code># Simple synthetic data for demonstration\nclass SimpleTextDataset:\n    \"\"\"Simple dataset for language modeling\"\"\"\n\n    def __init__(self, vocab_size=32000, seq_length=512, num_samples=1000):\n        self.vocab_size = vocab_size\n        self.seq_length = seq_length\n        self.num_samples = num_samples\n\n        # Generate random token sequences\n        self.data = genesis.tensor(\n            np.random.randint(0, vocab_size, (num_samples, seq_length))\n        )\n\n    def __len__(self):\n        return self.num_samples\n\n    def get_batch(self, batch_size=4, start_idx=0):\n        \"\"\"Get a batch of sequences\"\"\"\n        end_idx = min(start_idx + batch_size, self.num_samples)\n        batch_data = self.data[start_idx:end_idx]\n\n        # For language modeling: input = tokens[:-1], target = tokens[1:]\n        input_ids = batch_data[:, :-1]\n        labels = batch_data[:, 1:]\n\n        return input_ids, labels\n\n# Create dataset\ndataset = SimpleTextDataset(vocab_size=config.vocab_size, seq_length=512, num_samples=100)\nprint(f\"Dataset created with {len(dataset)} samples\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#4-training-setup","title":"4. Training Setup","text":"Python<pre><code>import genesis.optim as optim\n\n# Set up optimizer and loss\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.SoftmaxLoss()\n\nprint(f\"Optimizer: {type(optimizer).__name__}\")\nprint(f\"Learning rate: 1e-4\")\nprint(f\"Loss function: {type(criterion).__name__}\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#5-training-loop","title":"5. Training Loop","text":"Python<pre><code>def train_step(model, input_ids, labels, criterion, optimizer):\n    \"\"\"Single training step\"\"\"\n    # Forward pass\n    logits = model(input_ids)\n\n    # Reshape for loss calculation\n    # logits: [batch_size, seq_len, vocab_size]\n    # labels: [batch_size, seq_len]\n    batch_size, seq_len, vocab_size = logits.shape\n    logits_flat = logits.view(batch_size * seq_len, vocab_size)\n    labels_flat = labels.view(batch_size * seq_len)\n\n    # Calculate loss\n    loss = criterion(logits_flat, labels_flat)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n\n    # Gradient clipping\n    total_norm = 0.0\n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm.data ** 2\n    total_norm = total_norm ** 0.5\n\n    # Clip gradients\n    clip_coef = min(1.0, 1.0 / max(total_norm, 1e-6))\n    for p in model.parameters():\n        if p.grad is not None:\n            p.grad.data = p.grad.data * clip_coef\n\n    # Update weights\n    optimizer.step()\n\n    return loss.data.item() if hasattr(loss.data, 'item') else float(loss.data)\n\n# Training configuration\nnum_epochs = 5\nbatch_size = 2  # Small batch size for demo\nlog_interval = 10\n\nprint(\"Starting Genesis Qwen training...\")\nprint(f\"Epochs: {num_epochs}, Batch size: {batch_size}\")\nprint(\"-\" * 50)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    num_batches = len(dataset) // batch_size\n\n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * batch_size\n        input_ids, labels = dataset.get_batch(batch_size, start_idx)\n\n        # Training step\n        loss = train_step(model, input_ids, labels, criterion, optimizer)\n        total_loss += loss\n\n        if batch_idx % log_interval == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{num_batches}, Loss: {loss:.4f}\")\n\n    avg_loss = total_loss / num_batches\n    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n    print(\"-\" * 30)\n\nprint(\"Pure Genesis training completed!\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#approach-2-production-hybrid-training","title":"Approach 2: Production Hybrid Training","text":"<p>For production use, Genesis integrates with PyTorch and Transformers library. This is the approach used in <code>apps/llm/train_sft_qwen.py</code>.</p>"},{"location":"tutorials/llm-training_en/#1-setup-and-dependencies","title":"1. Setup and Dependencies","text":"Python<pre><code>import torch\nimport torch.nn as nn\nfrom transformers import (\n    AutoConfig, AutoModelForCausalLM, AutoTokenizer,\n    Trainer, TrainingArguments\n)\nfrom torch.utils.data import Dataset, DataLoader\nimport json\nimport pandas as pd\nimport os\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\nprint(f\"Vocabulary size: {len(tokenizer)}\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#2-data-preparation-for-sft","title":"2. Data Preparation for SFT","text":"Python<pre><code>class ConversationDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning with conversation format\"\"\"\n\n    def __init__(self, data_path, tokenizer, max_length=2048):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.conversations = self.load_conversations(data_path)\n\n    def load_conversations(self, data_path):\n        \"\"\"Load conversation data from JSON lines\"\"\"\n        conversations = []\n\n        # Example conversation format\n        sample_conversations = [\n            {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n                    {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}\n                ]\n            },\n            {\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Explain neural networks\"},\n                    {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks...\"}\n                ]\n            }\n        ]\n\n        return sample_conversations\n\n    def format_conversation(self, messages):\n        \"\"\"Format conversation into training text\"\"\"\n        text = \"\"\n        for i, message in enumerate(messages):\n            role = message[\"role\"]\n            content = message[\"content\"].strip()\n\n            if i != len(messages) - 1:\n                text += f\"&lt;|im_start|&gt;{role}\\n{content}&lt;|im_end|&gt;\\n\"\n            else:\n                # Last message (assistant response)\n                text += f\"&lt;|im_start|&gt;{role}\\n{content}&lt;|im_end|&gt;\"\n\n        return text\n\n    def __len__(self):\n        return len(self.conversations)\n\n    def __getitem__(self, idx):\n        conversation = self.conversations[idx]\n        text = self.format_conversation(conversation[\"messages\"])\n\n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n\n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n\n        # For language modeling, labels are the same as input_ids\n        labels = input_ids.clone()\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n\n# Create dataset\ndataset = ConversationDataset(\"./data\", tokenizer)\nprint(f\"Dataset created with {len(dataset)} conversations\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#3-model-setup-with-transformers","title":"3. Model Setup with Transformers","text":"Python<pre><code># Load pre-trained Qwen model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2.5-0.5B\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(f\"Model loaded: {model.config.model_type}\")\nprint(f\"Parameters: {model.num_parameters() / 1e6:.1f}M\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#4-training-configuration","title":"4. Training Configuration","text":"Python<pre><code># Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./qwen_sft_output\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=100,\n    max_steps=1000,\n    learning_rate=5e-5,\n    fp16=True,  # Mixed precision training\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    evaluation_strategy=\"steps\",\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=None,  # Disable wandb logging\n    gradient_checkpointing=True,\n    dataloader_drop_last=True,\n    remove_unused_columns=False,\n)\n\nprint(\"Training configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Mixed precision: {training_args.fp16}\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#5-trainer-setup-and-training","title":"5. Trainer Setup and Training","text":"Python<pre><code># Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset,\n    eval_dataset=dataset,  # Using same dataset for demo\n    tokenizer=tokenizer,\n)\n\nprint(\"Trainer created successfully\")\n\n# Start training\nprint(\"Starting supervised fine-tuning...\")\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./qwen_sft_final\")\ntokenizer.save_pretrained(\"./qwen_sft_final\")\n\nprint(\"Training completed and model saved!\")\n</code></pre>"},{"location":"tutorials/llm-training_en/#6-inference-with-trained-model","title":"6. Inference with Trained Model","text":"Python<pre><code># Load trained model for inference\nfrom transformers import pipeline\n\n# Create text generation pipeline\ngenerator = pipeline(\n    \"text-generation\",\n    model=\"./qwen_sft_final\",\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Test the model\ntest_prompt = \"&lt;|im_start|&gt;user\\nWhat is artificial intelligence?&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n\nresponse = generator(\n    test_prompt,\n    max_new_tokens=100,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(\"Generated response:\")\nprint(response[0][\"generated_text\"])\n</code></pre>"},{"location":"tutorials/llm-training_en/#advanced-features","title":"Advanced Features","text":""},{"location":"tutorials/llm-training_en/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Genesis supports mixed precision training for both approaches:</p> Python<pre><code># For pure Genesis (in development)\ngenesis.enable_autocast = True\n\n# For hybrid approach (using PyTorch)\nfrom torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nwith autocast():\n    outputs = model(inputs)\n    loss = criterion(outputs, targets)\n\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"tutorials/llm-training_en/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"Python<pre><code># Genesis optimizer with learning rate scheduling\nfrom genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=100,\n    num_training_steps=1000\n)\n\n# Update learning rate after each step\nscheduler.step()\n</code></pre>"},{"location":"tutorials/llm-training_en/#model-checkpointing","title":"Model Checkpointing","text":"Python<pre><code># Save Genesis model\ngenesis.save_checkpoint({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'epoch': epoch,\n    'loss': loss,\n}, 'checkpoint.pkl')\n\n# Load Genesis model\ncheckpoint = genesis.load_checkpoint('checkpoint.pkl')\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n</code></pre>"},{"location":"tutorials/llm-training_en/#performance-tips","title":"Performance Tips","text":"<ol> <li>Batch Size: Start with smaller batch sizes (2-4) for Genesis implementation</li> <li>Gradient Accumulation: Use gradient accumulation for effective larger batch sizes</li> <li>Mixed Precision: Enable FP16 to reduce memory usage and increase speed</li> <li>Gradient Clipping: Prevent gradient explosion in transformer training</li> <li>Learning Rate: Use warmup and cosine decay scheduling</li> </ol>"},{"location":"tutorials/llm-training_en/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorials/llm-training_en/#common-issues","title":"Common Issues","text":"<ol> <li>Out of Memory: Reduce batch size or sequence length</li> <li>Gradient Explosion: Enable gradient clipping</li> <li>Slow Convergence: Check learning rate and warmup schedule</li> <li>NaN Loss: Reduce learning rate or check data quality</li> </ol>"},{"location":"tutorials/llm-training_en/#memory-optimization","title":"Memory Optimization","text":"Python<pre><code># Reduce memory usage\n- Use smaller batch sizes\n- Enable gradient checkpointing\n- Use mixed precision (FP16)\n- Reduce sequence length\n- Use gradient accumulation instead of large batches\n</code></pre>"},{"location":"tutorials/llm-training_en/#next-steps","title":"Next Steps","text":"<ol> <li>Scale up: Try larger models and datasets</li> <li>Fine-tuning: Experiment with different fine-tuning strategies</li> <li>Evaluation: Implement proper evaluation metrics</li> <li>Deployment: Set up inference pipelines</li> <li>Optimization: Profile and optimize training performance</li> </ol> <p>This guide demonstrates both the educational pure Genesis approach and the production-ready hybrid approach used in real applications.</p>"},{"location":"tutorials/mixed-precision.zh/","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6307\u5357","text":"<p>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u662f\u4e00\u79cd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u540c\u65f6\u4f7f\u752816\u4f4d\uff08\u534a\u7cbe\u5ea6\uff09\u548c32\u4f4d\uff08\u5355\u7cbe\u5ea6\uff09\u6d6e\u70b9\u6570\u7684\u6280\u672f\uff0c\u7528\u4e8e\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u52a0\u901f\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002Genesis\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u652f\u6301\u548c\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09\u529f\u80fd\u3002</p>"},{"location":"tutorials/mixed-precision.zh/#_2","title":"\u6982\u8ff0","text":""},{"location":"tutorials/mixed-precision.zh/#_3","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u4f18\u52bf","text":"<ul> <li>\u5185\u5b58\u6548\u7387\uff1a\u51cf\u5c11\u7ea650%\u7684\u5185\u5b58\u4f7f\u7528</li> <li>\u901f\u5ea6\u63d0\u5347\uff1a\u5728\u5e26\u6709Tensor Cores\u7684\u73b0\u4ee3GPU\u4e0a\u8bad\u7ec3\u66f4\u5feb</li> <li>\u6a21\u578b\u7cbe\u5ea6\uff1a\u901a\u8fc7\u81ea\u52a8\u635f\u5931\u7f29\u653e\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027</li> <li>\u66f4\u5927\u6a21\u578b\uff1a\u5728\u540c\u6837\u786c\u4ef6\u4e0a\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b</li> </ul>"},{"location":"tutorials/mixed-precision.zh/#_4","title":"\u652f\u6301\u7684\u7cbe\u5ea6\u7c7b\u578b","text":"<p>Genesis\u652f\u6301\u591a\u79cd\u7cbe\u5ea6\u683c\u5f0f\uff1a</p> <ul> <li>float32 (FP32)\uff1a\u6807\u51c6\u5355\u7cbe\u5ea6\uff08\u9ed8\u8ba4\uff09</li> <li>float16 (FP16)\uff1aIEEE\u534a\u7cbe\u5ea6</li> <li>bfloat16 (BF16)\uff1a\u5177\u6709\u66f4\u5927\u52a8\u6001\u8303\u56f4\u7684Brain Float\u683c\u5f0f</li> </ul>"},{"location":"tutorials/mixed-precision.zh/#_5","title":"\u6570\u636e\u7c7b\u578b\u7cfb\u7edf","text":""},{"location":"tutorials/mixed-precision.zh/#genesis","title":"\u7406\u89e3Genesis\u6570\u636e\u7c7b\u578b","text":"Python<pre><code>import genesis\n\n# \u53ef\u7528\u7684\u7cbe\u5ea6\u7c7b\u578b\nprint(\"\u53ef\u7528\u7684\u6570\u636e\u7c7b\u578b\uff1a\")\nprint(f\"FP32: {genesis.float32}\")  # \u6807\u51c6\u7cbe\u5ea6\nprint(f\"FP16: {genesis.float16}\")  # \u534a\u7cbe\u5ea6\nprint(f\"BF16: {genesis.bfloat16}\") # Brain Float\n\n# \u68c0\u67e5\u6570\u636e\u7c7b\u578b\u5c5e\u6027\ndtype = genesis.float16\nprint(f\"\u540d\u79f0: {dtype.name}\")\nprint(f\"\u5927\u5c0f: {dtype.itemsize} \u5b57\u8282\")\nprint(f\"\u662f\u5426\u6d6e\u70b9: {dtype.is_floating_point}\")\nprint(f\"NumPy\u7c7b\u578b: {dtype.numpy_dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_6","title":"\u521b\u5efa\u6df7\u5408\u7cbe\u5ea6\u5f20\u91cf","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u4e0d\u540c\u7cbe\u5ea6\u7684\u5f20\u91cf\nfp32_tensor = genesis.randn(1000, 1000, dtype=genesis.float32)\nfp16_tensor = genesis.randn(1000, 1000, dtype=genesis.float16) \nbf16_tensor = genesis.randn(1000, 1000, dtype=genesis.bfloat16)\n\nprint(f\"FP32\u5185\u5b58: {fp32_tensor.numel() * 4} \u5b57\u8282\")\nprint(f\"FP16\u5185\u5b58: {fp16_tensor.numel() * 2} \u5b57\u8282\") \nprint(f\"BF16\u5185\u5b58: {bf16_tensor.numel() * 2} \u5b57\u8282\")\n\n# \u7c7b\u578b\u8f6c\u6362\nfp16_from_fp32 = fp32_tensor.half()    # \u8f6c\u6362\u4e3aFP16\nfp32_from_fp16 = fp16_tensor.float()   # \u8f6c\u6362\u4e3aFP32\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#amp","title":"\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff08AMP\uff09","text":""},{"location":"tutorials/mixed-precision.zh/#amp_1","title":"\u57fa\u7840AMP\u4f7f\u7528","text":"<p>Genesis\u901a\u8fc7<code>autocast</code>\u4e0a\u4e0b\u6587\u548c\u542f\u7528\u6807\u5fd7\u63d0\u4f9b\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\uff1a</p> Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# \u5168\u5c40\u542f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\n# \u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\nmodel = nn.Linear(784, 10).cuda()\nx = genesis.randn(32, 784, device='cuda')\nlabels = genesis.randint(0, 10, (32,), device='cuda')\n\n# \u4f7f\u7528\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\u7684\u524d\u5411\u4f20\u64ad\noutputs = model(x)  # \u81ea\u52a8\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\n\n# \u635f\u5931\u8ba1\u7b97\uff08\u901a\u5e38\u5728FP32\u4e2d\u8fdb\u884c\uff09\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(outputs, labels)\n\nprint(f\"\u8f93\u5165\u6570\u636e\u7c7b\u578b: {x.dtype}\")\nprint(f\"\u8f93\u51fa\u6570\u636e\u7c7b\u578b: {outputs.dtype}\")\nprint(f\"\u635f\u5931\u6570\u636e\u7c7b\u578b: {loss.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#amp_2","title":"\u624b\u52a8AMP\u63a7\u5236","text":"<p>\u5bf9\u4e8e\u7cbe\u7ec6\u63a7\u5236\uff0c\u4f7f\u7528<code>autocast</code>\u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff1a</p> Python<pre><code>import genesis\n\n# \u7981\u7528\u5168\u5c40autocast\ngenesis.enable_autocast = False\n\n# \u6a21\u578b\u8bbe\u7f6e\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n).cuda()\n\nx = genesis.randn(32, 784, device='cuda')\n\n# \u624b\u52a8\u6df7\u5408\u7cbe\u5ea6\u63a7\u5236\nwith genesis.autocast():\n    # \u6b64\u5757\u5185\u7684\u64cd\u4f5c\u4f7f\u7528FP16/BF16\n    hidden = model[0](x)  # Linear\u5c42\u4f7f\u7528FP16\n    activated = model[1](hidden)  # ReLU\u4f7f\u7528FP16\n\n# \u5757\u5916\u64cd\u4f5c\u4f7f\u7528\u9ed8\u8ba4\u7cbe\u5ea6\noutputs = model[2](activated)  # \u8fd9\u5c06\u662fFP32\n\nprint(f\"\u9690\u85cf\u5c42\u6570\u636e\u7c7b\u578b: {hidden.dtype}\")\nprint(f\"\u6fc0\u6d3b\u5c42\u6570\u636e\u7c7b\u578b: {activated.dtype}\")\nprint(f\"\u8f93\u51fa\u6570\u636e\u7c7b\u578b: {outputs.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_7","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":""},{"location":"tutorials/mixed-precision.zh/#_8","title":"\u7b80\u5355\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u6a21\u578b\u8bbe\u7f6e\nmodel = nn.Sequential(\n    nn.Linear(784, 512),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, 10)\n).cuda()\n\n# \u4f18\u5316\u5668\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# \u635f\u5931\u51fd\u6570\ncriterion = nn.CrossEntropyLoss()\n\n# \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\ndef train_epoch_amp(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0.0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        # \u6e05\u96f6\u68af\u5ea6\n        optimizer.zero_grad()\n\n        # \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u7684\u524d\u5411\u4f20\u64ad\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\uff08\u5bf9\u7a33\u5b9a\u6027\u5f88\u91cd\u8981\uff09\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u4f18\u5316\u5668\u6b65\u9aa4\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'\u6279\u6b21 {batch_idx}: loss={loss.item():.4f}')\n\n    return total_loss / len(dataloader)\n\n# \u8bad\u7ec3\nfor epoch in range(10):\n    avg_loss = train_epoch_amp(model, train_loader, optimizer, criterion)\n    print(f'\u8f6e\u6b21 {epoch}: \u5e73\u5747\u635f\u5931 = {avg_loss:.4f}')\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_9","title":"\u5e26\u635f\u5931\u7f29\u653e\u7684\u9ad8\u7ea7\u6df7\u5408\u7cbe\u5ea6","text":"<p>\u4e3a\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u4f7f\u7528FP16\u65f6\uff0c\u5efa\u8bae\u4f7f\u7528\u635f\u5931\u7f29\u653e\uff1a</p> Python<pre><code>class GradScaler:\n    \"\"\"\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u68af\u5ea6\u7f29\u653e\u5668\u3002\"\"\"\n\n    def __init__(self, init_scale=2**16, growth_factor=2.0, backoff_factor=0.5, \n                 growth_interval=2000):\n        self.scale = init_scale\n        self.growth_factor = growth_factor\n        self.backoff_factor = backoff_factor\n        self.growth_interval = growth_interval\n        self._growth_tracker = 0\n\n    def scale_loss(self, loss):\n        \"\"\"\u7f29\u653e\u635f\u5931\u4ee5\u9632\u6b62\u68af\u5ea6\u4e0b\u6ea2\u3002\"\"\"\n        return loss * self.scale\n\n    def unscale_gradients(self, optimizer):\n        \"\"\"\u5728\u4f18\u5316\u5668\u6b65\u9aa4\u524d\u53cd\u7f29\u653e\u68af\u5ea6\u3002\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    param.grad.data.div_(self.scale)\n\n    def step(self, optimizer):\n        \"\"\"\u5e26\u68af\u5ea6\u6ea2\u51fa\u68c0\u6d4b\u7684\u4f18\u5316\u5668\u6b65\u9aa4\u3002\"\"\"\n        # \u68c0\u67e5\u68af\u5ea6\u6ea2\u51fa\n        has_overflow = self._check_overflow(optimizer)\n\n        if has_overflow:\n            # \u8df3\u8fc7\u4f18\u5316\u5668\u6b65\u9aa4\u5e76\u51cf\u5c11\u7f29\u653e\n            self.scale *= self.backoff_factor\n            self.scale = max(self.scale, 1.0)\n            self._growth_tracker = 0\n            return False\n        else:\n            # \u6b63\u5e38\u4f18\u5316\u5668\u6b65\u9aa4\n            optimizer.step()\n\n            # \u5b9a\u671f\u589e\u52a0\u7f29\u653e\n            self._growth_tracker += 1\n            if self._growth_tracker &gt;= self.growth_interval:\n                self.scale *= self.growth_factor\n                self._growth_tracker = 0\n\n            return True\n\n    def _check_overflow(self, optimizer):\n        \"\"\"\u68c0\u67e5\u662f\u5426\u6709\u68af\u5ea6\u6ea2\u51fa\u3002\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    if genesis.isnan(param.grad).any() or genesis.isinf(param.grad).any():\n                        return True\n        return False\n\n# \u5e26\u68af\u5ea6\u7f29\u653e\u7684\u8bad\u7ec3\nscaler = GradScaler()\n\ndef train_with_scaling(model, dataloader, optimizer, criterion, scaler):\n    model.train()\n    total_loss = 0.0\n    successful_steps = 0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        optimizer.zero_grad()\n\n        # \u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u7684\u524d\u5411\u4f20\u64ad\n        with genesis.autocast():\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n\n        # \u7f29\u653e\u635f\u5931\u4ee5\u9632\u6b62\u68af\u5ea6\u4e0b\u6ea2\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # \u53cd\u7f29\u653e\u68af\u5ea6\u5e76\u68c0\u67e5\u6ea2\u51fa\n        scaler.unscale_gradients(optimizer)\n\n        # \u5728\u53cd\u7f29\u653e\u68af\u5ea6\u4e0a\u8fdb\u884c\u68af\u5ea6\u88c1\u526a\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u5e26\u6ea2\u51fa\u68c0\u6d4b\u7684\u4f18\u5316\u5668\u6b65\u9aa4\n        if scaler.step(optimizer):\n            successful_steps += 1\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'\u6279\u6b21 {batch_idx}: loss={loss.item():.4f}, scale={scaler.scale:.0f}')\n\n    success_rate = successful_steps / len(dataloader)\n    print(f'\u8bad\u7ec3\u6210\u529f\u7387: {success_rate:.1%}')\n\n    return total_loss / len(dataloader)\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_10","title":"\u7cbe\u5ea6\u7279\u5b9a\u8003\u8651","text":""},{"location":"tutorials/mixed-precision.zh/#fp16","title":"FP16\uff08\u534a\u7cbe\u5ea6\uff09","text":"Python<pre><code>import genesis\n\n# FP16\u7279\u6027\nfp16_info = {\n    'range': '\u00b165,504',\n    'precision': '\u7ea63-4\u4f4d\u5c0f\u6570',\n    'special_values': ['inf', '-inf', 'nan'],\n    'benefits': ['\u5728Tensor Cores\u4e0a\u66f4\u5feb', '50%\u5185\u5b58\u51cf\u5c11'],\n    'challenges': ['\u6709\u9650\u7684\u8303\u56f4', '\u68af\u5ea6\u4e0b\u6ea2']\n}\n\n# FP16\u6700\u4f73\u5b9e\u8df5\ndef create_fp16_model():\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.LayerNorm(256),  # LayerNorm\u5728FP16\u4e0b\u8868\u73b0\u826f\u597d\n        nn.ReLU(),\n        nn.Linear(256, 10)\n    )\n\n    # \u4e3aFP16\u521d\u59cb\u5316\u9002\u5f53\u7684\u7f29\u653e\n    for module in model.modules():\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=1.0)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    return model\n\n# \u76d1\u63a7FP16\u8bad\u7ec3\ndef check_fp16_health(model):\n    \"\"\"\u68c0\u67e5FP16\u8bad\u7ec3\u671f\u95f4\u7684\u6a21\u578b\u5065\u5eb7\u72b6\u51b5\u3002\"\"\"\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            param_norm = param.norm().item()\n\n            print(f\"{name}:\")\n            print(f\"  \u53c2\u6570\u8303\u6570: {param_norm:.2e}\")\n            print(f\"  \u68af\u5ea6\u8303\u6570: {grad_norm:.2e}\")\n\n            # \u68c0\u67e5\u95ee\u9898\u503c\n            if grad_norm &lt; 1e-7:\n                print(f\"  \u8b66\u544a: \u68c0\u6d4b\u5230\u975e\u5e38\u5c0f\u7684\u68af\u5ea6!\")\n            if grad_norm &gt; 1e4:\n                print(f\"  \u8b66\u544a: \u68c0\u6d4b\u5230\u975e\u5e38\u5927\u7684\u68af\u5ea6!\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#bf16brain-float","title":"BF16\uff08Brain Float\uff09","text":"Python<pre><code>import genesis\n\n# BF16\u4f18\u52bf\nbf16_info = {\n    'range': '\u4e0eFP32\u76f8\u540c (\u00b13.4\u00d710^38)',\n    'precision': '\u7ea62-3\u4f4d\u5c0f\u6570', \n    'benefits': ['\u6bd4FP16\u8303\u56f4\u66f4\u5927', '\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3'],\n    'hardware': ['A100', 'H100', 'TPUs']\n}\n\n# BF16\u901a\u5e38\u6bd4FP16\u66f4\u7a33\u5b9a\ndef train_with_bf16():\n    # \u4f7f\u7528BF16\u521b\u5efa\u6a21\u578b\n    model = nn.Linear(1000, 100).cuda()\n    x = genesis.randn(32, 1000, dtype=genesis.bfloat16, device='cuda')\n\n    # BF16\u524d\u5411\u4f20\u64ad\n    output = model(x)\n    print(f\"\u8f93\u5165: {x.dtype}, \u8f93\u51fa: {output.dtype}\")\n\n    # BF16\u901a\u5e38\u4e0d\u9700\u8981\u635f\u5931\u7f29\u653e\n    loss = output.sum()\n    loss.backward()\n\n    return model\n\n# \u6bd4\u8f83\u7cbe\u5ea6\ndef compare_precisions():\n    sizes = [100, 1000, 10000]\n\n    for size in sizes:\n        # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\n        data_fp32 = genesis.randn(size, size)\n        data_fp16 = data_fp32.half()\n        data_bf16 = data_fp32.to(genesis.bfloat16)\n\n        # \u7b80\u5355\u8ba1\u7b97\n        result_fp32 = genesis.matmul(data_fp32, data_fp32)\n        result_fp16 = genesis.matmul(data_fp16, data_fp16)\n        result_bf16 = genesis.matmul(data_bf16, data_bf16)\n\n        # \u6bd4\u8f83\u7cbe\u5ea6\n        error_fp16 = (result_fp32 - result_fp16.float()).abs().mean()\n        error_bf16 = (result_fp32 - result_bf16.float()).abs().mean()\n\n        print(f\"\u5927\u5c0f {size}x{size}:\")\n        print(f\"  FP16\u8bef\u5dee: {error_fp16:.2e}\")\n        print(f\"  BF16\u8bef\u5dee: {error_bf16:.2e}\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_11","title":"\u5185\u5b58\u4f18\u5316","text":""},{"location":"tutorials/mixed-precision.zh/#_12","title":"\u5185\u5b58\u4f7f\u7528\u5206\u6790","text":"Python<pre><code>import genesis\n\ndef analyze_memory_usage():\n    \"\"\"\u5206\u6790\u4e0d\u540c\u7cbe\u5ea6\u7c7b\u578b\u7684\u5185\u5b58\u4f7f\u7528\u3002\"\"\"\n\n    # \u6a21\u578b\u5927\u5c0f\n    sizes = [(1000, 1000), (2000, 2000), (5000, 5000)]\n\n    for h, w in sizes:\n        print(f\"\\n\u5f20\u91cf\u5927\u5c0f: {h}x{w}\")\n\n        # \u521b\u5efa\u5f20\u91cf\n        fp32_tensor = genesis.randn(h, w, dtype=genesis.float32, device='cuda')\n        fp16_tensor = genesis.randn(h, w, dtype=genesis.float16, device='cuda')\n        bf16_tensor = genesis.randn(h, w, dtype=genesis.bfloat16, device='cuda')\n\n        # \u5185\u5b58\u4f7f\u7528\n        fp32_memory = fp32_tensor.numel() * 4  # \u6bcf\u4e2afloat32 4\u5b57\u8282\n        fp16_memory = fp16_tensor.numel() * 2  # \u6bcf\u4e2afloat16 2\u5b57\u8282\n        bf16_memory = bf16_tensor.numel() * 2  # \u6bcf\u4e2abfloat16 2\u5b57\u8282\n\n        print(f\"  FP32: {fp32_memory / 1e6:.1f} MB\")\n        print(f\"  FP16: {fp16_memory / 1e6:.1f} MB ({fp16_memory/fp32_memory:.1%})\")\n        print(f\"  BF16: {bf16_memory / 1e6:.1f} MB ({bf16_memory/fp32_memory:.1%})\")\n\n        # \u6e05\u7406\n        del fp32_tensor, fp16_tensor, bf16_tensor\n        genesis.cuda.empty_cache()\n\nanalyze_memory_usage()\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_13","title":"\u68af\u5ea6\u68c0\u67e5\u70b9\u4e0e\u6df7\u5408\u7cbe\u5ea6","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\nclass CheckpointedModule(nn.Module):\n    \"\"\"\u652f\u6301\u68af\u5ea6\u68c0\u67e5\u70b9\u7684\u6a21\u5757\u3002\"\"\"\n\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.checkpoint = True\n\n    def forward(self, x):\n        def run_layers(x, layers):\n            for layer in layers:\n                x = layer(x)\n            return x\n\n        if self.training and self.checkpoint:\n            # \u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\u8282\u7701\u5185\u5b58\n            return genesis.utils.checkpoint(run_layers, x, self.layers)\n        else:\n            return run_layers(x, self.layers)\n\n# \u521b\u5efa\u5185\u5b58\u9ad8\u6548\u6a21\u578b\ndef create_checkpointed_model():\n    layers = [\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Linear(512, 10)\n    ]\n\n    return CheckpointedModule(layers)\n\n# \u4f7f\u7528\u68c0\u67e5\u70b9\u548c\u6df7\u5408\u7cbe\u5ea6\u8fdb\u884c\u8bad\u7ec3\ndef train_memory_efficient():\n    model = create_checkpointed_model().cuda()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\n    genesis.enable_autocast = True\n\n    for epoch in range(10):\n        for batch in dataloader:\n            data, targets = batch\n            data = data.cuda()\n            targets = targets.cuda()\n\n            optimizer.zero_grad()\n\n            # \u4f7f\u7528\u68c0\u67e5\u70b9\u548c\u6df7\u5408\u7cbe\u5ea6\u7684\u524d\u5411\u4f20\u64ad\n            outputs = model(data)\n            loss = nn.CrossEntropyLoss()(outputs, targets)\n\n            # \u53cd\u5411\u4f20\u64ad\n            loss.backward()\n            optimizer.step()\n\n        print(f\"\u8f6e\u6b21 {epoch} \u5b8c\u6210\")\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_14","title":"\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"tutorials/mixed-precision.zh/#_15","title":"\u6df7\u5408\u7cbe\u5ea6\u6027\u80fd\u6bd4\u8f83","text":"Python<pre><code>import genesis\nimport time\n\ndef benchmark_precision_performance():\n    \"\"\"\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u540c\u7cbe\u5ea6\u683c\u5f0f\u3002\"\"\"\n\n    # \u6a21\u578b\u8bbe\u7f6e\n    sizes = [512, 1024, 2048]\n    batch_sizes = [16, 32, 64]\n\n    results = {}\n\n    for size in sizes:\n        for batch_size in batch_sizes:\n            print(f\"\\n\u57fa\u51c6\u6d4b\u8bd5: size={size}, batch_size={batch_size}\")\n\n            # \u521b\u5efa\u6a21\u578b\n            model_fp32 = nn.Linear(size, size).cuda()\n            model_fp16 = nn.Linear(size, size).cuda().half()\n\n            # \u521b\u5efa\u6570\u636e\n            data_fp32 = genesis.randn(batch_size, size, device='cuda')\n            data_fp16 = data_fp32.half()\n\n            # \u57fa\u51c6\u6d4b\u8bd5FP32\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp32 = model_fp32(data_fp32)\n\n            torch.cuda.synchronize()\n            fp32_time = time.time() - start_time\n\n            # \u57fa\u51c6\u6d4b\u8bd5FP16\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp16 = model_fp16(data_fp16)\n\n            torch.cuda.synchronize()\n            fp16_time = time.time() - start_time\n\n            # \u7ed3\u679c\n            speedup = fp32_time / fp16_time\n            print(f\"  FP32\u65f6\u95f4: {fp32_time:.3f}s\")\n            print(f\"  FP16\u65f6\u95f4: {fp16_time:.3f}s\") \n            print(f\"  \u52a0\u901f\u6bd4: {speedup:.2f}x\")\n\n            results[(size, batch_size)] = {\n                'fp32_time': fp32_time,\n                'fp16_time': fp16_time,\n                'speedup': speedup\n            }\n\n    return results\n\n# \u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\nbenchmark_results = benchmark_precision_performance()\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_16","title":"\u6700\u4f73\u5b9e\u8df5\u548c\u6545\u969c\u6392\u9664","text":""},{"location":"tutorials/mixed-precision.zh/#_17","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4ece\u7b80\u5355\u5f00\u59cb\uff1a\u5728\u624b\u52a8\u63a7\u5236\u4e4b\u524d\u5148\u5c1d\u8bd5\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6</li> <li>\u76d1\u63a7\u8bad\u7ec3\uff1a\u5173\u6ce8\u68af\u5ea6\u4e0b\u6ea2/\u6ea2\u51fa</li> <li>\u4f7f\u7528\u635f\u5931\u7f29\u653e\uff1a\u5bf9FP16\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981</li> <li>\u68af\u5ea6\u88c1\u526a\uff1a\u6709\u52a9\u4e8e\u9632\u6b62\u68af\u5ea6\u7206\u70b8</li> <li>\u5206\u5c42\u7cbe\u5ea6\uff1a\u67d0\u4e9b\u5c42\u53ef\u80fd\u9700\u8981FP32\uff08\u5982\u6279\u6807\u51c6\u5316\uff09</li> </ol>"},{"location":"tutorials/mixed-precision.zh/#_18","title":"\u5e38\u89c1\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848","text":"Python<pre><code># \u95ee\u98981: \u68af\u5ea6\u4e0b\u6ea2\ndef handle_gradient_underflow():\n    \"\"\"\u5904\u7406FP16\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u4e0b\u6ea2\u3002\"\"\"\n\n    # \u89e3\u51b3\u65b9\u68481: \u4f7f\u7528\u635f\u5931\u7f29\u653e\n    scaler = GradScaler(init_scale=2**16)\n\n    # \u89e3\u51b3\u65b9\u68482: \u8df3\u8fc7\u6709\u95ee\u9898\u7684\u6279\u6b21\n    def safe_backward(loss, scaler):\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # \u5728\u4f18\u5316\u5668\u6b65\u9aa4\u524d\u68c0\u67e5\u95ee\u9898\n        has_inf_or_nan = any(\n            genesis.isinf(p.grad).any() or genesis.isnan(p.grad).any()\n            for p in model.parameters() \n            if p.grad is not None\n        )\n\n        if has_inf_or_nan:\n            print(\"\u7531\u4e8einf/nan\u68af\u5ea6\u8df3\u8fc7\u6b65\u9aa4\")\n            optimizer.zero_grad()\n            return False\n\n        return True\n\n# \u95ee\u98982: \u6a21\u578b\u53d1\u6563\ndef prevent_model_divergence():\n    \"\"\"\u9632\u6b62\u6df7\u5408\u7cbe\u5ea6\u4e2d\u7684\u6a21\u578b\u53d1\u6563\u3002\"\"\"\n\n    # \u89e3\u51b3\u65b9\u68481: \u964d\u4f4e\u5b66\u4e60\u7387\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # \u66f4\u4f4e\u7684\u5b66\u4e60\u7387\n\n    # \u89e3\u51b3\u65b9\u68482: \u9884\u70ed\u8ba1\u5212\n    scheduler = optim.get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=1000, num_training_steps=10000\n    )\n\n    # \u89e3\u51b3\u65b9\u68483: \u5bc6\u5207\u76d1\u63a7\u635f\u5931\n    def check_loss_stability(loss, loss_history):\n        loss_history.append(loss.item())\n\n        if len(loss_history) &gt; 100:\n            recent_losses = loss_history[-50:]\n            if any(l &gt; 10 * min(recent_losses) for l in recent_losses):\n                print(\"\u8b66\u544a: \u68c0\u6d4b\u5230\u635f\u5931\u4e0d\u7a33\u5b9a!\")\n                return False\n\n        return True\n\n# \u95ee\u98983: \u7cbe\u5ea6\u964d\u4f4e\ndef maintain_accuracy():\n    \"\"\"\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002\"\"\"\n\n    # \u89e3\u51b3\u65b9\u68481: \u4f7f\u7528BF16\u800c\u4e0d\u662fFP16\n    genesis.enable_autocast = True\n    default_dtype = genesis.bfloat16\n\n    # \u89e3\u51b3\u65b9\u68482: \u4fdd\u6301\u5173\u952e\u5c42\u5728FP32\n    class MixedPrecisionModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Linear(784, 256),  # FP16/BF16\n                nn.ReLU(),\n                nn.Linear(256, 128),  # FP16/BF16\n                nn.ReLU()\n            )\n\n            # \u4fdd\u6301\u8f93\u51fa\u5c42\u5728FP32\u4ee5\u83b7\u5f97\u7a33\u5b9a\u6027\n            self.classifier = nn.Linear(128, 10).float()\n\n        def forward(self, x):\n            with genesis.autocast():\n                features = self.features(x)\n\n            # \u8f93\u51fa\u5c42\u5728FP32\n            output = self.classifier(features.float())\n            return output\n</code></pre>"},{"location":"tutorials/mixed-precision.zh/#_19","title":"\u8c03\u8bd5\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code>def debug_mixed_precision():\n    \"\"\"\u8c03\u8bd5\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u95ee\u9898\u3002\"\"\"\n\n    # 1. \u68c0\u67e5\u6574\u4e2a\u6a21\u578b\u7684\u5f20\u91cf\u6570\u636e\u7c7b\u578b\n    def print_tensor_info(tensor, name):\n        print(f\"{name}:\")\n        print(f\"  \u5f62\u72b6: {tensor.shape}\")\n        print(f\"  \u6570\u636e\u7c7b\u578b: {tensor.dtype}\")\n        print(f\"  \u8bbe\u5907: {tensor.device}\")\n        print(f\"  \u9700\u8981\u68af\u5ea6: {tensor.requires_grad}\")\n        print(f\"  \u6700\u5c0f/\u6700\u5927\u503c: {tensor.min():.2e} / {tensor.max():.2e}\")\n        print()\n\n    # 2. \u76d1\u63a7\u68af\u5ea6\u8303\u6570\n    def check_gradient_norms(model):\n        total_norm = 0.0\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.norm().item()\n                total_norm += grad_norm ** 2\n                print(f\"{name}: grad_norm = {grad_norm:.2e}\")\n\n        total_norm = total_norm ** 0.5\n        print(f\"\u603b\u68af\u5ea6\u8303\u6570: {total_norm:.2e}\")\n        return total_norm\n\n    # 3. \u9a8c\u8bc1\u6570\u503c\u7a33\u5b9a\u6027\n    def check_numerical_stability(tensor):\n        \"\"\"\u68c0\u67e5\u6570\u503c\u95ee\u9898\u3002\"\"\"\n        has_nan = genesis.isnan(tensor).any()\n        has_inf = genesis.isinf(tensor).any()\n\n        if has_nan:\n            print(\"\u8b66\u544a: \u68c0\u6d4b\u5230NaN\u503c!\")\n        if has_inf:\n            print(\"\u8b66\u544a: \u68c0\u6d4b\u5230Inf\u503c!\")\n\n        return not (has_nan or has_inf)\n\n# \u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u4f7f\u7528\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        # \u524d\u5411\u4f20\u64ad\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # \u8c03\u8bd5\u4fe1\u606f\n        if batch_idx % 100 == 0:\n            print(f\"\u8f6e\u6b21 {epoch}, \u6279\u6b21 {batch_idx}:\")\n            print_tensor_info(data, \"\u8f93\u5165\")\n            print_tensor_info(outputs, \"\u8f93\u51fa\") \n            print_tensor_info(loss, \"\u635f\u5931\")\n\n            # \u53cd\u5411\u4f20\u64ad\u540e\u68c0\u67e5\u68af\u5ea6\n            loss.backward()\n            grad_norm = check_gradient_norms(model)\n\n            if grad_norm &gt; 10.0:\n                print(\"\u8b66\u544a: \u68c0\u6d4b\u5230\u5927\u68af\u5ea6\u8303\u6570!\")\n</code></pre> <p>\u8fd9\u4efd\u5168\u9762\u6307\u5357\u6db5\u76d6\u4e86Genesis\u4e2d\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u6240\u6709\u65b9\u9762\uff0c\u4ece\u57fa\u7840\u4f7f\u7528\u5230\u9ad8\u7ea7\u4f18\u5316\u6280\u672f\u548c\u6545\u969c\u6392\u9664\u7b56\u7565\u3002</p>"},{"location":"tutorials/mixed-precision_en/","title":"Mixed Precision Training Guide","text":"<p>Mixed precision training is a technique that uses both 16-bit (half precision) and 32-bit (single precision) floating-point numbers during training to reduce memory usage and accelerate training while maintaining model accuracy. Genesis provides comprehensive support for mixed precision training with automatic mixed precision (AMP) capabilities.</p>"},{"location":"tutorials/mixed-precision_en/#overview","title":"Overview","text":""},{"location":"tutorials/mixed-precision_en/#benefits-of-mixed-precision-training","title":"Benefits of Mixed Precision Training","text":"<ul> <li>Memory Efficiency: Reduces memory usage by ~50%</li> <li>Speed Improvement: Faster training on modern GPUs with Tensor Cores</li> <li>Model Accuracy: Maintains training stability with automatic loss scaling</li> <li>Larger Models: Enables training of larger models on the same hardware</li> </ul>"},{"location":"tutorials/mixed-precision_en/#supported-precision-types","title":"Supported Precision Types","text":"<p>Genesis supports multiple precision formats:</p> <ul> <li>float32 (FP32): Standard single precision (default)</li> <li>float16 (FP16): IEEE half precision </li> <li>bfloat16 (BF16): Brain float format with larger dynamic range</li> </ul>"},{"location":"tutorials/mixed-precision_en/#data-type-system","title":"Data Type System","text":""},{"location":"tutorials/mixed-precision_en/#understanding-genesis-dtypes","title":"Understanding Genesis DTypes","text":"Python<pre><code>import genesis\n\n# Available precision types\nprint(\"Available dtypes:\")\nprint(f\"FP32: {genesis.float32}\")  # Standard precision\nprint(f\"FP16: {genesis.float16}\")  # Half precision\nprint(f\"BF16: {genesis.bfloat16}\") # Brain float\n\n# Check dtype properties\ndtype = genesis.float16\nprint(f\"Name: {dtype.name}\")\nprint(f\"Size: {dtype.itemsize} bytes\")\nprint(f\"Is floating: {dtype.is_floating_point}\")\nprint(f\"NumPy type: {dtype.numpy_dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#creating-mixed-precision-tensors","title":"Creating Mixed Precision Tensors","text":"Python<pre><code>import genesis\n\n# Create tensors with different precisions\nfp32_tensor = genesis.randn(1000, 1000, dtype=genesis.float32)\nfp16_tensor = genesis.randn(1000, 1000, dtype=genesis.float16) \nbf16_tensor = genesis.randn(1000, 1000, dtype=genesis.bfloat16)\n\nprint(f\"FP32 memory: {fp32_tensor.numel() * 4} bytes\")\nprint(f\"FP16 memory: {fp16_tensor.numel() * 2} bytes\") \nprint(f\"BF16 memory: {bf16_tensor.numel() * 2} bytes\")\n\n# Type conversion\nfp16_from_fp32 = fp32_tensor.half()    # Convert to FP16\nfp32_from_fp16 = fp16_tensor.float()   # Convert to FP32\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#automatic-mixed-precision-amp","title":"Automatic Mixed Precision (AMP)","text":""},{"location":"tutorials/mixed-precision_en/#basic-amp-usage","title":"Basic AMP Usage","text":"<p>Genesis provides automatic mixed precision through the <code>autocast</code> context and enable flag:</p> Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# Enable automatic mixed precision globally\ngenesis.enable_autocast = True\n\n# Create model and data\nmodel = nn.Linear(784, 10).cuda()\nx = genesis.randn(32, 784, device='cuda')\nlabels = genesis.randint(0, 10, (32,), device='cuda')\n\n# Forward pass with automatic casting\noutputs = model(x)  # Automatically uses mixed precision\n\n# Loss computation (typically done in FP32)\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(outputs, labels)\n\nprint(f\"Input dtype: {x.dtype}\")\nprint(f\"Output dtype: {outputs.dtype}\")\nprint(f\"Loss dtype: {loss.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#manual-amp-control","title":"Manual AMP Control","text":"<p>For fine-grained control, use the <code>autocast</code> context manager:</p> Python<pre><code>import genesis\n\n# Disable global autocast\ngenesis.enable_autocast = False\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n).cuda()\n\nx = genesis.randn(32, 784, device='cuda')\n\n# Manual mixed precision control\nwith genesis.autocast():\n    # Operations inside this block use FP16/BF16\n    hidden = model[0](x)  # Linear layer in FP16\n    activated = model[1](hidden)  # ReLU in FP16\n\n# Operations outside use default precision\noutputs = model[2](activated)  # This will be FP32\n\nprint(f\"Hidden dtype: {hidden.dtype}\")\nprint(f\"Activated dtype: {activated.dtype}\")\nprint(f\"Output dtype: {outputs.dtype}\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#training-with-mixed-precision","title":"Training with Mixed Precision","text":""},{"location":"tutorials/mixed-precision_en/#simple-mixed-precision-training-loop","title":"Simple Mixed Precision Training Loop","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 512),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, 10)\n).cuda()\n\n# Optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Enable mixed precision\ngenesis.enable_autocast = True\n\ndef train_epoch_amp(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0.0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # Backward pass\n        loss.backward()\n\n        # Gradient clipping (important for stability)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Optimizer step\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}: loss={loss.item():.4f}')\n\n    return total_loss / len(dataloader)\n\n# Training\nfor epoch in range(10):\n    avg_loss = train_epoch_amp(model, train_loader, optimizer, criterion)\n    print(f'Epoch {epoch}: avg loss = {avg_loss:.4f}')\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#advanced-mixed-precision-with-loss-scaling","title":"Advanced Mixed Precision with Loss Scaling","text":"<p>For training stability, especially with FP16, loss scaling is recommended:</p> Python<pre><code>class GradScaler:\n    \"\"\"Gradient scaler for mixed precision training.\"\"\"\n\n    def __init__(self, init_scale=2**16, growth_factor=2.0, backoff_factor=0.5, \n                 growth_interval=2000):\n        self.scale = init_scale\n        self.growth_factor = growth_factor\n        self.backoff_factor = backoff_factor\n        self.growth_interval = growth_interval\n        self._growth_tracker = 0\n\n    def scale_loss(self, loss):\n        \"\"\"Scale loss to prevent gradient underflow.\"\"\"\n        return loss * self.scale\n\n    def unscale_gradients(self, optimizer):\n        \"\"\"Unscale gradients before optimizer step.\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    param.grad.data.div_(self.scale)\n\n    def step(self, optimizer):\n        \"\"\"Step optimizer with gradient overflow detection.\"\"\"\n        # Check for gradient overflow\n        has_overflow = self._check_overflow(optimizer)\n\n        if has_overflow:\n            # Skip optimizer step and reduce scale\n            self.scale *= self.backoff_factor\n            self.scale = max(self.scale, 1.0)\n            self._growth_tracker = 0\n            return False\n        else:\n            # Normal optimizer step\n            optimizer.step()\n\n            # Increase scale periodically\n            self._growth_tracker += 1\n            if self._growth_tracker &gt;= self.growth_interval:\n                self.scale *= self.growth_factor\n                self._growth_tracker = 0\n\n            return True\n\n    def _check_overflow(self, optimizer):\n        \"\"\"Check if any gradients have overflowed.\"\"\"\n        for param_group in optimizer.param_groups:\n            for param in param_group['params']:\n                if param.grad is not None:\n                    if genesis.isnan(param.grad).any() or genesis.isinf(param.grad).any():\n                        return True\n        return False\n\n# Training with gradient scaling\nscaler = GradScaler()\n\ndef train_with_scaling(model, dataloader, optimizer, criterion, scaler):\n    model.train()\n    total_loss = 0.0\n    successful_steps = 0\n\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        data = data.cuda()\n        targets = targets.cuda()\n\n        optimizer.zero_grad()\n\n        # Forward pass with mixed precision\n        with genesis.autocast():\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n\n        # Scale loss to prevent gradient underflow\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # Unscale gradients and check for overflow\n        scaler.unscale_gradients(optimizer)\n\n        # Gradient clipping on unscaled gradients\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Step optimizer with overflow detection\n        if scaler.step(optimizer):\n            successful_steps += 1\n\n        total_loss += loss.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Batch {batch_idx}: loss={loss.item():.4f}, scale={scaler.scale:.0f}')\n\n    success_rate = successful_steps / len(dataloader)\n    print(f'Training success rate: {success_rate:.1%}')\n\n    return total_loss / len(dataloader)\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#precision-specific-considerations","title":"Precision-Specific Considerations","text":""},{"location":"tutorials/mixed-precision_en/#fp16-half-precision","title":"FP16 (Half Precision)","text":"Python<pre><code>import genesis\n\n# FP16 characteristics\nfp16_info = {\n    'range': '\u00b165,504',\n    'precision': '~3-4 decimal digits',\n    'special_values': ['inf', '-inf', 'nan'],\n    'benefits': ['Faster on Tensor Cores', '50% memory reduction'],\n    'challenges': ['Limited range', 'Gradient underflow']\n}\n\n# Best practices for FP16\ndef create_fp16_model():\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.LayerNorm(256),  # LayerNorm works well with FP16\n        nn.ReLU(),\n        nn.Linear(256, 10)\n    )\n\n    # Initialize with appropriate scale for FP16\n    for module in model.modules():\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=1.0)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    return model\n\n# Monitor FP16 training\ndef check_fp16_health(model):\n    \"\"\"Check model health during FP16 training.\"\"\"\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            param_norm = param.norm().item()\n\n            print(f\"{name}:\")\n            print(f\"  Param norm: {param_norm:.2e}\")\n            print(f\"  Grad norm: {grad_norm:.2e}\")\n\n            # Check for problematic values\n            if grad_norm &lt; 1e-7:\n                print(f\"  WARNING: Very small gradients detected!\")\n            if grad_norm &gt; 1e4:\n                print(f\"  WARNING: Very large gradients detected!\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#bf16-brain-float","title":"BF16 (Brain Float)","text":"Python<pre><code>import genesis\n\n# BF16 advantages\nbf16_info = {\n    'range': 'Same as FP32 (\u00b13.4\u00d710^38)',\n    'precision': '~2-3 decimal digits', \n    'benefits': ['Larger range than FP16', 'More stable training'],\n    'hardware': ['A100', 'H100', 'TPUs']\n}\n\n# BF16 is often more stable than FP16\ndef train_with_bf16():\n    # Create model with BF16\n    model = nn.Linear(1000, 100).cuda()\n    x = genesis.randn(32, 1000, dtype=genesis.bfloat16, device='cuda')\n\n    # BF16 forward pass\n    output = model(x)\n    print(f\"Input: {x.dtype}, Output: {output.dtype}\")\n\n    # BF16 typically doesn't need loss scaling\n    loss = output.sum()\n    loss.backward()\n\n    return model\n\n# Compare precisions\ndef compare_precisions():\n    sizes = [100, 1000, 10000]\n\n    for size in sizes:\n        # Create test data\n        data_fp32 = genesis.randn(size, size)\n        data_fp16 = data_fp32.half()\n        data_bf16 = data_fp32.to(genesis.bfloat16)\n\n        # Simple computation\n        result_fp32 = genesis.matmul(data_fp32, data_fp32)\n        result_fp16 = genesis.matmul(data_fp16, data_fp16)\n        result_bf16 = genesis.matmul(data_bf16, data_bf16)\n\n        # Compare accuracy\n        error_fp16 = (result_fp32 - result_fp16.float()).abs().mean()\n        error_bf16 = (result_fp32 - result_bf16.float()).abs().mean()\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  FP16 error: {error_fp16:.2e}\")\n        print(f\"  BF16 error: {error_bf16:.2e}\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#memory-optimization","title":"Memory Optimization","text":""},{"location":"tutorials/mixed-precision_en/#memory-usage-analysis","title":"Memory Usage Analysis","text":"Python<pre><code>import genesis\n\ndef analyze_memory_usage():\n    \"\"\"Analyze memory usage of different precision types.\"\"\"\n\n    # Model sizes\n    sizes = [(1000, 1000), (2000, 2000), (5000, 5000)]\n\n    for h, w in sizes:\n        print(f\"\\nTensor size: {h}x{w}\")\n\n        # Create tensors\n        fp32_tensor = genesis.randn(h, w, dtype=genesis.float32, device='cuda')\n        fp16_tensor = genesis.randn(h, w, dtype=genesis.float16, device='cuda')\n        bf16_tensor = genesis.randn(h, w, dtype=genesis.bfloat16, device='cuda')\n\n        # Memory usage\n        fp32_memory = fp32_tensor.numel() * 4  # 4 bytes per float32\n        fp16_memory = fp16_tensor.numel() * 2  # 2 bytes per float16\n        bf16_memory = bf16_tensor.numel() * 2  # 2 bytes per bfloat16\n\n        print(f\"  FP32: {fp32_memory / 1e6:.1f} MB\")\n        print(f\"  FP16: {fp16_memory / 1e6:.1f} MB ({fp16_memory/fp32_memory:.1%})\")\n        print(f\"  BF16: {bf16_memory / 1e6:.1f} MB ({bf16_memory/fp32_memory:.1%})\")\n\n        # Cleanup\n        del fp32_tensor, fp16_tensor, bf16_tensor\n        genesis.cuda.empty_cache()\n\nanalyze_memory_usage()\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#gradient-checkpointing-with-mixed-precision","title":"Gradient Checkpointing with Mixed Precision","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\nclass CheckpointedModule(nn.Module):\n    \"\"\"Module with gradient checkpointing support.\"\"\"\n\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n        self.checkpoint = True\n\n    def forward(self, x):\n        def run_layers(x, layers):\n            for layer in layers:\n                x = layer(x)\n            return x\n\n        if self.training and self.checkpoint:\n            # Use gradient checkpointing to save memory\n            return genesis.utils.checkpoint(run_layers, x, self.layers)\n        else:\n            return run_layers(x, self.layers)\n\n# Create memory-efficient model\ndef create_checkpointed_model():\n    layers = [\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Linear(512, 10)\n    ]\n\n    return CheckpointedModule(layers)\n\n# Training with checkpointing and mixed precision\ndef train_memory_efficient():\n    model = create_checkpointed_model().cuda()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Enable mixed precision\n    genesis.enable_autocast = True\n\n    for epoch in range(10):\n        for batch in dataloader:\n            data, targets = batch\n            data = data.cuda()\n            targets = targets.cuda()\n\n            optimizer.zero_grad()\n\n            # Forward pass with checkpointing and mixed precision\n            outputs = model(data)\n            loss = nn.CrossEntropyLoss()(outputs, targets)\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch} completed\")\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#performance-benchmarking","title":"Performance Benchmarking","text":""},{"location":"tutorials/mixed-precision_en/#mixed-precision-performance-comparison","title":"Mixed Precision Performance Comparison","text":"Python<pre><code>import genesis\nimport time\n\ndef benchmark_precision_performance():\n    \"\"\"Benchmark different precision formats.\"\"\"\n\n    # Model setup\n    sizes = [512, 1024, 2048]\n    batch_sizes = [16, 32, 64]\n\n    results = {}\n\n    for size in sizes:\n        for batch_size in batch_sizes:\n            print(f\"\\nBenchmarking: size={size}, batch_size={batch_size}\")\n\n            # Create models\n            model_fp32 = nn.Linear(size, size).cuda()\n            model_fp16 = nn.Linear(size, size).cuda().half()\n\n            # Create data\n            data_fp32 = genesis.randn(batch_size, size, device='cuda')\n            data_fp16 = data_fp32.half()\n\n            # Benchmark FP32\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp32 = model_fp32(data_fp32)\n\n            torch.cuda.synchronize()\n            fp32_time = time.time() - start_time\n\n            # Benchmark FP16\n            torch.cuda.synchronize()\n            start_time = time.time()\n\n            for _ in range(100):\n                output_fp16 = model_fp16(data_fp16)\n\n            torch.cuda.synchronize()\n            fp16_time = time.time() - start_time\n\n            # Results\n            speedup = fp32_time / fp16_time\n            print(f\"  FP32 time: {fp32_time:.3f}s\")\n            print(f\"  FP16 time: {fp16_time:.3f}s\") \n            print(f\"  Speedup: {speedup:.2f}x\")\n\n            results[(size, batch_size)] = {\n                'fp32_time': fp32_time,\n                'fp16_time': fp16_time,\n                'speedup': speedup\n            }\n\n    return results\n\n# Run benchmark\nbenchmark_results = benchmark_precision_performance()\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#best-practices-and-troubleshooting","title":"Best Practices and Troubleshooting","text":""},{"location":"tutorials/mixed-precision_en/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple: Begin with automatic mixed precision before manual control</li> <li>Monitor Training: Watch for gradient underflow/overflow</li> <li>Use Loss Scaling: Essential for FP16 stability</li> <li>Gradient Clipping: Helps prevent gradient explosion</li> <li>Layer-wise Precision: Some layers may need FP32 (e.g., batch norm)</li> </ol>"},{"location":"tutorials/mixed-precision_en/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"Python<pre><code># Issue 1: Gradient Underflow\ndef handle_gradient_underflow():\n    \"\"\"Handle gradient underflow in FP16 training.\"\"\"\n\n    # Solution 1: Use loss scaling\n    scaler = GradScaler(init_scale=2**16)\n\n    # Solution 2: Skip problematic batches\n    def safe_backward(loss, scaler):\n        scaled_loss = scaler.scale_loss(loss)\n        scaled_loss.backward()\n\n        # Check for problems before optimizer step\n        has_inf_or_nan = any(\n            genesis.isinf(p.grad).any() or genesis.isnan(p.grad).any()\n            for p in model.parameters() \n            if p.grad is not None\n        )\n\n        if has_inf_or_nan:\n            print(\"Skipping step due to inf/nan gradients\")\n            optimizer.zero_grad()\n            return False\n\n        return True\n\n# Issue 2: Model Divergence\ndef prevent_model_divergence():\n    \"\"\"Prevent model divergence in mixed precision.\"\"\"\n\n    # Solution 1: Lower learning rate\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower LR\n\n    # Solution 2: Warmup schedule\n    scheduler = optim.get_cosine_schedule_with_warmup(\n        optimizer, num_warmup_steps=1000, num_training_steps=10000\n    )\n\n    # Solution 3: Monitor loss closely\n    def check_loss_stability(loss, loss_history):\n        loss_history.append(loss.item())\n\n        if len(loss_history) &gt; 100:\n            recent_losses = loss_history[-50:]\n            if any(l &gt; 10 * min(recent_losses) for l in recent_losses):\n                print(\"WARNING: Loss instability detected!\")\n                return False\n\n        return True\n\n# Issue 3: Accuracy Degradation\ndef maintain_accuracy():\n    \"\"\"Maintain model accuracy with mixed precision.\"\"\"\n\n    # Solution 1: Use BF16 instead of FP16\n    genesis.enable_autocast = True\n    default_dtype = genesis.bfloat16\n\n    # Solution 2: Keep critical layers in FP32\n    class MixedPrecisionModel(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.features = nn.Sequential(\n                nn.Linear(784, 256),  # FP16/BF16\n                nn.ReLU(),\n                nn.Linear(256, 128),  # FP16/BF16\n                nn.ReLU()\n            )\n\n            # Keep output layer in FP32 for stability\n            self.classifier = nn.Linear(128, 10).float()\n\n        def forward(self, x):\n            with genesis.autocast():\n                features = self.features(x)\n\n            # Output layer in FP32\n            output = self.classifier(features.float())\n            return output\n</code></pre>"},{"location":"tutorials/mixed-precision_en/#debugging-mixed-precision-training","title":"Debugging Mixed Precision Training","text":"Python<pre><code>def debug_mixed_precision():\n    \"\"\"Debug mixed precision training issues.\"\"\"\n\n    # 1. Check tensor dtypes throughout the model\n    def print_tensor_info(tensor, name):\n        print(f\"{name}:\")\n        print(f\"  Shape: {tensor.shape}\")\n        print(f\"  Dtype: {tensor.dtype}\")\n        print(f\"  Device: {tensor.device}\")\n        print(f\"  Requires grad: {tensor.requires_grad}\")\n        print(f\"  Min/Max: {tensor.min():.2e} / {tensor.max():.2e}\")\n        print()\n\n    # 2. Monitor gradient norms\n    def check_gradient_norms(model):\n        total_norm = 0.0\n        for name, param in model.named_parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.norm().item()\n                total_norm += grad_norm ** 2\n                print(f\"{name}: grad_norm = {grad_norm:.2e}\")\n\n        total_norm = total_norm ** 0.5\n        print(f\"Total gradient norm: {total_norm:.2e}\")\n        return total_norm\n\n    # 3. Validate numerical stability\n    def check_numerical_stability(tensor):\n        \"\"\"Check for numerical issues.\"\"\"\n        has_nan = genesis.isnan(tensor).any()\n        has_inf = genesis.isinf(tensor).any()\n\n        if has_nan:\n            print(\"WARNING: NaN values detected!\")\n        if has_inf:\n            print(\"WARNING: Inf values detected!\")\n\n        return not (has_nan or has_inf)\n\n# Usage in training loop\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(dataloader):\n        # Forward pass\n        outputs = model(data)\n        loss = criterion(outputs, targets)\n\n        # Debug information\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch}, Batch {batch_idx}:\")\n            print_tensor_info(data, \"Input\")\n            print_tensor_info(outputs, \"Output\") \n            print_tensor_info(loss, \"Loss\")\n\n            # Check gradients after backward pass\n            loss.backward()\n            grad_norm = check_gradient_norms(model)\n\n            if grad_norm &gt; 10.0:\n                print(\"WARNING: Large gradient norm detected!\")\n</code></pre> <p>This comprehensive guide covers all aspects of mixed precision training in Genesis, from basic usage to advanced optimization techniques and troubleshooting strategies.</p>"},{"location":"tutorials/performance-tuning.zh/","title":"\u6027\u80fd\u8c03\u4f18\u6307\u5357","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>\u672c\u6307\u5357\u5c06\u6559\u4f60\u5982\u4f55\u4f18\u5316Genesis\u6a21\u578b\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5305\u62ec\u5185\u5b58\u4f7f\u7528\u3001\u8ba1\u7b97\u6548\u7387\u548cI/O\u4f18\u5316\u7b49\u65b9\u9762\u3002</p>"},{"location":"tutorials/performance-tuning.zh/#_2","title":"\ud83c\udfaf \u4f18\u5316\u76ee\u6807","text":"<ul> <li>\u8bad\u7ec3\u901f\u5ea6: \u63d0\u9ad8\u6bcf\u79d2\u5904\u7406\u7684\u6837\u672c\u6570</li> <li>\u5185\u5b58\u6548\u7387: \u51cf\u5c11GPU\u663e\u5b58\u5360\u7528</li> <li>\u541e\u5410\u91cf: \u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387</li> </ul>"},{"location":"tutorials/performance-tuning.zh/#_3","title":"\ud83d\udcca \u6027\u80fd\u5206\u6790\u5de5\u5177","text":""},{"location":"tutorials/performance-tuning.zh/#_4","title":"\u5185\u7f6e\u6027\u80fd\u5206\u6790\u5668","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# WIP: \u6027\u80fd\u5206\u6790\u4ee3\u7801\u793a\u4f8b\nwith profiler.profile() as prof:\n    # \u8bad\u7ec3\u4ee3\u7801\n    pass\n\nprof.print_stats()\n</code></pre>"},{"location":"tutorials/performance-tuning.zh/#_5","title":"\u26a1 \u4f18\u5316\u7b56\u7565","text":""},{"location":"tutorials/performance-tuning.zh/#1","title":"1. \u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u68af\u5ea6\u7d2f\u79ef</li> <li>\u68c0\u67e5\u70b9\u6280\u672f</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> </ul>"},{"location":"tutorials/performance-tuning.zh/#2","title":"2. \u8ba1\u7b97\u4f18\u5316","text":"<ul> <li>\u7b97\u5b50\u878d\u5408</li> <li>Triton kernel\u4f18\u5316</li> <li>CUDA\u6d41\u91cd\u53e0</li> </ul>"},{"location":"tutorials/performance-tuning.zh/#3-io","title":"3. I/O\u4f18\u5316","text":"<ul> <li>\u6570\u636e\u9884\u53d6</li> <li>\u591a\u8fdb\u7a0b\u6570\u636e\u52a0\u8f7d</li> <li>\u5185\u5b58\u6620\u5c04</li> </ul>"},{"location":"tutorials/performance-tuning.zh/#_6","title":"\ud83d\udcc8 \u57fa\u51c6\u6d4b\u8bd5","text":"<ul> <li>\u4e0ePyTorch\u6027\u80fd\u5bf9\u6bd4</li> <li>\u4e0d\u540c\u914d\u7f6e\u7684\u6027\u80fd\u6d4b\u8bd5</li> <li>\u74f6\u9888\u8bc6\u522b\u65b9\u6cd5</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"},{"location":"tutorials/performance-tuning_en/","title":"Performance Tuning Guide","text":"<p>Under Development</p> <p>This document is being written and content will be continuously updated.</p> <p>This guide will teach you how to optimize Genesis model training performance, including memory usage, computational efficiency, and I/O optimization.</p>"},{"location":"tutorials/performance-tuning_en/#optimization-goals","title":"\ud83c\udfaf Optimization Goals","text":"<ul> <li>Training Speed: Increase samples processed per second</li> <li>Memory Efficiency: Reduce GPU memory usage</li> <li>Throughput: Maximize hardware utilization</li> </ul>"},{"location":"tutorials/performance-tuning_en/#performance-analysis-tools","title":"\ud83d\udcca Performance Analysis Tools","text":""},{"location":"tutorials/performance-tuning_en/#built-in-profiler","title":"Built-in Profiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# WIP: Performance analysis code example\nwith profiler.profile() as prof:\n    # Training code\n    pass\n\nprof.print_stats()\n</code></pre>"},{"location":"tutorials/performance-tuning_en/#optimization-strategies","title":"\u26a1 Optimization Strategies","text":""},{"location":"tutorials/performance-tuning_en/#1-memory-optimization","title":"1. Memory Optimization","text":"<ul> <li>Gradient accumulation</li> <li>Checkpoint techniques</li> <li>Mixed precision training</li> </ul>"},{"location":"tutorials/performance-tuning_en/#2-compute-optimization","title":"2. Compute Optimization","text":"<ul> <li>Operator fusion</li> <li>Triton kernel optimization</li> <li>CUDA stream overlap</li> </ul>"},{"location":"tutorials/performance-tuning_en/#3-io-optimization","title":"3. I/O Optimization","text":"<ul> <li>Data prefetching</li> <li>Multi-process data loading</li> <li>Memory mapping</li> </ul>"},{"location":"tutorials/performance-tuning_en/#benchmarking","title":"\ud83d\udcc8 Benchmarking","text":"<ul> <li>Performance comparison with PyTorch</li> <li>Performance testing with different configurations</li> <li>Bottleneck identification methods</li> </ul> <p>\ud83d\udcd8 Documentation Status: Under development, expected to be completed in v0.2.0.</p>"}]}