{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Genesis Deep Learning Framework","text":"\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6 | \u4ece\u96f6\u6784\u5efa | Python + Triton + CUDA"},{"location":"#_1","title":"\ud83d\ude80 \u9879\u76ee\u6982\u8ff0","text":"<p>Genesis\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528Python\u5f00\u53d1\u3002CPU\u540e\u7aef\u57fa\u4e8ePyTorch\u5f20\u91cf\u64cd\u4f5c\uff0cGPU\u540e\u7aef\u5219\u5b8c\u5168\u72ec\u7acb\u5b9e\u73b0\uff0c\u4f7f\u7528CUDA Python API\u76f4\u63a5\u7ba1\u7406GPU\u5185\u5b58\uff0c\u5e76\u901a\u8fc7Triton\u7f16\u5199\u9ad8\u6027\u80fd\u7684GPU kernels\u3002\u9879\u76ee\u65e8\u5728\u63d0\u4f9b\u6e05\u6670\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u6559\u80b2\u4ef7\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</p>"},{"location":"#_2","title":"\u2728 \u6838\u5fc3\u7279\u6027","text":"<ul> <li>\ud83c\udfaf \u8f7b\u91cf\u7ea7\u8bbe\u8ba1 - \u7b80\u6d01\u7684API\u8bbe\u8ba1\uff0c\u6613\u4e8e\u7406\u89e3\u548c\u4f7f\u7528</li> <li>\u26a1 \u9ad8\u6027\u80fd\u8ba1\u7b97 - Triton\u4f18\u5316\u7684GPU kernels\uff0c\u5ab2\u7f8e\u4e3b\u6d41\u6846\u67b6\u6027\u80fd</li> <li>\ud83d\udd04 \u81ea\u52a8\u5fae\u5206 - \u5b8c\u6574\u7684\u53cd\u5411\u4f20\u64ad\u548c\u68af\u5ea6\u8ba1\u7b97\u7cfb\u7edf</li> <li>\ud83e\udde0 \u795e\u7ecf\u7f51\u7edc - \u4e30\u5bcc\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u548c\u4f18\u5316\u5668\u5b9e\u73b0</li> <li>\ud83d\udd27 \u6df7\u5408\u7cbe\u5ea6 - \u652f\u6301FP16/BF16\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08AMP\uff09</li> <li>\ud83d\udcca \u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3\u652f\u6301</li> <li>\ud83c\udfa8 \u6a21\u578b\u5e93 - \u5185\u7f6eQwen\u7b49\u4e3b\u6d41LLM\u6a21\u578b\u5b9e\u73b0</li> <li>\ud83d\udcbe \u6a21\u578b\u7ba1\u7406 - \u5b8c\u6574\u7684\u68c0\u67e5\u70b9\u4fdd\u5b58\u548c\u52a0\u8f7d\u7cfb\u7edf</li> <li>\ud83d\udcc8 \u5b66\u4e60\u7387\u8c03\u5ea6 - \u591a\u79cd\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u548c\u68af\u5ea6\u88c1\u526a</li> </ul>"},{"location":"#_3","title":"\ud83c\udfd7\ufe0f \u67b6\u6784\u4eae\u70b9","text":"<pre><code>graph TB\n    A[\u7528\u6237API] --&gt; B[\u81ea\u52a8\u5fae\u5206\u5f15\u64ce]\n    A --&gt; C[\u795e\u7ecf\u7f51\u7edc\u6a21\u5757]\n    B --&gt; D[\u5f20\u91cf\u7cfb\u7edf]\n    C --&gt; D\n    D --&gt; E[\u540e\u7aef\u62bd\u8c61\u5c42]\n    E --&gt; F[CPU Backend]\n    E --&gt; G[CUDA Backend]\n    G --&gt; H[Triton Kernels]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e3f2fd\n    style H fill:#fff8e1</code></pre>"},{"location":"#_4","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":""},{"location":"#_5","title":"\u6559\u80b2\u4ef7\u503c","text":"<ul> <li>\u6e05\u6670\u7684\u4ee3\u7801\u7ed3\u6784 - \u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u660e\u786e\u7684\u804c\u8d23\u5206\u5de5</li> <li>\u8be6\u7ec6\u7684\u6587\u6863\u8bf4\u660e - \u4ece\u8bbe\u8ba1\u7406\u5ff5\u5230\u5b9e\u73b0\u7ec6\u8282\u7684\u5b8c\u6574\u6587\u6863</li> <li>\u6e10\u8fdb\u5f0f\u5b66\u4e60 - \u4ece\u57fa\u7840\u6982\u5ff5\u5230\u9ad8\u7ea7\u7279\u6027\u7684\u5b66\u4e60\u8def\u5f84</li> </ul>"},{"location":"#_6","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u73b0\u4ee3\u5316\u67b6\u6784 - \u501f\u9274PyTorch\u7b49\u4e3b\u6d41\u6846\u67b6\u7684\u4f18\u79c0\u8bbe\u8ba1</li> <li>\u9ad8\u6548\u5b9e\u73b0 - \u4f7f\u7528Triton\u7b49\u73b0\u4ee3\u5de5\u5177\u4f18\u5316\u6027\u80fd</li> <li>\u53ef\u6269\u5c55\u6027 - \u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6613\u4e8e\u6dfb\u52a0\u65b0\u529f\u80fd</li> </ul>"},{"location":"#_7","title":"\u5b9e\u7528\u6027","text":"<ul> <li>\u5b8c\u6574\u529f\u80fd - \u652f\u6301\u4ece\u6a21\u578b\u5b9a\u4e49\u5230\u8bad\u7ec3\u90e8\u7f72\u7684\u5b8c\u6574\u6d41\u7a0b</li> <li>\u6027\u80fd\u4f18\u5316 - \u591a\u79cd\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u7528\u7684\u8bad\u7ec3\u6027\u80fd</li> <li>\u751f\u6001\u517c\u5bb9 - \u4e0e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u826f\u597d\u517c\u5bb9</li> </ul>"},{"location":"#_8","title":"\ud83d\udcca \u6027\u80fd\u5bf9\u6bd4","text":"\u64cd\u4f5c\u7c7b\u578b Genesis PyTorch \u6027\u80fd\u6bd4\u4f8b \u77e9\u9635\u4e58\u6cd5 (4096\u00d74096) 2.1ms 2.0ms 95% Softmax (8192\u00d78192) 0.8ms 0.9ms 112% LayerNorm (4096\u00d74096) 0.5ms 0.6ms 120% \u6ce8\u610f\u529b\u673a\u5236 3.2ms 3.1ms 97% <p>\u6027\u80fd\u8bf4\u660e</p> <p>\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\uff1aNVIDIA A100 GPU, CUDA 11.8, \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6a21\u5f0f</p>"},{"location":"#_9","title":"\ud83d\udee0\ufe0f \u6280\u672f\u6808","text":""},{"location":"#_10","title":"\u6838\u5fc3\u4f9d\u8d56","text":"<ul> <li>Python 3.8+ - \u4e3b\u8981\u5f00\u53d1\u8bed\u8a00</li> <li>PyTorch - \u5185\u5b58\u7ba1\u7406\u548c\u90e8\u5206\u64cd\u4f5c</li> <li>Triton 2.0+ - GPU kernel\u4f18\u5316</li> <li>CUDA 11.0+ - GPU\u8ba1\u7b97\u652f\u6301</li> <li>NumPy - CPU\u6570\u503c\u8ba1\u7b97</li> </ul>"},{"location":"#_11","title":"\u5f00\u53d1\u5de5\u5177","text":"<ul> <li>pytest - \u5355\u5143\u6d4b\u8bd5\u6846\u67b6</li> <li>black - \u4ee3\u7801\u683c\u5f0f\u5316</li> <li>mypy - \u7c7b\u578b\u68c0\u67e5</li> <li>MkDocs - \u6587\u6863\u751f\u6210</li> </ul>"},{"location":"#_12","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84","text":""},{"location":"#_13","title":"\u521d\u5b66\u8005","text":"<ol> <li>\u5feb\u901f\u5f00\u59cb - \u5b89\u88c5\u548c\u7b2c\u4e00\u4e2a\u7a0b\u5e8f</li> <li>\u57fa\u7840\u6559\u7a0b - \u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3</li> <li>API\u53c2\u8003 - \u5e38\u7528API\u4f7f\u7528\u65b9\u6cd5</li> </ol>"},{"location":"#_14","title":"\u8fdb\u9636\u7528\u6237","text":"<ol> <li>\u67b6\u6784\u8bbe\u8ba1 - \u6df1\u5165\u7406\u89e3\u7cfb\u7edf\u8bbe\u8ba1</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u64cd\u4f5c</li> <li>\u6027\u80fd\u4f18\u5316 - \u8bad\u7ec3\u6027\u80fd\u8c03\u4f18</li> </ol>"},{"location":"#_15","title":"\u8d21\u732e\u8005","text":"<ol> <li>\u5f00\u53d1\u73af\u5883 - \u642d\u5efa\u5f00\u53d1\u73af\u5883</li> <li>\u6838\u5fc3\u7ec4\u4ef6 - \u7406\u89e3\u5185\u90e8\u5b9e\u73b0</li> <li>\u6d4b\u8bd5\u89c4\u8303 - \u4ee3\u7801\u8d21\u732e\u6307\u5357</li> </ol>"},{"location":"#_16","title":"\ud83c\udf1f \u9879\u76ee\u4eae\u70b9","text":""},{"location":"#_17","title":"\u4ee3\u7801\u8d28\u91cf","text":"<ul> <li>\u7c7b\u578b\u6ce8\u89e3 - \u5b8c\u6574\u7684\u7c7b\u578b\u63d0\u793a\uff0cIDE\u53cb\u597d</li> <li>\u5355\u5143\u6d4b\u8bd5 - 95%+\u6d4b\u8bd5\u8986\u76d6\u7387</li> <li>\u6587\u6863\u5b8c\u6574 - \u4eceAPI\u5230\u8bbe\u8ba1\u7684\u5168\u9762\u6587\u6863</li> <li>\u4ee3\u7801\u89c4\u8303 - \u7edf\u4e00\u7684\u4ee3\u7801\u98ce\u683c\u548c\u6700\u4f73\u5b9e\u8df5</li> </ul>"},{"location":"#_18","title":"\u521b\u65b0\u7279\u6027","text":"<ul> <li>\u5185\u5b58\u4f18\u5316 - \u667a\u80fd\u7684\u5185\u5b58\u7ba1\u7406\u548c\u7f13\u5b58\u7b56\u7565</li> <li>\u52a8\u6001\u56fe\u4f18\u5316 - \u9ad8\u6548\u7684\u8ba1\u7b97\u56fe\u6784\u5efa\u548c\u6267\u884c</li> <li>\u6a21\u5757\u5316\u8bbe\u8ba1 - \u6e05\u6670\u7684\u6a21\u5757\u8fb9\u754c\u548c\u63a5\u53e3\u5b9a\u4e49</li> </ul>"},{"location":"#_19","title":"\ud83e\udd1d \u793e\u533a\u4e0e\u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u5404\u79cd\u5f62\u5f0f\u7684\u8d21\u732e\uff1a</p> <ul> <li>\ud83d\udc1b \u95ee\u9898\u62a5\u544a - \u53d1\u73b0bug\u8bf7\u53ca\u65f6\u53cd\u9988</li> <li>\ud83d\udca1 \u529f\u80fd\u5efa\u8bae - \u6b22\u8fce\u63d0\u51fa\u65b0\u529f\u80fd\u60f3\u6cd5</li> <li>\ud83d\udcdd \u6587\u6863\u6539\u8fdb - \u5e2e\u52a9\u5b8c\u5584\u6587\u6863\u5185\u5bb9</li> <li>\ud83d\udcbb \u4ee3\u7801\u8d21\u732e - \u76f4\u63a5\u53c2\u4e0e\u4ee3\u7801\u5f00\u53d1</li> </ul> <p>\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u8003\u8d21\u732e\u6307\u5357\u3002</p>"},{"location":"#_20","title":"\ud83d\udcde \u8054\u7cfb\u6211\u4eec","text":"<ul> <li>GitHub Issues - \u95ee\u9898\u62a5\u544a\u548c\u529f\u80fd\u8bf7\u6c42</li> <li>Discussions - \u6280\u672f\u8ba8\u8bba\u548c\u4f7f\u7528\u4ea4\u6d41</li> <li>Email - genesis-dev@example.com</li> </ul> <p>\u5f00\u59cb\u4f60\u7684\u6df1\u5ea6\u5b66\u4e60\u4e4b\u65c5 \ud83d\ude80</p> <ul> <li> <p> \u5feb\u901f\u5f00\u59cb</p> <p>\u7acb\u5373\u5f00\u59cb\u4f7f\u7528Genesis\u6784\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc</p> <p> \u5feb\u901f\u5f00\u59cb</p> </li> <li> <p> \u67e5\u770b\u6e90\u7801</p> <p>\u5728GitHub\u4e0a\u63a2\u7d22Genesis\u7684\u5b8c\u6574\u6e90\u4ee3\u7801\u5b9e\u73b0</p> <p> GitHub\u4ed3\u5e93</p> </li> </ul>"},{"location":"api/autograd/","title":"\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf (genesis.autograd)","text":"<p>Genesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u662f\u6846\u67b6\u7684\u6838\u5fc3\uff0c\u63d0\u4f9b\u4e86\u5f20\u91cf\u64cd\u4f5c\u548c\u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97\u529f\u80fd\u3002</p>"},{"location":"api/autograd/#_1","title":"\u6a21\u5757\u6982\u8ff0","text":"<p><code>genesis.autograd</code>\u6a21\u5757\u5b9e\u73b0\u4e86\u52a8\u6001\u8ba1\u7b97\u56fe\u548c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u652f\u6301\uff1a - \u81ea\u52a8\u68af\u5ea6\u8ba1\u7b97 - \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u68af\u5ea6\u94a9\u5b50\u548c\u7d2f\u79ef - \u8ba1\u7b97\u56fe\u6784\u5efa\u548c\u4f18\u5316</p>"},{"location":"api/autograd/#_2","title":"\u6838\u5fc3\u7c7b","text":""},{"location":"api/autograd/#tensor","title":"Tensor","text":"<p>Genesis\u6846\u67b6\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u81ea\u52a8\u5fae\u5206\u7684\u591a\u7ef4\u6570\u7ec4\u3002</p> Python<pre><code>class Tensor:\n    def __init__(self, array, device=None, dtype=None, requires_grad=False, **kwargs)\n</code></pre>"},{"location":"api/autograd/#_3","title":"\u53c2\u6570","text":"<ul> <li><code>array</code>: array-like - \u8f93\u5165\u6570\u636e\uff0c\u53ef\u4ee5\u662flist\u3001numpy\u6570\u7ec4\u6216NDArray</li> <li><code>device</code>: Device, optional - \u8bbe\u5907\u5bf9\u8c61(cpu/cuda)\uff0c\u9ed8\u8ba4\u4f7f\u7528default_device</li> <li><code>dtype</code>: DType, optional - \u6570\u636e\u7c7b\u578b\uff0c\u9ed8\u8ba4\u4ece\u8f93\u5165\u6570\u636e\u63a8\u65ad</li> <li><code>requires_grad</code>: bool - \u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\uff0c\u9ed8\u8ba4False</li> <li><code>**kwargs</code>: \u5176\u4ed6NDArray\u6784\u9020\u53c2\u6570</li> </ul>"},{"location":"api/autograd/#_4","title":"\u5c5e\u6027","text":""},{"location":"api/autograd/#_5","title":"\u57fa\u7840\u5c5e\u6027","text":"Python<pre><code>@property\ndef shape(self) -&gt; Tuple[int, ...]:\n    \"\"\"\u8fd4\u56de\u5f20\u91cf\u5f62\u72b6\"\"\"\n\n@property\ndef dtype(self) -&gt; DType:\n    \"\"\"\u8fd4\u56de\u6570\u636e\u7c7b\u578b\"\"\"\n\n@property\ndef device(self) -&gt; Device:\n    \"\"\"\u8fd4\u56de\u8bbe\u5907\u5bf9\u8c61\"\"\"\n\n@property\ndef ndim(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u5f20\u91cf\u7ef4\u5ea6\u6570\"\"\"\n\n@property\ndef size(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u5f20\u91cf\u5143\u7d20\u603b\u6570\"\"\"\n</code></pre>"},{"location":"api/autograd/#_6","title":"\u68af\u5ea6\u76f8\u5173\u5c5e\u6027","text":"Python<pre><code>@property\ndef requires_grad(self) -&gt; bool:\n    \"\"\"\u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\"\"\"\n\n@property\ndef grad(self) -&gt; Optional[Tensor]:\n    \"\"\"\u8bbf\u95ee\u68af\u5ea6\u5f20\u91cf\"\"\"\n\n@grad.setter\ndef grad(self, value: Optional[Tensor]):\n    \"\"\"\u8bbe\u7f6e\u68af\u5ea6\u5f20\u91cf\"\"\"\n\n@property\ndef is_leaf(self) -&gt; bool:\n    \"\"\"\u662f\u5426\u4e3a\u53f6\u5b50\u8282\u70b9\uff08\u7528\u6237\u521b\u5efa\u7684\u5f20\u91cf\uff09\"\"\"\n</code></pre>"},{"location":"api/autograd/#_7","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"api/autograd/#_8","title":"\u68af\u5ea6\u64cd\u4f5c","text":"Python<pre><code>def backward(self, gradient=None):\n    \"\"\"\n    \u6267\u884c\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6240\u6709\u9700\u8981\u68af\u5ea6\u7684\u5f20\u91cf\u7684\u68af\u5ea6\n\n    \u53c2\u6570:\n        gradient: Tensor, optional - \u8f93\u51fa\u68af\u5ea6\uff0c\u9ed8\u8ba4\u4e3a1.0\uff08\u6807\u91cf\u60c5\u51b5\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2., 3.], requires_grad=True)\n        &gt;&gt;&gt; y = (x ** 2).sum()\n        &gt;&gt;&gt; y.backward()\n        &gt;&gt;&gt; print(x.grad)  # tensor([2., 4., 6.])\n    \"\"\"\n\ndef detach(self) -&gt; Tensor:\n    \"\"\"\n    \u8fd4\u56de\u4e00\u4e2a\u65b0\u5f20\u91cf\uff0c\u4e0e\u8ba1\u7b97\u56fe\u5206\u79bb\n\n    \u8fd4\u56de:\n        Tensor - \u5206\u79bb\u7684\u5f20\u91cf\uff0crequires_grad=False\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.], requires_grad=True)\n        &gt;&gt;&gt; y = x.detach()\n        &gt;&gt;&gt; print(y.requires_grad)  # False\n    \"\"\"\n\ndef requires_grad_(self, requires_grad=True) -&gt; Tensor:\n    \"\"\"\n    \u539f\u5730\u4fee\u6539requires_grad\u5c5e\u6027\n\n    \u53c2\u6570:\n        requires_grad: bool - \u662f\u5426\u9700\u8981\u68af\u5ea6\n\n    \u8fd4\u56de:\n        self - \u8fd4\u56de\u81ea\u8eab\u4ee5\u652f\u6301\u94fe\u5f0f\u8c03\u7528\n    \"\"\"\n\ndef zero_grad(self):\n    \"\"\"\u6e05\u96f6\u68af\u5ea6\"\"\"\n\ndef register_hook(self, hook):\n    \"\"\"\n    \u6ce8\u518c\u68af\u5ea6\u94a9\u5b50\u51fd\u6570\n\n    \u53c2\u6570:\n        hook: callable - \u94a9\u5b50\u51fd\u6570\uff0c\u63a5\u6536\u68af\u5ea6\u4f5c\u4e3a\u53c2\u6570\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; def print_grad(grad):\n        ...     print(f\"Gradient: {grad}\")\n        &gt;&gt;&gt; x.register_hook(print_grad)\n    \"\"\"\n</code></pre>"},{"location":"api/autograd/#_9","title":"\u8bbe\u5907\u548c\u7c7b\u578b\u8f6c\u6362","text":"Python<pre><code>def cpu(self) -&gt; Tensor:\n    \"\"\"\u5c06\u5f20\u91cf\u79fb\u5230CPU\"\"\"\n\ndef cuda(self, device_id=0) -&gt; Tensor:\n    \"\"\"\n    \u5c06\u5f20\u91cf\u79fb\u5230CUDA\u8bbe\u5907\n\n    \u53c2\u6570:\n        device_id: int - GPU\u8bbe\u5907ID\n    \"\"\"\n\ndef to(self, device=None, dtype=None) -&gt; Tensor:\n    \"\"\"\n    \u8f6c\u6362\u8bbe\u5907\u6216\u6570\u636e\u7c7b\u578b\n\n    \u53c2\u6570:\n        device: Device, optional - \u76ee\u6807\u8bbe\u5907\n        dtype: DType, optional - \u76ee\u6807\u6570\u636e\u7c7b\u578b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1, 2, 3])\n        &gt;&gt;&gt; x_gpu = x.to(genesis.cuda())\n        &gt;&gt;&gt; x_fp16 = x.to(dtype=genesis.float16)\n    \"\"\"\n\ndef float(self) -&gt; Tensor:\n    \"\"\"\u8f6c\u6362\u4e3afloat32\u7c7b\u578b\"\"\"\n\ndef half(self) -&gt; Tensor:\n    \"\"\"\u8f6c\u6362\u4e3afloat16\u7c7b\u578b\"\"\"\n\ndef long(self) -&gt; Tensor:\n    \"\"\"\u8f6c\u6362\u4e3aint64\u7c7b\u578b\"\"\"\n\ndef int(self) -&gt; Tensor:\n    \"\"\"\u8f6c\u6362\u4e3aint32\u7c7b\u578b\"\"\"\n\ndef bool(self) -&gt; Tensor:\n    \"\"\"\u8f6c\u6362\u4e3abool\u7c7b\u578b\"\"\"\n</code></pre>"},{"location":"api/autograd/#_10","title":"\u5f62\u72b6\u64cd\u4f5c","text":"Python<pre><code>def reshape(self, *shape) -&gt; Tensor:\n    \"\"\"\n    \u6539\u53d8\u5f20\u91cf\u5f62\u72b6\n\n    \u53c2\u6570:\n        *shape: int - \u65b0\u5f62\u72b6\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u6539\u53d8\u5f62\u72b6\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([[1, 2], [3, 4]])\n        &gt;&gt;&gt; y = x.reshape(4)  # [1, 2, 3, 4]\n        &gt;&gt;&gt; z = x.reshape(1, 4)  # [[1, 2, 3, 4]]\n    \"\"\"\n\ndef view(self, *shape) -&gt; Tensor:\n    \"\"\"reshape\u7684\u522b\u540d\uff0c\u4e0ePyTorch\u517c\u5bb9\"\"\"\n\ndef transpose(self, dim0=None, dim1=None) -&gt; Tensor:\n    \"\"\"\n    \u8f6c\u7f6e\u5f20\u91cf\n\n    \u53c2\u6570:\n        dim0, dim1: int, optional - \u8981\u4ea4\u6362\u7684\u7ef4\u5ea6\uff0c\u9ed8\u8ba4\u8f6c\u7f6e\u6700\u540e\u4e24\u7ef4\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y = x.transpose(0, 2)  # shape: (4, 3, 2)\n    \"\"\"\n\ndef permute(self, *dims) -&gt; Tensor:\n    \"\"\"\n    \u6309\u6307\u5b9a\u987a\u5e8f\u91cd\u6392\u7ef4\u5ea6\n\n    \u53c2\u6570:\n        *dims: int - \u65b0\u7684\u7ef4\u5ea6\u987a\u5e8f\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y = x.permute(2, 0, 1)  # shape: (4, 2, 3)\n    \"\"\"\n\ndef squeeze(self, dim=None) -&gt; Tensor:\n    \"\"\"\u79fb\u9664\u5927\u5c0f\u4e3a1\u7684\u7ef4\u5ea6\"\"\"\n\ndef unsqueeze(self, dim) -&gt; Tensor:\n    \"\"\"\u5728\u6307\u5b9a\u4f4d\u7f6e\u63d2\u5165\u5927\u5c0f\u4e3a1\u7684\u7ef4\u5ea6\"\"\"\n\ndef expand(self, *shape) -&gt; Tensor:\n    \"\"\"\u6269\u5c55\u5f20\u91cf\u5230\u65b0\u5f62\u72b6\uff08\u901a\u8fc7\u5e7f\u64ad\uff09\"\"\"\n</code></pre>"},{"location":"api/autograd/#_11","title":"\u6570\u5b66\u8fd0\u7b97","text":"Python<pre><code># \u7b97\u672f\u8fd0\u7b97\ndef __add__(self, other) -&gt; Tensor\ndef __sub__(self, other) -&gt; Tensor  \ndef __mul__(self, other) -&gt; Tensor\ndef __truediv__(self, other) -&gt; Tensor\ndef __pow__(self, other) -&gt; Tensor\ndef __matmul__(self, other) -&gt; Tensor\n\n# \u539f\u5730\u8fd0\u7b97\ndef add_(self, other) -&gt; Tensor\ndef sub_(self, other) -&gt; Tensor\ndef mul_(self, other) -&gt; Tensor\ndef div_(self, other) -&gt; Tensor\n\n# \u805a\u5408\u8fd0\u7b97\ndef sum(self, axis=None, keepdims=False) -&gt; Tensor:\n    \"\"\"\n    \u6c42\u548c\n\n    \u53c2\u6570:\n        axis: int or tuple, optional - \u6c42\u548c\u7684\u8f74\n        keepdims: bool - \u662f\u5426\u4fdd\u6301\u7ef4\u5ea6\n    \"\"\"\n\ndef mean(self, axis=None, keepdims=False) -&gt; Tensor:\n    \"\"\"\u6c42\u5e73\u5747\u503c\"\"\"\n\ndef max(self, axis=None, keepdims=False) -&gt; Tensor:\n    \"\"\"\u6c42\u6700\u5927\u503c\"\"\"\n\ndef min(self, axis=None, keepdims=False) -&gt; Tensor:\n    \"\"\"\u6c42\u6700\u5c0f\u503c\"\"\"\n\n# \u6570\u5b66\u51fd\u6570\ndef exp(self) -&gt; Tensor\ndef log(self) -&gt; Tensor\ndef sqrt(self) -&gt; Tensor\ndef sin(self) -&gt; Tensor\ndef cos(self) -&gt; Tensor\ndef tanh(self) -&gt; Tensor\ndef sigmoid(self) -&gt; Tensor\ndef relu(self) -&gt; Tensor\n</code></pre>"},{"location":"api/autograd/#_12","title":"\u7d22\u5f15\u548c\u5207\u7247","text":"Python<pre><code>def __getitem__(self, key) -&gt; Tensor:\n    \"\"\"\n    \u5f20\u91cf\u7d22\u5f15\u548c\u5207\u7247\n\n    \u53c2\u6570:\n        key: int, slice, tuple - \u7d22\u5f15\u952e\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y = x[0]  # \u83b7\u53d6\u7b2c\u4e00\u4e2a\u5143\u7d20\n        &gt;&gt;&gt; z = x[:, 1:3, :]  # \u5207\u7247\u64cd\u4f5c\n        &gt;&gt;&gt; w = x[..., -1]  # \u4f7f\u7528\u7701\u7565\u53f7\n    \"\"\"\n\ndef __setitem__(self, key, value):\n    \"\"\"\u5f20\u91cf\u8d4b\u503c\"\"\"\n</code></pre>"},{"location":"api/autograd/#_13","title":"\u5b9e\u7528\u65b9\u6cd5","text":"Python<pre><code>def item(self) -&gt; float:\n    \"\"\"\u8fd4\u56de\u6807\u91cf\u5f20\u91cf\u7684Python\u6570\u503c\"\"\"\n\ndef numpy(self) -&gt; numpy.ndarray:\n    \"\"\"\u8f6c\u6362\u4e3anumpy\u6570\u7ec4\"\"\"\n\ndef contiguous(self) -&gt; Tensor:\n    \"\"\"\u8fd4\u56de\u5185\u5b58\u8fde\u7eed\u7684\u5f20\u91cf\"\"\"\n\ndef clone(self) -&gt; Tensor:\n    \"\"\"\u6df1\u62f7\u8d1d\u5f20\u91cf\"\"\"\n\ndef data_ptr(self) -&gt; int:\n    \"\"\"\u8fd4\u56de\u6570\u636e\u6307\u9488\uff08\u7528\u4e8eTriton\uff09\"\"\"\n\ndef stride(self, dim=None):\n    \"\"\"\u8fd4\u56de\u6b65\u957f\u4fe1\u606f\"\"\"\n\ndef is_contiguous(self) -&gt; bool:\n    \"\"\"\u68c0\u67e5\u662f\u5426\u5185\u5b58\u8fde\u7eed\"\"\"\n</code></pre>"},{"location":"api/autograd/#function","title":"Function","text":"<p>\u6240\u6709\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\u3002</p> Python<pre><code>class Function:\n    \"\"\"\u81ea\u5b9a\u4e49\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\"\"\"\n\n    @staticmethod\n    def forward(ctx: Context, *args, **kwargs):\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\u8ba1\u7b97\n\n        \u53c2\u6570:\n            ctx: Context - \u7528\u4e8e\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c\u7684\u4e0a\u4e0b\u6587\n            *args: \u8f93\u5165\u53c2\u6570\n            **kwargs: \u5173\u952e\u5b57\u53c2\u6570\n\n        \u8fd4\u56de:\n            \u8f93\u51fa\u5f20\u91cf\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def backward(ctx: Context, *grad_outputs):\n        \"\"\"\n        \u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\n\n        \u53c2\u6570:\n            ctx: Context - \u524d\u5411\u4f20\u64ad\u4fdd\u5b58\u7684\u4e0a\u4e0b\u6587\n            *grad_outputs: \u8f93\u51fa\u68af\u5ea6\n\n        \u8fd4\u56de:\n            tuple - \u5bf9\u5e94\u6bcf\u4e2a\u8f93\u5165\u7684\u68af\u5ea6\n        \"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def apply(cls, *args, **kwargs):\n        \"\"\"\n        \u5e94\u7528\u51fd\u6570\uff0c\u81ea\u52a8\u5904\u7406\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; class Exp(Function):\n            ...     @staticmethod\n            ...     def forward(ctx, x):\n            ...         y = x.exp()\n            ...         ctx.save_for_backward(y)\n            ...         return y\n            ...     \n            ...     @staticmethod\n            ...     def backward(ctx, grad_output):\n            ...         y, = ctx.saved_tensors\n            ...         return grad_output * y\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; y = Exp.apply(x)\n        \"\"\"\n</code></pre>"},{"location":"api/autograd/#context","title":"Context","text":"<p>\u7528\u4e8e\u5728\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u4e4b\u95f4\u4f20\u9012\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u5bf9\u8c61\u3002</p> Python<pre><code>class Context:\n    \"\"\"\u64cd\u4f5c\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c\"\"\"\n\n    def save_for_backward(self, *tensors):\n        \"\"\"\n        \u4fdd\u5b58\u5f20\u91cf\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\n\n        \u53c2\u6570:\n            *tensors: Tensor - \u9700\u8981\u4fdd\u5b58\u7684\u5f20\u91cf\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; def forward(ctx, x, y):\n            ...     z = x * y\n            ...     ctx.save_for_backward(x, y)\n            ...     return z\n        \"\"\"\n\n    @property\n    def saved_tensors(self) -&gt; List[Tensor]:\n        \"\"\"\u83b7\u53d6\u4fdd\u5b58\u7684\u5f20\u91cf\u5217\u8868\"\"\"\n</code></pre>"},{"location":"api/autograd/#_14","title":"\u5168\u5c40\u51fd\u6570","text":""},{"location":"api/autograd/#_15","title":"\u68af\u5ea6\u7ba1\u7406","text":"Python<pre><code>@contextmanager\ndef no_grad():\n    \"\"\"\n    \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u7981\u7528\u68af\u5ea6\u8ba1\u7b97\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; with genesis.no_grad():\n        ...     y = x * 2  # \u4e0d\u4f1a\u6784\u5efa\u8ba1\u7b97\u56fe\n    \"\"\"\n\n@contextmanager\ndef enable_grad():\n    \"\"\"\n    \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u542f\u7528\u68af\u5ea6\u8ba1\u7b97\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; with genesis.enable_grad():\n        ...     y = x * 2  # \u4f1a\u6784\u5efa\u8ba1\u7b97\u56fe\n    \"\"\"\n\ndef set_grad_enabled(enabled: bool):\n    \"\"\"\n    \u8bbe\u7f6e\u68af\u5ea6\u8ba1\u7b97\u5f00\u5173\n\n    \u53c2\u6570:\n        enabled: bool - \u662f\u5426\u542f\u7528\u68af\u5ea6\u8ba1\u7b97\n    \"\"\"\n</code></pre>"},{"location":"api/autograd/#_16","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"api/autograd/#_17","title":"\u57fa\u7840\u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([[1., 2.], [3., 4.]], requires_grad=True)\ny = genesis.tensor([[2., 0.], [0., 2.]], requires_grad=True)\n\n# \u524d\u5411\u8ba1\u7b97\nz = genesis.matmul(x, y)\nloss = z.sum()\n\n# \u53cd\u5411\u4f20\u64ad\nloss.backward()\n\nprint(x.grad)  # \u68af\u5ea6: [[2., 2.], [2., 2.]]\nprint(y.grad)  # \u68af\u5ea6: [[4., 4.], [6., 6.]]\n</code></pre>"},{"location":"api/autograd/#function_1","title":"\u81ea\u5b9a\u4e49Function","text":"Python<pre><code>class CustomReLU(genesis.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return genesis.maximum(x, 0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, = ctx.saved_tensors\n        grad_input = grad_output * (x &gt; 0)\n        return grad_input\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\nx = genesis.randn(10, requires_grad=True)\ny = CustomReLU.apply(x)\ny.sum().backward()\n</code></pre>"},{"location":"api/autograd/#_18","title":"\u68af\u5ea6\u94a9\u5b50","text":"Python<pre><code>def grad_hook(grad):\n    print(f\"Gradient norm: {grad.norm()}\")\n    return grad * 0.1  # \u7f29\u653e\u68af\u5ea6\n\nx = genesis.randn(10, requires_grad=True)\nx.register_hook(grad_hook)\n\ny = (x ** 2).sum()\ny.backward()  # \u4f1a\u6253\u5370\u68af\u5ea6\u8303\u6570\u5e76\u7f29\u653e\u68af\u5ea6\n</code></pre>"},{"location":"api/autograd/#_19","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code>genesis.enable_autocast = True\n\nwith genesis.autocast():\n    # \u81ea\u52a8\u8f6c\u6362\u4e3aFP16\u8fdb\u884c\u8ba1\u7b97\n    x = genesis.randn(1000, 1000)\n    y = genesis.matmul(x, x)\n    loss = y.mean()\n\nloss.backward()  # \u68af\u5ea6\u8ba1\u7b97\u81ea\u52a8\u5904\u7406\u7cbe\u5ea6\u8f6c\u6362\n</code></pre>"},{"location":"api/autograd/#_20","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":"<ol> <li>\u4f7f\u7528no_grad\u8fdb\u884c\u63a8\u7406\uff1a\u5728\u4e0d\u9700\u8981\u68af\u5ea6\u7684\u573a\u666f\u4e0b\u4f7f\u7528<code>no_grad()</code>\u51cf\u5c11\u5185\u5b58\u6d88\u8017</li> <li>\u53ca\u65f6\u6e05\u7406\u68af\u5ea6\uff1a\u4f7f\u7528<code>zero_grad()</code>\u6e05\u7406\u4e0d\u9700\u8981\u7684\u68af\u5ea6</li> <li>\u4f7f\u7528detach\u5206\u79bb\u8ba1\u7b97\u56fe\uff1a\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u68af\u5ea6\u4f20\u64ad</li> <li>\u5229\u7528\u539f\u5730\u64cd\u4f5c\uff1a\u4f7f\u7528<code>add_()</code>, <code>mul_()</code>\u7b49\u539f\u5730\u64cd\u4f5c\u51cf\u5c11\u5185\u5b58\u5206\u914d</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff1a\u542f\u7528autocast\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6</li> </ol>"},{"location":"api/autograd/#_21","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u5f20\u91cf\u7684<code>requires_grad</code>\u5c5e\u6027\u53ea\u80fd\u5728\u53f6\u5b50\u8282\u70b9\u4e0a\u8bbe\u7f6e</li> <li>\u539f\u5730\u64cd\u4f5c\u53ef\u80fd\u4f1a\u7834\u574f\u8ba1\u7b97\u56fe\uff0c\u8c28\u614e\u4f7f\u7528</li> <li>\u68af\u5ea6\u9ed8\u8ba4\u4f1a\u7d2f\u79ef\uff0c\u9700\u8981\u624b\u52a8\u6e05\u96f6</li> <li>\u89c6\u56fe\u64cd\u4f5c\uff08reshape, transpose\u7b49\uff09\u5171\u4eab\u5185\u5b58\uff0c\u4fee\u6539\u4f1a\u5f71\u54cd\u539f\u5f20\u91cf</li> </ul>"},{"location":"api/nn/functional/","title":"\u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3 (genesis.nn.functional)","text":"<p>Genesis\u7684\u51fd\u6570\u5f0f\u63a5\u53e3\u63d0\u4f9b\u4e86\u65e0\u72b6\u6001\u7684\u5f20\u91cf\u64cd\u4f5c\u51fd\u6570\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u8c03\u7528\u800c\u65e0\u9700\u521b\u5efa\u6a21\u5757\u5b9e\u4f8b\u3002</p>"},{"location":"api/nn/functional/#_1","title":"\u6a21\u5757\u6982\u8ff0","text":"<p><code>genesis.nn.functional</code>\uff08\u901a\u5e38\u5bfc\u5165\u4e3a<code>F</code>\uff09\u5305\u542b\uff1a - \u6fc0\u6d3b\u51fd\u6570\uff08relu\u3001sigmoid\u3001softmax\u7b49\uff09 - \u635f\u5931\u51fd\u6570\uff08cross_entropy\u3001mse_loss\u7b49\uff09 - \u5f20\u91cf\u64cd\u4f5c\uff08matmul\u3001transpose\u3001reshape\u7b49\uff09 - \u5f52\u4e00\u5316\u51fd\u6570\uff08layer_norm\u3001batch_norm\u7b49\uff09 - \u6ce8\u610f\u529b\u673a\u5236\uff08scaled_dot_product_attention\u7b49\uff09</p>"},{"location":"api/nn/functional/#_2","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"api/nn/functional/#relu","title":"relu","text":"Python<pre><code>def relu(x: Tensor, inplace: bool = False) -&gt; Tensor:\n    \"\"\"\n    ReLU\u6fc0\u6d3b\u51fd\u6570: f(x) = max(0, x)\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        inplace: bool - \u662f\u5426\u539f\u5730\u64cd\u4f5c\n\n    \u8fd4\u56de:\n        Tensor - \u6fc0\u6d3b\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = F.relu(x)\n        &gt;&gt;&gt; # \u539f\u5730\u64cd\u4f5c\n        &gt;&gt;&gt; F.relu(x, inplace=True)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#sigmoid","title":"sigmoid","text":"Python<pre><code>def sigmoid(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Sigmoid\u6fc0\u6d3b\u51fd\u6570: f(x) = 1 / (1 + exp(-x))\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u8303\u56f4\u5728(0, 1)\u7684\u8f93\u51fa\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = F.sigmoid(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#tanh","title":"tanh","text":"Python<pre><code>def tanh(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u53cc\u66f2\u6b63\u5207\u6fc0\u6d3b\u51fd\u6570: f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u8303\u56f4\u5728(-1, 1)\u7684\u8f93\u51fa\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = F.tanh(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#softmax","title":"softmax","text":"Python<pre><code>def softmax(x: Tensor, dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    Softmax\u51fd\u6570\uff0c\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u6982\u7387\u5206\u5e03\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int - \u8ba1\u7b97softmax\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u6982\u7387\u5206\u5e03\uff0c\u5728\u6307\u5b9a\u7ef4\u5ea6\u4e0a\u548c\u4e3a1\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)\n        &gt;&gt;&gt; probs = F.softmax(logits, dim=-1)\n        &gt;&gt;&gt; print(probs.sum(dim=-1))  # \u5168\u4e3a1\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#log_softmax","title":"log_softmax","text":"Python<pre><code>def log_softmax(x: Tensor, dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    Log-Softmax\u51fd\u6570: log(softmax(x))\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int - \u8ba1\u7b97\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - log\u6982\u7387\n\n    \u6ce8\u610f: \u6570\u503c\u4e0a\u6bd4\u5148softmax\u518dlog\u66f4\u7a33\u5b9a\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)\n        &gt;&gt;&gt; log_probs = F.log_softmax(logits, dim=-1)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#silu","title":"silu","text":"Python<pre><code>def silu(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    SiLU/Swish\u6fc0\u6d3b\u51fd\u6570: f(x) = x * sigmoid(x)\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u6fc0\u6d3b\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = F.silu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#gelu","title":"gelu","text":"Python<pre><code>def gelu(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    GELU\u6fc0\u6d3b\u51fd\u6570: f(x) = x * \u03a6(x)\n    \u5176\u4e2d\u03a6(x)\u662f\u6807\u51c6\u6b63\u6001\u5206\u5e03\u7684\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u6fc0\u6d3b\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = F.gelu(x)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_3","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"api/nn/functional/#cross_entropy","title":"cross_entropy","text":"Python<pre><code>def cross_entropy(input: Tensor, target: Tensor, \n                  reduction: str = 'mean') -&gt; Tensor:\n    \"\"\"\n    \u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\n\n    \u53c2\u6570:\n        input: Tensor - \u9884\u6d4blogits\uff0cshape: (N, C)\n        target: Tensor - \u76ee\u6807\u6807\u7b7e\uff0cshape: (N,) \u6216 (N, C)\n        reduction: str - 'none'|'mean'|'sum'\n\n    \u8fd4\u56de:\n        Tensor - \u635f\u5931\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; logits = genesis.randn(32, 10)  # 32\u4e2a\u6837\u672c\uff0c10\u4e2a\u7c7b\u522b\n        &gt;&gt;&gt; labels = genesis.randint(0, 10, (32,))\n        &gt;&gt;&gt; loss = F.cross_entropy(logits, labels)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#mse_loss","title":"mse_loss","text":"Python<pre><code>def mse_loss(input: Tensor, target: Tensor, \n             reduction: str = 'mean') -&gt; Tensor:\n    \"\"\"\n    \u5747\u65b9\u8bef\u5dee\u635f\u5931\u51fd\u6570\n\n    \u53c2\u6570:\n        input: Tensor - \u9884\u6d4b\u503c\n        target: Tensor - \u76ee\u6807\u503c\n        reduction: str - 'none'|'mean'|'sum'\n\n    \u8fd4\u56de:\n        Tensor - MSE\u635f\u5931\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; pred = genesis.randn(32, 10)\n        &gt;&gt;&gt; target = genesis.randn(32, 10)\n        &gt;&gt;&gt; loss = F.mse_loss(pred, target)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#nll_loss","title":"nll_loss","text":"Python<pre><code>def nll_loss(input: Tensor, target: Tensor, \n             reduction: str = 'mean') -&gt; Tensor:\n    \"\"\"\n    \u8d1f\u5bf9\u6570\u4f3c\u7136\u635f\u5931\u51fd\u6570\n\n    \u53c2\u6570:\n        input: Tensor - log\u6982\u7387\uff0cshape: (N, C)\n        target: Tensor - \u76ee\u6807\u7c7b\u522b\u7d22\u5f15\uff0cshape: (N,)\n        reduction: str - 'none'|'mean'|'sum'\n\n    \u8fd4\u56de:\n        Tensor - NLL\u635f\u5931\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; log_probs = F.log_softmax(logits, dim=-1)\n        &gt;&gt;&gt; loss = F.nll_loss(log_probs, labels)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#binary_cross_entropy","title":"binary_cross_entropy","text":"Python<pre><code>def binary_cross_entropy(input: Tensor, target: Tensor,\n                        reduction: str = 'mean') -&gt; Tensor:\n    \"\"\"\n    \u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\n\n    \u53c2\u6570:\n        input: Tensor - \u9884\u6d4b\u6982\u7387\uff0c\u8303\u56f4[0, 1]\n        target: Tensor - \u4e8c\u5143\u76ee\u6807\uff0c0\u62161\n        reduction: str - 'none'|'mean'|'sum'\n\n    \u8fd4\u56de:\n        Tensor - BCE\u635f\u5931\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; probs = F.sigmoid(logits)\n        &gt;&gt;&gt; loss = F.binary_cross_entropy(probs, targets)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_4","title":"\u5f20\u91cf\u64cd\u4f5c","text":""},{"location":"api/nn/functional/#matmul","title":"matmul","text":"Python<pre><code>def matmul(a: Tensor, b: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u77e9\u9635\u4e58\u6cd5\n\n    \u53c2\u6570:\n        a: Tensor - \u7b2c\u4e00\u4e2a\u5f20\u91cf\n        b: Tensor - \u7b2c\u4e8c\u4e2a\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u77e9\u9635\u4e58\u79ef\n\n    \u652f\u6301:\n        - \u5411\u91cf\u00d7\u5411\u91cf: \u70b9\u79ef\n        - \u77e9\u9635\u00d7\u5411\u91cf: \u77e9\u9635\u5411\u91cf\u4e58\u6cd5\n        - \u77e9\u9635\u00d7\u77e9\u9635: \u77e9\u9635\u4e58\u6cd5\n        - \u6279\u91cf\u77e9\u9635\u4e58\u6cd5: \u5e7f\u64ad\u89c4\u5219\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u77e9\u9635\u4e58\u6cd5\n        &gt;&gt;&gt; a = genesis.randn(3, 4)\n        &gt;&gt;&gt; b = genesis.randn(4, 5)\n        &gt;&gt;&gt; c = F.matmul(a, b)  # shape: (3, 5)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # \u6279\u91cf\u77e9\u9635\u4e58\u6cd5\n        &gt;&gt;&gt; a = genesis.randn(10, 3, 4)\n        &gt;&gt;&gt; b = genesis.randn(10, 4, 5)\n        &gt;&gt;&gt; c = F.matmul(a, b)  # shape: (10, 3, 5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#transpose","title":"transpose","text":"Python<pre><code>def transpose(x: Tensor, dim0: int = -2, dim1: int = -1) -&gt; Tensor:\n    \"\"\"\n    \u8f6c\u7f6e\u5f20\u91cf\u7684\u4e24\u4e2a\u7ef4\u5ea6\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim0: int - \u7b2c\u4e00\u4e2a\u7ef4\u5ea6\n        dim1: int - \u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u8f6c\u7f6e\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y = F.transpose(x, 0, 2)  # shape: (4, 3, 2)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#reshape","title":"reshape","text":"Python<pre><code>def reshape(x: Tensor, *shape) -&gt; Tensor:\n    \"\"\"\n    \u6539\u53d8\u5f20\u91cf\u5f62\u72b6\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        *shape: int - \u65b0\u5f62\u72b6\uff0c-1\u8868\u793a\u81ea\u52a8\u63a8\u65ad\n\n    \u8fd4\u56de:\n        Tensor - \u65b0\u5f62\u72b6\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(2, 3, 4)\n        &gt;&gt;&gt; y = F.reshape(x, 6, 4)  # shape: (6, 4)\n        &gt;&gt;&gt; z = F.reshape(x, -1)  # shape: (24,)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#flatten","title":"flatten","text":"Python<pre><code>def flatten(x: Tensor, start_dim: int = 0, end_dim: int = -1) -&gt; Tensor:\n    \"\"\"\n    \u5c55\u5e73\u5f20\u91cf\u7ef4\u5ea6\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        start_dim: int - \u5f00\u59cb\u5c55\u5e73\u7684\u7ef4\u5ea6\n        end_dim: int - \u7ed3\u675f\u5c55\u5e73\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u5c55\u5e73\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(32, 3, 28, 28)\n        &gt;&gt;&gt; y = F.flatten(x, 1)  # shape: (32, 2352)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#squeeze","title":"squeeze","text":"Python<pre><code>def squeeze(x: Tensor, dim: Optional[int] = None) -&gt; Tensor:\n    \"\"\"\n    \u79fb\u9664\u5927\u5c0f\u4e3a1\u7684\u7ef4\u5ea6\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int, optional - \u6307\u5b9a\u8981\u79fb\u9664\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u538b\u7f29\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(1, 3, 1, 4)\n        &gt;&gt;&gt; y = F.squeeze(x)  # shape: (3, 4)\n        &gt;&gt;&gt; z = F.squeeze(x, dim=0)  # shape: (3, 1, 4)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#unsqueeze","title":"unsqueeze","text":"Python<pre><code>def unsqueeze(x: Tensor, dim: int) -&gt; Tensor:\n    \"\"\"\n    \u63d2\u5165\u5927\u5c0f\u4e3a1\u7684\u7ef4\u5ea6\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int - \u63d2\u5165\u7ef4\u5ea6\u7684\u4f4d\u7f6e\n\n    \u8fd4\u56de:\n        Tensor - \u6269\u5c55\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; y = F.unsqueeze(x, 0)  # shape: (1, 3, 4)\n        &gt;&gt;&gt; z = F.unsqueeze(x, -1)  # shape: (3, 4, 1)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#cat","title":"cat","text":"Python<pre><code>def cat(tensors: List[Tensor], dim: int = 0) -&gt; Tensor:\n    \"\"\"\n    \u8fde\u63a5\u5f20\u91cf\u5217\u8868\n\n    \u53c2\u6570:\n        tensors: List[Tensor] - \u5f20\u91cf\u5217\u8868\n        dim: int - \u8fde\u63a5\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u8fde\u63a5\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x1 = genesis.randn(2, 3)\n        &gt;&gt;&gt; x2 = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = F.cat([x1, x2], dim=0)  # shape: (4, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#stack","title":"stack","text":"Python<pre><code>def stack(tensors: List[Tensor], dim: int = 0) -&gt; Tensor:\n    \"\"\"\n    \u5806\u53e0\u5f20\u91cf\u5217\u8868\uff08\u65b0\u589e\u7ef4\u5ea6\uff09\n\n    \u53c2\u6570:\n        tensors: List[Tensor] - \u5f20\u91cf\u5217\u8868\uff0c\u5f62\u72b6\u5fc5\u987b\u76f8\u540c\n        dim: int - \u65b0\u7ef4\u5ea6\u7684\u4f4d\u7f6e\n\n    \u8fd4\u56de:\n        Tensor - \u5806\u53e0\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x1 = genesis.randn(2, 3)\n        &gt;&gt;&gt; x2 = genesis.randn(2, 3)\n        &gt;&gt;&gt; y = F.stack([x1, x2], dim=0)  # shape: (2, 2, 3)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#split","title":"split","text":"Python<pre><code>def split(x: Tensor, split_size_or_sections, dim: int = 0) -&gt; List[Tensor]:\n    \"\"\"\n    \u5206\u5272\u5f20\u91cf\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        split_size_or_sections: int or list - \u5206\u5272\u5927\u5c0f\u6216\u5404\u90e8\u5206\u5927\u5c0f\n        dim: int - \u5206\u5272\u7684\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        List[Tensor] - \u5206\u5272\u540e\u7684\u5f20\u91cf\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(10, 3)\n        &gt;&gt;&gt; # \u7b49\u5206\u4e3a5\u4efd\n        &gt;&gt;&gt; parts = F.split(x, 2, dim=0)  # 5\u4e2a(2, 3)\u5f20\u91cf\n        &gt;&gt;&gt; # \u6307\u5b9a\u5404\u90e8\u5206\u5927\u5c0f\n        &gt;&gt;&gt; parts = F.split(x, [3, 3, 4], dim=0)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_5","title":"\u5f52\u7ea6\u64cd\u4f5c","text":""},{"location":"api/nn/functional/#sum","title":"sum","text":"Python<pre><code>def sum(x: Tensor, dim: Optional[Union[int, Tuple[int]]] = None, \n        keepdim: bool = False) -&gt; Tensor:\n    \"\"\"\n    \u6c42\u548c\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int or tuple, optional - \u6c42\u548c\u7684\u7ef4\u5ea6\n        keepdim: bool - \u662f\u5426\u4fdd\u6301\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u6c42\u548c\u7ed3\u679c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y = F.sum(x)  # \u6807\u91cf\n        &gt;&gt;&gt; y = F.sum(x, dim=1)  # shape: (3, 5)\n        &gt;&gt;&gt; y = F.sum(x, dim=(1, 2))  # shape: (3,)\n        &gt;&gt;&gt; y = F.sum(x, dim=1, keepdim=True)  # shape: (3, 1, 5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#mean","title":"mean","text":"Python<pre><code>def mean(x: Tensor, dim: Optional[Union[int, Tuple[int]]] = None,\n         keepdim: bool = False) -&gt; Tensor:\n    \"\"\"\n    \u6c42\u5e73\u5747\u503c\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int or tuple, optional - \u6c42\u5e73\u5747\u7684\u7ef4\u5ea6\n        keepdim: bool - \u662f\u5426\u4fdd\u6301\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor - \u5e73\u5747\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4, 5)\n        &gt;&gt;&gt; y = F.mean(x, dim=1)  # shape: (3, 5)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#max","title":"max","text":"Python<pre><code>def max(x: Tensor, dim: Optional[int] = None, \n        keepdim: bool = False) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:\n    \"\"\"\n    \u6c42\u6700\u5927\u503c\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int, optional - \u6c42\u6700\u5927\u503c\u7684\u7ef4\u5ea6\n        keepdim: bool - \u662f\u5426\u4fdd\u6301\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor or (Tensor, Tensor) - \u6700\u5927\u503c\uff08\u548c\u7d22\u5f15\uff0c\u5982\u679c\u6307\u5b9adim\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(3, 4)\n        &gt;&gt;&gt; # \u5168\u5c40\u6700\u5927\u503c\n        &gt;&gt;&gt; max_val = F.max(x)\n        &gt;&gt;&gt; # \u6cbf\u7ef4\u5ea6\u6c42\u6700\u5927\u503c\n        &gt;&gt;&gt; max_vals, indices = F.max(x, dim=1)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#min","title":"min","text":"Python<pre><code>def min(x: Tensor, dim: Optional[int] = None,\n        keepdim: bool = False) -&gt; Union[Tensor, Tuple[Tensor, Tensor]]:\n    \"\"\"\n    \u6c42\u6700\u5c0f\u503c\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        dim: int, optional - \u6c42\u6700\u5c0f\u503c\u7684\u7ef4\u5ea6\n        keepdim: bool - \u662f\u5426\u4fdd\u6301\u7ef4\u5ea6\n\n    \u8fd4\u56de:\n        Tensor or (Tensor, Tensor) - \u6700\u5c0f\u503c\uff08\u548c\u7d22\u5f15\uff0c\u5982\u679c\u6307\u5b9adim\uff09\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_6","title":"\u6570\u5b66\u51fd\u6570","text":""},{"location":"api/nn/functional/#exp","title":"exp","text":"Python<pre><code>def exp(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u6307\u6570\u51fd\u6570: e^x\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n\n    \u8fd4\u56de:\n        Tensor - \u6307\u6570\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([0., 1., 2.])\n        &gt;&gt;&gt; y = F.exp(x)  # [1., 2.718, 7.389]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#log","title":"log","text":"Python<pre><code>def log(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u81ea\u7136\u5bf9\u6570: ln(x)\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\uff08\u5fc5\u987b\u4e3a\u6b63\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u5bf9\u6570\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2.718, 7.389])\n        &gt;&gt;&gt; y = F.log(x)  # [0., 1., 2.]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#sqrt","title":"sqrt","text":"Python<pre><code>def sqrt(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u5e73\u65b9\u6839\u51fd\u6570\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\uff08\u5fc5\u987b\u975e\u8d1f\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u5e73\u65b9\u6839\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([0., 1., 4., 9.])\n        &gt;&gt;&gt; y = F.sqrt(x)  # [0., 1., 2., 3.]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#pow","title":"pow","text":"Python<pre><code>def pow(x: Tensor, exponent: Union[float, Tensor]) -&gt; Tensor:\n    \"\"\"\n    \u5e42\u51fd\u6570: x^exponent\n\n    \u53c2\u6570:\n        x: Tensor - \u5e95\u6570\n        exponent: float or Tensor - \u6307\u6570\n\n    \u8fd4\u56de:\n        Tensor - \u5e42\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.tensor([1., 2., 3.])\n        &gt;&gt;&gt; y = F.pow(x, 2)  # [1., 4., 9.]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#sin","title":"sin","text":"Python<pre><code>def sin(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u6b63\u5f26\u51fd\u6570\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\uff08\u5f27\u5ea6\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u6b63\u5f26\u503c\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#cos","title":"cos","text":"Python<pre><code>def cos(x: Tensor) -&gt; Tensor:\n    \"\"\"\n    \u4f59\u5f26\u51fd\u6570\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\uff08\u5f27\u5ea6\uff09\n\n    \u8fd4\u56de:\n        Tensor - \u4f59\u5f26\u503c\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_7","title":"\u5f52\u4e00\u5316\u51fd\u6570","text":""},{"location":"api/nn/functional/#layer_norm","title":"layer_norm","text":"Python<pre><code>def layer_norm(x: Tensor, normalized_shape: Union[int, List[int]],\n               weight: Optional[Tensor] = None, bias: Optional[Tensor] = None,\n               eps: float = 1e-5) -&gt; Tensor:\n    \"\"\"\n    \u5c42\u5f52\u4e00\u5316\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        normalized_shape: int or list - \u5f52\u4e00\u5316\u7684\u5f62\u72b6\n        weight: Tensor, optional - \u7f29\u653e\u53c2\u6570\n        bias: Tensor, optional - \u504f\u79fb\u53c2\u6570\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\n\n    \u8fd4\u56de:\n        Tensor - \u5f52\u4e00\u5316\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 128)\n        &gt;&gt;&gt; y = F.layer_norm(x, 128)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#batch_norm","title":"batch_norm","text":"Python<pre><code>def batch_norm(x: Tensor, running_mean: Optional[Tensor], \n               running_var: Optional[Tensor], weight: Optional[Tensor],\n               bias: Optional[Tensor], training: bool = True,\n               momentum: float = 0.1, eps: float = 1e-5) -&gt; Tensor:\n    \"\"\"\n    \u6279\u91cf\u5f52\u4e00\u5316\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        running_mean: Tensor - \u79fb\u52a8\u5e73\u5747\u5747\u503c\n        running_var: Tensor - \u79fb\u52a8\u5e73\u5747\u65b9\u5dee\n        weight: Tensor - \u7f29\u653e\u53c2\u6570\n        bias: Tensor - \u504f\u79fb\u53c2\u6570\n        training: bool - \u662f\u5426\u8bad\u7ec3\u6a21\u5f0f\n        momentum: float - \u79fb\u52a8\u5e73\u5747\u52a8\u91cf\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\n\n    \u8fd4\u56de:\n        Tensor - \u5f52\u4e00\u5316\u540e\u7684\u5f20\u91cf\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_8","title":"\u6ce8\u610f\u529b\u673a\u5236","text":""},{"location":"api/nn/functional/#scaled_dot_product_attention","title":"scaled_dot_product_attention","text":"Python<pre><code>def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor,\n                                 attn_mask: Optional[Tensor] = None,\n                                 dropout_p: float = 0.0,\n                                 is_causal: bool = False) -&gt; Tensor:\n    \"\"\"\n    \u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\n\n    \u53c2\u6570:\n        query: Tensor - \u67e5\u8be2\u5f20\u91cf\uff0cshape: (*, seq_len, d_k)\n        key: Tensor - \u952e\u5f20\u91cf\uff0cshape: (*, seq_len, d_k)\n        value: Tensor - \u503c\u5f20\u91cf\uff0cshape: (*, seq_len, d_v)\n        attn_mask: Tensor, optional - \u6ce8\u610f\u529b\u63a9\u7801\n        dropout_p: float - dropout\u6982\u7387\n        is_causal: bool - \u662f\u5426\u4f7f\u7528\u56e0\u679c\u63a9\u7801\n\n    \u8fd4\u56de:\n        Tensor - \u6ce8\u610f\u529b\u8f93\u51fa\uff0cshape: (*, seq_len, d_v)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; q = genesis.randn(32, 10, 64)  # batch=32, seq=10, dim=64\n        &gt;&gt;&gt; k = genesis.randn(32, 10, 64)\n        &gt;&gt;&gt; v = genesis.randn(32, 10, 64)\n        &gt;&gt;&gt; output = F.scaled_dot_product_attention(q, k, v)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_9","title":"\u5176\u4ed6\u51fd\u6570","text":""},{"location":"api/nn/functional/#dropout","title":"dropout","text":"Python<pre><code>def dropout(x: Tensor, p: float = 0.5, training: bool = True,\n           inplace: bool = False) -&gt; Tensor:\n    \"\"\"\n    Dropout\u6b63\u5219\u5316\n\n    \u53c2\u6570:\n        x: Tensor - \u8f93\u5165\u5f20\u91cf\n        p: float - \u5931\u6d3b\u6982\u7387\n        training: bool - \u662f\u5426\u8bad\u7ec3\u6a21\u5f0f\n        inplace: bool - \u662f\u5426\u539f\u5730\u64cd\u4f5c\n\n    \u8fd4\u56de:\n        Tensor - dropout\u540e\u7684\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; x = genesis.randn(100)\n        &gt;&gt;&gt; # \u8bad\u7ec3\u65f6\u5e94\u7528dropout\n        &gt;&gt;&gt; y = F.dropout(x, p=0.2, training=True)\n        &gt;&gt;&gt; # \u63a8\u7406\u65f6\u4e0d\u5e94\u7528\n        &gt;&gt;&gt; y = F.dropout(x, p=0.2, training=False)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#embedding","title":"embedding","text":"Python<pre><code>def embedding(input: Tensor, weight: Tensor, padding_idx: Optional[int] = None) -&gt; Tensor:\n    \"\"\"\n    \u5d4c\u5165\u67e5\u627e\n\n    \u53c2\u6570:\n        input: Tensor - \u7d22\u5f15\u5f20\u91cf\n        weight: Tensor - \u5d4c\u5165\u77e9\u9635\uff0cshape: (num_embeddings, embedding_dim)\n        padding_idx: int, optional - \u586b\u5145\u7d22\u5f15\n\n    \u8fd4\u56de:\n        Tensor - \u5d4c\u5165\u5411\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; weight = genesis.randn(10000, 128)  # 10000\u4e2a\u8bcd\uff0c128\u7ef4\n        &gt;&gt;&gt; indices = genesis.tensor([1, 2, 3, 4])\n        &gt;&gt;&gt; embeddings = F.embedding(indices, weight)  # shape: (4, 128)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#one_hot","title":"one_hot","text":"Python<pre><code>def one_hot(x: Tensor, num_classes: int) -&gt; Tensor:\n    \"\"\"\n    One-hot\u7f16\u7801\n\n    \u53c2\u6570:\n        x: Tensor - \u7c7b\u522b\u7d22\u5f15\u5f20\u91cf\n        num_classes: int - \u7c7b\u522b\u603b\u6570\n\n    \u8fd4\u56de:\n        Tensor - one-hot\u7f16\u7801\u5f20\u91cf\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; labels = genesis.tensor([0, 1, 2, 1])\n        &gt;&gt;&gt; one_hot = F.one_hot(labels, num_classes=3)\n        &gt;&gt;&gt; # [[1, 0, 0],\n        &gt;&gt;&gt; #  [0, 1, 0],\n        &gt;&gt;&gt; #  [0, 0, 1],\n        &gt;&gt;&gt; #  [0, 1, 0]]\n    \"\"\"\n</code></pre>"},{"location":"api/nn/functional/#_10","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"api/nn/functional/#_11","title":"\u6784\u5efa\u524d\u5411\u4f20\u64ad","text":"Python<pre><code>import genesis.nn.functional as F\n\ndef forward(x, weight1, bias1, weight2, bias2):\n    # \u7b2c\u4e00\u5c42\n    x = F.matmul(x, weight1.T) + bias1\n    x = F.relu(x)\n    x = F.dropout(x, p=0.5, training=True)\n\n    # \u7b2c\u4e8c\u5c42\n    x = F.matmul(x, weight2.T) + bias2\n    x = F.softmax(x, dim=-1)\n\n    return x\n</code></pre>"},{"location":"api/nn/functional/#_12","title":"\u8ba1\u7b97\u635f\u5931","text":"Python<pre><code># \u5206\u7c7b\u4efb\u52a1\nlogits = model(x)\nloss = F.cross_entropy(logits, labels)\n\n# \u56de\u5f52\u4efb\u52a1\npredictions = model(x)\nloss = F.mse_loss(predictions, targets)\n\n# \u81ea\u5b9a\u4e49\u635f\u5931\nlog_probs = F.log_softmax(logits, dim=-1)\nloss = F.nll_loss(log_probs, labels)\n</code></pre>"},{"location":"api/nn/functional/#_13","title":"\u6570\u636e\u9884\u5904\u7406","text":"Python<pre><code># \u5f52\u4e00\u5316\nx = F.layer_norm(x, x.shape[-1])\n\n# \u6570\u636e\u589e\u5f3a\nx = F.dropout(x, p=0.1, training=True)\n\n# \u5f62\u72b6\u53d8\u6362\nx = F.flatten(x, start_dim=1)\nx = F.reshape(x, batch_size, -1)\n</code></pre>"},{"location":"api/nn/functional/#_14","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":"<ol> <li>\u4f7f\u7528fused\u64cd\u4f5c\uff1a\u5982<code>fused_layer_norm</code>\u6bd4\u5355\u72ec\u7684\u64cd\u4f5c\u66f4\u5feb</li> <li>\u907f\u514d\u5c0f\u6279\u91cf\u64cd\u4f5c\uff1a\u6279\u91cf\u5904\u7406\u6bd4\u9010\u4e2a\u5904\u7406\u6548\u7387\u9ad8</li> <li>\u4f7f\u7528\u539f\u5730\u64cd\u4f5c\uff1a<code>inplace=True</code>\u51cf\u5c11\u5185\u5b58\u5206\u914d</li> <li>\u5408\u5e76\u64cd\u4f5c\uff1a\u5982\u4f7f\u7528<code>cross_entropy</code>\u800c\u4e0d\u662f<code>softmax</code>+<code>nll_loss</code></li> <li>\u6ce8\u610f\u6570\u503c\u7a33\u5b9a\u6027\uff1a\u4f7f\u7528<code>log_softmax</code>\u800c\u4e0d\u662f<code>log(softmax())</code></li> </ol>"},{"location":"api/nn/functional/#_15","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u51fd\u6570\u5f0f\u63a5\u53e3\u662f\u65e0\u72b6\u6001\u7684\uff0c\u4e0d\u4fdd\u5b58\u53c2\u6570</li> <li>\u67d0\u4e9b\u51fd\u6570\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u5f0f\u4e0b\u884c\u4e3a\u4e0d\u540c\uff08\u5982dropout\uff09</li> <li>\u539f\u5730\u64cd\u4f5c\u53ef\u80fd\u7834\u574f\u68af\u5ea6\u8ba1\u7b97\uff0c\u8c28\u614e\u4f7f\u7528</li> <li>\u6ce8\u610f\u5f20\u91cf\u5f62\u72b6\u7684\u5e7f\u64ad\u89c4\u5219</li> <li>GPU\u64cd\u4f5c\u53ef\u80fd\u4f7f\u7528Triton\u4f18\u5316\u5185\u6838</li> </ul>"},{"location":"api/nn/modules/","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 (genesis.nn)","text":"<p>Genesis\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u63d0\u4f9b\u4e86\u6784\u5efa\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6240\u9700\u7684\u6240\u6709\u57fa\u7840\u7ec4\u4ef6\u3002</p>"},{"location":"api/nn/modules/#_1","title":"\u6a21\u5757\u6982\u8ff0","text":"<p><code>genesis.nn</code>\u6a21\u5757\u5305\u542b\uff1a - \u57fa\u7840\u6a21\u5757\u7c7b\uff08Module\u3001Parameter\uff09 - \u7ebf\u6027\u5c42\u548c\u5bb9\u5668\uff08Linear\u3001Sequential\u3001ModuleList\uff09 - \u6fc0\u6d3b\u51fd\u6570\uff08ReLU\u3001SiLU\u3001Softmax\uff09 - \u6b63\u5219\u5316\u5c42\uff08Dropout\u3001BatchNorm\u3001LayerNorm\uff09 - \u9ad8\u7ea7\u7ec4\u4ef6\uff08Embedding\u3001Attention\uff09</p>"},{"location":"api/nn/modules/#_2","title":"\u6838\u5fc3\u57fa\u7c7b","text":""},{"location":"api/nn/modules/#module","title":"Module","text":"<p>\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u7684\u57fa\u7c7b\uff0c\u63d0\u4f9b\u53c2\u6570\u7ba1\u7406\u3001\u524d\u5411\u4f20\u64ad\u3001\u72b6\u6001\u4fdd\u5b58\u7b49\u529f\u80fd\u3002</p> Python<pre><code>class Module:\n    \"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u57fa\u7c7b\"\"\"\n\n    def __init__(self):\n        \"\"\"\u521d\u59cb\u5316\u6a21\u5757\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_3","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"api/nn/modules/#_4","title":"\u524d\u5411\u4f20\u64ad","text":"Python<pre><code>def forward(self, *args, **kwargs):\n    \"\"\"\n    \u5b9a\u4e49\u524d\u5411\u4f20\u64ad\u903b\u8f91\uff08\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; class MyModule(nn.Module):\n        ...     def forward(self, x):\n        ...         return x * 2\n    \"\"\"\n    raise NotImplementedError()\n\ndef __call__(self, *args, **kwargs):\n    \"\"\"\n    \u8c03\u7528\u6a21\u5757\uff0c\u6267\u884c\u524d\u5411\u4f20\u64ad\n\n    \u6ce8\u610f: \u76f4\u63a5\u8c03\u7528\u6a21\u5757\u800c\u4e0d\u662fforward\u65b9\u6cd5\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = MyModule()\n        &gt;&gt;&gt; output = model(input)  # \u6b63\u786e\n        &gt;&gt;&gt; output = model.forward(input)  # \u4e0d\u63a8\u8350\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_5","title":"\u53c2\u6570\u7ba1\u7406","text":"Python<pre><code>def parameters(self) -&gt; List[Tensor]:\n    \"\"\"\n    \u8fd4\u56de\u6a21\u5757\u7684\u6240\u6709\u53c2\u6570\n\n    \u8fd4\u56de:\n        List[Tensor] - \u53c2\u6570\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Linear(10, 5)\n        &gt;&gt;&gt; params = model.parameters()\n        &gt;&gt;&gt; print(len(params))  # 2 (weight\u548cbias)\n    \"\"\"\n\ndef named_parameters(self) -&gt; List[Tuple[str, Tensor]]:\n    \"\"\"\n    \u8fd4\u56de\u53c2\u6570\u53ca\u5176\u540d\u79f0\n\n    \u8fd4\u56de:\n        List[Tuple[str, Tensor]] - (\u540d\u79f0, \u53c2\u6570)\u5bf9\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; for name, param in model.named_parameters():\n        ...     print(f\"{name}: {param.shape}\")\n    \"\"\"\n\ndef add_module(self, name: str, module: Optional[Module]):\n    \"\"\"\n    \u6dfb\u52a0\u5b50\u6a21\u5757\n\n    \u53c2\u6570:\n        name: str - \u5b50\u6a21\u5757\u540d\u79f0\n        module: Module - \u5b50\u6a21\u5757\u5b9e\u4f8b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Module()\n        &gt;&gt;&gt; model.add_module('fc', nn.Linear(10, 5))\n    \"\"\"\n\ndef modules(self) -&gt; Iterator[Module]:\n    \"\"\"\u8fd4\u56de\u6240\u6709\u5b50\u6a21\u5757\u7684\u8fed\u4ee3\u5668\uff08\u5305\u62ec\u81ea\u8eab\uff09\"\"\"\n\ndef children(self) -&gt; Iterator[Module]:\n    \"\"\"\u8fd4\u56de\u76f4\u63a5\u5b50\u6a21\u5757\u7684\u8fed\u4ee3\u5668\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_6","title":"\u72b6\u6001\u7ba1\u7406","text":"Python<pre><code>def state_dict(self, destination=None, prefix='', keep_vars=False) -&gt; dict:\n    \"\"\"\n    \u8fd4\u56de\u6a21\u5757\u72b6\u6001\u5b57\u5178\n\n    \u53c2\u6570:\n        destination: dict, optional - \u76ee\u6807\u5b57\u5178\n        prefix: str - \u53c2\u6570\u540d\u524d\u7f00\n        keep_vars: bool - \u662f\u5426\u4fdd\u6301\u5f20\u91cf\u7684\u68af\u5ea6\u4fe1\u606f\n\n    \u8fd4\u56de:\n        dict - \u53c2\u6570\u540d\u5230\u5f20\u91cf\u7684\u6620\u5c04\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; state = model.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'model.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: dict, strict: bool = True):\n    \"\"\"\n    \u52a0\u8f7d\u6a21\u5757\u72b6\u6001\n\n    \u53c2\u6570:\n        state_dict: dict - \u72b6\u6001\u5b57\u5178\n        strict: bool - \u662f\u5426\u4e25\u683c\u5339\u914d\u53c2\u6570\u540d\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; state = genesis.load('model.pth')\n        &gt;&gt;&gt; model.load_state_dict(state)\n    \"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_7","title":"\u8bad\u7ec3\u6a21\u5f0f\u63a7\u5236","text":"Python<pre><code>def train(self, mode: bool = True) -&gt; Module:\n    \"\"\"\n    \u8bbe\u7f6e\u8bad\u7ec3\u6a21\u5f0f\n\n    \u53c2\u6570:\n        mode: bool - True\u4e3a\u8bad\u7ec3\u6a21\u5f0f\uff0cFalse\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n\n    \u8fd4\u56de:\n        self - \u652f\u6301\u94fe\u5f0f\u8c03\u7528\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model.train()  # \u8bad\u7ec3\u6a21\u5f0f\n        &gt;&gt;&gt; model.train(False)  # \u8bc4\u4f30\u6a21\u5f0f\n    \"\"\"\n\ndef eval(self) -&gt; Module:\n    \"\"\"\n    \u8bbe\u7f6e\u8bc4\u4f30\u6a21\u5f0f\uff08\u7b49\u4ef7\u4e8etrain(False)\uff09\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model.eval()\n        &gt;&gt;&gt; with genesis.no_grad():\n        ...     output = model(input)\n    \"\"\"\n\n@property\ndef training(self) -&gt; bool:\n    \"\"\"\u8fd4\u56de\u662f\u5426\u5904\u4e8e\u8bad\u7ec3\u6a21\u5f0f\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_8","title":"\u8bbe\u5907\u7ba1\u7406","text":"Python<pre><code>def to(self, device=None, dtype=None) -&gt; Module:\n    \"\"\"\n    \u79fb\u52a8\u6a21\u5757\u5230\u6307\u5b9a\u8bbe\u5907\u6216\u8f6c\u6362\u6570\u636e\u7c7b\u578b\n\n    \u53c2\u6570:\n        device: Device - \u76ee\u6807\u8bbe\u5907\n        dtype: DType - \u76ee\u6807\u6570\u636e\u7c7b\u578b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = model.to(genesis.cuda())\n        &gt;&gt;&gt; model = model.to(dtype=genesis.float16)\n    \"\"\"\n\ndef cpu(self) -&gt; Module:\n    \"\"\"\u79fb\u52a8\u5230CPU\"\"\"\n\ndef cuda(self, device_id: int = 0) -&gt; Module:\n    \"\"\"\u79fb\u52a8\u5230CUDA\u8bbe\u5907\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_9","title":"\u5b9e\u7528\u65b9\u6cd5","text":"Python<pre><code>def apply(self, fn: Callable[[Module], None]) -&gt; Module:\n    \"\"\"\n    \u9012\u5f52\u5e94\u7528\u51fd\u6570\u5230\u6240\u6709\u5b50\u6a21\u5757\n\n    \u53c2\u6570:\n        fn: callable - \u5e94\u7528\u5230\u6bcf\u4e2a\u6a21\u5757\u7684\u51fd\u6570\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; def init_weights(m):\n        ...     if isinstance(m, nn.Linear):\n        ...         m.weight.data.normal_(0, 0.01)\n        &gt;&gt;&gt; model.apply(init_weights)\n    \"\"\"\n\ndef zero_grad(self):\n    \"\"\"\u6e05\u96f6\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\"\"\"\n\ndef extra_repr(self) -&gt; str:\n    \"\"\"\u8fd4\u56de\u6a21\u5757\u7684\u989d\u5916\u5b57\u7b26\u4e32\u8868\u793a\uff08\u5b50\u7c7b\u53ef\u91cd\u5199\uff09\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#parameter","title":"Parameter","text":"<p>\u6a21\u578b\u53c2\u6570\u7684\u7279\u6b8a\u5f20\u91cf\u7c7b\u578b\u3002</p> Python<pre><code>class Parameter(Tensor):\n    \"\"\"\n    \u6a21\u578b\u53c2\u6570\n\n    \u53c2\u6570:\n        data: Tensor - \u53c2\u6570\u6570\u636e\n        requires_grad: bool - \u662f\u5426\u9700\u8981\u68af\u5ea6\uff0c\u9ed8\u8ba4True\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; weight = nn.Parameter(genesis.randn(10, 5))\n        &gt;&gt;&gt; bias = nn.Parameter(genesis.zeros(5))\n    \"\"\"\n\n    def __init__(self, data: Tensor, requires_grad: bool = True):\n        \"\"\"\u521d\u59cb\u5316\u53c2\u6570\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_10","title":"\u57fa\u7840\u5c42","text":""},{"location":"api/nn/modules/#linear","title":"Linear","text":"<p>\u5168\u8fde\u63a5\u5c42\uff08\u7ebf\u6027\u53d8\u6362\uff09\u3002</p> Python<pre><code>class Linear(Module):\n    \"\"\"\n    \u7ebf\u6027\u5c42: y = xW^T + b\n\n    \u53c2\u6570:\n        in_features: int - \u8f93\u5165\u7279\u5f81\u7ef4\u5ea6\n        out_features: int - \u8f93\u51fa\u7279\u5f81\u7ef4\u5ea6\n        bias: bool - \u662f\u5426\u4f7f\u7528\u504f\u7f6e\uff0c\u9ed8\u8ba4True\n        device: Device - \u8bbe\u5907\n        dtype: DType - \u6570\u636e\u7c7b\u578b\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u7ebf\u6027\u5c42\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; layer = nn.Linear(784, 128)\n            &gt;&gt;&gt; x = genesis.randn(32, 784)\n            &gt;&gt;&gt; y = layer(x)  # shape: (32, 128)\n        \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\n\n        \u53c2\u6570:\n            x: Tensor - \u8f93\u5165\u5f20\u91cf\uff0cshape: (..., in_features)\n\n        \u8fd4\u56de:\n            Tensor - \u8f93\u51fa\u5f20\u91cf\uff0cshape: (..., out_features)\n        \"\"\"\n\n    @property\n    def weight(self) -&gt; Parameter:\n        \"\"\"\u6743\u91cd\u53c2\u6570\uff0cshape: (out_features, in_features)\"\"\"\n\n    @property\n    def bias(self) -&gt; Optional[Parameter]:\n        \"\"\"\u504f\u7f6e\u53c2\u6570\uff0cshape: (out_features,)\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#embedding","title":"Embedding","text":"<p>\u5d4c\u5165\u5c42\uff0c\u5c06\u79bb\u6563\u7d22\u5f15\u6620\u5c04\u5230\u8fde\u7eed\u5411\u91cf\u3002</p> Python<pre><code>class Embedding(Module):\n    \"\"\"\n    \u5d4c\u5165\u5c42\n\n    \u53c2\u6570:\n        num_embeddings: int - \u5d4c\u5165\u5b57\u5178\u5927\u5c0f\n        embedding_dim: int - \u5d4c\u5165\u5411\u91cf\u7ef4\u5ea6\n        padding_idx: int, optional - \u586b\u5145\u7d22\u5f15\n        device: Device - \u8bbe\u5907\n        dtype: DType - \u6570\u636e\u7c7b\u578b\n    \"\"\"\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, \n                 padding_idx: Optional[int] = None, device=None, dtype=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u5d4c\u5165\u5c42\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; embed = nn.Embedding(10000, 128)  # \u8bcd\u6c47\u8868\u5927\u5c0f10000\uff0c\u5d4c\u5165\u7ef4\u5ea6128\n            &gt;&gt;&gt; indices = genesis.tensor([1, 2, 3, 4])\n            &gt;&gt;&gt; vectors = embed(indices)  # shape: (4, 128)\n        \"\"\"\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u524d\u5411\u4f20\u64ad\n\n        \u53c2\u6570:\n            input: Tensor - \u7d22\u5f15\u5f20\u91cf\uff0cdtype\u5fc5\u987b\u662f\u6574\u6570\n\n        \u8fd4\u56de:\n            Tensor - \u5d4c\u5165\u5411\u91cf\uff0cshape: (*input.shape, embedding_dim)\n        \"\"\"\n\n    @property  \n    def weight(self) -&gt; Parameter:\n        \"\"\"\u5d4c\u5165\u6743\u91cd\u77e9\u9635\uff0cshape: (num_embeddings, embedding_dim)\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_11","title":"\u5bb9\u5668\u6a21\u5757","text":""},{"location":"api/nn/modules/#sequential","title":"Sequential","text":"<p>\u987a\u5e8f\u5bb9\u5668\uff0c\u6309\u987a\u5e8f\u6267\u884c\u5b50\u6a21\u5757\u3002</p> Python<pre><code>class Sequential(Module):\n    \"\"\"\n    \u987a\u5e8f\u5bb9\u5668\n\n    \u53c2\u6570:\n        *args: Module - \u6309\u987a\u5e8f\u6267\u884c\u7684\u6a21\u5757\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; model = nn.Sequential(\n        ...     nn.Linear(784, 256),\n        ...     nn.ReLU(),\n        ...     nn.Linear(256, 128),\n        ...     nn.ReLU(),\n        ...     nn.Linear(128, 10)\n        ... )\n        &gt;&gt;&gt; output = model(input)\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\u521d\u59cb\u5316\u987a\u5e8f\u5bb9\u5668\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u4f9d\u6b21\u901a\u8fc7\u6240\u6709\u5b50\u6a21\u5757\"\"\"\n\n    def append(self, module: Module) -&gt; Sequential:\n        \"\"\"\u6dfb\u52a0\u6a21\u5757\u5230\u672b\u5c3e\"\"\"\n\n    def __getitem__(self, idx: int) -&gt; Module:\n        \"\"\"\u901a\u8fc7\u7d22\u5f15\u8bbf\u95ee\u5b50\u6a21\u5757\"\"\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"\u8fd4\u56de\u5b50\u6a21\u5757\u6570\u91cf\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#modulelist","title":"ModuleList","text":"<p>\u6a21\u5757\u5217\u8868\u5bb9\u5668\u3002</p> Python<pre><code>class ModuleList(Module):\n    \"\"\"\n    \u6a21\u5757\u5217\u8868\n\n    \u53c2\u6570:\n        modules: list, optional - \u6a21\u5757\u5217\u8868\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; layers = nn.ModuleList([\n        ...     nn.Linear(10, 10) for _ in range(5)\n        ... ])\n        &gt;&gt;&gt; for layer in layers:\n        ...     x = layer(x)\n    \"\"\"\n\n    def __init__(self, modules: Optional[List[Module]] = None):\n        \"\"\"\u521d\u59cb\u5316\u6a21\u5757\u5217\u8868\"\"\"\n\n    def append(self, module: Module) -&gt; ModuleList:\n        \"\"\"\u6dfb\u52a0\u6a21\u5757\"\"\"\n\n    def extend(self, modules: List[Module]) -&gt; ModuleList:\n        \"\"\"\u6269\u5c55\u6a21\u5757\u5217\u8868\"\"\"\n\n    def __getitem__(self, idx: int) -&gt; Module:\n        \"\"\"\u7d22\u5f15\u8bbf\u95ee\"\"\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"\u8fd4\u56de\u6a21\u5757\u6570\u91cf\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_12","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"api/nn/modules/#relu","title":"ReLU","text":"<p>\u7ebf\u6027\u6574\u6d41\u5355\u5143\u6fc0\u6d3b\u51fd\u6570\u3002</p> Python<pre><code>class ReLU(Module):\n    \"\"\"\n    ReLU\u6fc0\u6d3b\u51fd\u6570: f(x) = max(0, x)\n\n    \u53c2\u6570:\n        inplace: bool - \u662f\u5426\u539f\u5730\u64cd\u4f5c\uff0c\u9ed8\u8ba4False\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; relu = nn.ReLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = relu(x)\n    \"\"\"\n\n    def __init__(self, inplace: bool = False):\n        \"\"\"\u521d\u59cb\u5316ReLU\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528ReLU\u6fc0\u6d3b\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#silu","title":"SiLU","text":"<p>Sigmoid\u7ebf\u6027\u5355\u5143\uff08Swish\u6fc0\u6d3b\u51fd\u6570\uff09\u3002</p> Python<pre><code>class SiLU(Module):\n    \"\"\"\n    SiLU\u6fc0\u6d3b\u51fd\u6570: f(x) = x * sigmoid(x)\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; silu = nn.SiLU()\n        &gt;&gt;&gt; x = genesis.randn(10)\n        &gt;&gt;&gt; y = silu(x)\n    \"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528SiLU\u6fc0\u6d3b\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#softmax","title":"Softmax","text":"<p>Softmax\u6fc0\u6d3b\u51fd\u6570\u3002</p> Python<pre><code>class Softmax(Module):\n    \"\"\"\n    Softmax\u6fc0\u6d3b\u51fd\u6570\n\n    \u53c2\u6570:\n        dim: int - \u8ba1\u7b97softmax\u7684\u7ef4\u5ea6\uff0c\u9ed8\u8ba4-1\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; softmax = nn.Softmax(dim=-1)\n        &gt;&gt;&gt; x = genesis.randn(10, 5)\n        &gt;&gt;&gt; y = softmax(x)  # \u6bcf\u884c\u548c\u4e3a1\n    \"\"\"\n\n    def __init__(self, dim: int = -1):\n        \"\"\"\u521d\u59cb\u5316Softmax\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528Softmax\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_13","title":"\u6b63\u5219\u5316\u5c42","text":""},{"location":"api/nn/modules/#dropout","title":"Dropout","text":"<p>\u968f\u673a\u5931\u6d3b\u6b63\u5219\u5316\u3002</p> Python<pre><code>class Dropout(Module):\n    \"\"\"\n    Dropout\u6b63\u5219\u5316\n\n    \u53c2\u6570:\n        p: float - \u5931\u6d3b\u6982\u7387\uff0c\u9ed8\u8ba40.5\n        inplace: bool - \u662f\u5426\u539f\u5730\u64cd\u4f5c\uff0c\u9ed8\u8ba4False\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; dropout = nn.Dropout(p=0.2)\n        &gt;&gt;&gt; model.train()  # \u8bad\u7ec3\u6a21\u5f0f\u4e0b\u5e94\u7528dropout\n        &gt;&gt;&gt; y = dropout(x)\n        &gt;&gt;&gt; model.eval()  # \u8bc4\u4f30\u6a21\u5f0f\u4e0b\u4e0d\u5e94\u7528dropout\n    \"\"\"\n\n    def __init__(self, p: float = 0.5, inplace: bool = False):\n        \"\"\"\u521d\u59cb\u5316Dropout\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u5e94\u7528Dropout\n\n        \u6ce8\u610f: \u8bad\u7ec3\u6a21\u5f0f\u4e0b\u968f\u673a\u5931\u6d3b\uff0c\u8bc4\u4f30\u6a21\u5f0f\u4e0b\u76f4\u63a5\u8fd4\u56de\u8f93\u5165\n        \"\"\"\n</code></pre>"},{"location":"api/nn/modules/#batchnorm1d","title":"BatchNorm1d","text":"<p>\u4e00\u7ef4\u6279\u91cf\u5f52\u4e00\u5316\u3002</p> Python<pre><code>class BatchNorm1d(Module):\n    \"\"\"\n    \u6279\u91cf\u5f52\u4e00\u5316\uff08\u7528\u4e8e\u5168\u8fde\u63a5\u5c42\uff09\n\n    \u53c2\u6570:\n        num_features: int - \u7279\u5f81\u7ef4\u5ea6\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\uff0c\u9ed8\u8ba41e-5\n        momentum: float - \u79fb\u52a8\u5e73\u5747\u52a8\u91cf\uff0c\u9ed8\u8ba40.1\n        device: Device - \u8bbe\u5907\n        dtype: DType - \u6570\u636e\u7c7b\u578b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; bn = nn.BatchNorm1d(128)\n        &gt;&gt;&gt; x = genesis.randn(32, 128)  # batch_size=32, features=128\n        &gt;&gt;&gt; y = bn(x)\n    \"\"\"\n\n    def __init__(self, num_features: int, eps: float = 1e-5, \n                 momentum: float = 0.1, device=None, dtype=None):\n        \"\"\"\u521d\u59cb\u5316BatchNorm1d\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        \u5e94\u7528\u6279\u91cf\u5f52\u4e00\u5316\n\n        \u53c2\u6570:\n            x: Tensor - \u8f93\u5165\u5f20\u91cf\uff0cshape: (N, C) \u6216 (N, C, L)\n\n        \u8fd4\u56de:\n            Tensor - \u5f52\u4e00\u5316\u540e\u7684\u5f20\u91cf\n        \"\"\"\n\n    @property\n    def weight(self) -&gt; Parameter:\n        \"\"\"\u7f29\u653e\u53c2\u6570\u03b3\uff0cshape: (num_features,)\"\"\"\n\n    @property\n    def bias(self) -&gt; Parameter:\n        \"\"\"\u504f\u79fb\u53c2\u6570\u03b2\uff0cshape: (num_features,)\"\"\"\n\n    @property\n    def running_mean(self) -&gt; Tensor:\n        \"\"\"\u79fb\u52a8\u5e73\u5747\u5747\u503c\"\"\"\n\n    @property\n    def running_var(self) -&gt; Tensor:\n        \"\"\"\u79fb\u52a8\u5e73\u5747\u65b9\u5dee\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#layernorm","title":"LayerNorm","text":"<p>\u5c42\u5f52\u4e00\u5316\u3002</p> Python<pre><code>class LayerNorm(Module):\n    \"\"\"\n    \u5c42\u5f52\u4e00\u5316\n\n    \u53c2\u6570:\n        normalized_shape: int or tuple - \u5f52\u4e00\u5316\u5f62\u72b6\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\uff0c\u9ed8\u8ba41e-5\n        elementwise_affine: bool - \u662f\u5426\u4f7f\u7528\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u9ed8\u8ba4True\n        device: Device - \u8bbe\u5907\n        dtype: DType - \u6570\u636e\u7c7b\u578b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; ln = nn.LayerNorm(128)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 128)\n        &gt;&gt;&gt; y = ln(x)  # \u5728\u6700\u540e\u4e00\u7ef4\u5f52\u4e00\u5316\n    \"\"\"\n\n    def __init__(self, normalized_shape, eps: float = 1e-5,\n                 elementwise_affine: bool = True, device=None, dtype=None):\n        \"\"\"\u521d\u59cb\u5316LayerNorm\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528\u5c42\u5f52\u4e00\u5316\"\"\"\n\n    @property\n    def weight(self) -&gt; Optional[Parameter]:\n        \"\"\"\u7f29\u653e\u53c2\u6570\"\"\"\n\n    @property\n    def bias(self) -&gt; Optional[Parameter]:\n        \"\"\"\u504f\u79fb\u53c2\u6570\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#rmsnorm","title":"RMSNorm","text":"<p>RMS\u5f52\u4e00\u5316\uff08Root Mean Square Normalization\uff09\u3002</p> Python<pre><code>class RMSNorm(Module):\n    \"\"\"\n    RMS\u5f52\u4e00\u5316\n\n    \u53c2\u6570:\n        dim: int - \u5f52\u4e00\u5316\u7ef4\u5ea6\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\uff0c\u9ed8\u8ba41e-6\n        device: Device - \u8bbe\u5907\n        dtype: DType - \u6570\u636e\u7c7b\u578b\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; rms_norm = nn.RMSNorm(128)\n        &gt;&gt;&gt; x = genesis.randn(32, 10, 128)\n        &gt;&gt;&gt; y = rms_norm(x)\n    \"\"\"\n\n    def __init__(self, dim: int, eps: float = 1e-6, device=None, dtype=None):\n        \"\"\"\u521d\u59cb\u5316RMSNorm\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528RMS\u5f52\u4e00\u5316\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_14","title":"\u5176\u4ed6\u6a21\u5757","text":""},{"location":"api/nn/modules/#flatten","title":"Flatten","text":"<p>\u5c55\u5e73\u5c42\u3002</p> Python<pre><code>class Flatten(Module):\n    \"\"\"\n    \u5c55\u5e73\u5c42\uff0c\u5c06\u591a\u7ef4\u8f93\u5165\u5c55\u5e73\u4e3a\u4e8c\u7ef4\n\n    \u53c2\u6570:\n        start_dim: int - \u5f00\u59cb\u5c55\u5e73\u7684\u7ef4\u5ea6\uff0c\u9ed8\u8ba41\n        end_dim: int - \u7ed3\u675f\u5c55\u5e73\u7684\u7ef4\u5ea6\uff0c\u9ed8\u8ba4-1\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; flatten = nn.Flatten()\n        &gt;&gt;&gt; x = genesis.randn(32, 3, 28, 28)\n        &gt;&gt;&gt; y = flatten(x)  # shape: (32, 2352)\n    \"\"\"\n\n    def __init__(self, start_dim: int = 1, end_dim: int = -1):\n        \"\"\"\u521d\u59cb\u5316Flatten\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5c55\u5e73\u5f20\u91cf\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#residual","title":"Residual","text":"<p>\u6b8b\u5dee\u8fde\u63a5\u5305\u88c5\u5668\u3002</p> Python<pre><code>class Residual(Module):\n    \"\"\"\n    \u6b8b\u5dee\u8fde\u63a5: output = x + fn(x)\n\n    \u53c2\u6570:\n        fn: Module - \u8981\u5e94\u7528\u7684\u51fd\u6570/\u6a21\u5757\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; residual = nn.Residual(\n        ...     nn.Sequential(\n        ...         nn.Linear(128, 128),\n        ...         nn.ReLU()\n        ...     )\n        ... )\n        &gt;&gt;&gt; y = residual(x)  # y = x + fn(x)\n    \"\"\"\n\n    def __init__(self, fn: Module):\n        \"\"\"\u521d\u59cb\u5316\u6b8b\u5dee\u6a21\u5757\"\"\"\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\u5e94\u7528\u6b8b\u5dee\u8fde\u63a5\"\"\"\n</code></pre>"},{"location":"api/nn/modules/#_15","title":"\u4f7f\u7528\u793a\u4f8b","text":""},{"location":"api/nn/modules/#mlp","title":"\u6784\u5efa\u7b80\u5355\u7684MLP","text":"Python<pre><code>import genesis.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.dropout(self.relu(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n\n# \u4f7f\u7528\u6a21\u578b\nmodel = MLP(784, 256, 10)\nx = genesis.randn(32, 784)\noutput = model(x)\nprint(output.shape)  # (32, 10)\n</code></pre>"},{"location":"api/nn/modules/#sequential_1","title":"\u4f7f\u7528Sequential\u6784\u5efa\u6a21\u578b","text":"Python<pre><code>model = nn.Sequential(\n    nn.Linear(784, 256),\n    nn.BatchNorm1d(256),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(256, 128),\n    nn.BatchNorm1d(128),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(128, 10)\n)\n\n# \u8bad\u7ec3\u6a21\u5f0f\nmodel.train()\noutput = model(x)\n\n# \u8bc4\u4f30\u6a21\u5f0f\nmodel.eval()\nwith genesis.no_grad():\n    output = model(x)\n</code></pre>"},{"location":"api/nn/modules/#_16","title":"\u53c2\u6570\u521d\u59cb\u5316","text":"Python<pre><code>def init_weights(m):\n    if isinstance(m, nn.Linear):\n        # Xavier\u521d\u59cb\u5316\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Embedding):\n        nn.init.normal_(m.weight, mean=0, std=0.02)\n\nmodel.apply(init_weights)\n</code></pre>"},{"location":"api/nn/modules/#_17","title":"\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d","text":"Python<pre><code># \u4fdd\u5b58\u6a21\u578b\u72b6\u6001\ngenesis.save(model.state_dict(), 'model.pth')\n\n# \u52a0\u8f7d\u6a21\u578b\u72b6\u6001\nmodel = MLP(784, 256, 10)\nstate_dict = genesis.load('model.pth')\nmodel.load_state_dict(state_dict)\n\n# \u4fdd\u5b58\u5b8c\u6574\u68c0\u67e5\u70b9\ncheckpoint = {\n    'epoch': 100,\n    'model_state': model.state_dict(),\n    'optimizer_state': optimizer.state_dict(),\n    'loss': loss.item()\n}\ngenesis.save_checkpoint(checkpoint, 'checkpoint.pth')\n</code></pre>"},{"location":"api/nn/modules/#_18","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":"<ol> <li>\u4f7f\u7528eval\u6a21\u5f0f\u8fdb\u884c\u63a8\u7406\uff1a\u8c03\u7528<code>model.eval()</code>\u7981\u7528dropout\u548c\u6279\u91cf\u5f52\u4e00\u5316\u7684\u8bad\u7ec3\u884c\u4e3a</li> <li>\u53c2\u6570\u5171\u4eab\uff1a\u4f7f\u7528\u540c\u4e00\u4e2a\u6a21\u5757\u5b9e\u4f8b\u53ef\u4ee5\u5171\u4eab\u53c2\u6570</li> <li>\u539f\u5730\u64cd\u4f5c\uff1a\u4f7f\u7528<code>inplace=True</code>\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff08\u6ce8\u610f\u68af\u5ea6\u8ba1\u7b97\uff09</li> <li>\u6279\u91cf\u5904\u7406\uff1a\u5c3d\u53ef\u80fd\u4f7f\u7528\u8f83\u5927\u7684\u6279\u91cf\u5927\u5c0f\u63d0\u5347GPU\u5229\u7528\u7387</li> <li>\u6df7\u5408\u7cbe\u5ea6\uff1a\u7ed3\u5408autocast\u4f7f\u7528\u534a\u7cbe\u5ea6\u8bad\u7ec3</li> </ol>"},{"location":"api/nn/modules/#_19","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>Module\u7684forward\u65b9\u6cd5\u5fc5\u987b\u88ab\u5b50\u7c7b\u5b9e\u73b0</li> <li>\u4f7f\u7528<code>model(x)</code>\u800c\u4e0d\u662f<code>model.forward(x)</code>\u8c03\u7528\u6a21\u578b</li> <li>\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u5f0f\u4f1a\u5f71\u54cdDropout\u548cBatchNorm\u7684\u884c\u4e3a</li> <li>Parameter\u4f1a\u81ea\u52a8\u6ce8\u518c\u4e3a\u6a21\u5757\u53c2\u6570\uff0c\u666e\u901aTensor\u4e0d\u4f1a</li> <li>\u6a21\u5757\u7684\u8bbe\u5907\u548c\u6570\u636e\u7c7b\u578b\u9700\u8981\u4e0e\u8f93\u5165\u4e00\u81f4</li> </ul>"},{"location":"api/optim/optimizers/","title":"\u4f18\u5316\u5668 (genesis.optim)","text":"<p>Genesis\u4f18\u5316\u5668\u6a21\u5757\u63d0\u4f9b\u4e86\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6240\u9700\u7684\u5404\u79cd\u4f18\u5316\u7b97\u6cd5\u3002</p>"},{"location":"api/optim/optimizers/#_1","title":"\u6a21\u5757\u6982\u8ff0","text":"<p><code>genesis.optim</code>\u6a21\u5757\u5305\u542b\uff1a - \u57fa\u7840\u4f18\u5316\u5668\u7c7b\uff08Optimizer\uff09 - \u7ecf\u5178\u4f18\u5316\u5668\uff08SGD\u3001Adam\u3001AdamW\uff09 - \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff08\u5728schedulers.md\u4e2d\u8be6\u8ff0\uff09 - \u68af\u5ea6\u88c1\u526a\u5de5\u5177</p>"},{"location":"api/optim/optimizers/#_2","title":"\u57fa\u7840\u7c7b","text":""},{"location":"api/optim/optimizers/#optimizer","title":"Optimizer","text":"<p>\u6240\u6709\u4f18\u5316\u5668\u7684\u62bd\u8c61\u57fa\u7c7b\u3002</p> Python<pre><code>class Optimizer:\n    \"\"\"\u4f18\u5316\u5668\u57fa\u7c7b\"\"\"\n\n    def __init__(self, params, defaults):\n        \"\"\"\n        \u521d\u59cb\u5316\u4f18\u5316\u5668\n\n        \u53c2\u6570:\n            params: list - \u53c2\u6570\u5217\u8868\u6216\u53c2\u6570\u7ec4\n            defaults: dict - \u9ed8\u8ba4\u8d85\u53c2\u6570\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#_3","title":"\u6838\u5fc3\u65b9\u6cd5","text":"Python<pre><code>def step(self, closure=None):\n    \"\"\"\n    \u6267\u884c\u4e00\u6b65\u4f18\u5316\n\n    \u53c2\u6570:\n        closure: callable, optional - \u91cd\u65b0\u8ba1\u7b97\u635f\u5931\u7684\u95ed\u5305\u51fd\u6570\n\n    \u8fd4\u56de:\n        loss - \u5982\u679c\u63d0\u4f9bclosure\uff0c\u8fd4\u56de\u635f\u5931\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; loss = criterion(output, target)\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef zero_grad(self):\n    \"\"\"\n    \u6e05\u96f6\u6240\u6709\u53c2\u6570\u7684\u68af\u5ea6\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\u65f6\u6e05\u96f6\u68af\u5ea6\n        &gt;&gt;&gt; optimizer.zero_grad()\n        &gt;&gt;&gt; output = model(input)\n        &gt;&gt;&gt; loss = criterion(output, target)\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef state_dict(self) -&gt; dict:\n    \"\"\"\n    \u8fd4\u56de\u4f18\u5316\u5668\u72b6\u6001\u5b57\u5178\n\n    \u8fd4\u56de:\n        dict - \u5305\u542bstate\u548cparam_groups\u7684\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u4fdd\u5b58\u4f18\u5316\u5668\u72b6\u6001\n        &gt;&gt;&gt; state = optimizer.state_dict()\n        &gt;&gt;&gt; genesis.save(state, 'optimizer.pth')\n    \"\"\"\n\ndef load_state_dict(self, state_dict: dict):\n    \"\"\"\n    \u52a0\u8f7d\u4f18\u5316\u5668\u72b6\u6001\n\n    \u53c2\u6570:\n        state_dict: dict - \u72b6\u6001\u5b57\u5178\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u6062\u590d\u4f18\u5316\u5668\u72b6\u6001\n        &gt;&gt;&gt; state = genesis.load('optimizer.pth')\n        &gt;&gt;&gt; optimizer.load_state_dict(state)\n    \"\"\"\n\ndef add_param_group(self, param_group: dict):\n    \"\"\"\n    \u6dfb\u52a0\u53c2\u6570\u7ec4\n\n    \u53c2\u6570:\n        param_group: dict - \u65b0\u7684\u53c2\u6570\u7ec4\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; # \u4e3a\u65b0\u6dfb\u52a0\u7684\u5c42\u8bbe\u7f6e\u4e0d\u540c\u7684\u5b66\u4e60\u7387\n        &gt;&gt;&gt; optimizer.add_param_group({\n        ...     'params': new_layer.parameters(),\n        ...     'lr': 0.001\n        ... })\n    \"\"\"\n\n@property\ndef param_groups(self) -&gt; List[dict]:\n    \"\"\"\n    \u83b7\u53d6\u53c2\u6570\u7ec4\u5217\u8868\n\n    \u8fd4\u56de:\n        List[dict] - \u6bcf\u4e2a\u5b57\u5178\u5305\u542b'params'\u548c\u5176\u4ed6\u8d85\u53c2\u6570\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; for group in optimizer.param_groups:\n        ...     group['lr'] *= 0.95  # \u624b\u52a8\u8c03\u6574\u5b66\u4e60\u7387\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#sgd","title":"SGD\u4f18\u5316\u5668","text":"<p>\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\uff0c\u652f\u6301\u52a8\u91cf\u548c\u6743\u91cd\u8870\u51cf\u3002</p> Python<pre><code>class SGD(Optimizer):\n    \"\"\"\n    \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\n\n    \u53c2\u6570:\n        params: iterable - \u5f85\u4f18\u5316\u53c2\u6570\u7684\u8fed\u4ee3\u5668\n        lr: float - \u5b66\u4e60\u7387\uff0c\u5fc5\u9700\u53c2\u6570\n        momentum: float - \u52a8\u91cf\u56e0\u5b50\uff0c\u9ed8\u8ba40\n        dampening: float - \u52a8\u91cf\u6291\u5236\uff0c\u9ed8\u8ba40\n        weight_decay: float - \u6743\u91cd\u8870\u51cf\uff08L2\u6b63\u5219\u5316\uff09\uff0c\u9ed8\u8ba40\n        nesterov: bool - \u662f\u5426\u4f7f\u7528Nesterov\u52a8\u91cf\uff0c\u9ed8\u8ba4False\n    \"\"\"\n\n    def __init__(self, params, lr, momentum=0, dampening=0,\n                 weight_decay=0, nesterov=False):\n        \"\"\"\n        \u521d\u59cb\u5316SGD\u4f18\u5316\u5668\n\n        \u7b97\u6cd5:\n            v_t = momentum * v_{t-1} + g_t\n            p_t = p_{t-1} - lr * v_t\n\n        \u5176\u4e2dg_t\u662f\u68af\u5ea6\uff0cv_t\u662f\u901f\u5ea6\uff0cp_t\u662f\u53c2\u6570\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; # \u57fa\u7840SGD\n            &gt;&gt;&gt; optimizer = optim.SGD(model.parameters(), lr=0.01)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u5e26\u52a8\u91cf\u7684SGD\n            &gt;&gt;&gt; optimizer = optim.SGD(model.parameters(), lr=0.01, \n            ...                      momentum=0.9)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u5e26\u6743\u91cd\u8870\u51cf\u7684SGD\n            &gt;&gt;&gt; optimizer = optim.SGD(model.parameters(), lr=0.01,\n            ...                      momentum=0.9, weight_decay=1e-4)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # Nesterov\u52a8\u91cfSGD\n            &gt;&gt;&gt; optimizer = optim.SGD(model.parameters(), lr=0.01,\n            ...                      momentum=0.9, nesterov=True)\n        \"\"\"\n\n    def step(self, closure=None):\n        \"\"\"\n        \u6267\u884c\u4e00\u6b65SGD\u66f4\u65b0\n\n        \u66f4\u65b0\u89c4\u5219:\n            \u5982\u679c\u4f7f\u7528\u52a8\u91cf:\n                buf_t = momentum * buf_{t-1} + (1 - dampening) * g_t\n                \u5982\u679c\u4f7f\u7528nesterov:\n                    g_t = g_t + momentum * buf_t\n                \u5426\u5219:\n                    g_t = buf_t\n            p_t = p_{t-1} - lr * g_t\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#_4","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code>import genesis.optim as optim\n\n# \u57fa\u7840SGD\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n# \u5e26\u52a8\u91cf\u7684SGD\uff08\u63a8\u8350\u7528\u4e8e\u5927\u591a\u6570\u4efb\u52a1\uff09\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# \u4e0d\u540c\u53c2\u6570\u7ec4\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u7387\noptimizer = optim.SGD([\n    {'params': model.base.parameters(), 'lr': 0.001},\n    {'params': model.head.parameters(), 'lr': 0.01}\n], momentum=0.9)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        optimizer.zero_grad()\n        output = model(batch['input'])\n        loss = criterion(output, batch['target'])\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"api/optim/optimizers/#adam","title":"Adam\u4f18\u5316\u5668","text":"<p>\u81ea\u9002\u5e94\u77e9\u4f30\u8ba1\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86RMSprop\u548c\u52a8\u91cf\u3002</p> Python<pre><code>class Adam(Optimizer):\n    \"\"\"\n    Adam\u4f18\u5316\u5668\n\n    \u53c2\u6570:\n        params: iterable - \u5f85\u4f18\u5316\u53c2\u6570\u7684\u8fed\u4ee3\u5668\n        lr: float - \u5b66\u4e60\u7387\uff0c\u9ed8\u8ba41e-3\n        betas: Tuple[float, float] - \u7528\u4e8e\u8ba1\u7b97\u68af\u5ea6\u53ca\u5176\u5e73\u65b9\u7684\u79fb\u52a8\u5e73\u5747\u7684\u7cfb\u6570\n                                     \u9ed8\u8ba4(0.9, 0.999)\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\uff0c\u9ed8\u8ba41e-8\n        weight_decay: float - \u6743\u91cd\u8870\u51cf\uff0c\u9ed8\u8ba40\n        amsgrad: bool - \u662f\u5426\u4f7f\u7528AMSGrad\u53d8\u4f53\uff0c\u9ed8\u8ba4False\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        \"\"\"\n        \u521d\u59cb\u5316Adam\u4f18\u5316\u5668\n\n        \u7b97\u6cd5:\n            m_t = \u03b21 * m_{t-1} + (1 - \u03b21) * g_t\n            v_t = \u03b22 * v_{t-1} + (1 - \u03b22) * g_t^2\n            m\u0302_t = m_t / (1 - \u03b21^t)\n            v\u0302_t = v_t / (1 - \u03b22^t)\n            p_t = p_{t-1} - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n\n        \u5176\u4e2d:\n            g_t: \u68af\u5ea6\n            m_t: \u4e00\u9636\u77e9\u4f30\u8ba1\uff08\u52a8\u91cf\uff09\n            v_t: \u4e8c\u9636\u77e9\u4f30\u8ba1\uff08\u81ea\u9002\u5e94\u5b66\u4e60\u7387\uff09\n            m\u0302_t, v\u0302_t: \u504f\u5dee\u4fee\u6b63\u7684\u77e9\u4f30\u8ba1\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; # \u9ed8\u8ba4Adam\n            &gt;&gt;&gt; optimizer = optim.Adam(model.parameters())\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u81ea\u5b9a\u4e49\u5b66\u4e60\u7387\n            &gt;&gt;&gt; optimizer = optim.Adam(model.parameters(), lr=0.001)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u8c03\u6574beta\u53c2\u6570\n            &gt;&gt;&gt; optimizer = optim.Adam(model.parameters(), lr=0.001,\n            ...                       betas=(0.9, 0.98))\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u4f7f\u7528AMSGrad\n            &gt;&gt;&gt; optimizer = optim.Adam(model.parameters(), lr=0.001,\n            ...                       amsgrad=True)\n        \"\"\"\n\n    def step(self, closure=None):\n        \"\"\"\n        \u6267\u884c\u4e00\u6b65Adam\u66f4\u65b0\n\n        \u72b6\u6001\u53d8\u91cf:\n            - exp_avg: \u68af\u5ea6\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08m_t\uff09\n            - exp_avg_sq: \u68af\u5ea6\u5e73\u65b9\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08v_t\uff09\n            - max_exp_avg_sq: \u6700\u5927\u7684v_t\uff08\u4ec5AMSGrad\u4f7f\u7528\uff09\n            - step: \u5f53\u524d\u6b65\u6570\uff08\u7528\u4e8e\u504f\u5dee\u4fee\u6b63\uff09\n        \"\"\"\n\n    @property\n    def state(self) -&gt; dict:\n        \"\"\"\n        \u4f18\u5316\u5668\u72b6\u6001\n\n        \u6bcf\u4e2a\u53c2\u6570\u7684\u72b6\u6001\u5305\u542b:\n            - step: int - \u66f4\u65b0\u6b65\u6570\n            - exp_avg: Tensor - \u4e00\u9636\u77e9\u4f30\u8ba1\n            - exp_avg_sq: Tensor - \u4e8c\u9636\u77e9\u4f30\u8ba1\n            - max_exp_avg_sq: Tensor - \u6700\u5927\u4e8c\u9636\u77e9\uff08AMSGrad\uff09\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#_5","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code># \u9ed8\u8ba4Adam\uff08\u6700\u5e38\u7528\uff09\noptimizer = optim.Adam(model.parameters())\n\n# \u81ea\u5b9a\u4e49\u5b66\u4e60\u7387\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Transformer\u6a21\u578b\u5e38\u7528\u8bbe\u7f6e\noptimizer = optim.Adam(model.parameters(), lr=0.0001, \n                      betas=(0.9, 0.98), eps=1e-9)\n\n# \u5e26\u6743\u91cd\u8870\u51cf\noptimizer = optim.Adam(model.parameters(), lr=0.001,\n                      weight_decay=1e-5)\n\n# \u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4e0d\u540c\u5c42\u4e0d\u540c\u5b66\u4e60\u7387\uff09\noptimizer = optim.Adam([\n    {'params': model.encoder.parameters(), 'lr': 1e-5},\n    {'params': model.decoder.parameters(), 'lr': 1e-4},\n    {'params': model.head.parameters(), 'lr': 1e-3}\n])\n</code></pre>"},{"location":"api/optim/optimizers/#adamw","title":"AdamW\u4f18\u5316\u5668","text":"<p>Adam\u4f18\u5316\u5668\u7684\u6539\u8fdb\u7248\u672c\uff0c\u89e3\u8026\u6743\u91cd\u8870\u51cf\u3002</p> Python<pre><code>class AdamW(Optimizer):\n    \"\"\"\n    AdamW\u4f18\u5316\u5668\uff08\u89e3\u8026\u6743\u91cd\u8870\u51cf\u7684Adam\uff09\n\n    \u53c2\u6570:\n        params: iterable - \u5f85\u4f18\u5316\u53c2\u6570\u7684\u8fed\u4ee3\u5668\n        lr: float - \u5b66\u4e60\u7387\uff0c\u9ed8\u8ba41e-3\n        betas: Tuple[float, float] - \u52a8\u91cf\u7cfb\u6570\uff0c\u9ed8\u8ba4(0.9, 0.999)\n        eps: float - \u6570\u503c\u7a33\u5b9a\u6027\u53c2\u6570\uff0c\u9ed8\u8ba41e-8\n        weight_decay: float - \u6743\u91cd\u8870\u51cf\u7cfb\u6570\uff0c\u9ed8\u8ba40.01\n        amsgrad: bool - \u662f\u5426\u4f7f\u7528AMSGrad\uff0c\u9ed8\u8ba4False\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0.01, amsgrad=False):\n        \"\"\"\n        \u521d\u59cb\u5316AdamW\u4f18\u5316\u5668\n\n        \u4e0eAdam\u7684\u533a\u522b:\n            Adam: p_t = p_{t-1} - lr * (m\u0302_t / (\u221av\u0302_t + \u03b5) + wd * p_{t-1})\n            AdamW: p_t = p_{t-1} * (1 - lr * wd) - lr * m\u0302_t / (\u221av\u0302_t + \u03b5)\n\n        AdamW\u5c06\u6743\u91cd\u8870\u51cf\u4ece\u68af\u5ea6\u8ba1\u7b97\u4e2d\u89e3\u8026\uff0c\u76f4\u63a5\u4f5c\u7528\u4e8e\u53c2\u6570\n\n        \u793a\u4f8b:\n            &gt;&gt;&gt; # \u9ed8\u8ba4AdamW\uff08\u63a8\u8350\u7528\u4e8eTransformer\uff09\n            &gt;&gt;&gt; optimizer = optim.AdamW(model.parameters())\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # BERT/GPT\u5e38\u7528\u8bbe\u7f6e\n            &gt;&gt;&gt; optimizer = optim.AdamW(model.parameters(), lr=5e-5,\n            ...                        weight_decay=0.01)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # \u5927\u6a21\u578b\u8bad\u7ec3\u8bbe\u7f6e\n            &gt;&gt;&gt; optimizer = optim.AdamW(model.parameters(), lr=1e-4,\n            ...                        betas=(0.9, 0.95),\n            ...                        weight_decay=0.1)\n        \"\"\"\n\n    def step(self, closure=None):\n        \"\"\"\n        \u6267\u884c\u4e00\u6b65AdamW\u66f4\u65b0\n\n        \u66f4\u65b0\u89c4\u5219:\n            1. \u8ba1\u7b97Adam\u66f4\u65b0\uff08\u4e0d\u5305\u62ec\u6743\u91cd\u8870\u51cf\uff09\n            2. \u5355\u72ec\u5e94\u7528\u6743\u91cd\u8870\u51cf: p = p * (1 - lr * weight_decay)\n        \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":"Python<pre><code># Transformer\u6a21\u578b\u6807\u51c6\u914d\u7f6e\noptimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n\n# GPT\u98ce\u683c\u6a21\u578b\noptimizer = optim.AdamW(model.parameters(), lr=1e-4,\n                       betas=(0.9, 0.95), weight_decay=0.1)\n\n# \u6392\u9664\u67d0\u4e9b\u53c2\u6570\u7684\u6743\u91cd\u8870\u51cf\uff08\u5982\u504f\u7f6e\u548cLayerNorm\uff09\ndecay_params = []\nno_decay_params = []\nfor name, param in model.named_parameters():\n    if 'bias' in name or 'ln' in name or 'norm' in name:\n        no_decay_params.append(param)\n    else:\n        decay_params.append(param)\n\noptimizer = optim.AdamW([\n    {'params': decay_params, 'weight_decay': 0.01},\n    {'params': no_decay_params, 'weight_decay': 0.0}\n], lr=1e-4)\n</code></pre>"},{"location":"api/optim/optimizers/#_7","title":"\u68af\u5ea6\u88c1\u526a","text":"<p>\u9632\u6b62\u68af\u5ea6\u7206\u70b8\u7684\u5b9e\u7528\u51fd\u6570\u3002</p> Python<pre><code>def clip_grad_norm_(parameters, max_norm: float, norm_type: float = 2.0):\n    \"\"\"\n    \u88c1\u526a\u68af\u5ea6\u8303\u6570\n\n    \u53c2\u6570:\n        parameters: iterable - \u53c2\u6570\u8fed\u4ee3\u5668\n        max_norm: float - \u6700\u5927\u68af\u5ea6\u8303\u6570\n        norm_type: float - \u8303\u6570\u7c7b\u578b\uff081\u30012\u6216inf\uff09\n\n    \u8fd4\u56de:\n        float - \u88c1\u526a\u524d\u7684\u603b\u68af\u5ea6\u8303\u6570\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; # \u88c1\u526a\u68af\u5ea6\uff0c\u9632\u6b62\u68af\u5ea6\u7206\u70b8\n        &gt;&gt;&gt; genesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n\ndef clip_grad_value_(parameters, clip_value: float):\n    \"\"\"\n    \u88c1\u526a\u68af\u5ea6\u503c\n\n    \u53c2\u6570:\n        parameters: iterable - \u53c2\u6570\u8fed\u4ee3\u5668\n        clip_value: float - \u88c1\u526a\u9608\u503c\n\n    \u793a\u4f8b:\n        &gt;&gt;&gt; loss.backward()\n        &gt;&gt;&gt; # \u5c06\u68af\u5ea6\u9650\u5236\u5728[-1, 1]\u8303\u56f4\u5185\n        &gt;&gt;&gt; genesis.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n        &gt;&gt;&gt; optimizer.step()\n    \"\"\"\n</code></pre>"},{"location":"api/optim/optimizers/#_8","title":"\u5b8c\u6574\u8bad\u7ec3\u793a\u4f8b","text":""},{"location":"api/optim/optimizers/#_9","title":"\u57fa\u7840\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\n# \u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = MyModel()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # \u6e05\u96f6\u68af\u5ea6\n        optimizer.zero_grad()\n\n        # \u524d\u5411\u4f20\u64ad\n        output = model(data)\n        loss = criterion(output, target)\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n\n        # \u68af\u5ea6\u88c1\u526a\uff08\u53ef\u9009\uff09\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # \u66f4\u65b0\u53c2\u6570\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n</code></pre>"},{"location":"api/optim/optimizers/#_10","title":"\u5e26\u5b66\u4e60\u7387\u8c03\u5ea6\u7684\u8bad\u7ec3","text":"Python<pre><code>from genesis.optim.lr_scheduler import get_cosine_schedule_with_warmup\n\n# \u4f18\u5316\u5668\u548c\u8c03\u5ea6\u5668\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nnum_training_steps = len(train_loader) * num_epochs\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_training_steps * 0.1,\n    num_training_steps=num_training_steps\n)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        optimizer.zero_grad()\n        loss = compute_loss(model, batch)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()  # \u66f4\u65b0\u5b66\u4e60\u7387\n</code></pre>"},{"location":"api/optim/optimizers/#_11","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code>genesis.enable_autocast = True\n\n# \u4f7f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\nwith genesis.autocast():\n    output = model(data)\n    loss = criterion(output, target)\n\nloss.backward()\noptimizer.step()\n</code></pre>"},{"location":"api/optim/optimizers/#_12","title":"\u4f18\u5316\u5668\u9009\u62e9\u6307\u5357","text":""},{"location":"api/optim/optimizers/#sgd_1","title":"SGD","text":"<ul> <li>\u4f18\u70b9\uff1a\u7b80\u5355\u3001\u5185\u5b58\u6548\u7387\u9ad8\u3001\u6cdb\u5316\u6027\u80fd\u597d</li> <li>\u7f3a\u70b9\uff1a\u6536\u655b\u6162\u3001\u5bf9\u5b66\u4e60\u7387\u654f\u611f</li> <li>\u9002\u7528\u573a\u666f\uff1a</li> <li>\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08ResNet\u3001VGG\u7b49\uff09</li> <li>\u5185\u5b58\u53d7\u9650\u73af\u5883</li> <li>\u9700\u8981\u6700\u4f73\u6cdb\u5316\u6027\u80fd\u65f6</li> <li>\u63a8\u8350\u8bbe\u7f6e\uff1a<code>lr=0.1, momentum=0.9, weight_decay=1e-4</code></li> </ul>"},{"location":"api/optim/optimizers/#adam_1","title":"Adam","text":"<ul> <li>\u4f18\u70b9\uff1a\u6536\u655b\u5feb\u3001\u5bf9\u5b66\u4e60\u7387\u4e0d\u654f\u611f\u3001\u9002\u5e94\u6027\u5f3a</li> <li>\u7f3a\u70b9\uff1a\u5185\u5b58\u5360\u7528\u5927\uff08\u9700\u8981\u5b58\u50a8\u4e00\u9636\u548c\u4e8c\u9636\u77e9\uff09\u3001\u53ef\u80fd\u8fc7\u62df\u5408</li> <li>\u9002\u7528\u573a\u666f\uff1a</li> <li>NLP\u4efb\u52a1</li> <li>\u5feb\u901f\u539f\u578b\u5f00\u53d1</li> <li>\u7a00\u758f\u68af\u5ea6</li> <li>\u63a8\u8350\u8bbe\u7f6e\uff1a<code>lr=1e-3, betas=(0.9, 0.999)</code></li> </ul>"},{"location":"api/optim/optimizers/#adamw_1","title":"AdamW","text":"<ul> <li>\u4f18\u70b9\uff1a\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3001\u9002\u5408\u5927\u6a21\u578b</li> <li>\u7f3a\u70b9\uff1a\u5185\u5b58\u5360\u7528\u5927</li> <li>\u9002\u7528\u573a\u666f\uff1a</li> <li>Transformer\u6a21\u578b\uff08BERT\u3001GPT\u7b49\uff09</li> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3</li> <li>\u9700\u8981\u5f3a\u6b63\u5219\u5316\u65f6</li> <li>\u63a8\u8350\u8bbe\u7f6e\uff1a<code>lr=5e-5, weight_decay=0.01</code></li> </ul>"},{"location":"api/optim/optimizers/#_13","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":"<ol> <li>\u68af\u5ea6\u7d2f\u79ef\uff1a\u5c0f\u6279\u91cf\u65f6\u7d2f\u79ef\u591a\u6b65\u68af\u5ea6\u518d\u66f4\u65b0</li> <li>\u68af\u5ea6\u88c1\u526a\uff1a\u9632\u6b62\u68af\u5ea6\u7206\u70b8\uff0c\u7279\u522b\u662fRNN/Transformer</li> <li>\u53c2\u6570\u7ec4\uff1a\u4e0d\u540c\u5c42\u4f7f\u7528\u4e0d\u540c\u5b66\u4e60\u7387</li> <li>\u6743\u91cd\u8870\u51cf\uff1aAdamW\u901a\u5e38\u6bd4Adam+L2\u6b63\u5219\u5316\u6548\u679c\u597d</li> <li>\u5b66\u4e60\u7387\u9884\u70ed\uff1a\u5927\u6279\u91cf\u8bad\u7ec3\u65f6\u4f7f\u7528warmup</li> </ol>"},{"location":"api/optim/optimizers/#_14","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u4f18\u5316\u5668\u72b6\u6001\u4f1a\u5360\u7528\u989d\u5916\u5185\u5b58\uff08Adam/AdamW\u662f\u53c2\u6570\u76842\u500d\uff09</li> <li>\u5207\u6362\u8bbe\u5907\u65f6\u9700\u8981\u5c06\u4f18\u5316\u5668\u72b6\u6001\u4e5f\u79fb\u5230\u65b0\u8bbe\u5907</li> <li>\u4fdd\u5b58\u68c0\u67e5\u70b9\u65f6\u8bb0\u5f97\u4fdd\u5b58\u4f18\u5316\u5668\u72b6\u6001</li> <li>\u4e0d\u540c\u4f18\u5316\u5668\u7684\u5b66\u4e60\u7387\u8303\u56f4\u5dee\u5f02\u5f88\u5927</li> <li>\u68af\u5ea6\u88c1\u526a\u5e94\u5728optimizer.step()\u4e4b\u524d\u8fdb\u884c</li> </ul>"},{"location":"api-reference/","title":"API \u53c2\u8003\u6587\u6863","text":"<p>Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684API\u63a5\u53e3\uff0c\u672c\u8282\u63d0\u4f9b\u8be6\u7ec6\u7684\u4ee3\u7801\u7ea7\u6587\u6863\u548c\u4f7f\u7528\u793a\u4f8b\u3002</p>"},{"location":"api-reference/#_1","title":"\u6838\u5fc3\u6a21\u5757\u7ed3\u6784","text":""},{"location":"api-reference/#_2","title":"\u4e3b\u8981\u547d\u540d\u7a7a\u95f4","text":"<ul> <li><code>genesis</code>: \u6838\u5fc3\u5f20\u91cf\u548c\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf</li> <li><code>genesis.nn</code>: \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u548c\u5c42</li> <li><code>genesis.optim</code>: \u4f18\u5316\u5668\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\u5668</li> <li><code>genesis.functional</code>: \u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3</li> <li><code>genesis.utils</code>: \u5de5\u5177\u51fd\u6570\u548c\u8f85\u52a9\u7c7b</li> </ul>"},{"location":"api-reference/#_3","title":"\u5feb\u901f\u5bfc\u822a","text":"\u6a21\u5757 \u63cf\u8ff0 \u4e3b\u8981\u7c7b/\u51fd\u6570 genesis \u6838\u5fc3\u5f20\u91cf\u7cfb\u7edf <code>Tensor</code>, <code>autocast</code>, <code>no_grad</code> nn \u795e\u7ecf\u7f51\u7edc\u5c42 <code>Module</code>, <code>Linear</code>, <code>MultiHeadAttention</code> optim \u4f18\u5316\u5668 <code>SGD</code>, <code>Adam</code>, <code>AdamW</code> functional \u51fd\u6570\u5f0f\u64cd\u4f5c <code>relu</code>, <code>softmax</code>, <code>matmul</code> utils \u5de5\u5177\u51fd\u6570 <code>profile</code>, <code>DataLoader</code>"},{"location":"api-reference/#_4","title":"\u4ee3\u7801\u7ea6\u5b9a","text":""},{"location":"api-reference/#_5","title":"\u5bfc\u5165\u89c4\u8303","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nimport genesis.nn.functional as F\n</code></pre>"},{"location":"api-reference/#_6","title":"\u8bbe\u5907\u7ba1\u7406","text":"Python<pre><code># \u8bbe\u7f6e\u9ed8\u8ba4\u8bbe\u5907\ngenesis.set_default_device(genesis.cuda())\n\n# \u68c0\u67e5CUDA\u53ef\u7528\u6027\nif genesis.cuda.is_available():\n    device = genesis.cuda()\nelse:\n    device = genesis.cpu()\n</code></pre>"},{"location":"api-reference/#_7","title":"\u6570\u636e\u7c7b\u578b","text":"Python<pre><code># \u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\ngenesis.float32  # \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\ngenesis.float16  # \u534a\u7cbe\u5ea6\u6d6e\u70b9\ngenesis.int32    # 32\u4f4d\u6574\u6570\ngenesis.bool     # \u5e03\u5c14\u7c7b\u578b\n</code></pre>"},{"location":"api-reference/#_8","title":"\u5feb\u901f\u793a\u4f8b","text":""},{"location":"api-reference/#_9","title":"\u57fa\u7840\u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([[1, 2], [3, 4]], dtype=genesis.float32)\ny = genesis.randn(2, 2)\n\n# \u57fa\u7840\u8fd0\u7b97\nz = x + y\nresult = genesis.matmul(x, y.T)\n\n# \u68af\u5ea6\u8ba1\u7b97\nx.requires_grad_(True)\nloss = (x ** 2).sum()\nloss.backward()\nprint(x.grad)  # \u6253\u5370\u68af\u5ea6\n</code></pre>"},{"location":"api-reference/#_10","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u578b","text":"Python<pre><code>import genesis.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc2(x)\n\n# \u4f7f\u7528\u6a21\u578b\nmodel = MLP(784, 256, 10)\nx = genesis.randn(32, 784)\noutput = model(x)\n</code></pre>"},{"location":"api-reference/#_11","title":"\u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis.optim as optim\n\n# \u521d\u59cb\u5316\nmodel = MLP(784, 256, 10)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u8bad\u7ec3\u6b65\u9aa4\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n</code></pre>"},{"location":"api-reference/#_12","title":"\u6027\u80fd\u4f18\u5316\u63d0\u793a","text":""},{"location":"api-reference/#_13","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    output = model(input_tensor)\n    loss = criterion(output, target)\n</code></pre>"},{"location":"api-reference/#gpu","title":"GPU\u5185\u5b58\u4f18\u5316","text":"Python<pre><code># \u4f7f\u7528inplace\u64cd\u4f5c\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\nx.relu_()  # inplace ReLU\nx.add_(y)  # inplace \u52a0\u6cd5\n\n# \u91ca\u653e\u4e0d\u9700\u8981\u7684\u68af\u5ea6\nwith genesis.no_grad():\n    inference_result = model(data)\n</code></pre>"},{"location":"api-reference/#_14","title":"\u6279\u91cf\u64cd\u4f5c\u4f18\u5316","text":"Python<pre><code># \u6279\u91cf\u77e9\u9635\u4e58\u6cd5\nbatch_result = genesis.bmm(batch_a, batch_b)\n\n# \u5411\u91cf\u5316\u64cd\u4f5c\u66ff\u4ee3\u5faa\u73af\nresult = genesis.sum(tensor, dim=1, keepdim=True)\n</code></pre>"},{"location":"architecture/","title":"\u67b6\u6784\u6982\u8ff0","text":"<p>Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u91c7\u7528\u5206\u5c42\u7684\u6a21\u5757\u5316\u67b6\u6784\u8bbe\u8ba1\uff0c\u65e2\u4fdd\u6301\u4e86\u4ee3\u7801\u7684\u6e05\u6670\u6027\uff0c\u53c8\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u8ba1\u7b97\u80fd\u529b\u3002</p>"},{"location":"architecture/#_2","title":"\ud83c\udfd7\ufe0f \u603b\u4f53\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u7528\u6237API\u5c42\"\n        A[genesis.Tensor] --&gt; B[genesis.nn.*]\n        A --&gt; C[genesis.optim.*]\n        A --&gt; D[genesis.functional.*]\n    end\n\n    subgraph \"\u81ea\u52a8\u5fae\u5206\u5c42\"\n        E[autograd.Tensor] --&gt; F[Function\u57fa\u7c7b]\n        F --&gt; G[Context\u4e0a\u4e0b\u6587]\n    end\n\n    subgraph \"\u5f20\u91cf\u7cfb\u7edf\"\n        H[backend.py] --&gt; I[NDArray\u63a5\u53e3]\n    end\n\n    subgraph \"\u540e\u7aef\u5b9e\u73b0\"\n        I --&gt; J[CPU Backend&lt;br/&gt;PyTorch\u5f20\u91cf]\n        I --&gt; K[GPU Backend&lt;br/&gt;CUDA + Triton]\n    end\n\n    subgraph \"GPU\u72ec\u7acb\u5b9e\u73b0\"\n        K --&gt; L[cuda_tensor.py&lt;br/&gt;\u7eafCUDA\u5185\u5b58\u7ba1\u7406]\n        K --&gt; M[ndarray_ops_gpu.py&lt;br/&gt;Triton kernels]\n        L --&gt; N[CUDA Python API]\n        M --&gt; O[Triton\u7f16\u8bd1\u5668]\n    end\n\n    subgraph \"CPU\u5b9e\u73b0\"\n        J --&gt; P[ndarray_ops_cpu.py&lt;br/&gt;PyTorch\u64cd\u4f5c]\n        P --&gt; Q[PyTorch Backend]\n    end\n\n    A --&gt; E\n    B --&gt; E\n    E --&gt; H\n\n    style A fill:#e1f5fe\n    style E fill:#f3e5f5\n    style H fill:#fff3e0\n    style K fill:#e8f5e8\n    style J fill:#fce4ec</code></pre>"},{"location":"architecture/#_3","title":"\ud83d\udd11 \u6838\u5fc3\u8bbe\u8ba1\u7406\u5ff5","text":""},{"location":"architecture/#1","title":"1. \u53cc\u540e\u7aef\u67b6\u6784","text":"<p>Genesis\u91c7\u7528\u4e86\u521b\u65b0\u7684\u53cc\u540e\u7aef\u8bbe\u8ba1\uff1a</p> <ul> <li>CPU\u540e\u7aef\uff1a\u5229\u7528PyTorch\u6210\u719f\u7684CPU\u5f20\u91cf\u5b9e\u73b0\uff0c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u517c\u5bb9\u6027</li> <li>GPU\u540e\u7aef\uff1a\u5b8c\u5168\u72ec\u7acb\u7684CUDA\u5b9e\u73b0\uff0c\u5c55\u793a\u4e86\u4ece\u96f6\u6784\u5efaGPU\u8ba1\u7b97\u6808\u7684\u5b8c\u6574\u8fc7\u7a0b</li> </ul>"},{"location":"architecture/#2","title":"2. \u6559\u80b2\u4e0e\u6027\u80fd\u5e76\u91cd","text":"<ul> <li>\u4ee3\u7801\u53ef\u8bfb\u6027\uff1a\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u6e05\u6670\u7684\u804c\u8d23\u5206\u5de5\u548c\u8be6\u7ec6\u7684\u6587\u6863</li> <li>\u6027\u80fd\u4f18\u5316\uff1aGPU\u540e\u7aef\u4f7f\u7528Triton\u5b9e\u73b0\u9ad8\u6027\u80fdkernels</li> <li>\u6e10\u8fdb\u5f0f\u5b66\u4e60\uff1a\u4ece\u7b80\u5355\u7684CPU\u5b9e\u73b0\u5230\u590d\u6742\u7684GPU\u4f18\u5316</li> </ul>"},{"location":"architecture/#3","title":"3. \u6a21\u5757\u5316\u8bbe\u8ba1","text":"<p>\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u53ef\u4ee5\u72ec\u7acb\u7406\u89e3\u548c\u6269\u5c55\uff1a - \u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u72ec\u7acb\u4e8e\u5177\u4f53\u7684\u5f20\u91cf\u5b9e\u73b0 - \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u57fa\u4e8e\u901a\u7528\u7684\u5f20\u91cf\u64cd\u4f5c - \u540e\u7aef\u62bd\u8c61\u5141\u8bb8\u8f7b\u677e\u5207\u6362\u4e0d\u540c\u7684\u5b9e\u73b0</p>"},{"location":"architecture/#_4","title":"\ud83d\udcca \u4e3b\u8981\u7ec4\u4ef6\u8be6\u89e3","text":""},{"location":"architecture/#autogradpy","title":"\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf (<code>autograd.py</code>)","text":"Python<pre><code># \u6838\u5fc3\u7c7b\u7ed3\u6784\nclass Tensor:\n    data: NDArray          # \u5e95\u5c42\u6570\u636e\u5b58\u50a8\n    requires_grad: bool    # \u662f\u5426\u9700\u8981\u68af\u5ea6\n    creator: Function      # \u521b\u5efa\u6b64\u5f20\u91cf\u7684\u64cd\u4f5c\n    grad: Tensor          # \u68af\u5ea6\u5f20\u91cf\n\nclass Function:\n    @staticmethod\n    def forward(ctx, *args)    # \u524d\u5411\u4f20\u64ad\n    @staticmethod \n    def backward(ctx, grad)    # \u53cd\u5411\u4f20\u64ad\n</code></pre> <p>\u5173\u952e\u7279\u6027\uff1a - \u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362 - \u7075\u6d3b\u7684\u8ba1\u7b97\u56fe\u6784\u5efa\u548c\u904d\u5386 - \u5185\u7f6e\u7684\u68af\u5ea6\u7d2f\u79ef\u548c\u6e05\u96f6\u673a\u5236</p>"},{"location":"architecture/#_5","title":"\u5f20\u91cf\u540e\u7aef\u7cfb\u7edf","text":""},{"location":"architecture/#cpu-ndarray_ops_cpupy","title":"CPU\u540e\u7aef (<code>ndarray_ops_cpu.py</code>)","text":"Python<pre><code># \u76f4\u63a5\u4f7f\u7528PyTorch\u64cd\u4f5c\ndef add(x, y):\n    return x + y\n\ndef matmul(x, y):\n    return torch.matmul(x, y)\n</code></pre>"},{"location":"architecture/#gpu-ndarray_ops_gpupy","title":"GPU\u540e\u7aef (<code>ndarray_ops_gpu.py</code>)","text":"Python<pre><code># \u4f7f\u7528Triton\u5b9e\u73b0\u7684GPU kernels\n@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n</code></pre>"},{"location":"architecture/#cuda-cuda_tensorpy","title":"CUDA\u5185\u5b58\u7ba1\u7406 (<code>cuda_tensor.py</code>)","text":"Python<pre><code>class CUDATensor:\n    \"\"\"\u7eafCUDA\u5b9e\u73b0\u7684\u5f20\u91cf\uff0c\u4e0d\u4f9d\u8d56PyTorch\"\"\"\n    def __init__(self, shape, dtype):\n        self._cuda_device, self._cuda_context = _ensure_cuda_initialized()\n        self._allocate_memory(shape, dtype)\n\n    def _allocate_memory(self, shape, dtype):\n        # \u4f7f\u7528CUDA Python API\u76f4\u63a5\u5206\u914dGPU\u5185\u5b58\n        size_bytes = prod(shape) * dtype.itemsize\n        result = cuda.cuMemAlloc(size_bytes)\n        self._data_ptr = check_cuda_error(result)\n</code></pre>"},{"location":"architecture/#nnmodulespy","title":"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757 (<code>nn/modules.py</code>)","text":"Python<pre><code>class Module:\n    \"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u57fa\u7c7b\"\"\"\n    def parameters(self) -&gt; List[Tensor]:\n        # \u9012\u5f52\u6536\u96c6\u6240\u6709\u53c2\u6570\n        return _unpack_params(self.__dict__)\n\n    def forward(self, *args, **kwargs):\n        # \u5b50\u7c7b\u5b9e\u73b0\u5177\u4f53\u7684\u524d\u5411\u4f20\u64ad\u903b\u8f91\n        raise NotImplementedError\n\nclass Linear(Module):\n    \"\"\"\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0\"\"\"\n    def __init__(self, in_features, out_features):\n        self.weight = Parameter(genesis.randn(out_features, in_features))\n        self.bias = Parameter(genesis.zeros(out_features))\n</code></pre>"},{"location":"architecture/#_6","title":"\ud83d\udd27 \u5173\u952e\u6280\u672f\u5b9e\u73b0","text":""},{"location":"architecture/#1_1","title":"1. \u5185\u5b58\u7ba1\u7406\u7b56\u7565","text":"<p>CPU\u5185\u5b58\u7ba1\u7406\uff1a - \u4f9d\u8d56PyTorch\u7684\u5185\u5b58\u6c60\u548c\u5783\u573e\u56de\u6536 - \u81ea\u52a8\u5904\u7406\u5185\u5b58\u5bf9\u9f50\u548c\u7f13\u5b58\u4f18\u5316</p> <p>GPU\u5185\u5b58\u7ba1\u7406\uff1a Python<pre><code>class CUDATensor:\n    def __init__(self, shape, dtype, base=None):\n        if base is not None:\n            # \u89c6\u56fe\u5f20\u91cf\uff1a\u5171\u4eab\u5185\u5b58\u4f46\u4fdd\u6301\u5bf9\u539f\u5f20\u91cf\u7684\u5f15\u7528\n            self.base = base\n            self._data_ptr = base._data_ptr + offset\n        else:\n            # \u65b0\u5f20\u91cf\uff1a\u5206\u914d\u72ec\u7acb\u5185\u5b58\n            self.base = None\n            self._data_ptr = cuda.cuMemAlloc(size_bytes)\n\n    def __del__(self):\n        # \u53ea\u6709\u57fa\u7840\u5f20\u91cf\u624d\u91ca\u653e\u5185\u5b58\n        if self.base is None and self._data_ptr:\n            cuda.cuMemFree(self._data_ptr)\n</code></pre></p>"},{"location":"architecture/#2_1","title":"2. \u8bbe\u5907\u62bd\u8c61","text":"Python<pre><code>class Device:\n    def __init__(self, name: str, mod: Any, device_id: Optional[int] = None):\n        self.name = name        # \"cpu\" \u6216 \"cuda\"\n        self.mod = mod          # \u5bf9\u5e94\u7684\u64cd\u4f5c\u6a21\u5757\n        self.device_id = device_id  # GPU\u8bbe\u5907ID\n\n    def randn(self, *shape, dtype=genesis.float32):\n        if self.name == \"cuda\":\n            return NDArray(CUDATensor(shape, dtype), device=self)\n        else:\n            return NDArray(torch.randn(*shape), device=self)\n</code></pre>"},{"location":"architecture/#3_1","title":"3. \u7c7b\u578b\u7cfb\u7edf","text":"Python<pre><code># dtypes.py - \u7edf\u4e00\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\nclass DType:\n    def __init__(self, name: str, torch_dtype, numpy_dtype, itemsize: int):\n        self.name = name\n        self.torch_dtype = torch_dtype\n        self.numpy_dtype = numpy_dtype  \n        self.itemsize = itemsize\n\n# \u652f\u6301\u7684\u6570\u636e\u7c7b\u578b\nfloat32 = DType(\"float32\", torch.float32, np.float32, 4)\nfloat16 = DType(\"float16\", torch.float16, np.float16, 2)\nbfloat16 = DType(\"bfloat16\", torch.bfloat16, np.dtype('uint16'), 2)\n</code></pre>"},{"location":"architecture/#_7","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316\u7b56\u7565","text":""},{"location":"architecture/#1-triton-kernel","title":"1. Triton Kernel\u4f18\u5316","text":"<p>Softmax\u5b9e\u73b0\uff1a Python<pre><code>@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride, output_row_stride, \n                  n_cols, BLOCK_SIZE: tl.constexpr):\n    # \u9ad8\u6548\u7684\u5e76\u884csoftmax\u5b9e\u73b0\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets &lt; n_cols, other=-float('inf'))\n\n    # \u6570\u503c\u7a33\u5b9a\u7684softmax\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets &lt; n_cols)\n</code></pre></p>"},{"location":"architecture/#2_2","title":"2. \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># amp.py - \u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\nenable_autocast = False\n\ndef _cast(value, dtype):\n    \"\"\"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n</code></pre>"},{"location":"architecture/#_8","title":"\ud83d\udd0d \u67b6\u6784\u4f18\u52bf","text":""},{"location":"architecture/#_9","title":"\u6559\u80b2\u4ef7\u503c","text":"<ol> <li>\u6e10\u8fdb\u5f0f\u590d\u6742\u5ea6\uff1a\u4ece\u7b80\u5355\u7684CPU\u5b9e\u73b0\u5230\u590d\u6742\u7684GPU\u4f18\u5316</li> <li>\u5b8c\u6574\u5b9e\u73b0\u5c55\u793a\uff1a\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u5b8c\u6574\u6784\u5efa\u8fc7\u7a0b  </li> <li>\u6e05\u6670\u7684\u6a21\u5757\u8fb9\u754c\uff1a\u6bcf\u4e2a\u7ec4\u4ef6\u804c\u8d23\u660e\u786e\uff0c\u4fbf\u4e8e\u7406\u89e3</li> </ol>"},{"location":"architecture/#_10","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ol> <li>\u53cc\u540e\u7aef\u8bbe\u8ba1\uff1aCPU\u7a33\u5b9a\u6027 + GPU\u9ad8\u6027\u80fd</li> <li>\u5185\u5b58\u5b89\u5168\uff1aRAII\u6a21\u5f0f\u7684\u5185\u5b58\u7ba1\u7406\uff0c\u9632\u6b62\u5185\u5b58\u6cc4\u6f0f</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7edf\u4e00\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u907f\u514d\u7c7b\u578b\u9519\u8bef</li> </ol>"},{"location":"architecture/#_11","title":"\u6027\u80fd\u7279\u6027","text":"<ol> <li>Triton\u4f18\u5316\uff1a\u73b0\u4ee3GPU kernel\u7f16\u5199\u65b9\u5f0f</li> <li>\u96f6\u62f7\u8d1d\u89c6\u56fe\uff1a\u9ad8\u6548\u7684\u5f20\u91cf\u89c6\u56fe\u64cd\u4f5c</li> <li>\u5e76\u884c\u8ba1\u7b97\uff1a\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u80fd\u529b</li> </ol>"},{"location":"architecture/#_12","title":"\ud83c\udfaf \u8bbe\u8ba1\u6743\u8861","text":""},{"location":"architecture/#cpu-vs-gpu","title":"CPU vs GPU \u5b9e\u73b0\u9009\u62e9","text":"<ul> <li>CPU\uff1a\u4f7f\u7528PyTorch\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u517c\u5bb9\u6027</li> <li>GPU\uff1a\u72ec\u7acb\u5b9e\u73b0\u5c55\u793a\u5b8c\u6574\u7684GPU\u7f16\u7a0b\u6808</li> </ul>"},{"location":"architecture/#vs","title":"\u7b80\u6d01\u6027 vs \u6027\u80fd","text":"<ul> <li>\u4fdd\u6301API\u7b80\u6d01\u7684\u540c\u65f6\uff0c\u5e95\u5c42\u5b9e\u73b0\u9ad8\u5ea6\u4f18\u5316</li> <li>\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u5c06\u590d\u6742\u6027\u9694\u79bb\u5728\u5e95\u5c42</li> </ul>"},{"location":"architecture/#vs_1","title":"\u6559\u80b2 vs \u751f\u4ea7","text":"<ul> <li>\u4ee3\u7801\u6ce8\u91cd\u53ef\u8bfb\u6027\u548c\u6559\u80b2\u4ef7\u503c</li> <li>\u6027\u80fd\u4ecd\u7136\u8fbe\u5230\u5b9e\u7528\u7ea7\u522b</li> </ul> <p>\u8fd9\u79cd\u67b6\u6784\u8bbe\u8ba1\u4f7f\u5f97Genesis\u65e2\u662f\u4e00\u4e2a\u4f18\u79c0\u7684\u5b66\u4e60\u8d44\u6e90\uff0c\u4e5f\u662f\u4e00\u4e2a\u529f\u80fd\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002</p>"},{"location":"contributing/","title":"\u8d21\u732e\u6307\u5357","text":"<p>\u6b22\u8fce\u4e3aGenesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8d21\u732e\u4ee3\u7801\uff01\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u4e86\u89e3\u5982\u4f55\u53c2\u4e0e\u9879\u76ee\u5f00\u53d1\u3002</p>"},{"location":"contributing/#_2","title":"\ud83e\udd1d \u8d21\u732e\u65b9\u5f0f","text":""},{"location":"contributing/#_3","title":"\u4ee3\u7801\u8d21\u732e","text":"<ul> <li>\u4fee\u590dbug</li> <li>\u6dfb\u52a0\u65b0\u7279\u6027</li> <li>\u6027\u80fd\u4f18\u5316</li> <li>\u6d4b\u8bd5\u5b8c\u5584</li> </ul>"},{"location":"contributing/#_4","title":"\u6587\u6863\u8d21\u732e","text":"<ul> <li>\u6539\u8fdb\u73b0\u6709\u6587\u6863</li> <li>\u6dfb\u52a0\u6559\u7a0b\u548c\u793a\u4f8b</li> <li>\u7ffb\u8bd1\u6587\u6863</li> <li>API\u6587\u6863\u5b8c\u5584</li> </ul>"},{"location":"contributing/#_5","title":"\u793e\u533a\u8d21\u732e","text":"<ul> <li>\u56de\u7b54\u95ee\u9898</li> <li>\u4ee3\u7801\u5ba1\u67e5</li> <li>\u95ee\u9898\u62a5\u544a</li> <li>\u529f\u80fd\u5efa\u8bae</li> </ul>"},{"location":"contributing/#_6","title":"\ud83d\udccb \u5f00\u53d1\u6d41\u7a0b","text":""},{"location":"contributing/#1","title":"1. \u51c6\u5907\u5de5\u4f5c","text":"Bash<pre><code># Fork\u9879\u76ee\u5230\u4f60\u7684GitHub\u8d26\u6237\n# Clone\u4f60\u7684fork\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# \u6dfb\u52a0\u4e0a\u6e38\u4ed3\u5e93\ngit remote add upstream https://github.com/phonism/genesis.git\n\n# \u521b\u5efa\u5f00\u53d1\u5206\u652f\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2","title":"2. \u5f00\u53d1\u73af\u5883\u642d\u5efa","text":"<p>\u8be6\u89c1\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u6587\u6863\u3002</p>"},{"location":"contributing/#3","title":"3. \u4ee3\u7801\u5f00\u53d1","text":"<ul> <li>\u9075\u5faa\u4ee3\u7801\u89c4\u8303</li> <li>\u6dfb\u52a0\u5355\u5143\u6d4b\u8bd5</li> <li>\u66f4\u65b0\u76f8\u5173\u6587\u6863</li> <li>\u63d0\u4ea4\u6e05\u6670\u7684commit\u6d88\u606f</li> </ul>"},{"location":"contributing/#4","title":"4. \u6d4b\u8bd5\u9a8c\u8bc1","text":"Bash<pre><code># \u8fd0\u884c\u6d4b\u8bd5\u5957\u4ef6\npython -m pytest tests/ -v\n\n# \u8fd0\u884c\u4ee3\u7801\u683c\u5f0f\u68c0\u67e5\nblack genesis/ tests/\nflake8 genesis/ tests/\n\n# \u8fd0\u884c\u7c7b\u578b\u68c0\u67e5\nmypy genesis/\n</code></pre>"},{"location":"contributing/#5-pr","title":"5. \u63d0\u4ea4PR","text":"<ul> <li>\u786e\u4fdd\u6240\u6709\u6d4b\u8bd5\u901a\u8fc7</li> <li>\u586b\u5199\u8be6\u7ec6\u7684PR\u63cf\u8ff0</li> <li>\u94fe\u63a5\u76f8\u5173\u7684Issue</li> <li>\u7b49\u5f85\u4ee3\u7801\u5ba1\u67e5</li> </ul>"},{"location":"contributing/#_7","title":"\ud83d\udcdd \u4ee3\u7801\u89c4\u8303","text":""},{"location":"contributing/#python","title":"Python\u98ce\u683c\u6307\u5357","text":"<p>\u6211\u4eec\u9075\u5faaPEP 8\u89c4\u8303\uff1a</p> Python<pre><code># \u597d\u7684\u793a\u4f8b\ndef compute_attention_weights(query, key, scale_factor):\n    \"\"\"Compute scaled dot-product attention weights.\n\n    Args:\n        query: Query tensor of shape [batch, seq_len, hidden_dim]\n        key: Key tensor of shape [batch, seq_len, hidden_dim] \n        scale_factor: Scaling factor for attention scores\n\n    Returns:\n        Attention weights of shape [batch, seq_len, seq_len]\n    \"\"\"\n    scores = genesis.matmul(query, key.transpose(-2, -1))\n    scaled_scores = scores * scale_factor\n    return genesis.softmax(scaled_scores, dim=-1)\n</code></pre>"},{"location":"contributing/#_8","title":"\u6587\u6863\u5b57\u7b26\u4e32","text":"<p>\u4f7f\u7528Google\u98ce\u683c\u7684docstring\uff1a</p> Python<pre><code>def example_function(param1: int, param2: str = \"default\") -&gt; bool:\n    \"\"\"One line summary of the function.\n\n    More detailed description if needed. Can span multiple lines.\n\n    Args:\n        param1: Description of param1\n        param2: Description of param2 with default value\n\n    Returns:\n        Description of return value\n\n    Raises:\n        ValueError: When param1 is negative\n\n    Example:\n        &gt;&gt;&gt; result = example_function(42, \"test\")\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n    if param1 &lt; 0:\n        raise ValueError(\"param1 must be non-negative\")\n    return param1 &gt; 0\n</code></pre>"},{"location":"contributing/#_9","title":"\u6d4b\u8bd5\u7f16\u5199","text":"Python<pre><code>import pytest\nimport genesis\n\nclass TestAttention:\n    \"\"\"Test attention mechanisms.\"\"\"\n\n    def test_basic_attention(self):\n        \"\"\"Test basic attention computation.\"\"\"\n        batch_size, seq_len, hidden_dim = 2, 4, 8\n\n        query = genesis.randn(batch_size, seq_len, hidden_dim)\n        key = genesis.randn(batch_size, seq_len, hidden_dim)\n        value = genesis.randn(batch_size, seq_len, hidden_dim)\n\n        attention = genesis.nn.MultiHeadAttention(hidden_dim, num_heads=2)\n        output = attention(query, key, value)\n\n        assert output.shape == (batch_size, seq_len, hidden_dim)\n\n    @pytest.mark.parametrize(\"num_heads\", [1, 2, 4, 8])\n    def test_different_head_counts(self, num_heads):\n        \"\"\"Test attention with different head counts.\"\"\"\n        # \u6d4b\u8bd5\u5b9e\u73b0\n        pass\n</code></pre>"},{"location":"contributing/#_10","title":"\ud83d\ude80 \u5f00\u53d1\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"contributing/#1_1","title":"1. \u5206\u652f\u7ba1\u7406","text":"Bash<pre><code># \u4e3b\u8981\u5206\u652f\nmain          # \u7a33\u5b9a\u7248\u672c\ndevelop       # \u5f00\u53d1\u7248\u672c\n\n# \u529f\u80fd\u5206\u652f\nfeature/xxx   # \u65b0\u529f\u80fd\u5f00\u53d1\nbugfix/xxx    # bug\u4fee\u590d\nhotfix/xxx    # \u7d27\u6025\u4fee\u590d\n</code></pre>"},{"location":"contributing/#2-commit","title":"2. Commit\u6d88\u606f\u683c\u5f0f","text":"Text Only<pre><code>type(scope): brief description\n\nDetailed description (optional)\n\nFixes #123\n</code></pre> <p>\u7c7b\u578b\u8bf4\u660e\uff1a - <code>feat</code>: \u65b0\u529f\u80fd - <code>fix</code>: bug\u4fee\u590d - <code>docs</code>: \u6587\u6863\u66f4\u65b0 - <code>style</code>: \u4ee3\u7801\u683c\u5f0f\u8c03\u6574 - <code>refactor</code>: \u91cd\u6784 - <code>perf</code>: \u6027\u80fd\u4f18\u5316 - <code>test</code>: \u6d4b\u8bd5\u76f8\u5173 - <code>chore</code>: \u6784\u5efa\u5de5\u5177\u7b49</p>"},{"location":"contributing/#3_1","title":"3. \u6027\u80fd\u8003\u8651","text":"<ul> <li>\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\u62f7\u8d1d</li> <li>\u4f7f\u7528in-place\u64cd\u4f5cwhen\u53ef\u80fd</li> <li>\u8003\u8651CUDA kernel\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f</li> <li>\u6dfb\u52a0\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5</li> </ul>"},{"location":"contributing/#bug","title":"\ud83d\udc1b Bug\u62a5\u544a","text":"<p>\u63d0\u4ea4bug\u65f6\u8bf7\u5305\u542b\uff1a</p> <ol> <li>\u73af\u5883\u4fe1\u606f</li> <li>Genesis\u7248\u672c</li> <li>Python\u7248\u672c</li> <li>CUDA\u7248\u672c</li> <li> <p>\u64cd\u4f5c\u7cfb\u7edf</p> </li> <li> <p>\u590d\u73b0\u6b65\u9aa4</p> </li> <li>\u6700\u5c0f\u53ef\u590d\u73b0\u4ee3\u7801</li> <li>\u9884\u671f\u884c\u4e3a</li> <li>\u5b9e\u9645\u884c\u4e3a</li> <li> <p>\u9519\u8bef\u4fe1\u606f</p> </li> <li> <p>\u76f8\u5173\u65e5\u5fd7</p> </li> <li>\u5b8c\u6574\u7684\u9519\u8bef\u5806\u6808</li> <li>\u76f8\u5173\u914d\u7f6e\u4fe1\u606f</li> </ol> <p>\u793a\u4f8b\uff1a Python<pre><code># \u6700\u5c0f\u590d\u73b0\u6848\u4f8b\nimport genesis\n\nmodel = genesis.nn.Linear(10, 5)\nx = genesis.randn(3, 10)\ny = model(x)  # \u8fd9\u91cc\u51fa\u73b0\u9519\u8bef\n\n# \u9519\u8bef\u4fe1\u606f\uff1a\n# RuntimeError: CUDA kernel launch failed\n</code></pre></p>"},{"location":"contributing/#_11","title":"\ud83c\udfaf \u8d21\u732e\u91cd\u70b9\u9886\u57df","text":"<p>\u5f53\u524d\u6211\u4eec\u7279\u522b\u6b22\u8fce\u4ee5\u4e0b\u9886\u57df\u7684\u8d21\u732e\uff1a</p>"},{"location":"contributing/#_12","title":"\u9ad8\u4f18\u5148\u7ea7","text":"<ul> <li> \u6027\u80fd\u4f18\u5316\u548c\u57fa\u51c6\u6d4b\u8bd5</li> <li> CUDA\u7b97\u5b50\u5b9e\u73b0</li> <li> \u6587\u6863\u548c\u6559\u7a0b\u5b8c\u5584</li> <li> \u6d4b\u8bd5\u8986\u76d6\u7387\u63d0\u5347</li> </ul>"},{"location":"contributing/#_13","title":"\u4e2d\u4f18\u5148\u7ea7","text":"<ul> <li> \u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42</li> <li> \u6570\u636e\u52a0\u8f7d\u5668\u4f18\u5316</li> <li> \u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301</li> <li> \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> </ul>"},{"location":"contributing/#_14","title":"\u4f4e\u4f18\u5148\u7ea7","text":"<ul> <li> \u53ef\u89c6\u5316\u5de5\u5177</li> <li> \u6a21\u578b\u90e8\u7f72\u652f\u6301</li> <li> \u7b2c\u4e09\u65b9\u6846\u67b6\u96c6\u6210</li> </ul>"},{"location":"contributing/#_15","title":"\ud83d\udcde \u8054\u7cfb\u6211\u4eec","text":"<ul> <li>GitHub Issues: \u62a5\u544a\u95ee\u9898\u548c\u529f\u80fd\u8bf7\u6c42</li> <li>GitHub Discussions: \u6280\u672f\u8ba8\u8bba\u548c\u95ee\u7b54</li> <li>Email: genesis-dev@example.com</li> </ul>"},{"location":"contributing/#_16","title":"\ud83c\udfc6 \u8d21\u732e\u8005\u8ba4\u53ef","text":"<p>\u6211\u4eec\u91cd\u89c6\u6bcf\u4e00\u4f4d\u8d21\u732e\u8005\u7684\u52aa\u529b\uff1a</p> <ul> <li>\u8d21\u732e\u8005\u5c06\u5217\u5728\u9879\u76eeREADME\u4e2d</li> <li>\u91cd\u5927\u8d21\u732e\u8005\u5c06\u83b7\u5f97\u7ef4\u62a4\u8005\u6743\u9650</li> <li>\u5b9a\u671f\u53d1\u5e03\u8d21\u732e\u8005\u901a\u8baf</li> </ul>"},{"location":"contributing/#_17","title":"\ud83d\udcc4 \u8bb8\u53ef\u8bc1","text":"<p>\u901a\u8fc7\u8d21\u732e\u4ee3\u7801\uff0c\u4f60\u540c\u610f\u4f60\u7684\u8d21\u732e\u5c06\u5728MIT\u8bb8\u53ef\u8bc1\u4e0b\u53d1\u5e03\u3002</p> <p>\u5f00\u59cb\u8d21\u732e</p> <p>\u51c6\u5907\u597d\u5f00\u59cb\u8d21\u732e\u4e86\u5417\uff1f\u5148\u4ece\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u5f00\u59cb\u5427\uff01</p> <p>\u611f\u8c22\u4f60\u4e3aGenesis\u9879\u76ee\u7684\u8d21\u732e\uff01\ud83c\udf89</p>"},{"location":"contributing/development/","title":"\u5f00\u53d1\u73af\u5883\u914d\u7f6e","text":"<p>\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u642d\u5efaGenesis\u5f00\u53d1\u73af\u5883\uff0c\u5305\u62ec\u4ee3\u7801\u7f16\u8f91\u3001\u8c03\u8bd5\u3001\u6d4b\u8bd5\u7b49\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u3002</p>"},{"location":"contributing/development/#_2","title":"\ud83d\udee0\ufe0f \u7cfb\u7edf\u8981\u6c42","text":""},{"location":"contributing/development/#_3","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: x86_64\u67b6\u6784\uff0c\u652f\u6301AVX\u6307\u4ee4\u96c6</li> <li>\u5185\u5b58: \u6700\u5c1116GB\uff0c\u63a8\u835032GB+</li> <li>GPU: NVIDIA GPU with CUDA\u652f\u6301 (\u5f00\u53d1GPU\u7b97\u5b50\u65f6\u9700\u8981)</li> <li>\u5b58\u50a8: 20GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"contributing/development/#_4","title":"\u8f6f\u4ef6\u8981\u6c42","text":"<ul> <li>\u64cd\u4f5c\u7cfb\u7edf: Linux (\u63a8\u8350Ubuntu 20.04+), macOS 10.15+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>Git: \u6700\u65b0\u7248\u672c</li> <li>CUDA: 11.8+ (GPU\u5f00\u53d1\u9700\u8981)</li> </ul>"},{"location":"contributing/development/#_5","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"contributing/development/#1","title":"1. \u514b\u9686\u4ed3\u5e93","text":"Bash<pre><code># \u514b\u9686\u4f60\u7684fork (\u63a8\u8350)\ngit clone https://github.com/YOUR_USERNAME/genesis.git\ncd genesis\n\n# \u6216\u514b\u9686\u4e3b\u4ed3\u5e93\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u6dfb\u52a0\u4e0a\u6e38\u4ed3\u5e93 (\u5982\u679cfork\u7684\u8bdd)\ngit remote add upstream https://github.com/phonism/genesis.git\n</code></pre>"},{"location":"contributing/development/#2-python","title":"2. \u521b\u5efaPython\u73af\u5883","text":"\u4f7f\u7528conda\u4f7f\u7528venv Bash<pre><code># \u521b\u5efa\u73af\u5883\nconda create -n genesis-dev python=3.9\nconda activate genesis-dev\n\n# \u5b89\u88c5\u57fa\u7840\u4f9d\u8d56\nconda install numpy matplotlib ipython jupyter\n</code></pre> Bash<pre><code># \u521b\u5efa\u73af\u5883\npython -m venv genesis-dev\nsource genesis-dev/bin/activate  # Linux/macOS\n# genesis-dev\\\\Scripts\\\\activate  # Windows\n\n# \u5347\u7ea7pip\npip install --upgrade pip setuptools wheel\n</code></pre>"},{"location":"contributing/development/#3","title":"3. \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56","text":"Bash<pre><code># \u5b89\u88c5PyTorch (\u6839\u636e\u4f60\u7684CUDA\u7248\u672c\u9009\u62e9)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU\u7248\u672c\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# \u5b89\u88c5Triton\npip install triton\n\n# \u5b89\u88c5\u5f00\u53d1\u5de5\u5177\npip install -r requirements-dev.txt\n</code></pre>"},{"location":"contributing/development/#4-genesis","title":"4. \u5b89\u88c5Genesis (\u5f00\u53d1\u6a21\u5f0f)","text":"Bash<pre><code># \u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5 (\u63a8\u8350)\npip install -e .\n\n# \u9a8c\u8bc1\u5b89\u88c5\npython -c \"import genesis; print('Genesis\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u6210\u529f\uff01')\"\n</code></pre>"},{"location":"contributing/development/#_6","title":"\ud83d\udce6 \u4f9d\u8d56\u7ba1\u7406","text":""},{"location":"contributing/development/#requirementstxt","title":"\u6838\u5fc3\u4f9d\u8d56 (requirements.txt)","text":"Text Only<pre><code>torch&gt;=2.0.0\ntriton&gt;=2.0.0\nnumpy&gt;=1.21.0\ncuda-python&gt;=11.8.0\n</code></pre>"},{"location":"contributing/development/#requirements-devtxt","title":"\u5f00\u53d1\u4f9d\u8d56 (requirements-dev.txt)","text":"Text Only<pre><code>pytest&gt;=7.0.0\npytest-cov&gt;=4.0.0\nblack&gt;=22.0.0\nflake8&gt;=5.0.0\nmypy&gt;=1.0.0\nisort&gt;=5.0.0\npre-commit&gt;=2.20.0\nsphinx&gt;=5.0.0\nmatplotlib&gt;=3.5.0\njupyter&gt;=1.0.0\nipython&gt;=8.0.0\n</code></pre>"},{"location":"contributing/development/#_7","title":"\ud83d\udd27 \u5f00\u53d1\u5de5\u5177\u914d\u7f6e","text":""},{"location":"contributing/development/#1-git","title":"1. Git\u914d\u7f6e","text":"Bash<pre><code># \u914d\u7f6e\u7528\u6237\u4fe1\u606f\ngit config user.name \"Your Name\"\ngit config user.email \"your.email@example.com\"\n\n# \u914d\u7f6e\u63d0\u4ea4\u6a21\u677f\necho \"feat: brief description\n\nMore detailed explanation (optional)\n\n- Change 1\n- Change 2\n\nFixes #123\" &gt; ~/.gitmessage\ngit config commit.template ~/.gitmessage\n</code></pre>"},{"location":"contributing/development/#2-pre-commit","title":"2. Pre-commit\u94a9\u5b50","text":"Bash<pre><code># \u5b89\u88c5pre-commit\npip install pre-commit\n\n# \u5b89\u88c5\u94a9\u5b50\npre-commit install\n\n# \u624b\u52a8\u8fd0\u884c\u68c0\u67e5\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/development/#3-ide","title":"3. IDE\u914d\u7f6e","text":"VS CodePyCharm <p>\u63a8\u8350\u5b89\u88c5\u4ee5\u4e0b\u6269\u5c55\uff1a</p> JSON<pre><code>// .vscode/extensions.json\n{\n    \"recommendations\": [\n        \"ms-python.python\",\n        \"ms-python.black-formatter\", \n        \"ms-python.flake8\",\n        \"ms-python.mypy-type-checker\",\n        \"ms-toolsai.jupyter\",\n        \"ms-vscode.cpptools\"\n    ]\n}\n</code></pre> <p>\u914d\u7f6e\u6587\u4ef6\uff1a JSON<pre><code>// .vscode/settings.json\n{\n    \"python.defaultInterpreterPath\": \"./genesis-dev/bin/python\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.formatting.provider\": \"black\",\n    \"python.formatting.blackArgs\": [\"--line-length=88\"],\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\"tests/\"]\n}\n</code></pre></p> <ol> <li>\u6253\u5f00\u9879\u76ee\u8bbe\u7f6e (File -&gt; Settings)</li> <li>\u914d\u7f6ePython\u89e3\u91ca\u5668\u6307\u5411\u865a\u62df\u73af\u5883</li> <li>\u542f\u7528\u4ee3\u7801\u683c\u5f0f\u5316\u5de5\u5177 (Black, isort)</li> <li>\u914d\u7f6e\u6d4b\u8bd5\u8fd0\u884c\u5668\u4e3apytest</li> </ol>"},{"location":"contributing/development/#4","title":"4. \u73af\u5883\u53d8\u91cf","text":"Bash<pre><code># \u5f00\u53d1\u73af\u5883\u53d8\u91cf\nexport GENESIS_DEV=1\nexport PYTHONPATH=\"${PWD}:${PYTHONPATH}\"\nexport CUDA_VISIBLE_DEVICES=0  # \u6307\u5b9aGPU\u8bbe\u5907\n\n# \u6dfb\u52a0\u5230 ~/.bashrc \u6216 ~/.zshrc\necho 'export GENESIS_DEV=1' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"contributing/development/#_8","title":"\ud83e\uddea \u6d4b\u8bd5\u6846\u67b6","text":""},{"location":"contributing/development/#_9","title":"\u6d4b\u8bd5\u76ee\u5f55\u7ed3\u6784","text":"Text Only<pre><code>tests/\n\u251c\u2500\u2500 conftest.py              # pytest\u914d\u7f6e\n\u251c\u2500\u2500 test_autograd.py         # \u81ea\u52a8\u5fae\u5206\u6d4b\u8bd5\n\u251c\u2500\u2500 test_nn.py              # \u795e\u7ecf\u7f51\u7edc\u6d4b\u8bd5\n\u251c\u2500\u2500 test_cuda_tensor.py     # CUDA\u5f20\u91cf\u6d4b\u8bd5\n\u251c\u2500\u2500 test_functional.py      # \u51fd\u6570\u5f0f\u63a5\u53e3\u6d4b\u8bd5\n\u251c\u2500\u2500 benchmarks/             # \u6027\u80fd\u6d4b\u8bd5\n\u2502   \u251c\u2500\u2500 bench_matmul.py\n\u2502   \u2514\u2500\u2500 bench_attention.py\n\u2514\u2500\u2500 integration/            # \u96c6\u6210\u6d4b\u8bd5\n    \u251c\u2500\u2500 test_training.py\n    \u2514\u2500\u2500 test_models.py\n</code></pre>"},{"location":"contributing/development/#_10","title":"\u8fd0\u884c\u6d4b\u8bd5","text":"Bash<pre><code># \u8fd0\u884c\u6240\u6709\u6d4b\u8bd5\npytest tests/ -v\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\u6587\u4ef6\npytest tests/test_nn.py -v\n\n# \u8fd0\u884c\u7279\u5b9a\u6d4b\u8bd5\u51fd\u6570\npytest tests/test_nn.py::test_linear_layer -v\n\n# \u8fd0\u884c\u5e26\u8986\u76d6\u7387\u7684\u6d4b\u8bd5\npytest tests/ --cov=genesis --cov-report=html\n\n# \u8fd0\u884c\u6027\u80fd\u6d4b\u8bd5\npytest tests/benchmarks/ -v --benchmark-only\n</code></pre>"},{"location":"contributing/development/#_11","title":"\u7f16\u5199\u6d4b\u8bd5","text":"Python<pre><code># tests/test_example.py\nimport pytest\nimport genesis\nimport genesis.nn as nn\n\nclass TestExample:\n    \"\"\"Example test class.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Setup before each test method.\"\"\"\n        self.device = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\n\n    def test_basic_operation(self):\n        \"\"\"Test basic tensor operations.\"\"\"\n        x = genesis.randn(3, 4, device=self.device)\n        y = genesis.randn(3, 4, device=self.device)\n        z = x + y\n\n        assert z.shape == (3, 4)\n        assert z.device == self.device\n\n    @pytest.mark.parametrize(\"input_size,output_size\", [\n        (10, 5),\n        (128, 64),\n        (512, 256)\n    ])\n    def test_linear_layers(self, input_size, output_size):\n        \"\"\"Test linear layers with different sizes.\"\"\"\n        layer = nn.Linear(input_size, output_size).to(self.device)\n        x = genesis.randn(32, input_size, device=self.device)\n\n        output = layer(x)\n        assert output.shape == (32, output_size)\n\n    @pytest.mark.cuda\n    def test_cuda_specific(self):\n        \"\"\"Test CUDA-specific functionality.\"\"\"\n        if not genesis.cuda.is_available():\n            pytest.skip(\"CUDA not available\")\n\n        x = genesis.randn(10, 10, device='cuda')\n        assert x.is_cuda\n</code></pre>"},{"location":"contributing/development/#_12","title":"\ud83d\udcca \u6027\u80fd\u5206\u6790","text":""},{"location":"contributing/development/#1-profiler","title":"1. \u5185\u7f6eprofiler","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# \u4f7f\u7528context manager\nwith profiler.profile() as prof:\n    # \u4f60\u7684\u4ee3\u7801\n    x = genesis.randn(1000, 1000)\n    y = genesis.matmul(x, x)\n\n# \u6253\u5370\u7ed3\u679c\nprof.print_stats()\n\n# \u4fdd\u5b58\u7ed3\u679c\nprof.export_chrome_trace(\"profile.json\")\n</code></pre>"},{"location":"contributing/development/#2","title":"2. \u5185\u5b58\u5206\u6790","text":"Python<pre><code>import genesis\n\n# \u542f\u7528\u5185\u5b58\u8ddf\u8e2a\ngenesis.cuda.memory.enable_debug()\n\n# \u4f60\u7684\u4ee3\u7801\nx = genesis.randn(1000, 1000, device='cuda')\ny = genesis.matmul(x, x)\n\n# \u67e5\u770b\u5185\u5b58\u4f7f\u7528\nprint(f\"\u5185\u5b58\u4f7f\u7528: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"\u7f13\u5b58\u5185\u5b58: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# \u5185\u5b58\u5feb\u7167\nsnapshot = genesis.cuda.memory.memory_snapshot()\n</code></pre>"},{"location":"contributing/development/#3_1","title":"3. \u57fa\u51c6\u6d4b\u8bd5","text":"Python<pre><code># benchmark/bench_example.py\nimport time\nimport genesis\nimport torch\n\ndef benchmark_matmul():\n    \"\"\"Benchmark matrix multiplication.\"\"\"\n    sizes = [128, 256, 512, 1024]\n\n    for size in sizes:\n        # Genesis\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis_time = time.time() - start_time\n\n        # PyTorch\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        start_time = time.time()\n        for _ in range(100):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch_time = time.time() - start_time\n\n        print(f\"Size {size}x{size}:\")\n        print(f\"  Genesis: {genesis_time:.4f}s\")\n        print(f\"  PyTorch: {torch_time:.4f}s\")\n        print(f\"  Ratio: {genesis_time/torch_time:.2f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_matmul()\n</code></pre>"},{"location":"contributing/development/#_13","title":"\ud83d\udc1b \u8c03\u8bd5\u6280\u5de7","text":""},{"location":"contributing/development/#1_1","title":"1. \u8c03\u8bd5\u73af\u5883\u53d8\u91cf","text":"Bash<pre><code># \u542f\u7528\u8c03\u8bd5\u6a21\u5f0f\nexport GENESIS_DEBUG=1\nexport CUDA_LAUNCH_BLOCKING=1  # \u540c\u6b65CUDA\u6267\u884c\nexport PYTHONFAULTHANDLER=1    # Python\u9519\u8bef\u5904\u7406\n</code></pre>"},{"location":"contributing/development/#2_1","title":"2. \u65e5\u5fd7\u914d\u7f6e","text":"Python<pre><code>import logging\nimport genesis\n\n# \u914d\u7f6e\u65e5\u5fd7\nlogging.basicConfig(level=logging.DEBUG)\ngenesis.set_log_level('DEBUG')\n\n# \u4f7f\u7528\u65e5\u5fd7\nlogger = logging.getLogger(__name__)\nlogger.debug(\"\u8c03\u8bd5\u4fe1\u606f\")\n</code></pre>"},{"location":"contributing/development/#3_2","title":"3. \u65ad\u70b9\u8c03\u8bd5","text":"Python<pre><code>import pdb\n\ndef buggy_function(x):\n    pdb.set_trace()  # \u8bbe\u7f6e\u65ad\u70b9\n    y = x * 2\n    return y\n\n# \u6216\u4f7f\u7528ipdb (\u9700\u8981\u5b89\u88c5: pip install ipdb)\nimport ipdb\nipdb.set_trace()\n</code></pre>"},{"location":"contributing/development/#_14","title":"\ud83d\udcda \u6587\u6863\u5f00\u53d1","text":""},{"location":"contributing/development/#_15","title":"\u6784\u5efa\u6587\u6863","text":"Bash<pre><code># \u5b89\u88c5\u6587\u6863\u4f9d\u8d56\npip install -r docs/requirements.txt\n\n# \u672c\u5730\u670d\u52a1\u5668\nmkdocs serve\n\n# \u6784\u5efa\u9759\u6001\u6587\u4ef6\nmkdocs build\n\n# \u90e8\u7f72\u5230GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"contributing/development/#api","title":"API\u6587\u6863\u751f\u6210","text":"Bash<pre><code># \u81ea\u52a8\u751f\u6210API\u6587\u6863\npython scripts/generate_api_docs.py\n\n# \u68c0\u67e5docstring\u683c\u5f0f\npydocstyle genesis/\n</code></pre>"},{"location":"contributing/development/#_16","title":"\ud83d\ude80 \u63d0\u4ea4\u4ee3\u7801","text":""},{"location":"contributing/development/#1_2","title":"1. \u4ee3\u7801\u68c0\u67e5","text":"Bash<pre><code># \u683c\u5f0f\u5316\u4ee3\u7801\nblack genesis/ tests/\nisort genesis/ tests/\n\n# \u7c7b\u578b\u68c0\u67e5\nmypy genesis/\n\n# \u4ee3\u7801\u8d28\u91cf\u68c0\u67e5\nflake8 genesis/ tests/\n\n# \u8fd0\u884c\u6d4b\u8bd5\npytest tests/ -x\n</code></pre>"},{"location":"contributing/development/#2_2","title":"2. \u63d0\u4ea4\u6d41\u7a0b","text":"Bash<pre><code># 1. \u540c\u6b65\u6700\u65b0\u4ee3\u7801\ngit fetch upstream\ngit rebase upstream/main\n\n# 2. \u521b\u5efa\u529f\u80fd\u5206\u652f\ngit checkout -b feature/your-feature\n\n# 3. \u5f00\u53d1\u548c\u6d4b\u8bd5\n# ... \u4f60\u7684\u5f00\u53d1\u5de5\u4f5c ...\n\n# 4. \u63d0\u4ea4\u4ee3\u7801\ngit add .\ngit commit -m \"feat: add your feature\"\n\n# 5. \u63a8\u9001\u5206\u652f\ngit push origin feature/your-feature\n\n# 6. \u521b\u5efaPull Request\n</code></pre>"},{"location":"contributing/development/#_17","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"contributing/development/#q-cuda","title":"Q: CUDA\u76f8\u5173\u9519\u8bef\uff1f","text":"<p>A: \u68c0\u67e5CUDA\u7248\u672c\u517c\u5bb9\u6027\uff0c\u786e\u4fddPyTorch\u548cTriton\u7248\u672c\u5339\u914d\u3002</p>"},{"location":"contributing/development/#q","title":"Q: \u6d4b\u8bd5\u5931\u8d25\uff1f","text":"<p>A: \u8fd0\u884c <code>pytest tests/ -v</code> \u67e5\u770b\u8be6\u7ec6\u9519\u8bef\u4fe1\u606f\uff0c\u68c0\u67e5\u73af\u5883\u914d\u7f6e\u3002</p>"},{"location":"contributing/development/#q_1","title":"Q: \u6027\u80fd\u95ee\u9898\uff1f","text":"<p>A: \u4f7f\u7528profiler\u5206\u6790\u74f6\u9888\uff0c\u68c0\u67e5\u662f\u5426\u542f\u7528\u4e86GPU\u52a0\u901f\u3002</p>"},{"location":"contributing/development/#q_2","title":"Q: \u5185\u5b58\u4e0d\u8db3\uff1f","text":"<p>A: \u51cf\u5c0f\u6d4b\u8bd5\u7528\u4f8b\u7684\u6570\u636e\u89c4\u6a21\uff0c\u542f\u7528CPU\u56de\u9000\u6a21\u5f0f\u3002</p> <p>\u5f00\u53d1\u73af\u5883\u914d\u7f6e\u5b8c\u6210</p> <p>\u73b0\u5728\u4f60\u53ef\u4ee5\u5f00\u59cb\u4e3aGenesis\u8d21\u732e\u4ee3\u7801\u4e86\uff01</p> <p>\u4e0b\u4e00\u6b65\uff1a\u4e86\u89e3\u6d4b\u8bd5\u89c4\u8303 \u8fd4\u56de\u8d21\u732e\u6307\u5357</p>"},{"location":"contributing/testing/","title":"\u6d4b\u8bd5\u89c4\u8303","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>\u672c\u6587\u6863\u89c4\u5b9a\u4e86Genesis\u9879\u76ee\u7684\u6d4b\u8bd5\u6807\u51c6\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"contributing/testing/#_2","title":"\ud83c\udfaf \u6d4b\u8bd5\u539f\u5219","text":""},{"location":"contributing/testing/#1","title":"1. \u6d4b\u8bd5\u91d1\u5b57\u5854","text":"<ul> <li>\u5355\u5143\u6d4b\u8bd5 (70%): \u6d4b\u8bd5\u5355\u4e2a\u51fd\u6570\u548c\u7c7b</li> <li>\u96c6\u6210\u6d4b\u8bd5 (20%): \u6d4b\u8bd5\u7ec4\u4ef6\u95f4\u4ea4\u4e92</li> <li>\u7aef\u5230\u7aef\u6d4b\u8bd5 (10%): \u6d4b\u8bd5\u5b8c\u6574\u5de5\u4f5c\u6d41</li> </ul>"},{"location":"contributing/testing/#2","title":"2. \u6d4b\u8bd5\u8986\u76d6\u7387","text":"<ul> <li>\u76ee\u6807\u8986\u76d6\u7387: 90%+</li> <li>\u5173\u952e\u6a21\u5757\u8981\u6c42: 95%+</li> <li>\u65b0\u4ee3\u7801\u8981\u6c42: 100%</li> </ul>"},{"location":"contributing/testing/#_3","title":"\ud83d\udccb \u6d4b\u8bd5\u5206\u7c7b","text":""},{"location":"contributing/testing/#_4","title":"\u5355\u5143\u6d4b\u8bd5","text":"Python<pre><code>def test_tensor_creation():\n    \"\"\"Test basic tensor creation.\"\"\"\n    x = genesis.randn(3, 4)\n    assert x.shape == (3, 4)\n    assert x.dtype == genesis.float32\n</code></pre>"},{"location":"contributing/testing/#_5","title":"\u6027\u80fd\u6d4b\u8bd5","text":"Python<pre><code>@pytest.mark.benchmark\ndef test_matmul_performance():\n    \"\"\"Benchmark matrix multiplication performance.\"\"\"\n    # WIP: \u6027\u80fd\u6d4b\u8bd5\u5b9e\u73b0\n    pass\n</code></pre>"},{"location":"contributing/testing/#gpu","title":"GPU\u6d4b\u8bd5","text":"Python<pre><code>@pytest.mark.cuda\ndef test_cuda_operations():\n    \"\"\"Test CUDA-specific operations.\"\"\"\n    if not genesis.cuda.is_available():\n        pytest.skip(\"CUDA not available\")\n\n    x = genesis.randn(10, 10, device='cuda')\n    y = genesis.matmul(x, x)\n    assert y.device.type == 'cuda'\n</code></pre>"},{"location":"contributing/testing/#_6","title":"\ud83d\ude80 \u8fd0\u884c\u6d4b\u8bd5","text":"Bash<pre><code># \u6240\u6709\u6d4b\u8bd5\npytest tests/ -v\n\n# \u7279\u5b9a\u6807\u8bb0\npytest tests/ -m \"not slow\" -v\n\n# \u8986\u76d6\u7387\u62a5\u544a\npytest tests/ --cov=genesis --cov-report=html\n</code></pre>"},{"location":"contributing/testing/#_7","title":"\ud83d\udcca \u6d4b\u8bd5\u5de5\u5177","text":"<ul> <li>pytest: \u4e3b\u8981\u6d4b\u8bd5\u6846\u67b6</li> <li>pytest-cov: \u8986\u76d6\u7387\u7edf\u8ba1</li> <li>pytest-benchmark: \u6027\u80fd\u6d4b\u8bd5</li> <li>pytest-xdist: \u5e76\u884c\u6d4b\u8bd5</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"},{"location":"core-components/","title":"\u6838\u5fc3\u7ec4\u4ef6\u6982\u8ff0","text":"<p>Genesis\u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u5f20\u91cf\u7cfb\u7edf\u3001\u81ea\u52a8\u5fae\u5206\u5f15\u64ce\u3001\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\u548c\u51fd\u6570\u5f0f\u64cd\u4f5c\u63a5\u53e3\u3002</p>"},{"location":"core-components/#_2","title":"\ud83e\udde9 \u7ec4\u4ef6\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u6838\u5fc3\u7ec4\u4ef6\"\n        A[Tensor\u5f20\u91cf] --&gt; B[\u81ea\u52a8\u5fae\u5206\u5f15\u64ce]\n        C[\u6570\u636e\u7c7b\u578b\u7cfb\u7edf] --&gt; A\n        D[\u51fd\u6570\u5f0f\u64cd\u4f5c] --&gt; A\n        E[\u521d\u59cb\u5316\u51fd\u6570] --&gt; A\n    end\n\n    subgraph \"\u81ea\u52a8\u5fae\u5206\u8be6\u7ec6\"\n        B --&gt; F[Function\u57fa\u7c7b]\n        B --&gt; G[Context\u4e0a\u4e0b\u6587]\n        B --&gt; H[\u8ba1\u7b97\u56fe\u6784\u5efa]\n        B --&gt; I[\u53cd\u5411\u4f20\u64ad]\n    end\n\n    subgraph \"\u7c7b\u578b\u7cfb\u7edf\"\n        C --&gt; J[DType\u7c7b]\n        C --&gt; K[\u7c7b\u578b\u8f6c\u6362]\n        C --&gt; L[\u7cbe\u5ea6\u7ba1\u7406]\n    end\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0</code></pre>"},{"location":"core-components/#_3","title":"\ud83c\udfaf \u6838\u5fc3\u7ec4\u4ef6\u6e05\u5355","text":"\u7ec4\u4ef6 \u6587\u4ef6 \u4e3b\u8981\u529f\u80fd \u5f20\u91cf\u7cfb\u7edf <code>autograd.py</code> \u57fa\u7840\u6570\u636e\u7ed3\u6784\u3001\u81ea\u52a8\u5fae\u5206 \u6570\u636e\u7c7b\u578b <code>dtypes.py</code> \u7edf\u4e00\u7c7b\u578b\u7cfb\u7edf\u3001\u7cbe\u5ea6\u7ba1\u7406 \u51fd\u6570\u5f0f\u64cd\u4f5c <code>functional.py</code> \u5f20\u91cf\u64cd\u4f5c\u7684\u51fd\u6570\u5f0f\u63a5\u53e3 \u521d\u59cb\u5316 <code>init.py</code> \u5f20\u91cf\u521b\u5efa\u548c\u521d\u59cb\u5316 \u540e\u7aef\u62bd\u8c61 <code>backend.py</code> \u8bbe\u5907\u548c\u540e\u7aef\u7ba1\u7406"},{"location":"core-components/#_4","title":"\ud83d\ude80 \u8bbe\u8ba1\u7279\u8272","text":""},{"location":"core-components/#1","title":"1. \u7edf\u4e00\u7684\u5f20\u91cf\u63a5\u53e3","text":"<ul> <li>\u4e00\u81f4\u7684API\uff1a\u65e0\u8bbaCPU\u8fd8\u662fGPU\uff0c\u7528\u6237\u4f7f\u7528\u76f8\u540c\u7684\u63a5\u53e3</li> <li>\u900f\u660e\u7684\u8bbe\u5907\u5207\u6362\uff1a\u81ea\u52a8\u5904\u7406\u4e0d\u540c\u8bbe\u5907\u95f4\u7684\u6570\u636e\u8f6c\u6362</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u7684\u7c7b\u578b\u68c0\u67e5</li> </ul>"},{"location":"core-components/#2","title":"2. \u9ad8\u6548\u7684\u81ea\u52a8\u5fae\u5206","text":"<ul> <li>\u60f0\u6027\u8ba1\u7b97\u56fe\uff1a\u6309\u9700\u6784\u5efa\u8ba1\u7b97\u56fe\uff0c\u8282\u7701\u5185\u5b58</li> <li>\u667a\u80fd\u68af\u5ea6\u4f20\u64ad\uff1a\u4f18\u5316\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5</li> <li>\u5185\u5b58\u4f18\u5316\uff1a\u81ea\u52a8\u91ca\u653e\u4e0d\u518d\u9700\u8981\u7684\u4e2d\u95f4\u7ed3\u679c</li> </ul>"},{"location":"core-components/#3","title":"3. \u7075\u6d3b\u7684\u7c7b\u578b\u7cfb\u7edf","text":"<ul> <li>\u6df7\u5408\u7cbe\u5ea6\u652f\u6301\uff1a\u81ea\u52a8\u5728FP32\u548cFP16\u95f4\u8f6c\u6362</li> <li>\u8bbe\u5907\u65e0\u5173\uff1a\u7c7b\u578b\u5b9a\u4e49\u72ec\u7acb\u4e8e\u5177\u4f53\u8bbe\u5907</li> <li>NumPy\u517c\u5bb9\uff1a\u65e0\u7f1d\u5bf9\u63a5NumPy\u751f\u6001</li> </ul>"},{"location":"core-components/#_5","title":"\ud83d\udcca \u6027\u80fd\u7279\u6027","text":""},{"location":"core-components/#_6","title":"\u5185\u5b58\u6548\u7387","text":"<ul> <li>\u89c6\u56fe\u64cd\u4f5c\u96f6\u62f7\u8d1d\uff1areshape\u3001transpose\u7b49\u64cd\u4f5c\u4e0d\u590d\u5236\u6570\u636e</li> <li>\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff1a\u57fa\u4e8e\u5f15\u7528\u8ba1\u6570\u7684\u81ea\u52a8\u5185\u5b58\u91ca\u653e</li> <li>\u68af\u5ea6\u7d2f\u79ef\u4f18\u5316\uff1a\u51cf\u5c11\u4e34\u65f6\u5f20\u91cf\u521b\u5efa</li> </ul>"},{"location":"core-components/#_7","title":"\u8ba1\u7b97\u4f18\u5316","text":"<ul> <li>\u5ef6\u8fdf\u6267\u884c\uff1a\u64cd\u4f5c\u5728\u9700\u8981\u65f6\u624d\u771f\u6b63\u6267\u884c</li> <li>\u878d\u5408\u4f18\u5316\uff1a\u76f8\u90bb\u64cd\u4f5c\u81ea\u52a8\u878d\u5408\u4ee5\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee</li> <li>\u5e76\u884c\u8ba1\u7b97\uff1a\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u80fd\u529b</li> </ul>"},{"location":"core-components/#_8","title":"\ud83d\udd17 \u7ec4\u4ef6\u95f4\u534f\u4f5c","text":""},{"location":"core-components/#_9","title":"\u5f20\u91cf\u521b\u5efa\u6d41\u7a0b","text":"Python<pre><code># \u7528\u6237\u8c03\u7528\nx = genesis.randn(3, 4)\n\n# \u5185\u90e8\u6d41\u7a0b\ninit.randn() -&gt; \nNDArray.randn() -&gt; \nDevice.randn() -&gt; \nTensor.__init__() -&gt;\n\u8bbe\u7f6erequires_grad\u7b49\u5c5e\u6027\n</code></pre>"},{"location":"core-components/#_10","title":"\u81ea\u52a8\u5fae\u5206\u6d41\u7a0b","text":"Python<pre><code># \u524d\u5411\u4f20\u64ad\nz = x * y + x.sum()\n\n# \u6784\u5efa\u8ba1\u7b97\u56fe\nMulFunction.apply(x, y) -&gt; \nSumFunction.apply(x) -&gt;\nAddFunction.apply(mul_result, sum_result) -&gt;\n\u8bbe\u7f6ecreator\u5173\u7cfb\n\n# \u53cd\u5411\u4f20\u64ad\nz.backward()\n\n# \u8ba1\u7b97\u68af\u5ea6\ntopo_sort(z) -&gt;\n\u9006\u62d3\u6251\u5e8f\u904d\u5386 -&gt;\n\u8c03\u7528\u5404Function\u7684backward() -&gt;\n\u68af\u5ea6\u7d2f\u79ef\u5230\u53f6\u5b50\u8282\u70b9\n</code></pre>"},{"location":"core-components/#_11","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84\u5efa\u8bae","text":""},{"location":"core-components/#_12","title":"\u521d\u7ea7\u7528\u6237","text":"<ol> <li>\u5f20\u91cf\u57fa\u7840 - \u4e86\u89e3Tensor\u7684\u521b\u5efa\u548c\u57fa\u672c\u64cd\u4f5c</li> <li>\u81ea\u52a8\u5fae\u5206 - \u7406\u89e3requires_grad\u548cbackward()</li> <li>\u8bbe\u5907\u7ba1\u7406 - \u5b66\u4e60CPU/GPU\u5207\u6362</li> </ol>"},{"location":"core-components/#_13","title":"\u4e2d\u7ea7\u7528\u6237","text":"<ol> <li>\u6570\u636e\u7c7b\u578b - \u638c\u63e1\u4e0d\u540c\u7cbe\u5ea6\u7684\u4f7f\u7528\u573a\u666f</li> <li>\u51fd\u6570\u5f0f\u63a5\u53e3 - \u4f7f\u7528functional\u6a21\u5757</li> <li>\u5185\u5b58\u4f18\u5316 - \u7406\u89e3\u89c6\u56fe\u64cd\u4f5c\u548c\u5185\u5b58\u7ba1\u7406</li> </ol>"},{"location":"core-components/#_14","title":"\u9ad8\u7ea7\u7528\u6237","text":"<ol> <li>\u81ea\u5b9a\u4e49Function - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u7684\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u6548\u7387</li> <li>\u6e90\u7801\u7406\u89e3 - \u6df1\u5165\u7406\u89e3\u5404\u7ec4\u4ef6\u7684\u5b9e\u73b0\u7ec6\u8282</li> </ol> <p>\u5404\u7ec4\u4ef6\u7684\u8be6\u7ec6\u6587\u6863\u8bf7\u67e5\u770b\u5bf9\u5e94\u7684\u4e13\u95e8\u9875\u9762\uff1a</p> <ul> <li>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf - \u6df1\u5165\u7406\u89e3\u8ba1\u7b97\u56fe\u548c\u68af\u5ea6\u8ba1\u7b97</li> <li>\u5f20\u91cf\u64cd\u4f5c - \u5168\u9762\u7684\u5f20\u91cf\u64cd\u4f5c\u6307\u5357  </li> <li>\u6570\u636e\u7c7b\u578b - \u7c7b\u578b\u7cfb\u7edf\u548c\u7cbe\u5ea6\u7ba1\u7406</li> <li>\u51fd\u6570\u5f0f\u63a5\u53e3 - \u51fd\u6570\u5f0f\u7f16\u7a0b\u98ce\u683c\u7684\u64cd\u4f5c</li> </ul>"},{"location":"core-components/autograd/","title":"\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf","text":"<p>Genesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u662f\u6846\u67b6\u7684\u6838\u5fc3\uff0c\u8d1f\u8d23\u6784\u5efa\u8ba1\u7b97\u56fe\u3001\u6267\u884c\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u3002\u7cfb\u7edf\u8bbe\u8ba1\u7b80\u6d01\u800c\u9ad8\u6548\uff0c\u652f\u6301\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u3002</p>"},{"location":"core-components/autograd/#_2","title":"\ud83c\udfaf \u7cfb\u7edf\u6982\u8ff0","text":"<p>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u57fa\u4e8e\u52a8\u6001\u8ba1\u7b97\u56fe\u5b9e\u73b0\uff0c\u4e3b\u8981\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a</p> <ul> <li>Tensor - \u643a\u5e26\u68af\u5ea6\u4fe1\u606f\u7684\u5f20\u91cf</li> <li>Function - \u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u62bd\u8c61\u57fa\u7c7b</li> <li>Context - \u4fdd\u5b58\u524d\u5411\u4f20\u64ad\u4e2d\u95f4\u7ed3\u679c\u7684\u4e0a\u4e0b\u6587</li> </ul>"},{"location":"core-components/autograd/#_3","title":"\ud83c\udfd7\ufe0f \u6838\u5fc3\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"\u8ba1\u7b97\u56fe\u8282\u70b9\"\n        A[Tensor] --&gt; B[data NDArray]\n        A --&gt; C[grad Tensor]\n        A --&gt; D[creator Function]\n        A --&gt; E[requires_grad bool]\n    end\n\n    subgraph \"\u64cd\u4f5c\u8282\u70b9\"\n        F[Function] --&gt; G[forward]\n        F --&gt; H[backward]\n        F --&gt; I[Context]\n        I --&gt; J[saved_tensors]\n    end\n\n    subgraph \"\u6267\u884c\u6d41\u7a0b\"\n        K[\u524d\u5411\u4f20\u64ad] --&gt; L[\u6784\u5efa\u8ba1\u7b97\u56fe]\n        L --&gt; M[\u4fdd\u5b58\u4e2d\u95f4\u7ed3\u679c]\n        M --&gt; N[\u53cd\u5411\u4f20\u64ad]\n        N --&gt; O[\u68af\u5ea6\u8ba1\u7b97]\n        O --&gt; P[\u68af\u5ea6\u7d2f\u79ef]\n    end\n\n    A --&gt; F\n    F --&gt; A\n\n    style A fill:#e1f5fe\n    style F fill:#f3e5f5\n    style I fill:#e8f5e8</code></pre>"},{"location":"core-components/autograd/#tensor","title":"\ud83e\uddee Tensor\u7c7b\u8be6\u89e3","text":""},{"location":"core-components/autograd/#_4","title":"\u6838\u5fc3\u5c5e\u6027","text":"Python<pre><code>class Tensor:\n    grad: \"Tensor\"          # \u68af\u5ea6\u5f20\u91cf\n    creator: Function       # \u521b\u5efa\u6b64\u5f20\u91cf\u7684\u64cd\u4f5c\n    inputs: List[\"Tensor\"]  # \u8f93\u5165\u5f20\u91cf\u5217\u8868\n    data: NDArray          # \u5e95\u5c42\u6570\u636e\u5b58\u50a8\n    requires_grad: bool    # \u662f\u5426\u9700\u8981\u8ba1\u7b97\u68af\u5ea6\n    hooks: List[Callable]  # \u68af\u5ea6\u94a9\u5b50\u51fd\u6570\n</code></pre>"},{"location":"core-components/autograd/#_5","title":"\u5173\u952e\u65b9\u6cd5","text":""},{"location":"core-components/autograd/#1","title":"1. \u5f20\u91cf\u521b\u5efa","text":"Python<pre><code># \u4ece\u6570\u7ec4\u521b\u5efa\u5f20\u91cf\ndef __init__(self, array, *, device=None, dtype=None, requires_grad=True):\n    if dtype is not None:\n        dtype = get_dtype(dtype)  # \u8f6c\u6362\u4e3aDType\u5bf9\u8c61\n\n    # \u5904\u7406\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\n    if isinstance(array, Tensor):\n        # \u4ece\u73b0\u6709\u5f20\u91cf\u521b\u5efa\n        data = array.data if same_device_dtype else convert_data\n    elif isinstance(array, NDArray):\n        # \u4eceNDArray\u521b\u5efa\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n    else:\n        # \u4ecePython\u5bf9\u8c61\u521b\u5efa\n        device = device if device else default_device()\n        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\n\n    self.init([], data=data, requires_grad=requires_grad)\n</code></pre>"},{"location":"core-components/autograd/#2","title":"2. \u53cd\u5411\u4f20\u64ad","text":"Python<pre><code>def backward(self, out_grad=None):\n    # \u8bbe\u7f6e\u8f93\u51fa\u68af\u5ea6\n    out_grad = out_grad if out_grad else init.ones(*self.shape, dtype=self.dtype, device=self.device)\n\n    # \u521d\u59cb\u5316\u68af\u5ea6\u7d2f\u79ef\u5b57\u5178\n    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}\n    node_to_output_grads_list[self] = [out_grad]\n\n    # \u62d3\u6251\u6392\u5e8f\u83b7\u53d6\u8ba1\u7b97\u987a\u5e8f\n    topo_order = topo_sort(self)\n\n    # \u9006\u62d3\u6251\u5e8f\u904d\u5386\u8ba1\u7b97\u68af\u5ea6\n    for node in reversed(topo_order):\n        if not node.requires_grad:\n            continue\n\n        # \u7d2f\u79ef\u5f53\u524d\u8282\u70b9\u7684\u68af\u5ea6\n        if node.grad is None:\n            node.grad = reduce(operator.add, node_to_output_grads_list[node])\n            # \u786e\u4fdd\u68af\u5ea6\u8fde\u7eed\u6027\uff08\u89e3\u51b3\u5e7f\u64ad\u5f20\u91cf\u95ee\u9898\uff09\n            if hasattr(node.grad, 'data') and hasattr(node.grad.data, 'data'):\n                cuda_tensor = node.grad.data.data\n                if hasattr(cuda_tensor, 'is_contiguous') and not cuda_tensor.is_contiguous():\n                    node.grad.data.data = cuda_tensor.contiguous()\n        else:\n            node.grad += reduce(operator.add, node_to_output_grads_list[node])\n\n        # \u5e94\u7528\u68af\u5ea6\u94a9\u5b50\n        node.apply_hooks(node.grad)\n\n        # \u8ba1\u7b97\u8f93\u5165\u8282\u70b9\u7684\u68af\u5ea6\n        if node.creator is not None:\n            # \u5904\u7406\u6df7\u5408\u7cbe\u5ea6\n            grad = node.grad.half() if check_dtype(node.creator.ctx.saved_tensors, genesis.float16) else node.grad\n\n            # \u8c03\u7528\u5bf9\u5e94\u64cd\u4f5c\u7684\u53cd\u5411\u4f20\u64ad\n            if node.creator.is_tuple_result:\n                backward_grad = node.creator.backward(node.creator.ctx, grad, node.idx)\n            else:\n                backward_grad = node.creator.backward(node.creator.ctx, grad)\n\n            # \u5206\u53d1\u68af\u5ea6\u5230\u8f93\u5165\u8282\u70b9\n            for i, input_node in enumerate(node.creator.inputs):\n                if input_node.requires_grad:\n                    if input_node not in node_to_output_grads_list:\n                        node_to_output_grads_list[input_node] = []\n                    node_to_output_grads_list[input_node].append(backward_grad[i].float())\n</code></pre>"},{"location":"core-components/autograd/#3","title":"3. \u62d3\u6251\u6392\u5e8f","text":"Python<pre><code>def topo_sort(node):\n    \"\"\"\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u5b9e\u73b0\u62d3\u6251\u6392\u5e8f\"\"\"\n    visited = set()\n    topo_order = []\n\n    def dfs(n):\n        if n in visited:\n            return\n        visited.add(n)\n\n        # \u9012\u5f52\u8bbf\u95ee\u8f93\u5165\u8282\u70b9\n        if n.creator is not None:\n            for input_node in n.creator.inputs:\n                if isinstance(input_node, Tensor):\n                    dfs(input_node)\n\n        topo_order.append(n)\n\n    dfs(node)\n    return topo_order\n</code></pre>"},{"location":"core-components/autograd/#function","title":"\u2699\ufe0f Function\u57fa\u7c7b","text":"<p>Function\u662f\u6240\u6709\u53ef\u5fae\u5206\u64cd\u4f5c\u7684\u57fa\u7c7b\uff0c\u5b9a\u4e49\u4e86\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7684\u63a5\u53e3\u3002</p>"},{"location":"core-components/autograd/#_6","title":"\u57fa\u672c\u7ed3\u6784","text":"Python<pre><code>class Function:\n    @staticmethod\n    def forward(ctx: Context, *args) -&gt; Union[Tensor, Tuple[Tensor, ...]]:\n        \"\"\"\u524d\u5411\u4f20\u64ad\u5b9e\u73b0\"\"\"\n        raise NotImplementedError\n\n    @staticmethod  \n    def backward(ctx: Context, grad_output, out_idx=None) -&gt; Tuple[Tensor, ...]:\n        \"\"\"\u53cd\u5411\u4f20\u64ad\u5b9e\u73b0\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def apply(cls, *args, **kwargs):\n        \"\"\"\u7edf\u4e00\u7684\u8c03\u7528\u63a5\u53e3\"\"\"\n        # \u5904\u7406\u6df7\u5408\u7cbe\u5ea6\n        instance = cls()\n        instance.ctx = Context()\n\n        # \u6267\u884c\u524d\u5411\u4f20\u64ad\n        if genesis.enable_autocast:\n            result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n        else:\n            result = cls.forward(instance.ctx, *args, **kwargs)\n\n        # \u8bbe\u7f6e\u8ba1\u7b97\u56fe\u8fde\u63a5\n        instance.is_tuple_result = isinstance(result, tuple)\n\n        if instance.is_tuple_result:\n            for idx, res in enumerate(result):\n                if isinstance(res, Tensor) and res.requires_grad:\n                    res.set_creator(instance, idx)\n        elif isinstance(result, Tensor) and result.requires_grad:\n            result.set_creator(instance)\n\n        # \u8bb0\u5f55\u8f93\u5165\u5f20\u91cf\n        instance.inputs = []\n        for t in args:\n            if isinstance(t, Tensor):\n                instance.inputs.append(t)\n            elif isinstance(t, list) and all(isinstance(item, Tensor) for item in t):\n                instance.inputs.extend(t)\n\n        return result\n</code></pre>"},{"location":"core-components/autograd/#_7","title":"\u5b9e\u9645\u64cd\u4f5c\u793a\u4f8b","text":""},{"location":"core-components/autograd/#_8","title":"\u77e9\u9635\u4e58\u6cd5","text":"Python<pre><code>class MatMul(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        # \u4fdd\u5b58\u8f93\u5165\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\n        ctx.save_for_backward(a, b)\n        return a @ b  # \u8c03\u7528\u5e95\u5c42NDArray\u7684\u77e9\u9635\u4e58\u6cd5\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        a, b = ctx.saved_tensors\n        # \u8ba1\u7b97\u8f93\u5165\u68af\u5ea6\n        grad_a = grad_output @ b.T\n        grad_b = a.T @ grad_output\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd/#_9","title":"\u52a0\u6cd5\uff08\u652f\u6301\u5e7f\u64ad\uff09","text":"Python<pre><code>class Add(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        ctx.a_shape = a.shape\n        ctx.b_shape = b.shape\n        return a + b\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # \u5904\u7406\u5e7f\u64ad\u7684\u68af\u5ea6\n        grad_a = grad_output\n        grad_b = grad_output\n\n        # \u5bf9\u88ab\u5e7f\u64ad\u7684\u7ef4\u5ea6\u6c42\u548c\n        for i, (da, db) in enumerate(zip(ctx.a_shape, ctx.b_shape)):\n            if da == 1 and db &gt; 1:\n                grad_a = grad_a.sum(axis=i, keepdims=True)\n            elif db == 1 and da &gt; 1:\n                grad_b = grad_b.sum(axis=i, keepdims=True)\n\n        return grad_a, grad_b\n</code></pre>"},{"location":"core-components/autograd/#context","title":"\ud83d\udcdd Context\u7c7b","text":"<p>Context\u7c7b\u7528\u4e8e\u5728\u524d\u5411\u4f20\u64ad\u548c\u53cd\u5411\u4f20\u64ad\u4e4b\u95f4\u4f20\u9012\u4fe1\u606f\u3002</p> Python<pre><code>class Context:\n    def __init__(self):\n        self.saved_tensors = []\n\n    def save_for_backward(self, *tensors):\n        \"\"\"\u4fdd\u5b58\u5f20\u91cf\u7528\u4e8e\u53cd\u5411\u4f20\u64ad\"\"\"\n        self.saved_tensors.extend(tensors)\n\n    @property\n    def saved_tensors(self):\n        return self._saved_tensors\n\n    @saved_tensors.setter  \n    def saved_tensors(self, tensors):\n        self._saved_tensors = tensors\n</code></pre>"},{"location":"core-components/autograd/#_10","title":"\ud83d\udd04 \u6df7\u5408\u7cbe\u5ea6\u652f\u6301","text":"<p>\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u5185\u7f6e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u652f\u6301\uff1a</p> Python<pre><code># \u5168\u5c40\u5f00\u5173\ngenesis.enable_autocast = True\n\n# \u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\ndef _cast(value, dtype):\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# \u5728Function.apply\u4e2d\u5e94\u7528\nif genesis.enable_autocast:\n    result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/autograd/#_11","title":"\ud83e\ude9d \u68af\u5ea6\u94a9\u5b50\u7cfb\u7edf","text":"<p>\u652f\u6301\u5728\u68af\u5ea6\u8ba1\u7b97\u65f6\u6267\u884c\u81ea\u5b9a\u4e49\u51fd\u6570\uff1a</p> Python<pre><code>class Tensor:\n    def register_hook(self, hook):\n        \"\"\"\u6ce8\u518c\u68af\u5ea6\u94a9\u5b50\"\"\"\n        self.hooks.append(hook)\n\n    def apply_hooks(self, grad):\n        \"\"\"\u5e94\u7528\u6240\u6709\u94a9\u5b50\"\"\"\n        for hook in self.hooks:\n            hook(grad)\n\n# \u4f7f\u7528\u793a\u4f8b\ndef grad_clipping_hook(grad):\n    \"\"\"\u68af\u5ea6\u88c1\u526a\u94a9\u5b50\"\"\"\n    grad.clamp_(-1.0, 1.0)\n\ntensor.register_hook(grad_clipping_hook)\n</code></pre>"},{"location":"core-components/autograd/#_12","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/autograd/#1_1","title":"1. \u5185\u5b58\u7ba1\u7406\u4f18\u5316","text":"<ul> <li>\u89c6\u56fe\u64cd\u4f5c\uff1areshape\u3001transpose\u7b49\u64cd\u4f5c\u521b\u5efa\u89c6\u56fe\u800c\u975e\u62f7\u8d1d\u6570\u636e</li> <li>\u5c31\u5730\u64cd\u4f5c\uff1a\u652f\u6301<code>+=</code>\u3001<code>*=</code>\u7b49\u5c31\u5730\u66f4\u65b0\u64cd\u4f5c</li> <li>\u68af\u5ea6\u7d2f\u79ef\u4f18\u5316\uff1a\u667a\u80fd\u7684\u68af\u5ea6\u7d2f\u79ef\u7b56\u7565</li> </ul>"},{"location":"core-components/autograd/#2_1","title":"2. \u8ba1\u7b97\u56fe\u4f18\u5316","text":"<ul> <li>\u60f0\u6027\u6784\u5efa\uff1a\u53ea\u6709\u5728\u9700\u8981\u68af\u5ea6\u65f6\u624d\u6784\u5efa\u8ba1\u7b97\u56fe</li> <li>\u667a\u80fd\u91ca\u653e\uff1a\u81ea\u52a8\u91ca\u653e\u4e0d\u518d\u9700\u8981\u7684\u4e2d\u95f4\u7ed3\u679c</li> <li>\u62d3\u6251\u6392\u5e8f\u7f13\u5b58\uff1a\u7f13\u5b58\u62d3\u6251\u6392\u5e8f\u7ed3\u679c</li> </ul>"},{"location":"core-components/autograd/#3_1","title":"3. \u8bbe\u5907\u95f4\u4f18\u5316","text":"<ul> <li>\u81ea\u52a8\u8bbe\u5907\u63a8\u65ad\uff1a\u81ea\u52a8\u9009\u62e9\u5408\u9002\u7684\u8ba1\u7b97\u8bbe\u5907</li> <li>\u5f02\u6b65\u6267\u884c\uff1a\u652f\u6301GPU\u5f02\u6b65\u8ba1\u7b97</li> <li>\u5185\u5b58\u9884\u5206\u914d\uff1a\u51cf\u5c11\u52a8\u6001\u5185\u5b58\u5206\u914d</li> </ul>"},{"location":"core-components/autograd/#_13","title":"\ud83c\udfaf \u4f7f\u7528\u793a\u4f8b","text":""},{"location":"core-components/autograd/#_14","title":"\u57fa\u7840\u7528\u6cd5","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u9700\u8981\u68af\u5ea6\u7684\u5f20\u91cf\nx = genesis.randn(3, 4, requires_grad=True)\ny = genesis.randn(4, 2, requires_grad=True)\n\n# \u524d\u5411\u4f20\u64ad\uff08\u81ea\u52a8\u6784\u5efa\u8ba1\u7b97\u56fe\uff09\nz = x @ y\nloss = z.sum()\n\n# \u53cd\u5411\u4f20\u64ad\uff08\u8ba1\u7b97\u6240\u6709\u68af\u5ea6\uff09\nloss.backward()\n\nprint(f\"x\u7684\u68af\u5ea6: {x.grad}\")  # \u8f93\u51fax\u7684\u68af\u5ea6\nprint(f\"y\u7684\u68af\u5ea6: {y.grad}\")  # \u8f93\u51fay\u7684\u68af\u5ea6\n</code></pre>"},{"location":"core-components/autograd/#_15","title":"\u81ea\u5b9a\u4e49\u64cd\u4f5c","text":"Python<pre><code>class CustomFunction(genesis.autograd.Function):\n    @staticmethod\n    def forward(ctx, input_tensor):\n        # \u81ea\u5b9a\u4e49\u524d\u5411\u8ba1\u7b97\n        ctx.save_for_backward(input_tensor)\n        result = input_tensor ** 2 + 2 * input_tensor + 1\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_tensor, = ctx.saved_tensors\n        # \u81ea\u5b9a\u4e49\u68af\u5ea6\u8ba1\u7b97\uff1ad/dx(x^2 + 2x + 1) = 2x + 2\n        grad_input = grad_output * (2 * input_tensor + 2)\n        return grad_input\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u64cd\u4f5c\nx = genesis.randn(3, 4, requires_grad=True)\ny = CustomFunction.apply(x)\ny.sum().backward()\n</code></pre>"},{"location":"core-components/autograd/#_16","title":"\u68af\u5ea6\u94a9\u5b50","text":"Python<pre><code># \u68af\u5ea6\u76d1\u63a7\u94a9\u5b50\ndef monitor_grad(grad):\n    print(f\"\u68af\u5ea6\u7edf\u8ba1: \u5747\u503c={grad.mean():.4f}, \u6807\u51c6\u5dee={grad.std():.4f}\")\n\n# \u68af\u5ea6\u88c1\u526a\u94a9\u5b50\ndef clip_grad(grad):\n    grad.data.clamp_(-1.0, 1.0)\n\nx = genesis.randn(10, requires_grad=True)\nx.register_hook(monitor_grad)\nx.register_hook(clip_grad)\n\n# \u6267\u884c\u4e00\u4e9b\u8ba1\u7b97\ny = (x ** 3).sum()\ny.backward()  # \u4f1a\u89e6\u53d1\u94a9\u5b50\u51fd\u6570\n</code></pre> <p>Genesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u8bbe\u8ba1\u7b80\u6d01\u800c\u5f3a\u5927\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u68af\u5ea6\u8ba1\u7b97\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002</p>"},{"location":"core-components/cuda-tensor/","title":"CUDA\u5f20\u91cf\u7cfb\u7edf","text":"<p>Genesis\u7684CUDA\u5f20\u91cf\uff08CUDATensor\uff09\u662f\u6846\u67b6\u7684\u6838\u5fc3\u7ec4\u4ef6\u4e4b\u4e00\uff0c\u63d0\u4f9b\u4e86\u7eafCUDA\u5b9e\u73b0\u7684\u5f20\u91cf\u5185\u5b58\u7ba1\u7406\u548c\u64cd\u4f5c\uff0c\u5b8c\u5168\u72ec\u7acb\u4e8ePyTorch\uff0c\u76f4\u63a5\u4f7f\u7528CUDA Python API\u3002</p>"},{"location":"core-components/cuda-tensor/#_1","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":""},{"location":"core-components/cuda-tensor/#_2","title":"\u72ec\u7acb\u6027","text":"<ul> <li>\u7eafCUDA\u5b9e\u73b0: \u4e0d\u4f9d\u8d56PyTorch\u7684GPU\u540e\u7aef</li> <li>\u76f4\u63a5\u5185\u5b58\u7ba1\u7406: \u4f7f\u7528CUDA Python API\u76f4\u63a5\u7ba1\u7406GPU\u5185\u5b58</li> <li>\u9ad8\u6027\u80fd: \u9488\u5bf9GPU\u4f18\u5316\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f</li> </ul>"},{"location":"core-components/cuda-tensor/#_3","title":"\u517c\u5bb9\u6027","text":"<ul> <li>PyTorch\u98ce\u683cAPI: \u4fdd\u6301\u4e0ePyTorch\u5f20\u91cf\u7684\u63a5\u53e3\u517c\u5bb9\u6027</li> <li>\u81ea\u52a8\u5fae\u5206\u652f\u6301: \u4e0eGenesis\u7684\u81ea\u52a8\u5fae\u5206\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210</li> <li>\u7c7b\u578b\u5b89\u5168: \u5b8c\u6574\u7684\u7c7b\u578b\u6ce8\u89e3\u548c\u8fd0\u884c\u65f6\u68c0\u67e5</li> </ul>"},{"location":"core-components/cuda-tensor/#_4","title":"\ud83c\udfd7\ufe0f \u67b6\u6784\u8bbe\u8ba1","text":""},{"location":"core-components/cuda-tensor/#indexplan","title":"IndexPlan\u67b6\u6784","text":"<p>CUDATensor\u4f7f\u7528\u5148\u8fdb\u7684IndexPlan\u67b6\u6784\u6765\u5904\u7406\u590d\u6742\u7684\u5f20\u91cf\u7d22\u5f15\u64cd\u4f5c\uff1a</p> Python<pre><code>class IndexKind(Enum):\n    VIEW = \"view\"           # \u7eaf\u89c6\u56fe\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\n    GATHER = \"gather\"       # \u6536\u96c6\u64cd\u4f5c\uff0c\u7528\u4e8e\u9ad8\u7ea7\u7d22\u5f15  \n    SCATTER = \"scatter\"     # \u6563\u5e03\u64cd\u4f5c\uff0c\u7528\u4e8e\u8d4b\u503c\n    COPY = \"copy\"          # \u6b65\u957f\u62f7\u8d1d\n    FILL = \"fill\"          # \u586b\u5145\u64cd\u4f5c\n\n@dataclass\nclass IndexPlan:\n    \"\"\"\u7edf\u4e00\u7684\u7d22\u5f15\u8ba1\u5212\"\"\"\n    kind: IndexKind\n    result_shape: Optional[Tuple[int, ...]] = None\n    result_strides: Optional[Tuple[int, ...]] = None\n    ptr_offset_bytes: int = 0\n    index_tensor: Optional['CUDATensor'] = None\n    needs_mask_compaction: bool = False\n    temp_memory_bytes: int = 0\n</code></pre>"},{"location":"core-components/cuda-tensor/#_5","title":"\u5185\u5b58\u7ba1\u7406","text":"Python<pre><code>class AsyncMemoryPool:\n    \"\"\"\u5f02\u6b65\u5185\u5b58\u6c60\uff0c\u4f18\u5316GPU\u5185\u5b58\u5206\u914d\u6027\u80fd\"\"\"\n\n    def __init__(self):\n        self.free_blocks = {}  # \u6309\u5927\u5c0f\u7ec4\u7ec7\u7684\u7a7a\u95f2\u5757\n        self.allocated_blocks = {}  # \u5df2\u5206\u914d\u7684\u5757\n        self.alignment = 512  # \u5185\u5b58\u5bf9\u9f50\uff0c\u4e0ePyTorch\u4e00\u81f4\n\n    def allocate(self, size_bytes: int) -&gt; int:\n        \"\"\"\u5206\u914d\u5bf9\u9f50\u7684GPU\u5185\u5b58\"\"\"\n\n    def deallocate(self, ptr: int):\n        \"\"\"\u91ca\u653eGPU\u5185\u5b58\u5230\u6c60\u4e2d\u91cd\u7528\"\"\"\n</code></pre>"},{"location":"core-components/cuda-tensor/#_6","title":"\ud83d\udca1 \u6838\u5fc3\u7279\u6027","text":""},{"location":"core-components/cuda-tensor/#1","title":"1. \u9ad8\u6548\u7684\u7d22\u5f15\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efaCUDA\u5f20\u91cf\nx = genesis.randn(1000, 1000, device='cuda')\n\n# \u57fa\u7840\u7d22\u5f15 - \u4f7f\u7528VIEW\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\ny = x[10:20, 50:100]  # IndexPlan.kind = VIEW\n\n# \u9ad8\u7ea7\u7d22\u5f15 - \u4f7f\u7528GATHER\u64cd\u4f5c  \nindices = genesis.tensor([1, 3, 5, 7], device='cuda')\nz = x[indices]  # IndexPlan.kind = GATHER\n\n# \u5e03\u5c14\u7d22\u5f15 - \u81ea\u52a8\u4f18\u5316\nmask = x &gt; 0.5\nw = x[mask]  # \u6839\u636e\u7a20\u5bc6\u5ea6\u9009\u62e9\u6700\u4f18\u7b56\u7565\n</code></pre>"},{"location":"core-components/cuda-tensor/#2","title":"2. \u5185\u5b58\u9ad8\u6548\u7684\u64cd\u4f5c","text":"Python<pre><code># \u5c31\u5730\u64cd\u4f5c\uff0c\u907f\u514d\u5185\u5b58\u5206\u914d\nx = genesis.randn(1000, 1000, device='cuda')\nx += 1.0  # \u5c31\u5730\u52a0\u6cd5\n\n# \u89c6\u56fe\u64cd\u4f5c\uff0c\u96f6\u62f7\u8d1d\ny = x.view(100, 10000)  # \u6539\u53d8\u5f62\u72b6\u4f46\u4e0d\u590d\u5236\u6570\u636e\nz = x.transpose(0, 1)   # \u8f6c\u7f6e\u89c6\u56fe\n\n# \u6b65\u957f\u64cd\u4f5c\uff0c\u9ad8\u6548\u5b9e\u73b0\nw = x[::2, ::3]  # \u6b65\u957f\u7d22\u5f15\uff0c\u4f7f\u7528\u4f18\u5316\u7684COPY\u64cd\u4f5c\n</code></pre>"},{"location":"core-components/cuda-tensor/#3-triton","title":"3. Triton\u5185\u6838\u96c6\u6210","text":"Python<pre><code>@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u4f18\u5316\u7684Triton\u52a0\u6cd5\u5185\u6838\"\"\"\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets &lt; n_elements\n\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n# CUDATensor\u81ea\u52a8\u8c03\u7528\u4f18\u5316\u7684Triton\u5185\u6838\ndef add_cuda_tensor(x: CUDATensor, y: CUDATensor) -&gt; CUDATensor:\n    \"\"\"CUDA\u5f20\u91cf\u52a0\u6cd5\uff0c\u4f7f\u7528Triton\u4f18\u5316\"\"\"\n    output = CUDATensor(x.shape, x.dtype)\n\n    n_elements = x.numel()\n    BLOCK_SIZE = 1024\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n\n    add_kernel[grid](x.data_ptr(), y.data_ptr(), output.data_ptr(), \n                     n_elements, BLOCK_SIZE=BLOCK_SIZE)\n\n    return output\n</code></pre>"},{"location":"core-components/cuda-tensor/#_7","title":"\ud83d\ude80 \u57fa\u7840\u4f7f\u7528","text":""},{"location":"core-components/cuda-tensor/#_8","title":"\u521b\u5efa\u5f20\u91cf","text":"Python<pre><code>import genesis\n\n# \u4ece\u6570\u636e\u521b\u5efa\ndata = [[1.0, 2.0], [3.0, 4.0]]\ntensor = genesis.tensor(data, device='cuda')\n\n# \u76f4\u63a5\u521b\u5efa\u7279\u5b9a\u5f62\u72b6\nzeros = genesis.zeros(100, 100, device='cuda')\nones = genesis.ones(50, 50, device='cuda')  \nrandom = genesis.randn(200, 200, device='cuda')\n\n# \u6307\u5b9a\u6570\u636e\u7c7b\u578b\nfloat16_tensor = genesis.randn(100, 100, dtype=genesis.float16, device='cuda')\nint_tensor = genesis.randint(0, 10, (50, 50), device='cuda')\n\nprint(f\"\u5f20\u91cf\u5f62\u72b6: {tensor.shape}\")\nprint(f\"\u6570\u636e\u7c7b\u578b: {tensor.dtype}\")\nprint(f\"\u8bbe\u5907: {tensor.device}\")\nprint(f\"\u6b65\u957f: {tensor.strides}\")\n</code></pre>"},{"location":"core-components/cuda-tensor/#_9","title":"\u57fa\u7840\u64cd\u4f5c","text":"Python<pre><code># \u6570\u5b66\u8fd0\u7b97\nx = genesis.randn(100, 100, device='cuda')\ny = genesis.randn(100, 100, device='cuda')\n\n# \u9010\u5143\u7d20\u8fd0\u7b97\nz = x + y      # \u52a0\u6cd5\nw = x * y      # \u4e58\u6cd5  \nu = x.pow(2)   # \u5e42\u8fd0\u7b97\nv = x.exp()    # \u6307\u6570\u51fd\u6570\n\n# \u5f52\u7ea6\u64cd\u4f5c\nsum_all = x.sum()           # \u5168\u5c40\u6c42\u548c\nsum_dim = x.sum(dim=0)      # \u6309\u7ef4\u5ea6\u6c42\u548c\nmean_val = x.mean()         # \u5e73\u5747\u503c\nmax_val, indices = x.max(dim=1)  # \u6700\u5927\u503c\u548c\u7d22\u5f15\n\n# \u7ebf\u6027\u4ee3\u6570\na = genesis.randn(100, 50, device='cuda')\nb = genesis.randn(50, 200, device='cuda') \nc = genesis.matmul(a, b)    # \u77e9\u9635\u4e58\u6cd5\n\n# \u5f62\u72b6\u64cd\u4f5c\nreshaped = x.view(10, 1000)        # \u6539\u53d8\u5f62\u72b6\ntransposed = x.transpose(0, 1)     # \u8f6c\u7f6e  \nflattened = x.flatten()            # \u5c55\u5e73\n</code></pre>"},{"location":"core-components/cuda-tensor/#_10","title":"\u9ad8\u7ea7\u7d22\u5f15","text":"Python<pre><code># \u521b\u5efa\u6d4b\u8bd5\u5f20\u91cf\ndata = genesis.arange(0, 100, device='cuda').view(10, 10)\nprint(\"\u539f\u59cb\u6570\u636e:\")\nprint(data)\n\n# \u57fa\u7840\u5207\u7247\nslice_basic = data[2:5, 3:7]  # \u884c2-4\uff0c\u52173-6\nprint(\"\u57fa\u7840\u5207\u7247:\", slice_basic.shape)\n\n# \u6b65\u957f\u7d22\u5f15\nslice_stride = data[::2, 1::2]  # \u6bcf\u9694\u4e00\u884c\uff0c\u4ece\u7b2c1\u5217\u5f00\u59cb\u6bcf\u9694\u4e00\u5217\nprint(\"\u6b65\u957f\u7d22\u5f15:\", slice_stride.shape)\n\n# \u9ad8\u7ea7\u7d22\u5f15 - \u6574\u6570\u6570\u7ec4\nrow_indices = genesis.tensor([0, 2, 4, 6], device='cuda')\ncol_indices = genesis.tensor([1, 3, 5, 7], device='cuda')\nadvanced = data[row_indices, col_indices]  # \u9009\u62e9\u7279\u5b9a\u4f4d\u7f6e\nprint(\"\u9ad8\u7ea7\u7d22\u5f15\u7ed3\u679c:\", advanced)\n\n# \u5e03\u5c14\u7d22\u5f15\nmask = data &gt; 50\nmasked_data = data[mask]  # \u9009\u62e9\u5927\u4e8e50\u7684\u5143\u7d20\nprint(\"\u5e03\u5c14\u7d22\u5f15\u7ed3\u679c:\", masked_data)\n\n# \u6df7\u5408\u7d22\u5f15\nmixed = data[row_indices, 2:8]  # \u7279\u5b9a\u884c\u7684\u5217\u8303\u56f4\nprint(\"\u6df7\u5408\u7d22\u5f15:\", mixed.shape)\n</code></pre>"},{"location":"core-components/cuda-tensor/#_11","title":"\ud83d\udd27 \u5185\u5b58\u7ba1\u7406","text":""},{"location":"core-components/cuda-tensor/#_12","title":"\u5185\u5b58\u6c60\u4f18\u5316","text":"Python<pre><code># \u67e5\u770b\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\nprint(f\"\u5df2\u5206\u914d\u5185\u5b58: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\nprint(f\"\u7f13\u5b58\u5185\u5b58: {genesis.cuda.memory_cached() / 1024**2:.1f} MB\")\n\n# \u624b\u52a8\u5185\u5b58\u7ba1\u7406\nx = genesis.randn(1000, 1000, device='cuda')\nprint(f\"\u521b\u5efa\u5f20\u91cf\u540e: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\ndel x  # \u5220\u9664\u5f15\u7528\ngenesis.cuda.empty_cache()  # \u6e05\u7a7a\u7f13\u5b58\nprint(f\"\u6e05\u7406\u540e: {genesis.cuda.memory_allocated() / 1024**2:.1f} MB\")\n\n# \u5185\u5b58\u5feb\u7167\uff08\u8c03\u8bd5\u7528\uff09\nsnapshot = genesis.cuda.memory_snapshot()\nfor entry in snapshot[:3]:  # \u663e\u793a\u524d3\u4e2a\u6761\u76ee\n    print(f\"\u5730\u5740: {entry['address']}, \u5927\u5c0f: {entry['size']} bytes\")\n</code></pre>"},{"location":"core-components/cuda-tensor/#_13","title":"\u5f02\u6b65\u64cd\u4f5c","text":"Python<pre><code># \u5f02\u6b65\u5185\u5b58\u64cd\u4f5c\nwith genesis.cuda.stream():\n    x = genesis.randn(1000, 1000, device='cuda')\n    y = genesis.randn(1000, 1000, device='cuda')\n    z = genesis.matmul(x, y)  # \u5f02\u6b65\u6267\u884c\n\n    # \u5176\u4ed6CPU\u5de5\u4f5c\u53ef\u4ee5\u5e76\u884c\u8fdb\u884c\n    print(\"\u77e9\u9635\u4e58\u6cd5\u6b63\u5728GPU\u4e0a\u5f02\u6b65\u6267\u884c...\")\n\n    # \u540c\u6b65\u7b49\u5f85\u7ed3\u679c  \n    genesis.cuda.synchronize()\n    print(\"\u8ba1\u7b97\u5b8c\u6210:\", z.shape)\n</code></pre>"},{"location":"core-components/cuda-tensor/#_14","title":"\u26a1 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/cuda-tensor/#1_1","title":"1. \u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4f18\u5316","text":"Python<pre><code>def inefficient_access():\n    \"\"\"\u4f4e\u6548\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n    result = genesis.zeros(1000, device='cuda')\n\n    # \u975e\u8fde\u7eed\u8bbf\u95ee\uff0c\u7f13\u5b58\u672a\u547d\u4e2d\n    for i in range(1000):\n        result[i] = x[i, ::10].sum()  # \u6b65\u957f\u8bbf\u95ee\n\n    return result\n\ndef efficient_access():  \n    \"\"\"\u9ad8\u6548\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\"\"\"\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u8fde\u7eed\u8bbf\u95ee\uff0c\u5145\u5206\u5229\u7528\u7f13\u5b58\n    indices = genesis.arange(0, 1000, 10, device='cuda')\n    selected = x[:, indices]  # \u6279\u91cf\u9009\u62e9\n    result = selected.sum(dim=1)  # \u5411\u91cf\u5316\u6c42\u548c\n\n    return result\n\n# \u6027\u80fd\u5bf9\u6bd4\nimport time\n\nstart = time.time()\nresult1 = inefficient_access()\ntime1 = time.time() - start\n\nstart = time.time()  \nresult2 = efficient_access()\ntime2 = time.time() - start\n\nprint(f\"\u4f4e\u6548\u65b9\u6cd5: {time1:.4f}s\")\nprint(f\"\u9ad8\u6548\u65b9\u6cd5: {time2:.4f}s\")  \nprint(f\"\u52a0\u901f\u6bd4: {time1/time2:.2f}x\")\n</code></pre>"},{"location":"core-components/cuda-tensor/#2_1","title":"2. \u6279\u91cf\u64cd\u4f5c\u4f18\u5316","text":"Python<pre><code>def batch_operations_demo():\n    \"\"\"\u5c55\u793a\u6279\u91cf\u64cd\u4f5c\u7684\u6027\u80fd\u4f18\u52bf\"\"\"\n\n    # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\n    matrices = [genesis.randn(100, 100, device='cuda') for _ in range(10)]\n\n    # \u65b9\u6cd51: \u9010\u4e2a\u5904\u7406\uff08\u4f4e\u6548\uff09\n    start = time.time()\n    results1 = []\n    for matrix in matrices:\n        result = matrix.exp().sum()\n        results1.append(result)\n    time1 = time.time() - start\n\n    # \u65b9\u6cd52: \u6279\u91cf\u5904\u7406\uff08\u9ad8\u6548\uff09\n    start = time.time()\n    batched = genesis.stack(matrices, dim=0)  # [10, 100, 100]\n    results2 = batched.exp().sum(dim=(1, 2))  # [10]\n    time2 = time.time() - start\n\n    print(f\"\u9010\u4e2a\u5904\u7406: {time1:.4f}s\")\n    print(f\"\u6279\u91cf\u5904\u7406: {time2:.4f}s\")\n    print(f\"\u52a0\u901f\u6bd4: {time1/time2:.2f}x\")\n\nbatch_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-tensor/#3","title":"3. \u5c31\u5730\u64cd\u4f5c","text":"Python<pre><code>def inplace_operations_demo():\n    \"\"\"\u5c55\u793a\u5c31\u5730\u64cd\u4f5c\u7684\u5185\u5b58\u6548\u7387\"\"\"\n\n    # \u975e\u5c31\u5730\u64cd\u4f5c\uff08\u521b\u5efa\u65b0\u5f20\u91cf\uff09\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    y = x + 1.0      # \u521b\u5efa\u65b0\u5f20\u91cf\n    z = y * 2.0      # \u518d\u521b\u5efa\u65b0\u5f20\u91cf\n    w = z.exp()      # \u53c8\u521b\u5efa\u65b0\u5f20\u91cf\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"\u975e\u5c31\u5730\u64cd\u4f5c\u5185\u5b58\u589e\u957f: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\n    # \u5c31\u5730\u64cd\u4f5c\uff08\u4fee\u6539\u539f\u5f20\u91cf\uff09\n    x = genesis.randn(1000, 1000, device='cuda')\n    start_memory = genesis.cuda.memory_allocated()\n\n    x += 1.0         # \u5c31\u5730\u52a0\u6cd5\n    x *= 2.0         # \u5c31\u5730\u4e58\u6cd5  \n    x.exp_()         # \u5c31\u5730\u6307\u6570\u51fd\u6570\n\n    memory_after = genesis.cuda.memory_allocated()\n    print(f\"\u5c31\u5730\u64cd\u4f5c\u5185\u5b58\u589e\u957f: {(memory_after - start_memory) / 1024**2:.1f} MB\")\n\ninplace_operations_demo()\n</code></pre>"},{"location":"core-components/cuda-tensor/#_15","title":"\ud83d\udc1b \u8c03\u8bd5\u548c\u8bca\u65ad","text":""},{"location":"core-components/cuda-tensor/#_16","title":"\u5185\u5b58\u6cc4\u6f0f\u68c0\u6d4b","text":"Python<pre><code>def detect_memory_leaks():\n    \"\"\"\u68c0\u6d4b\u5185\u5b58\u6cc4\u6f0f\"\"\"\n    genesis.cuda.empty_cache()\n    initial_memory = genesis.cuda.memory_allocated()\n\n    # \u6267\u884c\u4e00\u4e9b\u64cd\u4f5c\n    for i in range(100):\n        x = genesis.randn(100, 100, device='cuda')\n        y = x.matmul(x)\n        del x, y\n\n    genesis.cuda.empty_cache()\n    final_memory = genesis.cuda.memory_allocated()\n\n    if final_memory &gt; initial_memory:\n        print(f\"\u53ef\u80fd\u5b58\u5728\u5185\u5b58\u6cc4\u6f0f: {(final_memory - initial_memory) / 1024**2:.1f} MB\")\n    else:\n        print(\"\u672a\u68c0\u6d4b\u5230\u5185\u5b58\u6cc4\u6f0f\")\n\ndetect_memory_leaks()\n</code></pre>"},{"location":"core-components/cuda-tensor/#_17","title":"\u9519\u8bef\u8bca\u65ad","text":"Python<pre><code>def diagnose_cuda_errors():\n    \"\"\"CUDA\u9519\u8bef\u8bca\u65ad\"\"\"\n    try:\n        # \u53ef\u80fd\u51fa\u9519\u7684\u64cd\u4f5c\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = genesis.randn(500, 500, device='cuda')  # \u5f62\u72b6\u4e0d\u5339\u914d\n        z = genesis.matmul(x, y)\n\n    except RuntimeError as e:\n        print(f\"CUDA\u9519\u8bef: {e}\")\n\n        # \u68c0\u67e5CUDA\u72b6\u6001\n        if genesis.cuda.is_available():\n            print(f\"CUDA\u8bbe\u5907: {genesis.cuda.get_device_name()}\")\n            print(f\"CUDA\u80fd\u529b: {genesis.cuda.get_device_capability()}\")\n            print(f\"\u53ef\u7528\u5185\u5b58: {genesis.cuda.get_device_properties().total_memory / 1024**3:.1f} GB\")\n        else:\n            print(\"CUDA\u4e0d\u53ef\u7528\")\n\ndiagnose_cuda_errors()\n</code></pre>"},{"location":"core-components/cuda-tensor/#pytorch","title":"\ud83d\udd04 \u4e0ePyTorch\u4e92\u64cd\u4f5c","text":"Python<pre><code>import torch\n\ndef pytorch_interop_demo():\n    \"\"\"\u5c55\u793a\u4e0ePyTorch\u7684\u4e92\u64cd\u4f5c\u6027\"\"\"\n\n    # Genesis\u5f20\u91cf\u8f6cPyTorch\n    genesis_tensor = genesis.randn(100, 100, device='cuda')\n\n    # \u8f6c\u6362\u4e3aPyTorch\uff08\u5171\u4eab\u5185\u5b58\uff09\n    pytorch_tensor = torch.as_tensor(genesis_tensor.detach().cpu().numpy()).cuda()\n\n    print(f\"Genesis\u5f62\u72b6: {genesis_tensor.shape}\")\n    print(f\"PyTorch\u5f62\u72b6: {pytorch_tensor.shape}\")\n\n    # PyTorch\u5f20\u91cf\u8f6cGenesis  \n    torch_data = torch.randn(50, 50, device='cuda')\n    genesis_from_torch = genesis.tensor(torch_data.cpu().numpy(), device='cuda')\n\n    print(f\"\u8f6c\u6362\u6210\u529f\uff0cGenesis\u5f20\u91cf: {genesis_from_torch.shape}\")\n\npytorch_interop_demo()\n</code></pre>"},{"location":"core-components/cuda-tensor/#_18","title":"\ud83d\udcca \u6027\u80fd\u57fa\u51c6","text":"Python<pre><code>def benchmark_cuda_tensor():\n    \"\"\"CUDA\u5f20\u91cf\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\"\"\"\n\n    sizes = [100, 500, 1000, 2000]\n\n    print(\"\u77e9\u9635\u4e58\u6cd5\u6027\u80fd\u5bf9\u6bd4 (Genesis vs PyTorch):\")\n    print(\"-\" * 50)\n\n    for size in sizes:\n        # Genesis\u6d4b\u8bd5\n        x_gen = genesis.randn(size, size, device='cuda')\n        y_gen = genesis.randn(size, size, device='cuda')\n\n        genesis.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_gen = genesis.matmul(x_gen, y_gen)\n        genesis.cuda.synchronize()\n        genesis_time = (time.time() - start) / 10\n\n        # PyTorch\u6d4b\u8bd5\n        x_torch = torch.randn(size, size, device='cuda')\n        y_torch = torch.randn(size, size, device='cuda')\n\n        torch.cuda.synchronize()\n        start = time.time()\n        for _ in range(10):\n            z_torch = torch.matmul(x_torch, y_torch)\n        torch.cuda.synchronize() \n        pytorch_time = (time.time() - start) / 10\n\n        ratio = genesis_time / pytorch_time\n        print(f\"{size}x{size}: Genesis {genesis_time:.4f}s, PyTorch {pytorch_time:.4f}s, \u6bd4\u7387 {ratio:.2f}\")\n\nbenchmark_cuda_tensor()\n</code></pre>"},{"location":"core-components/cuda-tensor/#_19","title":"\ud83c\udfaf \u6700\u4f73\u5b9e\u8df5","text":""},{"location":"core-components/cuda-tensor/#1_2","title":"1. \u5185\u5b58\u7ba1\u7406\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u2705 \u597d\u7684\u505a\u6cd5\ndef good_memory_practice():\n    with genesis.cuda.device(0):  # \u660e\u786e\u6307\u5b9a\u8bbe\u5907\n        x = genesis.randn(1000, 1000, device='cuda')\n\n        # \u4f7f\u7528\u5c31\u5730\u64cd\u4f5c\n        x += 1.0\n        x *= 0.5\n\n        # \u53ca\u65f6\u91ca\u653e\u5927\u5f20\u91cf\n        del x\n        genesis.cuda.empty_cache()\n\n# \u274c \u907f\u514d\u7684\u505a\u6cd5  \ndef bad_memory_practice():\n    tensors = []\n    for i in range(100):\n        x = genesis.randn(1000, 1000, device='cuda')\n        y = x + 1.0  # \u521b\u5efa\u989d\u5916\u526f\u672c\n        tensors.append(y)  # \u4fdd\u6301\u6240\u6709\u5f15\u7528\uff0c\u5185\u5b58\u65e0\u6cd5\u91ca\u653e\n    # \u5185\u5b58\u4f1a\u5feb\u901f\u8017\u5c3d\n</code></pre>"},{"location":"core-components/cuda-tensor/#2_2","title":"2. \u6027\u80fd\u4f18\u5316\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u2705 \u5411\u91cf\u5316\u64cd\u4f5c\ndef vectorized_operations():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u4f7f\u7528\u5411\u91cf\u5316\u51fd\u6570\n    result = genesis.relu(x).sum(dim=1).mean()\n\n# \u274c \u907f\u514d\u5faa\u73af\ndef avoid_loops():\n    x = genesis.randn(1000, 1000, device='cuda')\n\n    # \u907f\u514dPython\u5faa\u73af\n    result = 0\n    for i in range(1000):\n        result += x[i].sum()  # \u6bcf\u6b21\u90fd\u542f\u52a8CUDA kernel\n</code></pre>"},{"location":"core-components/cuda-tensor/#3_1","title":"3. \u8c03\u8bd5\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code># \u542f\u7528CUDA\u9519\u8bef\u68c0\u67e5\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# \u4f7f\u7528\u65ad\u8a00\u68c0\u67e5\u5f20\u91cf\u5c5e\u6027\ndef safe_tensor_operation(x, y):\n    assert x.device == y.device, \"\u5f20\u91cf\u5fc5\u987b\u5728\u540c\u4e00\u8bbe\u5907\u4e0a\"\n    assert x.shape == y.shape, f\"\u5f62\u72b6\u4e0d\u5339\u914d: {x.shape} vs {y.shape}\"\n\n    return x + y\n</code></pre>"},{"location":"core-components/cuda-tensor/#_20","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"core-components/cuda-tensor/#q-cuda","title":"Q: CUDA\u5185\u5b58\u4e0d\u8db3\u600e\u4e48\u529e\uff1f","text":"<p>A:  Python<pre><code># \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\nbatch_size = 32  # \u6539\u4e3a16\u62168\n\n# \u4f7f\u7528\u68af\u5ea6\u7d2f\u79ef\naccumulation_steps = 4\neffective_batch_size = batch_size * accumulation_steps\n\n# \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')\n\n# \u5b9a\u671f\u6e05\u7406\u5185\u5b58\ngenesis.cuda.empty_cache()\n</code></pre></p>"},{"location":"core-components/cuda-tensor/#q-cuda_1","title":"Q: \u4e3a\u4ec0\u4e48CUDA\u64cd\u4f5c\u5f88\u6162\uff1f","text":"<p>A: \u68c0\u67e5\u4ee5\u4e0b\u51e0\u70b9\uff1a Python<pre><code># 1. \u786e\u4fdd\u5f20\u91cf\u5728GPU\u4e0a\nassert x.device.type == 'cuda'\n\n# 2. \u907f\u514d\u9891\u7e41\u7684CPU-GPU\u4f20\u8f93\n# \u9519\u8bef\u505a\u6cd5\nfor i in range(1000):\n    cpu_data = x.cpu().numpy()  # \u6bcf\u6b21\u90fd\u4f20\u8f93\n\n# \u6b63\u786e\u505a\u6cd5\ncpu_data = x.cpu().numpy()  # \u53ea\u4f20\u8f93\u4e00\u6b21\n\n# 3. \u4f7f\u7528\u9002\u5f53\u7684\u6570\u636e\u7c7b\u578b\nx = genesis.randn(1000, 1000, dtype=genesis.float16, device='cuda')  # \u66f4\u5feb\n</code></pre></p>"},{"location":"core-components/cuda-tensor/#q-cuda-kernel","title":"Q: \u5982\u4f55\u8c03\u8bd5CUDA kernel\u9519\u8bef\uff1f","text":"<p>A: Python<pre><code># 1. \u542f\u7528\u540c\u6b65\u9519\u8bef\u68c0\u67e5\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# 2. \u68c0\u67e5tensor\u6709\u6548\u6027\ndef check_tensor(tensor, name):\n    assert not torch.isnan(tensor).any(), f\"{name}\u5305\u542bNaN\"\n    assert not torch.isinf(tensor).any(), f\"{name}\u5305\u542bInf\"\n    print(f\"{name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n\n# 3. \u4f7f\u7528CUDA\u8c03\u8bd5\u5de5\u5177\n# cuda-memcheck python your_script.py\n# compute-sanitizer python your_script.py\n</code></pre></p> <p>\u6027\u80fd\u63d0\u793a</p> <p>CUDA\u5f20\u91cf\u7684\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u548c\u6279\u91cf\u64cd\u4f5c\u7684\u4f7f\u7528\u3002\u4f18\u5148\u8003\u8651\u5411\u91cf\u5316\u64cd\u4f5c\u548c\u5408\u7406\u7684\u5185\u5b58\u5e03\u5c40\u3002</p> <p>\u51c6\u5907\u6df1\u5165\u4e86\u89e3\u66f4\u591a\u5417\uff1f</p> <p>\u4e0b\u4e00\u6b65\uff1a\u5f20\u91cf\u64cd\u4f5c\u4f18\u5316 \u8fd4\u56de\u6838\u5fc3\u7ec4\u4ef6</p>"},{"location":"core-components/dtypes/","title":"\u6570\u636e\u7c7b\u578b\u7cfb\u7edf","text":"<p>Genesis\u5b9e\u73b0\u4e86\u4e00\u5957\u7edf\u4e00\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e0ePyTorch\u5bf9\u9f50\u7684\u7c7b\u578b\u7ba1\u7406\uff0c\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u8de8\u8bbe\u5907\u7c7b\u578b\u8f6c\u6362\u3002</p>"},{"location":"core-components/dtypes/#_2","title":"\ud83c\udfaf \u8bbe\u8ba1\u76ee\u6807","text":"<ul> <li>\u7edf\u4e00\u63a5\u53e3\uff1aCPU\u548cGPU\u540e\u7aef\u4f7f\u7528\u76f8\u540c\u7684\u7c7b\u578b\u5b9a\u4e49</li> <li>PyTorch\u517c\u5bb9\uff1a\u4e0ePyTorch\u7684dtype\u7cfb\u7edf\u4fdd\u6301\u4e00\u81f4\u6027</li> <li>\u6df7\u5408\u7cbe\u5ea6\uff1a\u65e0\u7f1d\u652f\u6301FP16\u3001BF16\u7b49\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> <li>\u7c7b\u578b\u5b89\u5168\uff1a\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u7684\u7c7b\u578b\u68c0\u67e5</li> </ul>"},{"location":"core-components/dtypes/#_3","title":"\ud83c\udfd7\ufe0f \u6838\u5fc3\u67b6\u6784","text":"<pre><code>graph TB\n    subgraph \"DType\u6838\u5fc3\u7c7b\"\n        A[DType] --&gt; B[name str]\n        A --&gt; C[itemsize int]\n        A --&gt; D[numpy_dtype]\n        A --&gt; E[triton_name str]\n        A --&gt; F[is_floating_point bool]\n    end\n\n    subgraph \"\u9884\u5b9a\u4e49\u7c7b\u578b\"\n        G[\u6d6e\u70b9\u7c7b\u578b] --&gt; H[float32]\n        G --&gt; I[float16] \n        G --&gt; J[bfloat16]\n        G --&gt; K[float64]\n\n        L[\u6574\u6570\u7c7b\u578b] --&gt; M[int32]\n        L --&gt; N[int64]\n        L --&gt; O[int16]\n        L --&gt; P[int8]\n        L --&gt; Q[uint8]\n\n        R[\u5e03\u5c14\u7c7b\u578b] --&gt; S[bool]\n    end\n\n    subgraph \"\u7c7b\u578b\u8f6c\u6362\"\n        T[get_dtype] --&gt; U[\u5b57\u7b26\u4e32\u8f6c\u6362]\n        T --&gt; V[NumPy\u517c\u5bb9]\n        T --&gt; W[\u7c7b\u578b\u63a8\u65ad]\n    end\n\n    A --&gt; G\n    A --&gt; L  \n    A --&gt; R\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style L fill:#fff3e0\n    style T fill:#fce4ec</code></pre>"},{"location":"core-components/dtypes/#dtype","title":"\ud83d\udcca DType\u7c7b\u8be6\u89e3","text":""},{"location":"core-components/dtypes/#_4","title":"\u7c7b\u5b9a\u4e49","text":"Python<pre><code>class DType:\n    \"\"\"Genesis\u6570\u636e\u7c7b\u578b\uff0c\u7c7b\u4f3ctorch.dtype\"\"\"\n\n    def __init__(self, name, itemsize, numpy_dtype, triton_name=None, is_floating_point=None):\n        self.name = name                    # \u7c7b\u578b\u540d\u79f0\uff0c\u5982\"float32\"\n        self.itemsize = itemsize           # \u5b57\u8282\u5927\u5c0f\n        self.numpy_dtype = numpy_dtype     # \u5bf9\u5e94\u7684NumPy\u7c7b\u578b\n        self.triton_name = triton_name or name  # Triton\u4e2d\u7684\u7c7b\u578b\u540d\n\n        # \u81ea\u52a8\u68c0\u6d4b\u662f\u5426\u4e3a\u6d6e\u70b9\u7c7b\u578b\n        if is_floating_point is None:\n            self.is_floating_point = np.issubdtype(numpy_dtype, np.floating)\n        else:\n            self.is_floating_point = is_floating_point\n</code></pre>"},{"location":"core-components/dtypes/#_5","title":"\u6838\u5fc3\u65b9\u6cd5","text":""},{"location":"core-components/dtypes/#_6","title":"\u5b57\u7b26\u4e32\u8868\u793a","text":"Python<pre><code>def __str__(self):\n    return f\"genesis.{self.name}\"\n\ndef __repr__(self):\n    return f\"genesis.{self.name}\"\n\n# \u4f7f\u7528\u793a\u4f8b\nprint(genesis.float32)  # \u8f93\u51fa: genesis.float32\n</code></pre>"},{"location":"core-components/dtypes/#_7","title":"\u76f8\u7b49\u6027\u6bd4\u8f83","text":"Python<pre><code>def __eq__(self, other):\n    if isinstance(other, DType):\n        return self.name == other.name\n    elif isinstance(other, str):\n        return self.name == other  # \u5411\u540e\u517c\u5bb9\u5b57\u7b26\u4e32\u6bd4\u8f83\n    return False\n\n# \u4f7f\u7528\u793a\u4f8b\ngenesis.float32 == genesis.float32  # True\ngenesis.float32 == \"float32\"        # True (\u5411\u540e\u517c\u5bb9)\ngenesis.float32 == genesis.float16  # False\n</code></pre>"},{"location":"core-components/dtypes/#_8","title":"\ud83d\udd22 \u9884\u5b9a\u4e49\u6570\u636e\u7c7b\u578b","text":""},{"location":"core-components/dtypes/#_9","title":"\u6d6e\u70b9\u7c7b\u578b","text":"\u7c7b\u578b \u5b57\u8282\u6570 \u7cbe\u5ea6 \u7528\u9014 <code>float32</code> 4 \u5355\u7cbe\u5ea6 \u9ed8\u8ba4\u6d6e\u70b9\u7c7b\u578b\uff0c\u5e73\u8861\u7cbe\u5ea6\u548c\u6027\u80fd <code>float16</code> 2 \u534a\u7cbe\u5ea6 \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u8282\u7701\u5185\u5b58 <code>float64</code> 8 \u53cc\u7cbe\u5ea6 \u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u9700\u6c42 <code>bfloat16</code> 2 \u8111\u6d6e\u70b9 Google TPU\u4f18\u5316\uff0c\u52a8\u6001\u8303\u56f4\u5927 Python<pre><code># \u6d6e\u70b9\u7c7b\u578b\u5b9a\u4e49\nfloat32 = DType(\"float32\", 4, np.float32)\nfloat16 = DType(\"float16\", 2, np.float16)\nfloat64 = DType(\"float64\", 8, np.float64)\n\n# bfloat16\u7279\u6b8a\u5904\u7406 - Triton\u652f\u6301\u4f46NumPy\u4e0d\u539f\u751f\u652f\u6301\nbfloat16 = DType(\"bfloat16\", 2, np.float32, \"bfloat16\", is_floating_point=True)\n</code></pre>"},{"location":"core-components/dtypes/#_10","title":"\u6574\u6570\u7c7b\u578b","text":"\u7c7b\u578b \u5b57\u8282\u6570 \u8303\u56f4 \u7528\u9014 <code>int64</code> 8 -2^63 ~ 2^63-1 \u9ed8\u8ba4\u6574\u6570\u7c7b\u578b <code>int32</code> 4 -2^31 ~ 2^31-1 \u5185\u5b58\u4f18\u5316\u7684\u6574\u6570 <code>int16</code> 2 -32,768 ~ 32,767 \u5c0f\u6574\u6570\u5b58\u50a8 <code>int8</code> 1 -128 ~ 127 \u91cf\u5316\u8ba1\u7b97 <code>uint8</code> 1 0 ~ 255 \u56fe\u50cf\u6570\u636e Python<pre><code># \u6574\u6570\u7c7b\u578b\u5b9a\u4e49\nint32 = DType(\"int32\", 4, np.int32)\nint64 = DType(\"int64\", 8, np.int64)\nint16 = DType(\"int16\", 2, np.int16)\nint8 = DType(\"int8\", 1, np.int8)\nuint8 = DType(\"uint8\", 1, np.uint8)\n</code></pre>"},{"location":"core-components/dtypes/#_11","title":"\u5e03\u5c14\u7c7b\u578b","text":"Python<pre><code># \u5e03\u5c14\u7c7b\u578b\nbool = DType(\"bool\", 1, np.bool_, is_floating_point=False)\n</code></pre>"},{"location":"core-components/dtypes/#_12","title":"\ud83d\udd04 \u7c7b\u578b\u8f6c\u6362\u7cfb\u7edf","text":""},{"location":"core-components/dtypes/#_13","title":"\u6838\u5fc3\u8f6c\u6362\u51fd\u6570","text":"Python<pre><code>def get_dtype(obj):\n    \"\"\"\n    \u5c06\u5404\u79cd\u7c7b\u578b\u8868\u793a\u8f6c\u6362\u4e3aGenesis DType\u5bf9\u8c61\n\n    \u652f\u6301\u7684\u8f93\u5165\u7c7b\u578b:\n    - DType\u5bf9\u8c61: \u76f4\u63a5\u8fd4\u56de\n    - \u5b57\u7b26\u4e32: \"float32\", \"int64\"\u7b49\n    - NumPy dtype: np.float32, np.int64\u7b49\n    - NumPy\u7c7b\u578b: np.float32, np.int64\u7c7b\u7b49\n    - None: \u8fd4\u56de\u9ed8\u8ba4float32\n    \"\"\"\n    if obj is None:\n        return float32  # \u9ed8\u8ba4\u7c7b\u578b\n    elif isinstance(obj, DType):\n        return obj\n    elif isinstance(obj, str):\n        return _name_to_dtype[obj]\n    elif isinstance(obj, np.dtype):\n        return _numpy_to_dtype[obj.type]\n    elif isinstance(obj, type) and issubclass(obj, np.generic):\n        return _numpy_to_dtype[obj]\n    else:\n        raise ValueError(f\"Cannot convert {type(obj)} to Genesis DType: {obj}\")\n</code></pre>"},{"location":"core-components/dtypes/#_14","title":"\u7c7b\u578b\u6620\u5c04\u8868","text":"Python<pre><code># \u540d\u79f0\u5230\u7c7b\u578b\u7684\u6620\u5c04\n_name_to_dtype = {\n    \"float32\": float32,\n    \"float16\": float16,\n    \"float64\": float64,\n    \"bfloat16\": bfloat16,\n    \"int32\": int32,\n    \"int64\": int64,\n    \"int16\": int16,\n    \"int8\": int8,\n    \"uint8\": uint8,\n    \"bool\": bool,\n}\n\n# NumPy\u7c7b\u578b\u5230Genesis\u7c7b\u578b\u7684\u6620\u5c04\n_numpy_to_dtype = {\n    np.float32: float32,\n    np.float16: float16,\n    np.float64: float64,\n    np.int32: int32,\n    np.int64: int64,\n    np.int16: int16,\n    np.int8: int8,\n    np.uint8: uint8,\n    np.bool_: bool,\n}\n</code></pre>"},{"location":"core-components/dtypes/#_15","title":"\ud83e\uddee \u7c7b\u578b\u68c0\u67e5\u5de5\u5177","text":""},{"location":"core-components/dtypes/#_16","title":"\u6d6e\u70b9\u7c7b\u578b\u68c0\u67e5","text":"Python<pre><code>def is_floating_point(dtype):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u4e3a\u6d6e\u70b9\u7c7b\u578b\"\"\"\n    dtype = get_dtype(dtype)\n    return dtype.is_floating_point\n\n# \u4f7f\u7528\u793a\u4f8b\nis_floating_point(genesis.float32)  # True\nis_floating_point(genesis.int32)    # False\nis_floating_point(\"float16\")        # True\n</code></pre>"},{"location":"core-components/dtypes/#_17","title":"\u6574\u6570\u7c7b\u578b\u68c0\u67e5","text":"Python<pre><code>def is_integer(dtype):\n    \"\"\"\u68c0\u67e5\u662f\u5426\u4e3a\u6574\u6570\u7c7b\u578b\"\"\"\n    dtype = get_dtype(dtype)\n    return not dtype.is_floating_point and dtype != bool\n\n# \u4f7f\u7528\u793a\u4f8b\nis_integer(genesis.int32)   # True\nis_integer(genesis.float32) # False\nis_integer(genesis.bool)    # False\n</code></pre>"},{"location":"core-components/dtypes/#_18","title":"\u7c7b\u578b\u5206\u7c7b","text":"Python<pre><code># \u6240\u6709\u652f\u6301\u7684\u7c7b\u578b\nall_dtypes = [float32, float16, float64, bfloat16, int32, int64, int16, int8, uint8, bool]\n\n# \u6d6e\u70b9\u7c7b\u578b\u5217\u8868\nfloating_dtypes = [dt for dt in all_dtypes if dt.is_floating_point]\n# [float32, float16, float64, bfloat16]\n\n# \u6574\u6570\u7c7b\u578b\u5217\u8868\ninteger_dtypes = [dt for dt in all_dtypes if is_integer(dt)]\n# [int32, int64, int16, int8, uint8]\n</code></pre>"},{"location":"core-components/dtypes/#_19","title":"\ud83d\udd00 \u6df7\u5408\u7cbe\u5ea6\u652f\u6301","text":""},{"location":"core-components/dtypes/#_20","title":"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362","text":"Python<pre><code>def _cast(value, dtype):\n    \"\"\"\u81ea\u52a8\u7c7b\u578b\u8f6c\u6362\uff0c\u7528\u4e8e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\"\"\"\n    if isinstance(value, Tensor) and value.is_floating_point():\n        if dtype == genesis.float16:\n            return value.half()\n        else:\n            return value.float()\n    return value\n\n# \u5728autograd\u4e2d\u7684\u5e94\u7528\nif genesis.enable_autocast:\n    result = cls.forward(ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))\n</code></pre>"},{"location":"core-components/dtypes/#_21","title":"\u7c7b\u578b\u63a8\u65ad","text":"Python<pre><code>def check_dtype(value, dtype):\n    \"\"\"\u9012\u5f52\u68c0\u67e5\u6570\u636e\u7ed3\u6784\u4e2d\u662f\u5426\u5305\u542b\u6307\u5b9a\u7c7b\u578b\"\"\"\n    if isinstance(value, Tensor):\n        return value.dtype == dtype\n    elif isinstance(value, dict):\n        return any(check_dtype(k, dtype) or check_dtype(v, dtype) for k, v in value.items())\n    elif isinstance(value, (list, tuple)):\n        return any(check_dtype(v, dtype) for v in value)\n    else:\n        return False\n</code></pre>"},{"location":"core-components/dtypes/#_22","title":"\ud83c\udfaf \u4f7f\u7528\u793a\u4f8b","text":""},{"location":"core-components/dtypes/#_23","title":"\u57fa\u7840\u7c7b\u578b\u64cd\u4f5c","text":"Python<pre><code>import genesis\n\n# \u521b\u5efa\u4e0d\u540c\u7c7b\u578b\u7684\u5f20\u91cf\nx_f32 = genesis.randn(3, 4, dtype=genesis.float32)\nx_f16 = genesis.randn(3, 4, dtype=genesis.float16)\nx_int = genesis.randint(0, 10, (3, 4), dtype=genesis.int32)\n\n# \u68c0\u67e5\u7c7b\u578b\nprint(f\"x_f32\u7c7b\u578b: {x_f32.dtype}\")          # genesis.float32\nprint(f\"\u662f\u5426\u6d6e\u70b9: {x_f32.dtype.is_floating_point}\")  # True\nprint(f\"\u5b57\u8282\u5927\u5c0f: {x_f32.dtype.itemsize}\")          # 4\n</code></pre>"},{"location":"core-components/dtypes/#_24","title":"\u7c7b\u578b\u8f6c\u6362","text":"Python<pre><code># \u5b57\u7b26\u4e32\u5230\u7c7b\u578b\ndtype1 = genesis.get_dtype(\"float16\")    # genesis.float16\ndtype2 = genesis.get_dtype(np.float32)   # genesis.float32\ndtype3 = genesis.get_dtype(None)         # genesis.float32 (\u9ed8\u8ba4)\n\n# \u5f20\u91cf\u7c7b\u578b\u8f6c\u6362\nx = genesis.randn(3, 4, dtype=\"float32\")\nx_half = x.half()      # \u8f6c\u6362\u4e3afloat16\nx_float = x.float()    # \u8f6c\u6362\u4e3afloat32\n</code></pre>"},{"location":"core-components/dtypes/#_25","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\n# \u6a21\u578b\u4f1a\u81ea\u52a8\u5728fp16\u548cfp32\u95f4\u8f6c\u6362\nimport genesis.nn as nn\n\nmodel = nn.Linear(784, 128)\nx = genesis.randn(32, 784, dtype=genesis.float16)\n\n# \u524d\u5411\u4f20\u64ad\u65f6\u81ea\u52a8\u5904\u7406\u7c7b\u578b\u8f6c\u6362\noutput = model(x)\n</code></pre>"},{"location":"core-components/dtypes/#_26","title":"\u8bbe\u5907\u95f4\u7c7b\u578b\u4e00\u81f4\u6027","text":"Python<pre><code># CPU\u548cGPU\u4f7f\u7528\u76f8\u540c\u7684\u7c7b\u578b\u7cfb\u7edf\ncpu_tensor = genesis.randn(3, 4, device=\"cpu\", dtype=genesis.float32)\ngpu_tensor = genesis.randn(3, 4, device=\"cuda\", dtype=genesis.float32)\n\nprint(cpu_tensor.dtype == gpu_tensor.dtype)  # True\nprint(cpu_tensor.dtype.name)                 # \"float32\"\nprint(gpu_tensor.dtype.name)                 # \"float32\"\n</code></pre>"},{"location":"core-components/dtypes/#bfloat16","title":"bfloat16\u7279\u6b8a\u5904\u7406","text":"Python<pre><code># bfloat16\u5728\u4e0d\u540c\u540e\u7aef\u7684\u5904\u7406\nx_bf16 = genesis.randn(3, 4, dtype=genesis.bfloat16)\n\n# CPU\u540e\u7aef: \u4f7f\u7528float32\u5b58\u50a8\u4f46\u6807\u8bb0\u4e3abfloat16\n# GPU\u540e\u7aef: \u539f\u751fbfloat16\u652f\u6301\uff08\u5982\u679c\u786c\u4ef6\u652f\u6301\uff09\nprint(f\"\u7c7b\u578b\u540d: {x_bf16.dtype.name}\")           # \"bfloat16\"\nprint(f\"Triton\u540d: {x_bf16.dtype.triton_name}\")  # \"bfloat16\"\nprint(f\"NumPy\u7c7b\u578b: {x_bf16.dtype.numpy_dtype}\") # &lt;class 'numpy.float32'&gt;\n</code></pre>"},{"location":"core-components/dtypes/#_27","title":"\ud83d\ude80 \u6027\u80fd\u4f18\u5316","text":""},{"location":"core-components/dtypes/#_28","title":"\u7c7b\u578b\u8f6c\u6362\u4f18\u5316","text":"<ul> <li>\u60f0\u6027\u8f6c\u6362\uff1a\u53ea\u6709\u5728\u771f\u6b63\u9700\u8981\u65f6\u624d\u8fdb\u884c\u7c7b\u578b\u8f6c\u6362</li> <li>\u7f13\u5b58\u673a\u5236\uff1a\u5e38\u7528\u7684\u7c7b\u578b\u8f6c\u6362\u7ed3\u679c\u4f1a\u88ab\u7f13\u5b58</li> <li>\u96f6\u62f7\u8d1d\uff1a\u540c\u7c7b\u578b\u4e0d\u540c\u8bbe\u5907\u95f4\u7684\u8f6c\u6362\u5c3d\u53ef\u80fd\u96f6\u62f7\u8d1d</li> </ul>"},{"location":"core-components/dtypes/#_29","title":"\u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u7d27\u51d1\u5b58\u50a8\uff1a\u4f7f\u7528\u5408\u9002\u7684\u6570\u636e\u7c7b\u578b\u51cf\u5c11\u5185\u5b58\u5360\u7528</li> <li>\u5bf9\u9f50\u4f18\u5316\uff1a\u6570\u636e\u7c7b\u578b\u5bf9\u9f50\u4ee5\u63d0\u9ad8\u8bbf\u95ee\u6548\u7387</li> <li>\u6279\u91cf\u8f6c\u6362\uff1a\u6279\u91cf\u5904\u7406\u7c7b\u578b\u8f6c\u6362\u4ee5\u63d0\u9ad8\u6548\u7387</li> </ul> <p>Genesis\u7684\u6570\u636e\u7c7b\u578b\u7cfb\u7edf\u4e3a\u6574\u4e2a\u6846\u67b6\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u9ad8\u6548\u3001\u7c7b\u578b\u5b89\u5168\u7684\u6570\u636e\u8868\u793a\uff0c\u662f\u5b9e\u73b0\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u8de8\u8bbe\u5907\u8ba1\u7b97\u7684\u57fa\u7840\u3002</p>"},{"location":"getting-started/","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u6b22\u8fce\u4f7f\u7528Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff01\u8fd9\u4e2a\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u5728\u51e0\u5206\u949f\u5185\u5f00\u59cb\u4f7f\u7528Genesis\u3002</p>"},{"location":"getting-started/#_2","title":"\ud83c\udfaf \u6982\u89c8","text":"<p>Genesis\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u5b66\u4e60\u548c\u7814\u7a76\u800c\u8bbe\u8ba1\u3002\u5b83\u63d0\u4f9b\u4e86\uff1a</p> <ul> <li>\u7b80\u6d01\u6613\u61c2\u7684API\u8bbe\u8ba1</li> <li>\u9ad8\u6027\u80fd\u7684GPU\u52a0\u901f\u8ba1\u7b97</li> <li>\u5b8c\u6574\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u529f\u80fd</li> <li>\u4e0ePyTorch\u751f\u6001\u7cfb\u7edf\u7684\u826f\u597d\u517c\u5bb9\u6027</li> </ul>"},{"location":"getting-started/#5","title":"\u26a1 5\u5206\u949f\u5feb\u901f\u4f53\u9a8c","text":""},{"location":"getting-started/#1-genesis","title":"1. \u5b89\u88c5Genesis","text":"Bash<pre><code># \u5b89\u88c5\u6838\u5fc3\u4f9d\u8d56\npip install torch triton\n\n# \u514b\u9686\u6e90\u7801\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u5b89\u88c5Genesis\npip install -e .\n</code></pre>"},{"location":"getting-started/#2","title":"2. \u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\n\n# \u5b9a\u4e49\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        return self.layer2(x)\n\n# \u521b\u5efa\u6a21\u578b\u548c\u6570\u636e\nmodel = MLP(784, 128, 10)\nx = genesis.randn(32, 784)  # \u6279\u91cf\u5927\u5c0f32\uff0c\u8f93\u5165\u7ef4\u5ea6784\n\n# \u524d\u5411\u4f20\u64ad\noutput = model(x)\nprint(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")  # torch.Size([32, 10])\n</code></pre>"},{"location":"getting-started/#3","title":"3. \u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>import genesis.optim as optim\n\n# \u521b\u5efa\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# \u6a21\u62df\u8bad\u7ec3\u6570\u636e\ntargets = genesis.randint(0, 10, (32,))\n\n# \u8bad\u7ec3\u4e00\u4e2a\u6279\u6b21\noptimizer.zero_grad()        # \u6e05\u96f6\u68af\u5ea6\noutput = model(x)           # \u524d\u5411\u4f20\u64ad\nloss = criterion(output, targets)  # \u8ba1\u7b97\u635f\u5931\nloss.backward()             # \u53cd\u5411\u4f20\u64ad\noptimizer.step()            # \u66f4\u65b0\u53c2\u6570\n\nprint(f\"\u635f\u5931\u503c: {loss.item():.4f}\")\n</code></pre>"},{"location":"getting-started/#_3","title":"\ud83d\udcda \u6838\u5fc3\u6982\u5ff5","text":""},{"location":"getting-started/#tensor","title":"\u5f20\u91cf (Tensor)","text":"<p>Genesis\u4e2d\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u81ea\u52a8\u5fae\u5206\uff1a</p> Python<pre><code>import genesis\n\n# \u521b\u5efa\u5f20\u91cf\nx = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)\ny = genesis.tensor([4.0, 5.0, 6.0], requires_grad=True)\n\n# \u8ba1\u7b97\u64cd\u4f5c\nz = x * y + x.sum()\nz.backward(genesis.ones_like(z))\n\nprint(f\"x\u7684\u68af\u5ea6: {x.grad}\")  # [5., 6., 7.]\nprint(f\"y\u7684\u68af\u5ea6: {y.grad}\")  # [1., 2., 3.]\n</code></pre>"},{"location":"getting-started/#module","title":"\u6a21\u5757 (Module)","text":"<p>\u795e\u7ecf\u7f51\u7edc\u7ec4\u4ef6\u7684\u57fa\u7c7b\uff1a</p> Python<pre><code>import genesis.nn as nn\n\nclass CustomLayer(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.weight = genesis.randn(out_features, in_features, requires_grad=True)\n        self.bias = genesis.zeros(out_features, requires_grad=True)\n\n    def forward(self, x):\n        return genesis.functional.linear(x, self.weight, self.bias)\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u5c42\nlayer = CustomLayer(10, 5)\ninput_tensor = genesis.randn(3, 10)\noutput = layer(input_tensor)\n</code></pre>"},{"location":"getting-started/#optimizer","title":"\u4f18\u5316\u5668 (Optimizer)","text":"<p>\u53c2\u6570\u66f4\u65b0\u7b97\u6cd5\uff1a</p> Python<pre><code>import genesis.optim as optim\n\n# \u4e0d\u540c\u7684\u4f18\u5316\u5668\u9009\u62e9\nsgd_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nadam_optimizer = optim.Adam(model.parameters(), lr=0.001)\nadamw_optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n</code></pre>"},{"location":"getting-started/#_4","title":"\ud83d\udee0\ufe0f \u73af\u5883\u914d\u7f6e","text":""},{"location":"getting-started/#_5","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: \u73b0\u4ee3\u591a\u6838\u5904\u7406\u5668</li> <li>\u5185\u5b58: \u6700\u5c118GB RAM\uff0c\u63a8\u835016GB+</li> <li>GPU: NVIDIA GPU with CUDA\u652f\u6301 (\u63a8\u8350)</li> <li>\u5b58\u50a8: \u81f3\u5c112GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"getting-started/#_6","title":"\u8f6f\u4ef6\u4f9d\u8d56","text":"Bash<pre><code># Python\u73af\u5883\nPython &gt;= 3.8\n\n# \u6838\u5fc3\u4f9d\u8d56\ntorch &gt;= 2.0.0\ntriton &gt;= 2.0.0\nnumpy &gt;= 1.21.0\ncuda-python &gt;= 11.8.0  # GPU\u652f\u6301\n\n# \u53ef\u9009\u4f9d\u8d56\nmatplotlib &gt;= 3.5.0  # \u7528\u4e8e\u53ef\u89c6\u5316\ntqdm &gt;= 4.64.0      # \u8fdb\u5ea6\u6761\nwandb &gt;= 0.13.0     # \u5b9e\u9a8c\u8ddf\u8e2a\n</code></pre>"},{"location":"getting-started/#_7","title":"\ud83d\udcd6 \u4e0b\u4e00\u6b65","text":"<p>\u73b0\u5728\u4f60\u5df2\u7ecf\u4e86\u89e3\u4e86Genesis\u7684\u57fa\u7840\u7528\u6cd5\uff0c\u53ef\u4ee5\u7ee7\u7eed\u63a2\u7d22\uff1a</p>"},{"location":"getting-started/#_8","title":"\ud83c\udf93 \u6df1\u5165\u5b66\u4e60","text":"<ul> <li>\u5b8c\u6574\u5b89\u88c5\u6307\u5357 - \u8be6\u7ec6\u7684\u5b89\u88c5\u548c\u914d\u7f6e\u6b65\u9aa4</li> <li>\u7b2c\u4e00\u4e2a\u5b8c\u6574\u7a0b\u5e8f - \u6784\u5efa\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b</li> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u7cfb\u7edf\u6027\u7684\u8bad\u7ec3\u6559\u7a0b</li> </ul>"},{"location":"getting-started/#_9","title":"\ud83d\udd0d \u67b6\u6784\u7406\u89e3","text":"<ul> <li>\u67b6\u6784\u6982\u8ff0 - \u4e86\u89e3Genesis\u7684\u6574\u4f53\u8bbe\u8ba1</li> <li>\u6838\u5fc3\u7ec4\u4ef6 - \u6df1\u5165\u7406\u89e3\u5185\u90e8\u5b9e\u73b0</li> <li>API\u53c2\u8003 - \u5b8c\u6574\u7684API\u6587\u6863</li> </ul>"},{"location":"getting-started/#_10","title":"\ud83d\ude80 \u9ad8\u7ea7\u7279\u6027","text":"<ul> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u64cd\u4f5c</li> <li>\u6027\u80fd\u4f18\u5316 - \u8bad\u7ec3\u6027\u80fd\u8c03\u4f18</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u8bad\u7ec3</li> </ul>"},{"location":"getting-started/#_11","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":""},{"location":"getting-started/#q-genesispytorch","title":"Q: Genesis\u4e0ePyTorch\u6709\u4ec0\u4e48\u533a\u522b\uff1f","text":"<p>A: Genesis\u662f\u6559\u80b2\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u4ee3\u7801\u66f4\u7b80\u6d01\u6613\u61c2\uff0c\u9002\u5408\u5b66\u4e60\u6df1\u5ea6\u5b66\u4e60\u7684\u5185\u90e8\u5b9e\u73b0\u3002PyTorch\u66f4\u9002\u5408\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u3002</p>"},{"location":"getting-started/#q-genesis","title":"Q: \u53ef\u4ee5\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528Genesis\u5417\uff1f","text":"<p>A: Genesis\u4e3b\u8981\u7528\u4e8e\u6559\u80b2\u548c\u7814\u7a76\uff0c\u867d\u7136\u529f\u80fd\u5b8c\u6574\uff0c\u4f46\u5efa\u8bae\u751f\u4ea7\u73af\u5883\u4f7f\u7528\u66f4\u6210\u719f\u7684\u6846\u67b6\u5982PyTorch\u3002</p>"},{"location":"getting-started/#q","title":"Q: \u5982\u4f55\u83b7\u5f97\u5e2e\u52a9\uff1f","text":"<p>A: \u53ef\u4ee5\u901a\u8fc7GitHub Issues\u3001Discussions\u6216\u67e5\u770b\u8be6\u7ec6\u6587\u6863\u83b7\u5f97\u5e2e\u52a9\u3002</p>"},{"location":"getting-started/#_12","title":"\ud83c\udf89 \u51c6\u5907\u597d\u4e86\u5417\uff1f","text":"<p>\u8ba9\u6211\u4eec\u5f00\u59cb\u6df1\u5165\u4e86\u89e3Genesis\u5427\uff01</p> <p>\u8be6\u7ec6\u5b89\u88c5\u6307\u5357 \u5b8c\u6574\u6559\u7a0b</p>"},{"location":"getting-started/first-steps/","title":"\u7b2c\u4e00\u4e2a\u5b8c\u6574\u7a0b\u5e8f","text":"<p>\u5b8c\u6210\u5b89\u88c5\u540e\uff0c\u8ba9\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u5b8c\u6574\u7684\u4f8b\u5b50\u6765\u5b66\u4e60Genesis\u7684\u57fa\u672c\u7528\u6cd5\u3002\u6211\u4eec\u5c06\u5b9e\u73b0\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u5668\u6765\u6f14\u793a\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\u3002</p>"},{"location":"getting-started/first-steps/#_2","title":"\ud83c\udfaf \u9879\u76ee\u76ee\u6807","text":"<p>\u6784\u5efa\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u8bc6\u522b\u5668\uff08\u7c7b\u4f3cMNIST\uff09\uff0c\u5b66\u4e60Genesis\u7684\u6838\u5fc3\u6982\u5ff5\uff1a</p> <ul> <li>\u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406</li> <li>\u6a21\u578b\u5b9a\u4e49\u548c\u521d\u59cb\u5316</li> <li>\u8bad\u7ec3\u5faa\u73af\u548c\u9a8c\u8bc1</li> <li>\u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d</li> </ul>"},{"location":"getting-started/first-steps/#_3","title":"\ud83d\udcca \u9879\u76ee\u7ed3\u6784","text":"<p>\u521b\u5efa\u9879\u76ee\u76ee\u5f55\u7ed3\u6784\uff1a</p> Text Only<pre><code>first_project/\n\u251c\u2500\u2500 data/                # \u6570\u636e\u76ee\u5f55\n\u251c\u2500\u2500 models/              # \u6a21\u578b\u4fdd\u5b58\u76ee\u5f55\n\u251c\u2500\u2500 train.py            # \u8bad\u7ec3\u811a\u672c\n\u251c\u2500\u2500 model.py            # \u6a21\u578b\u5b9a\u4e49\n\u251c\u2500\u2500 dataset.py          # \u6570\u636e\u52a0\u8f7d\n\u2514\u2500\u2500 utils.py            # \u5de5\u5177\u51fd\u6570\n</code></pre>"},{"location":"getting-started/first-steps/#1-datasetpy","title":"\ud83d\udcc1 1. \u6570\u636e\u5904\u7406 (<code>dataset.py</code>)","text":"Python<pre><code>\"\"\"\u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406\u6a21\u5757\"\"\"\nimport genesis\nimport numpy as np\nfrom typing import Tuple, List\nimport pickle\nimport os\n\nclass SimpleDataset:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u96c6\u7c7b\"\"\"\n\n    def __init__(self, data: np.ndarray, labels: np.ndarray, transform=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u6570\u636e\u96c6\n\n        Args:\n            data: \u8f93\u5165\u6570\u636e (N, H, W) \u6216 (N, D)\n            labels: \u6807\u7b7e (N,)\n            transform: \u6570\u636e\u53d8\u6362\u51fd\u6570\n        \"\"\"\n        self.data = data.astype(np.float32)\n        self.labels = labels.astype(np.int64)\n        self.transform = transform\n\n    def __len__(self) -&gt; int:\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[genesis.Tensor, genesis.Tensor]:\n        \"\"\"\u83b7\u53d6\u5355\u4e2a\u6837\u672c\"\"\"\n        x = self.data[idx]\n        y = self.labels[idx]\n\n        if self.transform:\n            x = self.transform(x)\n\n        return genesis.tensor(x), genesis.tensor(y)\n\nclass DataLoader:\n    \"\"\"\u7b80\u5355\u7684\u6570\u636e\u52a0\u8f7d\u5668\"\"\"\n\n    def __init__(self, dataset: SimpleDataset, batch_size: int = 32, shuffle: bool = True):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self._reset_indices()\n\n    def _reset_indices(self):\n        \"\"\"\u91cd\u7f6e\u7d22\u5f15\"\"\"\n        self.indices = np.arange(len(self.dataset))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n        self.current = 0\n\n    def __iter__(self):\n        self._reset_indices()\n        return self\n\n    def __next__(self):\n        if self.current &gt;= len(self.dataset):\n            raise StopIteration\n\n        # \u83b7\u53d6\u5f53\u524d\u6279\u6b21\u7684\u7d22\u5f15\n        end_idx = min(self.current + self.batch_size, len(self.dataset))\n        batch_indices = self.indices[self.current:end_idx]\n\n        # \u6536\u96c6\u6279\u6b21\u6570\u636e\n        batch_data = []\n        batch_labels = []\n\n        for idx in batch_indices:\n            x, y = self.dataset[idx]\n            batch_data.append(x)\n            batch_labels.append(y)\n\n        self.current = end_idx\n\n        # \u5806\u53e0\u6210\u6279\u6b21\n        batch_x = genesis.stack(batch_data, dim=0)\n        batch_y = genesis.stack(batch_labels, dim=0)\n\n        return batch_x, batch_y\n\ndef create_synthetic_data(n_samples: int = 1000, n_features: int = 784, n_classes: int = 10) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\u521b\u5efa\u5408\u6210\u6570\u636e\u7528\u4e8e\u6f14\u793a\"\"\"\n    np.random.seed(42)\n\n    # \u751f\u6210\u968f\u673a\u6570\u636e\n    data = np.random.randn(n_samples, n_features).astype(np.float32)\n\n    # \u4e3a\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u4e00\u4e9b\u6a21\u5f0f\n    labels = np.random.randint(0, n_classes, n_samples)\n    for i in range(n_classes):\n        mask = labels == i\n        # \u7ed9\u6bcf\u4e2a\u7c7b\u522b\u6dfb\u52a0\u7279\u5b9a\u7684\u504f\u7f6e\n        data[mask] += np.random.randn(n_features) * 0.5\n\n    return data, labels\n\ndef load_data() -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"\u52a0\u8f7d\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e\"\"\"\n    print(\"\ud83d\udd04 \u52a0\u8f7d\u6570\u636e...\")\n\n    # \u521b\u5efa\u5408\u6210\u6570\u636e (\u5b9e\u9645\u9879\u76ee\u4e2d\u66ff\u6362\u4e3a\u771f\u5b9e\u6570\u636e)\n    train_data, train_labels = create_synthetic_data(800, 784, 10)\n    val_data, val_labels = create_synthetic_data(200, 784, 10)\n\n    # \u6570\u636e\u6807\u51c6\u5316\n    def normalize(x):\n        return (x - x.mean()) / (x.std() + 1e-8)\n\n    # \u521b\u5efa\u6570\u636e\u96c6\n    train_dataset = SimpleDataset(train_data, train_labels, transform=normalize)\n    val_dataset = SimpleDataset(val_data, val_labels, transform=normalize)\n\n    # \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n    print(f\"\u2705 \u6570\u636e\u52a0\u8f7d\u5b8c\u6210 - \u8bad\u7ec3\u96c6: {len(train_dataset)}, \u9a8c\u8bc1\u96c6: {len(val_dataset)}\")\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"getting-started/first-steps/#2-modelpy","title":"\ud83e\udde0 2. \u6a21\u578b\u5b9a\u4e49 (<code>model.py</code>)","text":"Python<pre><code>\"\"\"\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b9a\u4e49\"\"\"\nimport genesis\nimport genesis.nn as nn\nimport genesis.nn.functional as F\n\nclass MLP(nn.Module):\n    \"\"\"\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\"\"\"\n\n    def __init__(self, input_dim: int = 784, hidden_dims: list = None, num_classes: int = 10, dropout_rate: float = 0.2):\n        \"\"\"\n        \u521d\u59cb\u5316MLP\u6a21\u578b\n\n        Args:\n            input_dim: \u8f93\u5165\u7ef4\u5ea6\n            hidden_dims: \u9690\u85cf\u5c42\u7ef4\u5ea6\u5217\u8868\n            num_classes: \u5206\u7c7b\u6570\u91cf\n            dropout_rate: Dropout\u6bd4\u7387\n        \"\"\"\n        super().__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [512, 256, 128]\n\n        # \u6784\u5efa\u7f51\u7edc\u5c42\n        layers = []\n        prev_dim = input_dim\n\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout_rate)\n            ])\n            prev_dim = hidden_dim\n\n        # \u8f93\u51fa\u5c42\n        layers.append(nn.Linear(prev_dim, num_classes))\n\n        self.network = nn.Sequential(*layers)\n\n        # \u521d\u59cb\u5316\u6743\u91cd\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"\u6743\u91cd\u521d\u59cb\u5316\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                # Xavier\u521d\u59cb\u5316\n                std = (2.0 / (module.in_features + module.out_features)) ** 0.5\n                module.weight.data.normal_(0, std)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        \"\"\"\u524d\u5411\u4f20\u64ad\"\"\"\n        # \u5c55\u5e73\u8f93\u5165 (\u5982\u679c\u662f\u56fe\u50cf\u6570\u636e)\n        if x.dim() &gt; 2:\n            x = x.view(x.size(0), -1)\n\n        return self.network(x)\n\nclass CNN(nn.Module):\n    \"\"\"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668 (\u5982\u679c\u9700\u8981\u5904\u7406\u56fe\u50cf)\"\"\"\n\n    def __init__(self, num_classes: int = 10):\n        super().__init__()\n\n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n\n        # \u6c60\u5316\u5c42\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(128 * 3 * 3, 512)  # \u5047\u8bbe\u8f93\u5165\u662f28x28\n        self.fc2 = nn.Linear(512, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x: genesis.Tensor) -&gt; genesis.Tensor:\n        # \u5377\u79ef + \u6c60\u5316\n        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -&gt; 7x7  \n        x = self.pool(F.relu(self.conv3(x)))  # 7x7 -&gt; 3x3\n\n        # \u5c55\u5e73\n        x = x.view(x.size(0), -1)\n\n        # \u5168\u8fde\u63a5\u5c42\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\ndef create_model(model_type: str = \"mlp\", **kwargs) -&gt; nn.Module:\n    \"\"\"\u5de5\u5382\u51fd\u6570\uff1a\u521b\u5efa\u6a21\u578b\"\"\"\n    if model_type.lower() == \"mlp\":\n        return MLP(**kwargs)\n    elif model_type.lower() == \"cnn\":\n        return CNN(**kwargs)\n    else:\n        raise ValueError(f\"\u672a\u77e5\u7684\u6a21\u578b\u7c7b\u578b: {model_type}\")\n</code></pre>"},{"location":"getting-started/first-steps/#3-utilspy","title":"\ud83d\udee0\ufe0f 3. \u5de5\u5177\u51fd\u6570 (<code>utils.py</code>)","text":"Python<pre><code>\"\"\"\u5de5\u5177\u51fd\u6570\u6a21\u5757\"\"\"\nimport genesis\nimport time\nimport os\nfrom typing import Dict, Any\nimport json\n\nclass AverageMeter:\n    \"\"\"\u5e73\u5747\u503c\u8ba1\u7b97\u5668\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val: float, n: int = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass Timer:\n    \"\"\"\u8ba1\u65f6\u5668\"\"\"\n\n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n\n    def start(self):\n        self.start_time = time.time()\n\n    def stop(self):\n        self.end_time = time.time()\n        return self.end_time - self.start_time\n\n    def elapsed(self):\n        if self.start_time is None:\n            return 0\n        return time.time() - self.start_time\n\ndef accuracy(output: genesis.Tensor, target: genesis.Tensor, topk: tuple = (1,)) -&gt; list:\n    \"\"\"\u8ba1\u7b97\u51c6\u786e\u7387\"\"\"\n    with genesis.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n\n        return res\n\ndef save_checkpoint(model: genesis.nn.Module, optimizer: genesis.optim.Optimizer, \n                   epoch: int, loss: float, accuracy: float, filepath: str):\n    \"\"\"\u4fdd\u5b58\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n        'accuracy': accuracy\n    }\n\n    # \u786e\u4fdd\u76ee\u5f55\u5b58\u5728\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    genesis.save(checkpoint, filepath)\n    print(f\"\ud83d\udcbe \u68c0\u67e5\u70b9\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef load_checkpoint(filepath: str, model: genesis.nn.Module, optimizer: genesis.optim.Optimizer = None) -&gt; Dict[str, Any]:\n    \"\"\"\u52a0\u8f7d\u68c0\u67e5\u70b9\"\"\"\n    checkpoint = genesis.load(filepath)\n\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    if optimizer and 'optimizer_state_dict' in checkpoint:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    print(f\"\ud83d\udcc1 \u68c0\u67e5\u70b9\u5df2\u52a0\u8f7d: {filepath}\")\n    print(f\"   \u8f6e\u6b21: {checkpoint['epoch']}, \u635f\u5931: {checkpoint['loss']:.4f}, \u51c6\u786e\u7387: {checkpoint['accuracy']:.2f}%\")\n\n    return checkpoint\n\ndef save_training_history(history: Dict[str, list], filepath: str):\n    \"\"\"\u4fdd\u5b58\u8bad\u7ec3\u5386\u53f2\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\n    with open(filepath, 'w') as f:\n        json.dump(history, f, indent=2)\n\n    print(f\"\ud83d\udcca \u8bad\u7ec3\u5386\u53f2\u5df2\u4fdd\u5b58: {filepath}\")\n\ndef print_model_summary(model: genesis.nn.Module, input_shape: tuple):\n    \"\"\"\u6253\u5370\u6a21\u578b\u6458\u8981\"\"\"\n    print(\"\ud83c\udfd7\ufe0f  \u6a21\u578b\u67b6\u6784:\")\n    print(\"=\" * 50)\n\n    # \u8ba1\u7b97\u53c2\u6570\u6570\u91cf\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"\u603b\u53c2\u6570\u91cf: {total_params:,}\")\n    print(f\"\u53ef\u8bad\u7ec3\u53c2\u6570: {trainable_params:,}\")\n    print(f\"\u8f93\u5165\u5f62\u72b6: {input_shape}\")\n\n    # \u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\n    dummy_input = genesis.randn(*input_shape)\n    try:\n        with genesis.no_grad():\n            output = model(dummy_input)\n        print(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")\n    except Exception as e:\n        print(f\"\u524d\u5411\u4f20\u64ad\u6d4b\u8bd5\u5931\u8d25: {e}\")\n\n    print(\"=\" * 50)\n</code></pre>"},{"location":"getting-started/first-steps/#4-trainpy","title":"\ud83d\ude82 4. \u8bad\u7ec3\u811a\u672c (<code>train.py</code>)","text":"<p> [{\"id\": \"1\", \"content\": \"\\u521b\\u5efa\\u6587\\u6863\\u76ee\\u5f55\\u7ed3\\u6784\\u548c\\u914d\\u7f6e\\u6587\\u4ef6\", \"status\": \"completed\"}, {\"id\": \"2\", \"content\": \"\\u7f16\\u5199\\u9996\\u9875\\u548c\\u5feb\\u901f\\u5f00\\u59cb\\u6587\\u6863\", \"status\": \"completed\"}, {\"id\": \"3\", \"content\": \"\\u7f16\\u5199\\u67b6\\u6784\\u8bbe\\u8ba1\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"4\", \"content\": \"\\u7f16\\u5199\\u6838\\u5fc3\\u7ec4\\u4ef6\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"5\", \"content\": \"\\u7f16\\u5199\\u795e\\u7ecf\\u7f51\\u7edc\\u6a21\\u5757\\u6587\\u6863\", \"status\": \"pending\"}, {\"id\": \"6\", \"content\": \"\\u7f16\\u5199API\\u53c2\\u8003\\u6587\\u6863\", \"status\": \"pending\"}]"},{"location":"getting-started/installation/","title":"\u5b89\u88c5\u6307\u5357","text":"<p>\u672c\u6307\u5357\u5c06\u5e2e\u52a9\u4f60\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u5b89\u88c5Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002</p>"},{"location":"getting-started/installation/#_2","title":"\ud83d\udccb \u7cfb\u7edf\u8981\u6c42","text":""},{"location":"getting-started/installation/#_3","title":"\u786c\u4ef6\u8981\u6c42","text":"<ul> <li>CPU: x86_64\u67b6\u6784\uff0c\u652f\u6301AVX\u6307\u4ee4\u96c6</li> <li>\u5185\u5b58: \u6700\u5c118GB\uff0c\u63a8\u835016GB+</li> <li>GPU: NVIDIA GPU with Compute Capability \u2265 6.0 (\u53ef\u9009\u4f46\u63a8\u8350)</li> <li>\u5b58\u50a8: 2GB\u53ef\u7528\u7a7a\u95f4</li> </ul>"},{"location":"getting-started/installation/#_4","title":"\u8f6f\u4ef6\u8981\u6c42","text":"<ul> <li>\u64cd\u4f5c\u7cfb\u7edf: Linux (Ubuntu 20.04+), macOS (10.15+), Windows 10+</li> <li>Python: 3.8, 3.9, 3.10, 3.11</li> <li>CUDA: 11.0+ (GPU\u52a0\u901f\u9700\u8981)</li> </ul>"},{"location":"getting-started/installation/#_5","title":"\ud83d\ude80 \u5feb\u901f\u5b89\u88c5","text":""},{"location":"getting-started/installation/#_6","title":"\u65b9\u5f0f\u4e00\uff1a\u4ece\u6e90\u7801\u5b89\u88c5 (\u63a8\u8350)","text":"Bash<pre><code># 1. \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# 2. \u521b\u5efa\u865a\u62df\u73af\u5883 (\u63a8\u8350)\npython -m venv genesis-env\nsource genesis-env/bin/activate  # Linux/macOS\n# genesis-env\\\\Scripts\\\\activate  # Windows\n\n# 3. \u5b89\u88c5\u4f9d\u8d56\npip install -r genesis/requirements.txt\n\n# 4. \u5b89\u88c5Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation/#pip","title":"\u65b9\u5f0f\u4e8c\uff1a\u4f7f\u7528pip\u5b89\u88c5","text":"Bash<pre><code># \u5b89\u88c5\u53d1\u5e03\u7248\u672c\npip install genesis-dl\n\n# \u5b89\u88c5\u9884\u53d1\u5e03\u7248\u672c\npip install --pre genesis-dl\n</code></pre>"},{"location":"getting-started/installation/#_7","title":"\ud83d\udd27 \u8be6\u7ec6\u5b89\u88c5\u6b65\u9aa4","text":""},{"location":"getting-started/installation/#python","title":"\u7b2c\u4e00\u6b65\uff1a\u51c6\u5907Python\u73af\u5883","text":"Ubuntu/DebianCentOS/RHELmacOSWindows Bash<pre><code># \u5b89\u88c5Python\u548cpip\nsudo apt update\nsudo apt install python3.9 python3.9-pip python3.9-venv\n\n# \u521b\u5efa\u8f6f\u94fe\u63a5 (\u53ef\u9009)\nsudo ln -sf /usr/bin/python3.9 /usr/bin/python\n</code></pre> Bash<pre><code># \u5b89\u88c5EPEL\u4ed3\u5e93\nsudo yum install epel-release\n\n# \u5b89\u88c5Python\nsudo yum install python39 python39-pip\n</code></pre> Bash<pre><code># \u4f7f\u7528Homebrew\u5b89\u88c5\nbrew install python@3.9\n\n# \u6216\u4f7f\u7528\u5b98\u65b9\u5b89\u88c5\u5305\n# \u4ece https://python.org \u4e0b\u8f7d\u5b89\u88c5\n</code></pre> PowerShell<pre><code># \u4e0b\u8f7dPython\u5b89\u88c5\u5305\n# https://python.org/downloads/windows/\n\n# \u6216\u4f7f\u7528Chocolatey\nchoco install python39\n</code></pre>"},{"location":"getting-started/installation/#cuda-gpu","title":"\u7b2c\u4e8c\u6b65\uff1a\u5b89\u88c5CUDA (GPU\u52a0\u901f)","text":"<p>GPU\u652f\u6301\u8bf4\u660e</p> <p>\u5982\u679c\u4f60\u53ea\u9700\u8981CPU\u7248\u672c\uff0c\u53ef\u4ee5\u8df3\u8fc7\u6b64\u6b65\u9aa4\u3002\u4f46\u5f3a\u70c8\u63a8\u8350\u5b89\u88c5CUDA\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002</p> Ubuntu/DebianCentOS/RHELWindows Bash<pre><code># \u4e0b\u8f7dCUDA Toolkit\nwget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\nsudo sh cuda_11.8.0_520.61.05_linux.run\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> Bash<pre><code># \u5b89\u88c5NVIDIA\u9a71\u52a8\u4ed3\u5e93\nsudo yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\n\n# \u5b89\u88c5CUDA\nsudo yum install cuda-11-8\n</code></pre> PowerShell<pre><code># \u4e0b\u8f7dCUDA\u5b89\u88c5\u5305\n# https://developer.nvidia.com/cuda-downloads\n\n# \u8fd0\u884c\u5b89\u88c5\u7a0b\u5e8f\u5e76\u6309\u7167\u63d0\u793a\u64cd\u4f5c\n</code></pre>"},{"location":"getting-started/installation/#_8","title":"\u7b2c\u4e09\u6b65\uff1a\u5b89\u88c5\u6838\u5fc3\u4f9d\u8d56","text":"Bash<pre><code># \u521b\u5efa\u5e76\u6fc0\u6d3b\u865a\u62df\u73af\u5883\npython -m venv genesis-env\nsource genesis-env/bin/activate\n\n# \u5347\u7ea7pip\npip install --upgrade pip setuptools wheel\n\n# \u5b89\u88c5PyTorch (\u6839\u636e\u4f60\u7684CUDA\u7248\u672c\u9009\u62e9)\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CPU\u7248\u672c\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n\n# \u5b89\u88c5Triton\npip install triton\n\n# \u5b89\u88c5\u5176\u4ed6\u4f9d\u8d56\npip install numpy matplotlib tqdm\n</code></pre>"},{"location":"getting-started/installation/#genesis","title":"\u7b2c\u56db\u6b65\uff1a\u5b89\u88c5Genesis","text":"Bash<pre><code># \u514b\u9686\u6e90\u7801\ngit clone https://github.com/phonism/genesis.git\ncd genesis\n\n# \u67e5\u770b\u53ef\u7528\u7248\u672c\ngit tag\n\n# \u5207\u6362\u5230\u7a33\u5b9a\u7248\u672c (\u53ef\u9009)\ngit checkout v0.1.0\n\n# \u5b89\u88c5Genesis\npip install -e genesis/\n</code></pre>"},{"location":"getting-started/installation/#_9","title":"\u2705 \u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\u9a8c\u8bc1\u5b89\u88c5\u662f\u5426\u6210\u529f\uff1a</p> Python<pre><code>#!/usr/bin/env python3\n\"\"\"Genesis\u5b89\u88c5\u9a8c\u8bc1\u811a\u672c\"\"\"\n\ndef test_basic_import():\n    \"\"\"\u6d4b\u8bd5\u57fa\u7840\u5bfc\u5165\"\"\"\n    try:\n        import genesis\n        print(\"\u2705 Genesis\u5bfc\u5165\u6210\u529f\")\n        print(f\"   \u7248\u672c: {genesis.__version__}\")\n    except ImportError as e:\n        print(f\"\u274c Genesis\u5bfc\u5165\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_tensor_operations():\n    \"\"\"\u6d4b\u8bd5\u5f20\u91cf\u64cd\u4f5c\"\"\"\n    try:\n        import genesis\n\n        # \u521b\u5efa\u5f20\u91cf\n        x = genesis.randn(3, 4)\n        y = genesis.randn(3, 4)\n\n        # \u57fa\u7840\u8fd0\u7b97\n        z = x + y\n        print(\"\u2705 \u5f20\u91cf\u8fd0\u7b97\u6b63\u5e38\")\n        print(f\"   \u5f20\u91cf\u5f62\u72b6: {z.shape}\")\n    except Exception as e:\n        print(f\"\u274c \u5f20\u91cf\u8fd0\u7b97\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_neural_networks():\n    \"\"\"\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\"\"\"\n    try:\n        import genesis.nn as nn\n\n        # \u521b\u5efa\u7b80\u5355\u6a21\u578b\n        model = nn.Sequential(\n            nn.Linear(10, 5),\n            nn.ReLU(),\n            nn.Linear(5, 1)\n        )\n\n        # \u6d4b\u8bd5\u524d\u5411\u4f20\u64ad\n        x = genesis.randn(2, 10)\n        y = model(x)\n        print(\"\u2705 \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u6b63\u5e38\")\n        print(f\"   \u8f93\u51fa\u5f62\u72b6: {y.shape}\")\n    except Exception as e:\n        print(f\"\u274c \u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_cuda_support():\n    \"\"\"\u6d4b\u8bd5CUDA\u652f\u6301\"\"\"\n    try:\n        import genesis\n\n        if genesis.cuda.is_available():\n            device = genesis.device('cuda')\n            x = genesis.randn(10, 10, device=device)\n            print(\"\u2705 CUDA\u652f\u6301\u6b63\u5e38\")\n            print(f\"   GPU\u8bbe\u5907\u6570\u91cf: {genesis.cuda.device_count()}\")\n            print(f\"   GPU\u540d\u79f0: {genesis.cuda.get_device_name()}\")\n        else:\n            print(\"\u26a0\ufe0f  CUDA\u4e0d\u53ef\u7528 (\u5c06\u4f7f\u7528CPU)\")\n    except Exception as e:\n        print(f\"\u274c CUDA\u6d4b\u8bd5\u5931\u8d25: {e}\")\n        return False\n    return True\n\ndef test_autograd():\n    \"\"\"\u6d4b\u8bd5\u81ea\u52a8\u5fae\u5206\"\"\"\n    try:\n        import genesis\n\n        x = genesis.randn(5, requires_grad=True)\n        y = (x ** 2).sum()\n        y.backward()\n\n        print(\"\u2705 \u81ea\u52a8\u5fae\u5206\u6b63\u5e38\")\n        print(f\"   \u68af\u5ea6\u5f62\u72b6: {x.grad.shape}\")\n    except Exception as e:\n        print(f\"\u274c \u81ea\u52a8\u5fae\u5206\u5931\u8d25: {e}\")\n        return False\n    return True\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd0d Genesis\u5b89\u88c5\u9a8c\u8bc1\\n\")\n\n    tests = [\n        test_basic_import,\n        test_tensor_operations,\n        test_neural_networks,\n        test_cuda_support,\n        test_autograd\n    ]\n\n    passed = 0\n    total = len(tests)\n\n    for test in tests:\n        if test():\n            passed += 1\n        print()\n\n    print(f\"\ud83d\udcca \u6d4b\u8bd5\u7ed3\u679c: {passed}/{total} \u901a\u8fc7\")\n\n    if passed == total:\n        print(\"\ud83c\udf89 \u606d\u559c\uff01Genesis\u5b89\u88c5\u6210\u529f\uff0c\u6240\u6709\u529f\u80fd\u6b63\u5e38\uff01\")\n    else:\n        print(\"\u26a0\ufe0f  \u90e8\u5206\u529f\u80fd\u5f02\u5e38\uff0c\u8bf7\u68c0\u67e5\u5b89\u88c5\u6b65\u9aa4\")\n</code></pre> <p>\u5c06\u4e0a\u8ff0\u4ee3\u7801\u4fdd\u5b58\u4e3a <code>test_installation.py</code> \u5e76\u8fd0\u884c\uff1a</p> Bash<pre><code>python test_installation.py\n</code></pre>"},{"location":"getting-started/installation/#_10","title":"\ud83d\udd27 \u5e38\u89c1\u95ee\u9898\u89e3\u51b3","text":""},{"location":"getting-started/installation/#1cuda","title":"\u95ee\u98981\uff1aCUDA\u7248\u672c\u4e0d\u5339\u914d","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>RuntimeError: CUDA version mismatch\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u68c0\u67e5\u7cfb\u7edfCUDA\u7248\u672c\nnvidia-smi\n\n# \u68c0\u67e5PyTorch CUDA\u7248\u672c\npython -c \"import torch; print(torch.version.cuda)\"\n\n# \u91cd\u65b0\u5b89\u88c5\u5339\u914d\u7248\u672c\u7684PyTorch\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p>"},{"location":"getting-started/installation/#2triton","title":"\u95ee\u98982\uff1aTriton\u7f16\u8bd1\u5931\u8d25","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>Failed to compile Triton kernel\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u5347\u7ea7Triton\npip install --upgrade triton\n\n# \u6216\u5b89\u88c5\u5f00\u53d1\u7248\u672c\npip install --pre triton\n</code></pre></p>"},{"location":"getting-started/installation/#3","title":"\u95ee\u98983\uff1a\u5185\u5b58\u4e0d\u8db3","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>CUDA out of memory\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Python<pre><code>import genesis\n\n# \u542f\u7528\u5185\u5b58\u4f18\u5316\ngenesis.cuda.empty_cache()\n\n# \u51cf\u5c0f\u6279\u91cf\u5927\u5c0f\nbatch_size = 16  # \u66ff\u4ee3\u539f\u6765\u768432\n\n# \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9 (\u5982\u679c\u652f\u6301)\nmodel.gradient_checkpointing = True\n</code></pre></p>"},{"location":"getting-started/installation/#4","title":"\u95ee\u98984\uff1a\u5bfc\u5165\u9519\u8bef","text":"<p>\u9519\u8bef\u4fe1\u606f\uff1a Text Only<pre><code>ModuleNotFoundError: No module named 'genesis'\n</code></pre></p> <p>\u89e3\u51b3\u65b9\u6848\uff1a Bash<pre><code># \u68c0\u67e5\u865a\u62df\u73af\u5883\nwhich python\npip list | grep genesis\n\n# \u91cd\u65b0\u5b89\u88c5\npip uninstall genesis-dl\npip install -e genesis/\n</code></pre></p>"},{"location":"getting-started/installation/#docker","title":"\ud83d\udc33 Docker\u5b89\u88c5","text":"<p>\u5982\u679c\u4f60\u9047\u5230\u73af\u5883\u95ee\u9898\uff0c\u53ef\u4ee5\u4f7f\u7528Docker\uff1a</p> Bash<pre><code># \u4e0b\u8f7d\u9884\u6784\u5efa\u955c\u50cf\ndocker pull genesis/genesis:latest\n\n# \u6216\u6784\u5efa\u81ea\u5df1\u7684\u955c\u50cf\ngit clone https://github.com/phonism/genesis.git\ncd genesis\ndocker build -t genesis:local .\n\n# \u8fd0\u884c\u5bb9\u5668\ndocker run -it --gpus all genesis:local bash\n</code></pre> <p>Dockerfile\u5185\u5bb9\uff1a Docker<pre><code>FROM nvidia/cuda:11.8-devel-ubuntu22.04\n\n# \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nENV DEBIAN_FRONTEND=noninteractive\nENV PATH=\"/opt/conda/bin:$PATH\"\n\n# \u5b89\u88c5\u7cfb\u7edf\u4f9d\u8d56\nRUN apt-get update &amp;&amp; apt-get install -y \\\\\n    wget git build-essential &amp;&amp; \\\\\n    rm -rf /var/lib/apt/lists/*\n\n# \u5b89\u88c5Miniconda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \\\\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda &amp;&amp; \\\\\n    rm Miniconda3-latest-Linux-x86_64.sh\n\n# \u521b\u5efa\u73af\u5883\u5e76\u5b89\u88c5\u4f9d\u8d56\nRUN conda create -n genesis python=3.9 -y\nSHELL [\"conda\", \"run\", \"-n\", \"genesis\", \"/bin/bash\", \"-c\"]\n\nRUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 &amp;&amp; \\\\\n    pip install triton numpy matplotlib tqdm\n\n# \u590d\u5236\u5e76\u5b89\u88c5Genesis\nCOPY . /workspace/genesis\nWORKDIR /workspace/genesis\nRUN pip install -e genesis/\n\n# \u8bbe\u7f6e\u542f\u52a8\u547d\u4ee4\nENTRYPOINT [\"conda\", \"run\", \"-n\", \"genesis\"]\nCMD [\"bash\"]\n</code></pre></p>"},{"location":"getting-started/installation/#_11","title":"\ud83d\udcca \u6027\u80fd\u4f18\u5316\u5efa\u8bae","text":"<p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u4f18\u5316\u6027\u80fd\uff1a</p> Bash<pre><code># \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\nexport CUDA_VISIBLE_DEVICES=0  # \u6307\u5b9aGPU\nexport PYTHONPATH=$PWD:$PYTHONPATH\n\n# \u542f\u7528\u4f18\u5316\u9009\u9879\nexport GENESIS_OPTIMIZE=1\nexport TRITON_CACHE_DIR=/tmp/triton_cache\n</code></pre>"},{"location":"getting-started/installation/#_12","title":"\ud83c\udfaf \u4e0b\u4e00\u6b65","text":"<p>\u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u5efa\u8bae\uff1a</p> <ol> <li>\u8fd0\u884c\u7b2c\u4e00\u4e2a\u7a0b\u5e8f - \u9a8c\u8bc1\u5b89\u88c5\u5e76\u5b66\u4e60\u57fa\u7840\u7528\u6cd5</li> <li>\u67e5\u770b\u6559\u7a0b - \u7cfb\u7edf\u5b66\u4e60Genesis\u7684\u4f7f\u7528</li> <li>\u9605\u8bfb\u67b6\u6784\u6587\u6863 - \u7406\u89e3\u6846\u67b6\u8bbe\u8ba1\u7406\u5ff5</li> </ol> <p>\u5982\u679c\u5728\u5b89\u88c5\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\uff0c\u8bf7\u67e5\u770bFAQ\u6216\u5728GitHub\u4e0a\u63d0\u4ea4issue\u3002</p>"},{"location":"models/qwen/","title":"Qwen\u6a21\u578b\u5b9e\u73b0","text":""},{"location":"models/qwen/#_1","title":"\u6982\u8ff0","text":"<p>Genesis\u6846\u67b6\u5185\u7f6e\u4e86Qwen\uff08\u901a\u4e49\u5343\u95ee\uff09\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u73b0\uff0c\u652f\u6301\u5b8c\u6574\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6d41\u7a0b\u3002</p>"},{"location":"models/qwen/#_2","title":"\u6a21\u578b\u67b6\u6784","text":"<p>Qwen\u6a21\u578b\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</p> <ul> <li>\u6ce8\u610f\u529b\u673a\u5236: Multi-Head Attention with RoPE (Rotary Position Embedding)</li> <li>\u6fc0\u6d3b\u51fd\u6570: SwiGLU activation</li> <li>\u5c42\u5f52\u4e00\u5316: RMSNorm</li> <li>\u4f4d\u7f6e\u7f16\u7801: Rotary Position Embedding (RoPE)</li> </ul>"},{"location":"models/qwen/#_3","title":"\u5feb\u901f\u4f7f\u7528","text":""},{"location":"models/qwen/#_4","title":"\u57fa\u7840\u63a8\u7406","text":"Python<pre><code>import genesis\nfrom genesis.models.qwen import QwenModel, QwenConfig\n\n# \u521b\u5efa\u6a21\u578b\u914d\u7f6e\nconfig = QwenConfig(\n    vocab_size=32000,\n    n_layer=24,\n    n_head=16,\n    n_embd=2048,\n    max_seq_len=2048\n)\n\n# \u521b\u5efa\u6a21\u578b\nmodel = QwenModel(config)\n\n# \u63a8\u7406\ninput_ids = genesis.tensor([[1, 2, 3, 4, 5]])  # [batch_size, seq_len]\noutput = model(input_ids)\nprint(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")  # [1, 5, 32000]\n</code></pre>"},{"location":"models/qwen/#_5","title":"\u8bad\u7ec3\u793a\u4f8b","text":"Python<pre><code>import genesis.optim as optim\nimport genesis.nn as nn\n\n# \u521b\u5efa\u4f18\u5316\u5668\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n\n# \u8bad\u7ec3\u5faa\u73af\nfor batch in dataloader:\n    input_ids, labels = batch\n\n    # \u524d\u5411\u4f20\u64ad\n    logits = model(input_ids)\n\n    # \u8ba1\u7b97\u635f\u5931\n    loss = nn.functional.cross_entropy(\n        logits.view(-1, logits.size(-1)),\n        labels.view(-1)\n    )\n\n    # \u53cd\u5411\u4f20\u64ad\n    optimizer.zero_grad()\n    loss.backward()\n\n    # \u68af\u5ea6\u88c1\u526a\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # \u53c2\u6570\u66f4\u65b0\n    optimizer.step()\n</code></pre>"},{"location":"models/qwen/#_6","title":"\u914d\u7f6e\u53c2\u6570","text":""},{"location":"models/qwen/#qwenconfig","title":"QwenConfig","text":"\u53c2\u6570\u540d \u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>vocab_size</code> int 32000 \u8bcd\u6c47\u8868\u5927\u5c0f <code>n_layer</code> int 24 Transformer\u5c42\u6570 <code>n_head</code> int 16 \u6ce8\u610f\u529b\u5934\u6570 <code>n_embd</code> int 2048 \u9690\u85cf\u5c42\u7ef4\u5ea6 <code>max_seq_len</code> int 2048 \u6700\u5927\u5e8f\u5217\u957f\u5ea6 <code>dropout</code> float 0.1 Dropout\u6982\u7387 <code>bias</code> bool False \u662f\u5426\u4f7f\u7528\u504f\u7f6e"},{"location":"models/qwen/#_7","title":"\u6027\u80fd\u4f18\u5316","text":""},{"location":"models/qwen/#_8","title":"\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3","text":"Python<pre><code># \u542f\u7528\u6df7\u5408\u7cbe\u5ea6\ngenesis.enable_autocast = True\n\nwith genesis.autocast():\n    logits = model(input_ids)\n    loss = criterion(logits, labels)\n\n# \u68af\u5ea6\u7f29\u653e\nscaler = genesis.GradScaler()\nscaler.scale(loss).backward()\nscaler.step(optimizer)\nscaler.update()\n</code></pre>"},{"location":"models/qwen/#_9","title":"\u68af\u5ea6\u68c0\u67e5\u70b9","text":"Python<pre><code># \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\u8282\u7701\u663e\u5b58\nmodel.gradient_checkpointing = True\n</code></pre>"},{"location":"models/qwen/#_10","title":"\u5e94\u7528\u793a\u4f8b","text":""},{"location":"models/qwen/#_11","title":"\u6587\u672c\u751f\u6210","text":"Python<pre><code>def generate_text(model, tokenizer, prompt, max_length=100):\n    input_ids = tokenizer.encode(prompt)\n    input_tensor = genesis.tensor([input_ids])\n\n    with genesis.no_grad():\n        for _ in range(max_length):\n            logits = model(input_tensor)\n            next_token = logits[0, -1].argmax()\n            input_tensor = genesis.cat([input_tensor, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n    return tokenizer.decode(input_tensor[0].tolist())\n\n# \u4f7f\u7528\u793a\u4f8b\ngenerated = generate_text(model, tokenizer, \"\u4eca\u5929\u5929\u6c14\")\nprint(generated)\n</code></pre>"},{"location":"models/qwen/#_12","title":"\u5fae\u8c03\u8bad\u7ec3","text":"<p>\u53c2\u8003 <code>apps/llm/train_sft_qwen.py</code> \u4e86\u89e3\u5b8c\u6574\u7684SFT (Supervised Fine-tuning) \u5b9e\u73b0\u3002</p>"},{"location":"models/qwen/#_13","title":"\u6587\u4ef6\u7ed3\u6784","text":"<ul> <li><code>genesis/models/qwen.py</code> - \u6a21\u578b\u5b9e\u73b0</li> <li><code>apps/llm/qwen_model.py</code> - \u8bad\u7ec3\u914d\u7f6e\u548c\u5de5\u5177</li> <li><code>apps/llm/train_sft_qwen.py</code> - SFT\u8bad\u7ec3\u811a\u672c</li> <li><code>apps/llm/chat_qwen.py</code> - \u63a8\u7406\u804a\u5929\u811a\u672c</li> </ul>"},{"location":"models/qwen/#_14","title":"\u76f8\u5173\u8d44\u6e90","text":"<ul> <li>Qwen\u5b98\u65b9\u8bba\u6587</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u8be6\u89e3</li> <li>\u5927\u6a21\u578b\u8bad\u7ec3\u6700\u4f73\u5b9e\u8df5</li> </ul>"},{"location":"tutorials/","title":"\u6559\u7a0b\u603b\u89c8","text":"<p>\u6b22\u8fce\u6765\u5230Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6559\u7a0b\u7cfb\u5217\uff01\u8fd9\u4e9b\u6559\u7a0b\u5c06\u5e2e\u52a9\u4f60\u4ece\u5165\u95e8\u5230\u7cbe\u901aGenesis\u6846\u67b6\u7684\u5404\u4e2a\u65b9\u9762\u3002</p>"},{"location":"tutorials/#_2","title":"\ud83d\udcda \u6559\u7a0b\u5206\u7c7b","text":""},{"location":"tutorials/#_3","title":"\ud83c\udfaf \u57fa\u7840\u6559\u7a0b","text":"<p>\u9002\u5408\u521d\u5b66\u8005\uff0c\u6db5\u76d6Genesis\u7684\u57fa\u672c\u6982\u5ff5\u548c\u7528\u6cd5\u3002</p> <ul> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u5b66\u4e60\u5982\u4f55\u4f7f\u7528Genesis\u8bad\u7ec3\u4f60\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc</li> <li>\u6570\u636e\u5904\u7406\u6559\u7a0b - \u6570\u636e\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6570\u636e\u7ba1\u9053\u6784\u5efa</li> <li>\u6a21\u578b\u5b9a\u4e49\u6559\u7a0b - \u5982\u4f55\u5b9a\u4e49\u548c\u7ec4\u7ec7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b</li> </ul>"},{"location":"tutorials/#_4","title":"\ud83d\ude80 \u8fdb\u9636\u6559\u7a0b","text":"<p>\u6df1\u5165\u4e86\u89e3Genesis\u7684\u9ad8\u7ea7\u7279\u6027\u548c\u4f18\u5316\u6280\u5de7\u3002</p> <ul> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u4f7f\u7528AMP\u52a0\u901f\u8bad\u7ec3\u5e76\u8282\u7701\u663e\u5b58</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u81ea\u5b9a\u4e49\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u8bad\u7ec3\u6027\u80fd\u548c\u5185\u5b58\u4f7f\u7528</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3\u5b9e\u73b0</li> </ul>"},{"location":"tutorials/#_5","title":"\ud83d\udee0\ufe0f \u5b9e\u6218\u9879\u76ee","text":"<p>\u901a\u8fc7\u5b8c\u6574\u7684\u9879\u76ee\u5b66\u4e60Genesis\u7684\u5b9e\u9645\u5e94\u7528\u3002</p> <ul> <li>Qwen\u5927\u6a21\u578b\u8bad\u7ec3 - \u4f7f\u7528Genesis\u8bad\u7ec3Qwen\u8bed\u8a00\u6a21\u578b</li> <li>\u56fe\u50cf\u5206\u7c7b\u9879\u76ee - \u6784\u5efa\u5b8c\u6574\u7684\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf</li> <li>\u8bed\u8a00\u6a21\u578b\u5fae\u8c03 - \u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5b9e\u6218</li> </ul>"},{"location":"tutorials/#_6","title":"\ud83c\udf93 \u5b66\u4e60\u8def\u5f84","text":""},{"location":"tutorials/#1-2","title":"\u521d\u5b66\u8005\u8def\u5f84 (1-2\u5468)","text":"<ol> <li>\u5b89\u88c5\u548c\u73af\u5883\u914d\u7f6e</li> <li>\u7b2c\u4e00\u4e2a\u7a0b\u5e8f </li> <li>\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b</li> <li>\u6570\u636e\u5904\u7406\u6559\u7a0b</li> </ol>"},{"location":"tutorials/#2-4","title":"\u8fdb\u9636\u7528\u6237\u8def\u5f84 (2-4\u5468)","text":"<ol> <li>\u5b8c\u6210\u521d\u5b66\u8005\u8def\u5f84</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 </li> <li>\u6027\u80fd\u8c03\u4f18</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3</li> </ol>"},{"location":"tutorials/#4-8","title":"\u7814\u7a76\u8005\u8def\u5f84 (4-8\u5468)","text":"<ol> <li>\u5b8c\u6210\u8fdb\u9636\u7528\u6237\u8def\u5f84</li> <li>\u67b6\u6784\u6df1\u5165\u7406\u89e3</li> <li>\u6838\u5fc3\u7ec4\u4ef6\u6e90\u7801\u5206\u6790</li> <li>Qwen\u5927\u6a21\u578b\u8bad\u7ec3</li> <li>\u8d21\u732e\u4ee3\u7801</li> </ol>"},{"location":"tutorials/#_7","title":"\ud83d\udca1 \u6559\u7a0b\u7279\u8272","text":"<ul> <li>\ud83c\udfaf \u5b9e\u6218\u5bfc\u5411 - \u6bcf\u4e2a\u6559\u7a0b\u90fd\u5305\u542b\u5b8c\u6574\u7684\u53ef\u8fd0\u884c\u4ee3\u7801</li> <li>\ud83d\udcca \u6027\u80fd\u5bf9\u6bd4 - \u4e0e\u5176\u4ed6\u6846\u67b6\u7684\u6027\u80fd\u5bf9\u6bd4\u548c\u5206\u6790</li> <li>\ud83d\udd0d \u6e90\u7801\u89e3\u6790 - \u6df1\u5165\u7406\u89e3Genesis\u5185\u90e8\u5b9e\u73b0\u539f\u7406</li> <li>\u26a1 \u6700\u4f73\u5b9e\u8df5 - \u603b\u7ed3\u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u7ecf\u9a8c\u548c\u6280\u5de7</li> </ul>"},{"location":"tutorials/#_8","title":"\ud83e\udd1d \u8d21\u732e\u6559\u7a0b","text":"<p>\u6211\u4eec\u6b22\u8fce\u793e\u533a\u8d21\u732e\u66f4\u591a\u9ad8\u8d28\u91cf\u7684\u6559\u7a0b\uff01</p>"},{"location":"tutorials/#_9","title":"\u5982\u4f55\u8d21\u732e","text":"<ol> <li>Fork\u9879\u76ee\u5230\u4f60\u7684GitHub\u8d26\u6237</li> <li>\u5728<code>docs/tutorials/</code>\u76ee\u5f55\u4e0b\u521b\u5efa\u65b0\u7684Markdown\u6587\u4ef6</li> <li>\u6309\u7167\u73b0\u6709\u6559\u7a0b\u7684\u683c\u5f0f\u7f16\u5199\u5185\u5bb9</li> <li>\u63d0\u4ea4Pull Request</li> </ol>"},{"location":"tutorials/#_10","title":"\u6559\u7a0b\u6807\u51c6","text":"<ul> <li>\u6e05\u6670\u7684\u6807\u9898\u548c\u7ed3\u6784 - \u4f7f\u7528\u9002\u5f53\u7684\u6807\u9898\u5c42\u7ea7</li> <li>\u5b8c\u6574\u7684\u4ee3\u7801\u793a\u4f8b - \u786e\u4fdd\u4ee3\u7801\u53ef\u4ee5\u76f4\u63a5\u8fd0\u884c</li> <li>\u8be6\u7ec6\u7684\u89e3\u91ca - \u89e3\u91ca\u6bcf\u4e2a\u6b65\u9aa4\u7684\u539f\u7406\u548c\u76ee\u7684</li> <li>\u5b9e\u9645\u7684\u5e94\u7528\u573a\u666f - \u7ed3\u5408\u771f\u5b9e\u7684\u4f7f\u7528\u6848\u4f8b</li> </ul>"},{"location":"tutorials/#_11","title":"\ud83d\udcde \u83b7\u53d6\u5e2e\u52a9","text":"<p>\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9047\u5230\u95ee\u9898\uff1f</p> <ul> <li>GitHub Issues - \u62a5\u544a\u95ee\u9898\u6216\u5efa\u8bae\u6539\u8fdb</li> <li>Discussions - \u4e0e\u793e\u533a\u8ba8\u8bba\u6280\u672f\u95ee\u9898</li> <li>API\u6587\u6863 - \u67e5\u770b\u8be6\u7ec6\u7684API\u53c2\u8003\u6587\u6863</li> </ul> <p>\u5b66\u4e60\u5efa\u8bae</p> <p>\u5efa\u8bae\u6309\u7167\u63a8\u8350\u7684\u5b66\u4e60\u8def\u5f84\u5faa\u5e8f\u6e10\u8fdb\uff0c\u6bcf\u5b8c\u6210\u4e00\u4e2a\u6559\u7a0b\u540e\u5b9e\u9645\u52a8\u624b\u7ec3\u4e60\uff0c\u52a0\u6df1\u7406\u89e3\u3002</p> <p>\u51c6\u5907\u5f00\u59cb\u5b66\u4e60\u4e86\u5417\uff1f</p> <p>\u5f00\u59cb\u57fa\u7840\u6559\u7a0b \u67e5\u770bAPI\u6587\u6863</p>"},{"location":"tutorials/basic-training/","title":"\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u5e26\u4f60\u4ece\u96f6\u5f00\u59cb\uff0c\u4f7f\u7528Genesis\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6784\u5efa\u548c\u8bad\u7ec3\u4f60\u7684\u7b2c\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u5c06\u901a\u8fc7\u4e00\u4e2a\u5b8c\u6574\u7684\u56fe\u50cf\u5206\u7c7b\u9879\u76ee\u6765\u5b66\u4e60Genesis\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u7528\u6cd5\u3002</p>"},{"location":"tutorials/basic-training/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u5b66\u4f1a\uff1a - Genesis\u7684\u57fa\u672cAPI\u548c\u6570\u636e\u7ed3\u6784 - \u5982\u4f55\u5b9a\u4e49\u548c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b - \u6570\u636e\u52a0\u8f7d\u548c\u9884\u5904\u7406 - \u8bad\u7ec3\u5faa\u73af\u7684\u6784\u5efa\u548c\u4f18\u5316 - \u6a21\u578b\u8bc4\u4f30\u548c\u4fdd\u5b58</p>"},{"location":"tutorials/basic-training/#_3","title":"\ud83d\udee0\ufe0f \u73af\u5883\u51c6\u5907","text":""},{"location":"tutorials/basic-training/#_4","title":"\u5b89\u88c5\u4f9d\u8d56","text":"Bash<pre><code># \u786e\u4fdd\u5df2\u5b89\u88c5Genesis\npip install torch triton\ngit clone https://github.com/phonism/genesis.git\ncd genesis\npip install -e .\n\n# \u5b89\u88c5\u989d\u5916\u4f9d\u8d56\npip install matplotlib torchvision tqdm\n</code></pre>"},{"location":"tutorials/basic-training/#_5","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\n\nprint(f\"Genesis\u7248\u672c: {genesis.__version__}\")\nprint(f\"CUDA\u53ef\u7528: {genesis.cuda.is_available()}\")\n</code></pre>"},{"location":"tutorials/basic-training/#_6","title":"\ud83d\udcca \u9879\u76ee\uff1a\u624b\u5199\u6570\u5b57\u8bc6\u522b","text":"<p>\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u624b\u5199\u6570\u5b57\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528\u7ecf\u5178\u7684MNIST\u6570\u636e\u96c6\u3002</p>"},{"location":"tutorials/basic-training/#1","title":"1. \u6570\u636e\u51c6\u5907","text":"Python<pre><code>import genesis\nimport genesis.nn as nn\nimport genesis.optim as optim\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# \u6570\u636e\u9884\u5904\u7406\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n# \u52a0\u8f7dMNIST\u6570\u636e\u96c6\ntrain_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('data', train=False, transform=transform)\n\n# \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\nbatch_size = 64\ntrain_loader = genesis.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = genesis.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\u8bad\u7ec3\u96c6\u5927\u5c0f: {len(train_dataset)}\")\nprint(f\"\u6d4b\u8bd5\u96c6\u5927\u5c0f: {len(test_dataset)}\")\n</code></pre>"},{"location":"tutorials/basic-training/#2","title":"2. \u6a21\u578b\u5b9a\u4e49","text":"<p>\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u4f46\u6709\u6548\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff1a</p> Python<pre><code>class MNISTNet(nn.Module):\n    \"\"\"MNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b\u7f51\u7edc\"\"\"\n\n    def __init__(self, num_classes=10):\n        super(MNISTNet, self).__init__()\n\n        # \u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # \u5168\u8fde\u63a5\u5c42\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n        # \u6fc0\u6d3b\u51fd\u6570\u548cDropout\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        # \u5377\u79ef\u57571\n        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -&gt; 14x14\n\n        # \u5377\u79ef\u57572  \n        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -&gt; 7x7\n\n        # \u5c55\u5e73\n        x = x.view(x.size(0), -1)  # [batch_size, 64*7*7]\n\n        # \u5168\u8fde\u63a5\u5c42\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x\n\n# \u521b\u5efa\u6a21\u578b\u5b9e\u4f8b\ndevice = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')\nmodel = MNISTNet().to(device)\n\nprint(\"\u6a21\u578b\u7ed3\u6784:\")\nprint(model)\nprint(f\"\\\\n\u53c2\u6570\u603b\u6570: {sum(p.numel() for p in model.parameters()):,}\")\n</code></pre>"},{"location":"tutorials/basic-training/#3","title":"3. \u8bad\u7ec3\u914d\u7f6e","text":"Python<pre><code># \u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n\n# \u8bad\u7ec3\u53c2\u6570\nnum_epochs = 10\nprint_every = 100  # \u6bcf100\u4e2abatch\u6253\u5370\u4e00\u6b21\n\nprint(f\"\u8bbe\u5907: {device}\")\nprint(f\"\u6279\u91cf\u5927\u5c0f: {batch_size}\")\nprint(f\"\u8bad\u7ec3\u8f6e\u6570: {num_epochs}\")\nprint(f\"\u5b66\u4e60\u7387: {optimizer.param_groups[0]['lr']}\")\n</code></pre>"},{"location":"tutorials/basic-training/#4","title":"4. \u8bad\u7ec3\u5faa\u73af","text":"Python<pre><code>def train_epoch(model, train_loader, criterion, optimizer, epoch):\n    \"\"\"\u8bad\u7ec3\u4e00\u4e2aepoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # \u6570\u636e\u79fb\u5230\u8bbe\u5907\n        data, target = data.to(device), target.to(device)\n\n        # \u524d\u5411\u4f20\u64ad\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n\n        # \u7edf\u8ba1\n        running_loss += loss.item()\n        _, predicted = genesis.max(output, dim=1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n        # \u6253\u5370\u8fdb\u5ea6\n        if batch_idx % print_every == 0:\n            print(f'Epoch {epoch+1}/{num_epochs}, '\n                  f'Batch {batch_idx}/{len(train_loader)}, '\n                  f'Loss: {loss.item():.4f}, '\n                  f'Acc: {100*correct/total:.2f}%')\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100 * correct / total\n\n    return epoch_loss, epoch_acc\n\ndef validate(model, test_loader, criterion):\n    \"\"\"\u9a8c\u8bc1\u6a21\u578b\"\"\"\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n\n    with genesis.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            test_loss += criterion(output, target).item()\n            _, predicted = genesis.max(output, dim=1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n    avg_loss = test_loss / len(test_loader)\n    accuracy = 100 * correct / total\n\n    return avg_loss, accuracy\n\n# \u5f00\u59cb\u8bad\u7ec3\nprint(\"\u5f00\u59cb\u8bad\u7ec3...\")\ntrain_losses, train_accs = [], []\nval_losses, val_accs = [], []\n\nfor epoch in range(num_epochs):\n    # \u8bad\u7ec3\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, epoch)\n\n    # \u9a8c\u8bc1\n    val_loss, val_acc = validate(model, test_loader, criterion)\n\n    # \u5b66\u4e60\u7387\u8c03\u5ea6\n    scheduler.step()\n\n    # \u8bb0\u5f55\u7ed3\u679c\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n    print(f\"  \u8bad\u7ec3 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n    print(f\"  \u9a8c\u8bc1 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n    print(f\"  \u5b66\u4e60\u7387: {optimizer.param_groups[0]['lr']:.6f}\")\n    print(\"-\" * 50)\n\nprint(\"\u8bad\u7ec3\u5b8c\u6210\uff01\")\n</code></pre>"},{"location":"tutorials/basic-training/#5","title":"5. \u7ed3\u679c\u53ef\u89c6\u5316","text":"Python<pre><code># \u7ed8\u5236\u8bad\u7ec3\u66f2\u7ebf\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# \u635f\u5931\u66f2\u7ebf\nax1.plot(train_losses, label='\u8bad\u7ec3\u635f\u5931', color='blue')\nax1.plot(val_losses, label='\u9a8c\u8bc1\u635f\u5931', color='red')\nax1.set_title('\u635f\u5931\u66f2\u7ebf')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True)\n\n# \u51c6\u786e\u7387\u66f2\u7ebf\nax2.plot(train_accs, label='\u8bad\u7ec3\u51c6\u786e\u7387', color='blue')\nax2.plot(val_accs, label='\u9a8c\u8bc1\u51c6\u786e\u7387', color='red')\nax2.set_title('\u51c6\u786e\u7387\u66f2\u7ebf')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Accuracy (%)')\nax2.legend()\nax2.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u6700\u7ec8\u6d4b\u8bd5\u51c6\u786e\u7387: {val_accs[-1]:.2f}%\")\n</code></pre>"},{"location":"tutorials/basic-training/#6","title":"6. \u6a21\u578b\u4fdd\u5b58\u548c\u52a0\u8f7d","text":"Python<pre><code># \u4fdd\u5b58\u6a21\u578b\nmodel_path = 'mnist_model.pth'\ngenesis.save_checkpoint({\n    'epoch': num_epochs,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'train_loss': train_losses[-1],\n    'val_loss': val_losses[-1],\n    'val_acc': val_accs[-1]\n}, model_path)\n\nprint(f\"\u6a21\u578b\u5df2\u4fdd\u5b58\u5230: {model_path}\")\n\n# \u52a0\u8f7d\u6a21\u578b\ndef load_model(model_path, model_class, num_classes=10):\n    \"\"\"\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\"\"\"\n    checkpoint = genesis.load_checkpoint(model_path)\n\n    model = model_class(num_classes)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n\n    print(f\"\u6a21\u578b\u52a0\u8f7d\u6210\u529f\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387: {checkpoint['val_acc']:.2f}%\")\n    return model\n\n# \u6d4b\u8bd5\u52a0\u8f7d\nloaded_model = load_model(model_path, MNISTNet)\n</code></pre>"},{"location":"tutorials/basic-training/#7","title":"7. \u5355\u5f20\u56fe\u7247\u9884\u6d4b","text":"Python<pre><code>def predict_single_image(model, image, class_names=None):\n    \"\"\"\u5bf9\u5355\u5f20\u56fe\u7247\u8fdb\u884c\u9884\u6d4b\"\"\"\n    model.eval()\n\n    if class_names is None:\n        class_names = [str(i) for i in range(10)]\n\n    with genesis.no_grad():\n        if image.dim() == 3:  # \u6dfb\u52a0batch\u7ef4\u5ea6\n            image = image.unsqueeze(0)\n\n        image = image.to(device)\n        output = model(image)\n        probabilities = genesis.softmax(output, dim=1)\n\n        confidence, predicted = genesis.max(probabilities, dim=1)\n\n    return predicted.item(), confidence.item()\n\n# \u6d4b\u8bd5\u9884\u6d4b\ntest_iter = iter(test_loader)\nimages, labels = next(test_iter)\n\n# \u9884\u6d4b\u524d5\u5f20\u56fe\u7247\nfig, axes = plt.subplots(1, 5, figsize=(15, 3))\nfor i in range(5):\n    image = images[i]\n    true_label = labels[i].item()\n\n    predicted, confidence = predict_single_image(model, image)\n\n    # \u663e\u793a\u56fe\u7247\n    axes[i].imshow(image.squeeze(), cmap='gray')\n    axes[i].set_title(f'\u771f\u5b9e: {true_label}\\\\n\u9884\u6d4b: {predicted}\\\\n\u7f6e\u4fe1\u5ea6: {confidence:.3f}')\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/basic-training/#_7","title":"\ud83d\udcc8 \u6027\u80fd\u5bf9\u6bd4","text":"<p>\u8ba9\u6211\u4eec\u6bd4\u8f83Genesis\u4e0ePyTorch\u7684\u6027\u80fd\uff1a</p> Python<pre><code>import time\n\ndef benchmark_training(model, train_loader, criterion, optimizer, device, num_batches=100):\n    \"\"\"\u8bad\u7ec3\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\"\"\"\n    model.train()\n    start_time = time.time()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if batch_idx &gt;= num_batches:\n            break\n\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n    elapsed_time = time.time() - start_time\n    return elapsed_time\n\n# \u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\nprint(\"\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5 (100\u4e2abatch):\")\ngenesis_time = benchmark_training(model, train_loader, criterion, optimizer, device)\nprint(f\"Genesis\u8bad\u7ec3\u65f6\u95f4: {genesis_time:.2f} \u79d2\")\nprint(f\"\u5e73\u5747\u6bcf\u4e2abatch: {genesis_time/100*1000:.1f} ms\")\n</code></pre>"},{"location":"tutorials/basic-training/#_8","title":"\ud83c\udfaf \u5173\u952e\u6982\u5ff5\u603b\u7ed3","text":""},{"location":"tutorials/basic-training/#1_1","title":"1. \u5f20\u91cf\u64cd\u4f5c","text":"Python<pre><code># \u521b\u5efa\u5f20\u91cf\nx = genesis.randn(3, 4, requires_grad=True)\ny = genesis.ones(3, 4)\n\n# \u57fa\u7840\u8fd0\u7b97\nz = x + y\nw = genesis.matmul(x, y.T)\n\n# \u68af\u5ea6\u8ba1\u7b97\nz.sum().backward()\nprint(x.grad)  # x\u7684\u68af\u5ea6\n</code></pre>"},{"location":"tutorials/basic-training/#2_1","title":"2. \u6a21\u578b\u5b9a\u4e49\u6700\u4f73\u5b9e\u8df5","text":"Python<pre><code>class BestPracticeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # \u4f7f\u7528nn.Sequential\u7b80\u5316\u5b9a\u4e49\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(64 * 7 * 7, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.classifier(x)\n</code></pre>"},{"location":"tutorials/basic-training/#3_1","title":"3. \u8bad\u7ec3\u6280\u5de7","text":"Python<pre><code># \u68af\u5ea6\u88c1\u526a\ngenesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n# \u6743\u91cd\u521d\u59cb\u5316\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        genesis.nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            genesis.nn.init.zeros_(m.bias)\n\nmodel.apply(init_weights)\n</code></pre>"},{"location":"tutorials/basic-training/#_9","title":"\ud83d\ude80 \u4e0b\u4e00\u6b65","text":"<p>\u606d\u559c\uff01\u4f60\u5df2\u7ecf\u5b8c\u6210\u4e86\u7b2c\u4e00\u4e2aGenesis\u8bad\u7ec3\u9879\u76ee\u3002\u63a5\u4e0b\u6765\u53ef\u4ee5\u63a2\u7d22\uff1a</p> <ol> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3 - \u52a0\u901f\u8bad\u7ec3\u5e76\u8282\u7701\u663e\u5b58</li> <li>\u81ea\u5b9a\u4e49\u7b97\u5b50 - \u5b9e\u73b0\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c</li> <li>\u6027\u80fd\u8c03\u4f18 - \u4f18\u5316\u8bad\u7ec3\u6027\u80fd</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3 - \u591aGPU\u5e76\u884c\u8bad\u7ec3</li> </ol>"},{"location":"tutorials/basic-training/#_10","title":"\u2753 \u5e38\u89c1\u95ee\u9898","text":"<p>Q: \u8bad\u7ec3\u901f\u5ea6\u6bd4\u9884\u671f\u6162\uff1f A: \u68c0\u67e5\u662f\u5426\u542f\u7528\u4e86CUDA\uff0c\u786e\u4fdd\u6570\u636e\u9884\u5904\u7406\u4e0d\u662f\u74f6\u9888\uff0c\u8003\u8651\u8c03\u6574batch_size\u3002</p> <p>Q: \u5185\u5b58\u4e0d\u8db3\u9519\u8bef\uff1f A: \u51cf\u5c0fbatch_size\uff0c\u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\uff0c\u6216\u4f7f\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u3002</p> <p>Q: \u6a21\u578b\u4e0d\u6536\u655b\uff1f A: \u68c0\u67e5\u5b66\u4e60\u7387\u8bbe\u7f6e\uff0c\u786e\u8ba4\u6570\u636e\u9884\u5904\u7406\u6b63\u786e\uff0c\u5c1d\u8bd5\u4e0d\u540c\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u3002</p> <p>\u5b8c\u6210\u4e86\u57fa\u7840\u6559\u7a0b\uff01</p> <p>\u4f60\u73b0\u5728\u5df2\u7ecf\u638c\u63e1\u4e86Genesis\u7684\u6838\u5fc3\u6982\u5ff5\u3002\u7ee7\u7eed\u63a2\u7d22\u66f4\u9ad8\u7ea7\u7684\u7279\u6027\u5427\uff01</p> <p>\u4e0b\u4e00\u6559\u7a0b\uff1a\u81ea\u5b9a\u4e49\u7b97\u5b50 \u8fd4\u56de\u6559\u7a0b\u76ee\u5f55</p>"},{"location":"tutorials/custom-ops/","title":"\u81ea\u5b9a\u4e49\u7b97\u5b50\u5f00\u53d1","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>Genesis\u6846\u67b6\u652f\u6301\u81ea\u5b9a\u4e49\u7b97\u5b50\u5f00\u53d1\uff0c\u8ba9\u4f60\u53ef\u4ee5\u5b9e\u73b0\u4e13\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u3002\u672c\u6559\u7a0b\u5c06\u6559\u4f60\u5982\u4f55\u4ece\u96f6\u5f00\u59cb\u521b\u5efa\u9ad8\u6027\u80fd\u7684\u81ea\u5b9a\u4e49\u7b97\u5b50\u3002</p>"},{"location":"tutorials/custom-ops/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\u7406\u89e3Genesis\u7684\u7b97\u5b50\u7cfb\u7edf\u67b6\u6784</li> <li>\u5b66\u4f1a\u5b9e\u73b0CPU\u548cGPU\u7248\u672c\u7684\u81ea\u5b9a\u4e49\u7b97\u5b50</li> <li>\u638c\u63e1Triton kernel\u7f16\u7a0b\u6280\u5de7</li> <li>\u4e86\u89e3\u7b97\u5b50\u4f18\u5316\u548c\u6027\u80fd\u8c03\u8bd5\u65b9\u6cd5</li> </ul>"},{"location":"tutorials/custom-ops/#_3","title":"\ud83d\udccb \u9884\u5907\u77e5\u8bc6","text":"<p>\u5728\u5f00\u59cb\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\uff1a - \u5b8c\u6210\u57fa\u7840\u8bad\u7ec3\u6559\u7a0b - \u4e86\u89e3CUDA\u7f16\u7a0b\u57fa\u7840 - \u719f\u6089Python C\u6269\u5c55\u5f00\u53d1</p>"},{"location":"tutorials/custom-ops/#_4","title":"\ud83d\udee0\ufe0f \u5f00\u53d1\u73af\u5883","text":"Bash<pre><code># \u5b89\u88c5\u5f00\u53d1\u4f9d\u8d56\npip install triton pybind11 cmake ninja\n</code></pre>"},{"location":"tutorials/custom-ops/#rmsnorm","title":"\ud83d\udcdd \u793a\u4f8b\uff1aRMSNorm\u7b97\u5b50","text":"<p>\u6211\u4eec\u5c06\u5b9e\u73b0RMSNorm\uff08Root Mean Square Normalization\uff09\u4f5c\u4e3a\u793a\u4f8b\u3002</p>"},{"location":"tutorials/custom-ops/#cpu","title":"CPU\u5b9e\u73b0","text":"Python<pre><code># WIP: CPU\u5b9e\u73b0\u4ee3\u7801\u5c06\u5728\u540e\u7eed\u7248\u672c\u4e2d\u6dfb\u52a0\n</code></pre>"},{"location":"tutorials/custom-ops/#gpu-triton","title":"GPU\u5b9e\u73b0 (Triton)","text":"Python<pre><code># WIP: Triton\u5b9e\u73b0\u4ee3\u7801\u5c06\u5728\u540e\u7eed\u7248\u672c\u4e2d\u6dfb\u52a0\n</code></pre>"},{"location":"tutorials/custom-ops/#_5","title":"\ud83d\ude80 \u9ad8\u7ea7\u7279\u6027","text":"<ul> <li>\u81ea\u52a8\u5fae\u5206\u652f\u6301</li> <li>\u5185\u5b58\u4f18\u5316\u6280\u5de7</li> <li>\u7b97\u5b50\u878d\u5408\u7b56\u7565</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"},{"location":"tutorials/performance-tuning/","title":"\u6027\u80fd\u8c03\u4f18\u6307\u5357","text":"<p>\u5f00\u53d1\u4e2d</p> <p>\u6b64\u6587\u6863\u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u5185\u5bb9\u5c06\u6301\u7eed\u66f4\u65b0\u3002</p> <p>\u672c\u6307\u5357\u5c06\u6559\u4f60\u5982\u4f55\u4f18\u5316Genesis\u6a21\u578b\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5305\u62ec\u5185\u5b58\u4f7f\u7528\u3001\u8ba1\u7b97\u6548\u7387\u548cI/O\u4f18\u5316\u7b49\u65b9\u9762\u3002</p>"},{"location":"tutorials/performance-tuning/#_2","title":"\ud83c\udfaf \u4f18\u5316\u76ee\u6807","text":"<ul> <li>\u8bad\u7ec3\u901f\u5ea6: \u63d0\u9ad8\u6bcf\u79d2\u5904\u7406\u7684\u6837\u672c\u6570</li> <li>\u5185\u5b58\u6548\u7387: \u51cf\u5c11GPU\u663e\u5b58\u5360\u7528</li> <li>\u541e\u5410\u91cf: \u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387</li> </ul>"},{"location":"tutorials/performance-tuning/#_3","title":"\ud83d\udcca \u6027\u80fd\u5206\u6790\u5de5\u5177","text":""},{"location":"tutorials/performance-tuning/#_4","title":"\u5185\u7f6e\u6027\u80fd\u5206\u6790\u5668","text":"Python<pre><code>import genesis.utils.profile as profiler\n\n# WIP: \u6027\u80fd\u5206\u6790\u4ee3\u7801\u793a\u4f8b\nwith profiler.profile() as prof:\n    # \u8bad\u7ec3\u4ee3\u7801\n    pass\n\nprof.print_stats()\n</code></pre>"},{"location":"tutorials/performance-tuning/#_5","title":"\u26a1 \u4f18\u5316\u7b56\u7565","text":""},{"location":"tutorials/performance-tuning/#1","title":"1. \u5185\u5b58\u4f18\u5316","text":"<ul> <li>\u68af\u5ea6\u7d2f\u79ef</li> <li>\u68c0\u67e5\u70b9\u6280\u672f</li> <li>\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3</li> </ul>"},{"location":"tutorials/performance-tuning/#2","title":"2. \u8ba1\u7b97\u4f18\u5316","text":"<ul> <li>\u7b97\u5b50\u878d\u5408</li> <li>Triton kernel\u4f18\u5316</li> <li>CUDA\u6d41\u91cd\u53e0</li> </ul>"},{"location":"tutorials/performance-tuning/#3-io","title":"3. I/O\u4f18\u5316","text":"<ul> <li>\u6570\u636e\u9884\u53d6</li> <li>\u591a\u8fdb\u7a0b\u6570\u636e\u52a0\u8f7d</li> <li>\u5185\u5b58\u6620\u5c04</li> </ul>"},{"location":"tutorials/performance-tuning/#_6","title":"\ud83d\udcc8 \u57fa\u51c6\u6d4b\u8bd5","text":"<ul> <li>\u4e0ePyTorch\u6027\u80fd\u5bf9\u6bd4</li> <li>\u4e0d\u540c\u914d\u7f6e\u7684\u6027\u80fd\u6d4b\u8bd5</li> <li>\u74f6\u9888\u8bc6\u522b\u65b9\u6cd5</li> </ul> <p>\ud83d\udcd8 \u6587\u6863\u72b6\u6001: \u6b63\u5728\u7f16\u5199\u4e2d\uff0c\u9884\u8ba1\u5728v0.2.0\u7248\u672c\u5b8c\u6210\u3002</p>"}]}