


# Genesis: A Lightweight Deep Learning Framework

<div align="center">

![Genesis Logo](https://img.shields.io/badge/Genesis-Deep%20Learning-blue?style=for-the-badge&logo=python)

[![License](https://img.shields.io/github/license/phonism/genesis?style=flat-square)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)](https://python.org)
[![CUDA](https://img.shields.io/badge/CUDA-11.0%2B-green?style=flat-square&logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)
[![Triton](https://img.shields.io/badge/Triton-2.0%2B-orange?style=flat-square)](https://github.com/openai/triton)
[![Tests](https://img.shields.io/badge/Tests-Passing-success?style=flat-square)](tests/)
[![Documentation](https://img.shields.io/badge/Docs-MkDocs-blue?style=flat-square)](docs/)

**A modern deep learning framework built from scratch with educational clarity and production performance**

[ğŸ“š Documentation](docs/) | [ğŸš€ Quick Start](#quick-start) | [ğŸ“Š Benchmarks](#performance) | [ğŸ¤ Contributing](CONTRIBUTING.md)

</div>

---

## ğŸŒŸ Highlights

Genesis is a lightweight yet powerful deep learning framework that combines **educational clarity** with **production-level performance**. Built from scratch in Python, it features a clean, modern architecture with modular backends for CPU and GPU operations.

**ğŸš€ v2.0 - Clean Architecture Update**:
- âœ… **Modular Backend System**: Separated CPU and CUDA backends in `backends/` for better maintainability
- âœ… **Unified Device Abstraction**: Centralized device management in `genesis.device`
- âœ… **Advanced Memory Management**: High-performance CUDA memory manager with lazy initialization
- âœ… **Modern Dispatcher**: Clean operation dispatch system routing to device-specific implementations
- âœ… **Enhanced Stability**: Improved error handling and CUDA initialization
- âœ… **Production Ready**: Complete training pipeline with mixed precision and distributed support

### Why Genesis?

- ğŸ¯ **Educational Excellence**: Clear, well-documented code that shows how deep learning frameworks work internally
- âš¡ **High Performance**: Triton-optimized kernels achieving 60-85% efficiency compared to PyTorch on large tensors
- ğŸ”§ **Modern Architecture**: Clean separation between automatic differentiation, tensor operations, and neural network modules
- ğŸš€ **Production Ready**: Complete training pipeline support including mixed precision, distributed training, and model serialization
- ğŸ“– **Learning Resource**: Perfect for understanding deep learning framework internals while building real models

## ğŸ¯ Key Features

### Core Capabilities
- âœ… **Automatic Differentiation**: Dynamic computational graph with full backpropagation support
- âœ… **Comprehensive Tensor Operations**: Complete tensor arithmetic with GPU acceleration
- âœ… **Neural Network Modules**: All essential layers including Multi-Head Attention, LayerNorm, etc.
- âœ… **Modern Optimizers**: Adam, AdamW, SGD with learning rate scheduling and gradient clipping
- âœ… **Mixed Precision Training**: Automatic Mixed Precision (AMP) with FP16/BF16 support
- âœ… **Model Management**: Checkpoint saving/loading, state dict management
- âœ… **LLM Support**: Built-in Qwen model implementation with SFT training and chat inference
- âœ… **Training Pipeline**: Complete LLM training with datasets, schedulers, and checkpointing
- âœ… **Chat Applications**: Ready-to-use chat interfaces for trained models

### Technical Innovations
- ğŸ—ï¸ **Modular Backend System**: Clean separation of CPU and CUDA implementations in `backends/`
- ğŸ¯ **Unified Operation Dispatch**: Central operation router automatically selects optimal backend
- ğŸ”¥ **Triton Kernels**: Hand-optimized GPU kernels for maximum performance
- ğŸ§® **Advanced Memory Management**: High-performance memory pooling with fragmentation control and statistics
- ğŸš€ **Lazy CUDA Initialization**: Reliable GPU initialization without import-time failures
- ğŸ“Š **Profiling Tools**: Built-in performance profiling, memory usage tracking, and optimization utilities
- ğŸ² **Random State Management**: PyTorch-compatible RNG with thread-safe state handling
- ğŸ›ï¸ **Device Abstraction**: Unified device interface supporting CPU, CUDA, and future backends

## ğŸ“Š Performance

Genesis achieves impressive performance through Triton-optimized kernels:

| Operation | Size | Genesis | PyTorch | Efficiency |
|-----------|------|---------|---------|------------|
| Add | 4096Ã—4096 | 0.025ms | 0.04ms | **66.7%** |
| MatMul | 4096Ã—4096 | 2.1ms | 2.0ms | **95%** |
| Softmax | 8192Ã—8192 | 0.8ms | 0.9ms | **112%** |
| LayerNorm | 4096Ã—4096 | 0.5ms | 0.6ms | **120%** |
| Attention | 32Ã—1024Ã—1024 | 3.2ms | 3.1ms | **97%** |

*Benchmarked on NVIDIA A100 GPU with CUDA 11.8*

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/phonism/genesis.git
cd genesis

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Basic installation (CPU only)
pip install -e .

# Full installation with LLM support and development tools
pip install -e ".[llm,dev]"

# Verify installation
python verify_install.py

# For GPU acceleration (Linux/Windows only)
export CUDA_VISIBLE_DEVICES=0  # Use first GPU
```

**Installation Options:**
- `pip install -e .` - Core framework only
- `pip install -e ".[llm]"` - Add LLM support (transformers, safetensors)
- `pip install -e ".[dev]"` - Add development tools (pytest, black, mypy)
- `pip install -e ".[docs]"` - Add documentation tools (mkdocs)
- `pip install -e ".[all]"` - Everything included

See [INSTALLATION.md](INSTALLATION.md) for detailed platform-specific instructions.

### Basic Usage

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim

# Create tensors with automatic differentiation
x = genesis.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
y = genesis.tensor([[2.0, 0.0], [0.0, 2.0]], requires_grad=True)

# Perform operations
z = genesis.matmul(x, y)
loss = z.sum()

# Automatic differentiation
loss.backward()
print(f"Gradient of x: {x.grad}")
```

### Neural Network Example

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim

class SimpleNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize model and optimizer
model = SimpleNet(784, 256, 10)
optimizer = optim.AdamW(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training loop
for epoch in range(10):
    for batch_data, batch_labels in dataloader:
        # Forward pass
        outputs = model(batch_data)
        loss = criterion(outputs, batch_labels)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping (optional)
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # Update weights
        optimizer.step()
```

### Mixed Precision Training

```python
import genesis

# Enable automatic mixed precision
genesis.enable_autocast = True

# Use autocast context
with genesis.autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

# Backward pass handles mixed precision automatically
loss.backward()
optimizer.step()
```

### Random Number Generation

```python
import genesis

# Set global random seed for reproducibility
genesis.manual_seed(42)

# Create random tensors
x = genesis.rand(100, 100, device=genesis.device('cuda'))
y = genesis.randn(50, 50, device=genesis.device('cpu'))

# Advanced RNG state management
generator = genesis.Generator()
generator.manual_seed(12345)

# Save and restore RNG states
state = genesis.get_rng_state()
# ... some random operations ...
genesis.set_rng_state(state)  # Restore previous state

# Thread-safe random generation
with genesis.fork_rng():
    genesis.manual_seed(999)
    # Random operations in this context don't affect global state
```

### Memory Management and Profiling

```python
import genesis

# Monitor memory usage
device = genesis.device('cuda')
print(f"Memory allocated: {device.memory_allocated() / 1e6:.1f} MB")
print(f"Memory cached: {device.memory_cached() / 1e6:.1f} MB")

# Advanced memory statistics
stats = device.memory_stats()
print(f"Cache hit rate: {stats['cache_hit_rate']:.1%}")
print(f"Peak memory usage: {stats['peak_allocated'] / 1e9:.2f} GB")

# Memory profiling for optimization
with genesis.profiler.profile() as prof:
    x = genesis.rand(4096, 4096, device=device)
    y = genesis.matmul(x, x.T)
    
print(prof.memory_summary())
```

## ğŸ—ï¸ Architecture

```
genesis/
â”œâ”€â”€ tensor.py                # Core Tensor class with autograd support
â”œâ”€â”€ function.py              # Automatic differentiation functions
â”œâ”€â”€ device.py                # Unified device abstraction
â”œâ”€â”€ storage.py               # Storage interface layer
â”œâ”€â”€ backends/                # Device-specific implementations
â”‚   â”œâ”€â”€ cpu.py               # CPU backend using PyTorch
â”‚   â”œâ”€â”€ cuda.py              # CUDA tensor storage
â”‚   â”œâ”€â”€ cuda_memory.py       # Advanced CUDA memory management
â”‚   â””â”€â”€ cuda_kernels.py      # Optimized CUDA kernels
â”œâ”€â”€ ops/                     # Operation dispatch system
â”‚   â”œâ”€â”€ dispatcher.py        # Central operation router
â”‚   â”œâ”€â”€ cpu/                 # CPU operation implementations
â”‚   â””â”€â”€ cuda/                # CUDA operation implementations
â”œâ”€â”€ nn/
â”‚   â”œâ”€â”€ modules/             # Neural network modules (modularized)
â”‚   â”‚   â”œâ”€â”€ module.py        # Base Module class
â”‚   â”‚   â”œâ”€â”€ linear.py        # Linear layers
â”‚   â”‚   â”œâ”€â”€ activation.py    # Activation functions
â”‚   â”‚   â”œâ”€â”€ normalization.py # LayerNorm, BatchNorm, RMSNorm
â”‚   â”‚   â”œâ”€â”€ transformer.py   # Multi-head attention, transformers
â”‚   â”‚   â””â”€â”€ loss.py          # Loss functions (CrossEntropy, MSE, etc.)
â”‚   â”œâ”€â”€ functional.py        # Functional NN operations
â”‚   â””â”€â”€ triton_ops/          # Triton-accelerated operations
â”œâ”€â”€ optim/
â”‚   â”œâ”€â”€ optimizer.py         # Base optimizer and Adam/AdamW/SGD
â”‚   â””â”€â”€ lr_scheduler.py      # Learning rate schedulers
â”œâ”€â”€ models/
â”‚   â””â”€â”€ qwen.py              # Qwen LLM implementation
â”œâ”€â”€ distributed/             # Distributed training support
â”‚   â”œâ”€â”€ parallel.py          # DDP implementation
â”‚   â””â”€â”€ nccl_backend.py      # NCCL communication
â””â”€â”€ cuda/
    â””â”€â”€ __init__.py          # CUDA utilities and initialization
```

## ğŸ“š Documentation

Comprehensive documentation is available in the [docs/](docs/) directory:

- [Getting Started Guide](docs/getting-started/)
- [API Reference](docs/api-reference/)
- [Architecture Overview](docs/architecture/)
- [Tutorials](docs/tutorials/)
- [Performance Guide](docs/performance/)

## ğŸ§ª Testing

Genesis maintains high code quality with comprehensive testing:

```bash
# Run all tests
python -m pytest tests/

# Run specific test module
python -m pytest tests/test_autograd.py

# Run with coverage
python -m pytest tests/ --cov=genesis --cov-report=html

# Run performance benchmarks
python benchmark/bench_ops.py
```

## ğŸ¤ Contributing

We welcome contributions! Genesis is designed to be hackable and extensible.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run code formatting
black genesis/
isort genesis/

# Run type checking
mypy genesis/
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed contribution guidelines.

## ğŸš¦ Roadmap

- [x] Core tensor operations and autograd
- [x] Essential neural network modules
- [x] Optimizers and schedulers
- [x] Mixed precision training
- [x] Qwen LLM implementation
- [ ] More model architectures (GPT, BERT, ViT)
- [ ] Distributed training improvements
- [ ] JIT compilation support
- [ ] Model quantization
- [ ] Mobile deployment

See [ROADMAP.md](ROADMAP.md) for detailed plans.

## ğŸ“Š Benchmarks

Detailed performance comparisons are available in [benchmark/](benchmark/):

- `bench_ops.py` - Elementwise operations
- `bench_matmul.py` - Matrix multiplication
- `bench_attention.py` - Attention mechanisms
- `bench_end_to_end.py` - Full model training

## ğŸŒŸ Examples

The [apps/](apps/) and [samples/](samples/) directories contain various examples:

**LLM Applications** (`apps/llm/`):
- `train_sft_qwen.py` - Qwen supervised fine-tuning
- `chat_qwen.py` - Interactive chat with trained models
- `torch_qwen.py` - PyTorch comparison benchmarks

**General Examples** (`samples/`):
- `sample.py` - Basic neural network training
- `mnist_cnn.py` - CNN for MNIST classification
- `transformer.py` - Transformer model implementation

**Quick Start Commands**:
```bash
# Train a Qwen model
cd apps/llm && python train_sft_qwen.py

# Chat with trained model
cd apps/llm && python chat_qwen.py

# Run benchmarks
python benchmark/simple_qwen_bench.py
```

## ğŸ“œ License

Genesis is released under the MIT License. See [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

Genesis is inspired by and learns from many excellent projects:
- [PyTorch](https://pytorch.org) - API design and tensor operations
- [Triton](https://github.com/openai/triton) - GPU kernel optimization
- [TinyGrad](https://github.com/tinygrad/tinygrad) - Minimalist design philosophy
- [JAX](https://github.com/google/jax) - Functional programming concepts

## ğŸ“® Contact

- **GitHub Issues**: Bug reports and feature requests
- **Discussions**: Questions and community support
- **Email**: genesis-dev@example.com

---

<div align="center">

**Built with â¤ï¸ for the deep learning community**

â­ Star us on GitHub to support the project!

</div>