# Genesis

<div align="center">

![Genesis](https://img.shields.io/badge/Genesis-Deep%20Learning-blue?style=for-the-badge&logo=python)

[![License](https://img.shields.io/github/license/phonism/genesis?style=flat-square)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)](https://python.org)
[![CUDA](https://img.shields.io/badge/CUDA-11.0%2B-green?style=flat-square&logo=nvidia)](https://developer.nvidia.com/cuda-toolkit)

**A high-performance deep learning framework with educational clarity**

[ğŸ“š Documentation](docs/) | [ğŸš€ Quick Start](#quick-start) | [ğŸ“Š Performance](#performance)

</div>

---

## Overview

Genesis is a modern deep learning framework built from scratch, combining **production-level performance** with **educational transparency**. Featuring Triton-optimized kernels, automatic differentiation, and comprehensive neural network modules, Genesis serves both as a learning resource and a practical training framework.

## Key Features

**Core Capabilities**
- ğŸ”¥ **High Performance**: Triton-optimized GPU kernels achieving near-native performance
- âš¡ **Automatic Differentiation**: Dynamic computational graph with full gradient support
- ğŸ§  **Neural Networks**: Complete module library including transformers and attention mechanisms
- ğŸ¯ **Mixed Precision**: AMP support with FP16/BF16 training
- ğŸš€ **Distributed Training**: Multi-GPU training with NCCL backend
- ğŸ“¦ **Model Support**: Built-in LLM implementations (Qwen) with training pipelines

**Technical Highlights**
- Modular backend system with clean CPU/CUDA separation
- Advanced CUDA memory management with pooling and statistics
- Unified operation dispatch routing to optimal implementations
- Complete optimizer suite (Adam, AdamW, SGD) with schedulers
- Production-ready training pipeline with checkpointing

## Performance

Genesis delivers competitive performance through hand-optimized Triton kernels:

| Operation | Efficiency vs Reference |
|-----------|------------------------|
| Matrix Multiplication | ~95% |
| Softmax | ~112% |
| LayerNorm | ~120% |
| Multi-Head Attention | ~97% |

*Benchmarked on NVIDIA A100 GPU*

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/phonism/genesis.git
cd genesis

# Install (CPU only)
pip install -e .

# Install with LLM support
pip install -e ".[llm]"

# Verify installation
python -c "import genesis; print(genesis.__version__)"
```

### Basic Usage

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim

# Define model
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.fc2 = nn.Linear(256, 10)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = self.fc1(x).relu()
        x = self.dropout(x)
        return self.fc2(x)

# Training setup
model = Net()
optimizer = optim.AdamW(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# Training loop
for data, target in dataloader:
    output = model(data)
    loss = criterion(output, target)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### Mixed Precision Training

```python
from genesis.cuda import amp

scaler = amp.GradScaler()

for data, target in dataloader:
    with amp.autocast():
        output = model(data)
        loss = criterion(output, target)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### Distributed Training

```bash
# Single command for multi-GPU training
torchrun --nproc_per_node=4 train.py
```

```python
import genesis.distributed as dist

# Initialize
dist.init_process_group(backend='nccl')

# Wrap model
from genesis.distributed import DistributedDataParallel as DDP
model = DDP(model)

# Train normally - gradients synchronized automatically
```

## Architecture

```
genesis/
â”œâ”€â”€ tensor.py              # Core tensor with autograd
â”œâ”€â”€ function.py            # Autodiff functions
â”œâ”€â”€ backends/              # CPU/CUDA implementations
â”‚   â”œâ”€â”€ cpu.py
â”‚   â”œâ”€â”€ cuda.py
â”‚   â””â”€â”€ cuda_memory.py
â”œâ”€â”€ ops/                   # Operation dispatch
â”œâ”€â”€ nn/                    # Neural network modules
â”‚   â”œâ”€â”€ modules/          # Layer implementations
â”‚   â”œâ”€â”€ functional.py     # Functional operations
â”‚   â””â”€â”€ triton_ops/       # Optimized kernels
â”œâ”€â”€ optim/                # Optimizers
â”œâ”€â”€ distributed/          # Multi-GPU support
â””â”€â”€ cuda/                 # CUDA utilities & AMP
```

## Examples

**Train Qwen LLM**
```bash
cd apps/llm
python train_sft_qwen.py --amp --dtype fp16
```

**Interactive Chat**
```bash
cd apps/llm
python chat_qwen.py --checkpoint model.pth
```

**Benchmarks**
```bash
python benchmark/bench_matmul.py
python benchmark/bench_qwen_training.py
```

## Documentation

- [Getting Started Guide](docs/getting-started/)
- [API Reference](docs/api-reference/)
- [Architecture Overview](docs/architecture/)
- [Performance Tuning](docs/performance/)

## Testing

```bash
# Run test suite
pytest tests/ -v

# With coverage
pytest tests/ --cov=genesis --cov-report=html
```

## Contributing

We welcome contributions! Genesis is designed to be hackable and educational.

```bash
# Development setup
pip install -e ".[dev]"
black genesis/ && isort genesis/
pytest tests/
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

Genesis builds on ideas from PyTorch, Triton, TinyGrad, and JAX. We thank these projects for their inspiration and the deep learning community for their support.

---

<div align="center">

**Built for deep learning researchers and practitioners**

â­ Star us on GitHub if you find Genesis useful!

</div>
