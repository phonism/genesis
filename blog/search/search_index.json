{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Genesis AI Blog","text":"<p>Welcome to the Genesis AI Blog - your go-to source for technical insights, optimization strategies, and development updates from the Genesis deep learning framework team.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<ul> <li>Performance Optimization: Deep dives into GPU computing, CUDA optimization, and framework performance tuning</li> <li>Architecture Insights: Technical details about Genesis framework design and implementation</li> <li>Research Updates: Latest developments in AI systems and deep learning infrastructure</li> <li>Community Contributions: Guest posts and community-driven content</li> </ul>"},{"location":"#featured-topics","title":"Featured Topics","text":"<ul> <li>GPU Computing: CUDA kernels, Triton optimization, memory management</li> <li>Deep Learning Systems: Framework architecture, autograd implementation</li> <li>Algorithmic Insights: Reduction operations, matrix computations, numerical stability</li> <li>Engineering Practices: Benchmarking, profiling, debugging techniques</li> </ul>"},{"location":"#about-this-blog","title":"About This Blog","text":"<p>This technical blog documents our journey building Genesis, a high-performance deep learning framework. We share:</p> <ul> <li>\ud83d\udd2c Research insights: Cutting-edge optimization techniques</li> <li>\u26a1 Performance analysis: Benchmarks, profiling, bottleneck identification  </li> <li>\ud83d\udee0 Implementation details: How we solve real-world engineering challenges</li> <li>\ud83d\udcca Experimental results: What works, what doesn't, and why</li> </ul>"},{"location":"#our-philosophy","title":"Our Philosophy","text":"<pre><code>def optimize():\n    while True:\n        measure()\n        analyze() \n        improve()\n        if breakthrough_achieved():\n            share_with_community()\n</code></pre> <p>The Genesis team is dedicated to advancing the state of deep learning infrastructure through open source contributions and knowledge sharing.</p>"},{"location":"about/","title":"About Genesis AI Blog","text":"<p>This blog is maintained by the Genesis development team to share insights, research findings, and technical deep-dives from our work on building a high-performance deep learning framework.</p>"},{"location":"about/#our-mission","title":"Our Mission","text":"<p>Genesis aims to push the boundaries of deep learning infrastructure by:</p> <ul> <li>Performance First: Optimizing every operation for maximum GPU utilization</li> <li>Developer Experience: Providing intuitive APIs without sacrificing performance</li> <li>Open Source: Contributing to the broader AI community through open research</li> <li>Innovation: Exploring cutting-edge techniques in system optimization</li> </ul>"},{"location":"about/#what-we-write-about","title":"What We Write About","text":"<ul> <li>GPU Computing: CUDA optimization, Triton kernels, memory management</li> <li>Framework Design: Architecture decisions, API design, performance trade-offs</li> <li>Numerical Computing: Precision strategies, algorithmic optimizations</li> <li>Research Insights: Latest findings from our optimization experiments</li> </ul>"},{"location":"about/#the-genesis-framework","title":"The Genesis Framework","text":"<p>Genesis is a deep learning framework designed from the ground up for performance. Key features include:</p> <ul> <li>Dual Backend Architecture: CPU operations via PyTorch, GPU via custom CUDA/Triton</li> <li>Mixed Precision Support: Automatic FP16/BF16 optimization</li> <li>Advanced Memory Management: Efficient GPU memory allocation and recycling</li> <li>Large Model Support: Optimized for training billion-parameter models</li> </ul>"},{"location":"about/#connect-with-us","title":"Connect With Us","text":"<ul> <li>GitHub: genesis-ai/genesis</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Join technical conversations</li> <li>Twitter: @GenesisAI (coming soon)</li> </ul>"},{"location":"about/#contributing","title":"Contributing","text":"<p>We welcome contributions to both the framework and this blog:</p> <ol> <li>Code Contributions: Submit PRs to improve the framework</li> <li>Bug Reports: Help us identify and fix issues</li> <li>Guest Posts: Share your Genesis optimization experiences</li> <li>Documentation: Improve our guides and examples</li> </ol> <p>Genesis is developed by researchers and engineers passionate about advancing the state of AI infrastructure.</p>"},{"location":"2025/08/20/reduction-ops-optimization/","title":"Genesis\u6846\u67b6\u4e2dReduction\u64cd\u4f5c\u7684\u4f18\u5316\u4e4b\u8def\uff1a\u4ece\u539f\u7406\u5230\u5b9e\u8df5","text":"<p>\u6df1\u5165\u5206\u6790GPU\u4e0areduction\u64cd\u4f5c\u7684\u6311\u6218\u4e0e\u4f18\u5316\u7b56\u7565\uff0c\u501f\u9274Flag-Gems\u7b49\u5148\u8fdb\u9879\u76ee\u7684\u8bbe\u8ba1\u601d\u60f3\uff0c\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_1","title":"\u5f15\u8a00","text":"<p>Reduction\u64cd\u4f5c\u662f\u5e76\u884c\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6838\u5fc3\u57fa\u77f3\uff0c\u5b83\u5c06\u9ad8\u7ef4\u5f20\u91cf\u6cbf\u6307\u5b9a\u7ef4\u5ea6\u805a\u5408\u4e3a\u4f4e\u7ef4\u7ed3\u679c\u3002\u4ece\u57fa\u7840\u7684sum\u3001max\uff0c\u5230numerically stable\u7684logsumexp\uff0c\u8fd9\u4e9b\u64cd\u4f5c\u5728\u795e\u7ecf\u7f51\u7edc\u7684forward/backward propagation\u3001\u68af\u5ea6\u805a\u5408\u3001\u635f\u5931\u8ba1\u7b97\u4e2d\u5360\u636e\u5173\u952e\u5730\u4f4d\u3002\u5728Genesis\u6846\u67b6\u7684\u5f00\u53d1\u5b9e\u8df5\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0reduction\u64cd\u4f5c\u5f80\u5f80\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u2014\u2014\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0cattention\u5c42\u7684softmax reduction\u3001layer normalization\u7b49\u64cd\u4f5c\u53ef\u6d88\u8017\u603b\u8ba1\u7b97\u65f6\u95f4\u768415-30%\u3002</p> <p>\u9488\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u6df1\u5165\u7814\u7a76\u4e86\u73b0\u4ee3GPU\u67b6\u6784\u7684\u5e76\u884creduction\u7b97\u6cd5\uff0c\u501f\u9274\u4e86Flag-Gems\u3001CUB (CUDA Unbound)\u7b49\u4e1a\u754c\u5148\u8fdb\u9879\u76ee\u7684\u8bbe\u8ba1\u601d\u60f3\uff0c\u5b9e\u73b0\u4e86\u4ece\u7406\u8bba\u5230\u5de5\u7a0b\u7684\u5168\u9762\u4f18\u5316\u3002\u672c\u6587\u5c06\u5256\u6790GPU\u4e0areduction\u64cd\u4f5c\u7684\u5e95\u5c42\u673a\u5236\u3001\u7b97\u6cd5\u590d\u6742\u5ea6\u3001\u5185\u5b58\u5c42\u6b21\u4f18\u5316\uff0c\u4ee5\u53ca\u6211\u4eec\u5728Genesis\u4e2d\u7684\u5177\u4f53\u5de5\u7a0b\u5b9e\u8df5\u3002</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#reduction","title":"Reduction\u64cd\u4f5c\u7684\u57fa\u672c\u6982\u5ff5","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#reduction_1","title":"\u4ec0\u4e48\u662fReduction\u64cd\u4f5c\uff1f","text":"<p>Reduction\u64cd\u4f5c\u662f\u6307\u5c06\u4e00\u4e2a\u591a\u7ef4\u5f20\u91cf\u6cbf\u7740\u6307\u5b9a\u7ef4\u5ea6\u8fdb\u884c\u805a\u5408\uff0c\u6700\u7ec8\u5f97\u5230\u66f4\u4f4e\u7ef4\u5ea6\u7ed3\u679c\u7684\u64cd\u4f5c\u3002\u5e38\u89c1\u7684reduction\u64cd\u4f5c\u5305\u62ec\uff1a</p> <ul> <li>Sum: \u6c42\u548c\u64cd\u4f5c <code>torch.sum(x, dim=1)</code></li> <li>Max: \u6700\u5927\u503c\u64cd\u4f5c <code>torch.max(x, dim=0)</code></li> <li>Mean: \u5e73\u5747\u503c\u64cd\u4f5c <code>torch.mean(x)</code></li> <li>LogSumExp: \u6570\u503c\u7a33\u5b9a\u7684\u6307\u6570\u6c42\u548c <code>torch.logsumexp(x, dim=-1)</code></li> </ul> <pre><code># \u793a\u4f8b\uff1a2D\u5f20\u91cf\u7684\u4e0d\u540creduction\u64cd\u4f5c\nx = [[1, 2, 3],\n     [4, 5, 6]]\n\nsum_all = sum(x)      # 21 (\u6240\u6709\u5143\u7d20\u6c42\u548c)\nsum_axis0 = sum(x, axis=0)  # [5, 7, 9] (\u6cbf\u7b2c0\u7ef4\u6c42\u548c)\nsum_axis1 = sum(x, axis=1)  # [6, 15] (\u6cbf\u7b2c1\u7ef4\u6c42\u548c)\n</code></pre>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#gpureduction","title":"GPU\u5e76\u884cReduction\u7684\u6839\u672c\u6311\u6218","text":"<p>\u5728\u73b0\u4ee3GPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u9ad8\u6548reduction\u64cd\u4f5c\u9762\u4e34\u591a\u91cd\u6280\u672f\u6311\u6218\uff1a</p> <p>1. Memory Coalescing\u4e0eBank Conflicts - GPU\u5185\u5b58\u5b50\u7cfb\u7edf\u8981\u6c42\u8fde\u7eed\u7ebf\u7a0b\u8bbf\u95ee\u8fde\u7eed\u5185\u5b58\u5730\u5740\u4ee5\u5b9e\u73b0coalesced memory access - Non-inner dimension reduction\u4f1a\u4ea7\u751fstrided memory pattern\uff0c\u5bfc\u81f4memory coalescing\u5931\u6548 - Shared memory\u7684bank conflicts\u53ef\u4e25\u91cd\u5f71\u54cdintra-warp\u6570\u636e\u4ea4\u6362\u6548\u7387</p> <p>2. Warp Divergence\u4e0eControl Flow - \u6761\u4ef6\u5206\u652f\u4f1a\u5bfc\u81f4\u540c\u4e00warp\u5185\u7ebf\u7a0b\u6267\u884c\u4e0d\u540c\u8def\u5f84\uff0c\u9020\u6210warp divergence - Reduction\u8fc7\u7a0b\u4e2d\u7684\u8fb9\u754c\u68c0\u67e5\u3001mask\u64cd\u4f5c\u9700\u8981careful branch optimization - SIMT\u6267\u884c\u6a21\u578b\u4e0b\u7684thread divergence\u53ef\u5c06\u6027\u80fd\u964d\u4f4e\u81f31/32</p> <p>3. Hierarchical Synchronization Overhead - Thread-level: register shuffle operations within warps - Warp-level: shared memory synchronization with <code>__syncthreads()</code> - Block-level: global memory atomics with potential contention - Grid-level: kernel launch overhead for multi-stage reductions</p> <p>4. Numerical Precision\u4e0eAssociativity - \u6d6e\u70b9\u8fd0\u7b97\u7684\u975e\u7ed3\u5408\u6027(non-associativity)\u5bfc\u81f4\u4e0d\u540creduction order\u4ea7\u751f\u4e0d\u540c\u7ed3\u679c - Half-precision (FP16/BF16)\u7684limited dynamic range\u589e\u52a0overflow/underflow\u98ce\u9669 - Catastrophic cancellation\u5728large-scale reduction\u4e2d\u5c24\u4e3a\u7a81\u51fa</p> <p>5. Load Balancing\u4e0eOccupancy - \u4e0d\u5747\u5300\u7684reduction workload\u5bfc\u81f4GPU SM utilization\u4e0d\u8db3 - Register pressure\u9650\u5236\u4e86achievable occupancy - Memory bandwidth vs compute intensity\u7684balance</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_2","title":"\u6df1\u5ea6\u4f18\u5316\u7b56\u7565\u89e3\u6790","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#1-hierarchical-two-stage-reduction","title":"1. Hierarchical Two-Stage Reduction\u7b97\u6cd5","text":"<p>\u6211\u4eec\u91c7\u7528\u4e86\u7c7b\u4f3cCUB\u548cFlag-Gems\u7684\u5c42\u6b21\u5316\u4e24\u9636\u6bb5reduction\u7b56\u7565\uff0c\u8fd9\u662f\u73b0\u4ee3GPU\u4e0a\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u7684\u6807\u51c6\u65b9\u6cd5\uff1a</p> <p>\u7b97\u6cd5\u590d\u6742\u5ea6\u5206\u6790\uff1a - \u4f20\u7edf\u5355\u9636\u6bb5: O(N) work, O(log N) depth, \u4f46\u5b58\u5728\u4e25\u91cd\u7684synchronization bottleneck - \u4e24\u9636\u6bb5\u65b9\u6cd5: \u603bwork\u4ecd\u4e3aO(N)\uff0c\u4f46\u5c06depth\u4eceO(log N)\u4f18\u5316\u4e3aO(log\u00b2 \u221aN)</p> <p>Stage 1: Intra-Block Reduction <pre><code>@triton.jit\ndef sum_kernel_two_stage_1(\n    inp_ptr, mid_ptr, N, BLOCK_SIZE: tl.constexpr\n):\n    \"\"\"\u6bcf\u4e2aCUDA block\u72ec\u7acb\u8ba1\u7b97partial result\"\"\"\n    # \u81ea\u52a8\u6570\u636e\u7c7b\u578b\u63d0\u5347\u907f\u514dprecision loss\n    if tl.constexpr(inp_ptr.dtype.element_ty == tl.float16):\n        cdtype = tl.float32  # \u5185\u90e8\u8ba1\u7b97\u63d0\u5347\u5230FP32\n    else:\n        cdtype = inp_ptr.dtype.element_ty\n\n    pid = tl.program_id(0)\n    # Coalesced memory access pattern\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset &lt; N\n\n    # Vectorized load with out-of-bounds protection\n    inp_val = tl.load(inp_ptr + offset, mask=mask, other=0.0).to(cdtype)\n    sum_val = tl.sum(inp_val)  # Hardware-accelerated warp reduction\n\n    tl.store(mid_ptr + pid, sum_val)  # Store partial result\n</code></pre></p> <p>Stage 2: Inter-Block Reduction <pre><code>@triton.jit\ndef sum_kernel_two_stage_2(\n    mid_ptr, out_ptr, mid_size, BLOCK_MID: tl.constexpr\n):\n    \"\"\"\u5355\u4e2ablock\u5904\u7406\u6240\u6709partial results\"\"\"\n    # \u786e\u4fddmid_size\u8db3\u591f\u5c0f\uff0c\u5355block\u53ef\u5904\u7406\n    offset = tl.arange(0, BLOCK_MID)\n    mask = offset &lt; mid_size\n\n    mid_val = tl.load(mid_ptr + offset, mask=mask, other=0.0)\n    final_sum = tl.sum(mid_val)\n\n    tl.store(out_ptr, final_sum)\n</code></pre></p> <p>\u7b97\u6cd5\u4f18\u52bf\uff1a - Memory Bandwidth\u4f18\u5316: Stage 1\u5b9e\u73b0perfect coalescing\uff0c\u6bcf\u4e2a\u7ebf\u7a0b\u8fde\u7eed\u8bbf\u95ee\u5185\u5b58 - Synchronization\u5f00\u9500: \u6d88\u9664\u4e86intra-block <code>__syncthreads()</code>\uff0c\u53ea\u9700\u4e24\u6b21kernel launch - Scalability: \u652f\u6301\u4efb\u610f\u5927\u5c0f\u5f20\u91cf\uff0cpartial results\u6570\u91cf\u53ef\u63a7\u5236\u5728O(\u221aN)\u7ea7\u522b - Load Balancing: \u6bcf\u4e2ablock\u5904\u7406\u76f8\u540cworkload\uff0c\u907f\u514dtail effect</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#2-block-size-selection","title":"2. \u81ea\u9002\u5e94Block Size Selection\u7b97\u6cd5","text":"<p>\u5757\u5927\u5c0f\u9009\u62e9\u76f4\u63a5\u5f71\u54cdGPU occupancy\u3001register pressure\u548cmemory throughput\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u591a\u56e0\u7d20\u6743\u8861\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\uff1a</p> <pre><code>def adaptive_block_size_v3(n_elements):\n    \"\"\"\u57fa\u4e8eGPU architecture\u548cworkload characteristics\u7684\u81ea\u9002\u5e94\u9009\u62e9\"\"\"\n    if n_elements &lt;= 1024:\n        # Small tensors: minimize kernel launch overhead\n        return triton.next_power_of_2(n_elements)\n    else:\n        # Large tensors: optimize for SM utilization and memory bandwidth\n        # \u9650\u5236\u6700\u5927block\u6570\u91cf\u907f\u514dstage 2\u6210\u4e3a\u74f6\u9888\n        optimal_blocks = min(triton.cdiv(n_elements, 256), 512)\n        block_size = triton.cdiv(n_elements, optimal_blocks)\n        # \u786e\u4fddblock size\u4e3a2\u7684\u5e42\uff0c\u5229\u7528hardware optimization\n        block_size = triton.next_power_of_2(block_size)\n        return max(block_size, 64)  # \u6700\u5c0f64\u786e\u4fddsufficient parallelism\n</code></pre> <p>\u8bbe\u8ba1\u539f\u7406\u6df1\u5165\u5206\u6790\uff1a</p> <p>Power-of-2 Alignment:  - GPU memory controller\u9488\u5bf92\u7684\u5e42\u6b21\u65b9\u5bf9\u9f50\u8fdb\u884c\u4e86\u4f18\u5316 - Triton compiler\u53ef\u4e3apower-of-2 block size\u751f\u6210\u66f4\u9ad8\u6548\u7684indexing code - Warp-level operations (shuffle, reduction)\u5728power-of-2 size\u4e0b\u6548\u7387\u66f4\u9ad8</p> <p>Register Pressure Management: - \u6bcf\u4e2aSM\u7684register file\u6709\u9650(\u4f8b\u5982A100\u768465536\u4e2a32-bit registers) - block_size\u8fc7\u5927\u4f1a\u5bfc\u81f4occupancy\u4e0b\u964d\uff1a<code>occupancy = min(max_blocks_per_SM, registers_per_SM / (registers_per_thread * threads_per_block))</code> - \u6211\u4eec\u768464-1024\u8303\u56f4\u5728\u73b0\u4ee3GPU\u4e0a\u80fd\u4fdd\u8bc1\u226550% occupancy</p> <p>Memory Bandwidth Optimization: - \u7406\u8bba\u5e26\u5bbd\uff1aA100\u76841555 GB/s\u9700\u8981\u8db3\u591f\u7684\u5e76\u53d1memory transactions - Block size\u5f71\u54cdmemory coalescing efficiency\u548cL1/L2 cache hit rate - \u221aN scaling\u786e\u4fdd\u968f\u6570\u636e\u91cf\u589e\u957f\u7684balanced partitioning</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#3-dimension-specialized-kernel-architecture","title":"3. Dimension-Specialized Kernel Architecture","text":"<p>\u4e0d\u540creduction\u7ef4\u5ea6\u7684\u5185\u5b58\u8bbf\u95eepattern\u5dee\u5f02\u5de8\u5927\uff0c\u9700\u8981specialized kernel\u8fdb\u884c\u4f18\u5316\uff1a</p> <p>Inner Dimension Reduction (Coalesced Access Pattern):</p> <p>\u5bf9\u4e8eshape <code>[M, N]</code>\u5f20\u91cf\u6cbf\u6700\u540e\u7ef4\u5ea6reduction\uff0c\u6bcf\u4e2athread\u8bbf\u95ee\u8fde\u7eed\u5185\u5b58\uff1a</p> <pre><code>@triton.jit\ndef sum_kernel_inner_dim(\n    output_ptr, input_ptr, M, N,\n    TILE_N: tl.constexpr, ONE_TILE_PER_CTA: tl.constexpr\n):\n    \"\"\"\u4e13\u4e3ainner dimension\u4f18\u5316\u7684\u9ad8\u6027\u80fdkernel\"\"\"\n    pid_m = tl.program_id(0)  # \u6bcf\u4e2ablock\u5904\u7406\u4e00\u884c\n\n    if ONE_TILE_PER_CTA:\n        # N\u7ef4\u5ea6\u5355tile\u5904\u7406\uff1a\u6700\u4f18memory coalescing\n        n_offsets = tl.arange(0, TILE_N)\n        inp_offset = pid_m * N + n_offsets\n        mask = n_offsets &lt; N\n        # Vectorized load: 32 threads simultaneously load consecutive elements\n        inp = tl.load(input_ptr + inp_offset, mask=mask, other=0.0)\n        # Warp-level tree reduction using shuffle operations\n        out = tl.sum(inp)  # Hardware-accelerated\n        tl.store(output_ptr + pid_m, out)\n    else:\n        # N\u7ef4\u5ea6\u591atile\u5904\u7406\uff1abalance memory bandwidth and register usage\n        sum_acc = tl.zeros((TILE_N,), dtype=cdtype)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)\n            inp_offsets = pid_m * N + n_offsets\n            mask = n_offsets &lt; N\n            inp = tl.load(input_ptr + inp_offsets, mask=mask, other=0.0)\n            sum_acc += inp  # Element-wise accumulation\n        out = tl.sum(sum_acc)  # Final intra-thread reduction\n        tl.store(output_ptr + pid_m, out)\n</code></pre> <p>Non-Inner Dimension Reduction (Strided Access Pattern):</p> <p>\u5bf9\u4e8e\u975e\u5185\u7ef4\u5ea6reduction\uff0c\u9700\u8981\u5904\u7406strided memory access\u548ccomplex indexing\uff1a</p> <pre><code>@triton.jit \ndef sum_kernel_non_inner_dim(\n    output_ptr, input_ptr, M, N, K,\n    TILE_N: tl.constexpr, TILE_K: tl.constexpr, ONE_TILE_PER_CTA: tl.constexpr\n):\n    \"\"\"\u5904\u7406\u975e\u5185\u7ef4\u5ea6\u7684strided reduction\"\"\"\n    pid_m = tl.program_id(0)\n    pid_k = tl.program_id(1)\n\n    # 2D thread block grid\u5904\u74063D tensor reshape\n    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)[None, :]\n\n    if ONE_TILE_PER_CTA:\n        n_offsets = tl.arange(0, TILE_N)[:, None]\n        # \u590d\u6742\u76843D indexing: [M, N, K] -&gt; linear offset\n        inp_offset = pid_m * N * K + n_offsets * K + k_offsets\n        mask = (n_offsets &lt; N) &amp; (k_offsets &lt; K)\n        inp = tl.load(input_ptr + inp_offset, mask=mask, other=0.0)\n        # Reduce along N dimension (axis=0 of loaded tile)\n        out = tl.sum(inp, axis=0, keep_dims=True)\n        out_offset = pid_m * K + k_offsets\n        tl.store(output_ptr + out_offset, out, mask=k_offsets &lt; K)\n    else:\n        # Multi-tile processing with accumulation\n        sum_acc = tl.zeros([TILE_N, TILE_K], dtype=cdtype)\n        for start_n in range(0, N, TILE_N):\n            n_offsets = start_n + tl.arange(0, TILE_N)[:, None]\n            inp_offsets = pid_m * N * K + n_offsets * K + k_offsets\n            mask = (n_offsets &lt; N) &amp; (k_offsets &lt; K)\n            inp = tl.load(input_ptr + inp_offsets, mask=mask, other=0.0)\n            sum_acc += inp\n        out = tl.sum(sum_acc, axis=0, keep_dims=True)\n        out_offset = pid_m * K + k_offsets\n        tl.store(output_ptr + out_offset, out, mask=k_offsets &lt; K)\n</code></pre> <p>Kernel Selection Logic: <pre><code>if ax == ndim - 1:  # Inner dimension\n    # Optimal: coalesced access, high memory bandwidth utilization\n    M = functools_reduce(operator.mul, shape[:-1], 1)\n    N = shape[-1]\n    # Expected memory throughput: ~80% of peak bandwidth\n    use_inner_dim_kernel(M, N)\nelse:  # Non-inner dimension  \n    # Suboptimal but necessary: strided access pattern\n    # Memory throughput drops to ~30-50% of peak\n    axes_to_keep = tuple(i for i in range(ndim) if i != ax)\n    new_order = axes_to_keep + (ax,)  # Move reduction dim to end\n    x = x.permute(new_order)  # Expensive transpose operation\n    use_non_inner_kernel(x)\n</code></pre></p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#genesis","title":"Genesis\u4e2d\u7684\u5177\u4f53\u5b9e\u73b0","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_3","title":"\u7248\u672c\u63a7\u5236\u7cfb\u7edf","text":"<p>\u6211\u4eec\u5b9e\u73b0\u4e86\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff0c\u5141\u8bb8\u5728\u8fd0\u884c\u65f6\u5207\u6362\u4e0d\u540c\u7684\u4f18\u5316\u7b56\u7565\uff1a</p> <pre><code>def reduce_sum(x, axis=None, keepdims=False):\n    version = os.environ.get('GENESIS_REDUCTION_VERSION', 'v3')\n\n    if version == 'v1':\n        return reduce_sum_v1(x, axis, keepdims)  # \u539f\u59cb\u5b9e\u73b0\n    elif version == 'v2':\n        return reduce_sum_v2(x, axis, keepdims)  # \u4e24\u9636\u6bb5reduction\n    elif version == 'v3':\n        return reduce_sum_v3(x, axis, keepdims)  # \u9ad8\u7ea7\u4f18\u5316\n    else:\n        return reduce_sum_v3(x, axis, keepdims)  # \u9ed8\u8ba4\u6700\u65b0\u7248\u672c\n</code></pre>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#triton","title":"Triton\u5185\u6838\u5b9e\u73b0","text":"<p>\u6211\u4eec\u4f7f\u7528Triton\u7f16\u5199\u4e86\u9ad8\u6027\u80fd\u7684GPU\u5185\u6838\uff1a</p> <pre><code>@triton.jit\ndef sum_kernel_two_stage_1(inp_ptr, partial_ptr, N, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u7b2c\u4e00\u9636\u6bb5\uff1a\u8ba1\u7b97\u5c40\u90e8sum\u503c\"\"\"\n    pid = tl.program_id(0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset &lt; N\n\n    # \u52a0\u8f7d\u6570\u636e\uff0cout-of-bounds\u586b0\n    vals = tl.load(inp_ptr + offset, mask=mask, other=0.0)\n\n    # \u8ba1\u7b97\u5757\u5185sum\n    block_sum = tl.sum(vals)\n\n    # \u5b58\u50a8\u5c40\u90e8\u7ed3\u679c\n    tl.store(partial_ptr + pid, block_sum)\n\n@triton.jit  \ndef sum_kernel_two_stage_2(partial_ptr, output_ptr, num_blocks, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\u7b2c\u4e8c\u9636\u6bb5\uff1a\u5408\u5e76\u5c40\u90e8\u7ed3\u679c\"\"\"\n    pid = tl.program_id(0)\n\n    if pid == 0:  # \u53ea\u7528\u4e00\u4e2a\u7ebf\u7a0b\u5757\n        offset = tl.arange(0, BLOCK_SIZE)\n        mask = offset &lt; num_blocks\n\n        # \u52a0\u8f7d\u5c40\u90e8\u7ed3\u679c\n        vals = tl.load(partial_ptr + offset, mask=mask, other=0.0)\n\n        # \u6700\u7ec8reduction\n        result = tl.sum(vals)\n\n        # \u5b58\u50a8\u6700\u7ec8\u7ed3\u679c\n        tl.store(output_ptr, result)\n</code></pre>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#4-numerical-precisionmixed-precision-strategy","title":"4. Numerical Precision\u4e0eMixed-Precision Strategy","text":"<p>\u6570\u503c\u7cbe\u5ea6\u662freduction\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u548c\u4f4e\u7cbe\u5ea6\u573a\u666f\u4e0b\uff1a</p> <p>Precision Loss Analysis: <pre><code># \u95ee\u9898\u793a\u4f8b\uff1aFP16\u7684precision loss\nimport numpy as np\n\n# FP16\u7684\u673a\u5668\u7cbe\u5ea6\u7ea6\u4e3a5e-4\nfp16_data = np.random.randn(1000000).astype(np.float16)\nfp32_result = np.sum(fp16_data.astype(np.float32))  # Ground truth\nfp16_result = np.sum(fp16_data)  # Naive FP16 reduction\n\nrelative_error = abs(fp32_result - fp16_result) / abs(fp32_result)\nprint(f\"Relative error: {relative_error:.2e}\")  # \u901a\u5e38&gt;1e-3\n</code></pre></p> <p>Genesis\u7684Mixed-Precision Strategy: <pre><code>@triton.jit\ndef precision_aware_reduction(inp_ptr, out_ptr, N):\n    \"\"\"\u81ea\u52a8\u7cbe\u5ea6\u63d0\u5347\u7b56\u7565\"\"\"\n    # \u7f16\u8bd1\u671f\u7c7b\u578b\u68c0\u67e5\u548c\u63d0\u5347\n    if tl.constexpr(inp_ptr.dtype.element_ty == tl.float16) or \\\n       tl.constexpr(inp_ptr.dtype.element_ty == tl.bfloat16):\n        # \u5185\u90e8\u8ba1\u7b97\u63d0\u5347\u5230FP32\u786e\u4fddnumerical stability\n        compute_dtype = tl.float32\n        # \u8f93\u51fa\u7cbe\u5ea6\u4fdd\u6301\u539f\u59cb\u7c7b\u578b\u5e73\u8861accuracy\u548cmemory\n        output_dtype = inp_ptr.dtype.element_ty\n    else:\n        compute_dtype = inp_ptr.dtype.element_ty\n        output_dtype = compute_dtype\n\n    # Load and convert to higher precision\n    vals = tl.load(inp_ptr + offsets, mask=mask, other=0.0)\n    compute_vals = vals.to(compute_dtype)  # Precision promotion\n\n    # High-precision computation\n    result = tl.sum(compute_vals)\n\n    # Convert back to output precision\n    final_result = result.to(output_dtype)\n    tl.store(out_ptr, final_result)\n</code></pre></p> <p>Advanced Numerical Techniques:</p> <ol> <li> <p>Kahan Summation for Ultra-High Precision: <pre><code>@triton.jit\ndef kahan_sum_kernel(inp_ptr, out_ptr, N):\n    \"\"\"Compensated summation for maximum precision\"\"\"\n    sum_val = tl.zeros((1,), dtype=tl.float64)\n    compensation = tl.zeros((1,), dtype=tl.float64)\n\n    for i in range(0, N, BLOCK_SIZE):\n        vals = tl.load(inp_ptr + i + tl.arange(0, BLOCK_SIZE), \n                      mask=i + tl.arange(0, BLOCK_SIZE) &lt; N)\n        vals_64 = vals.to(tl.float64)\n\n        # Kahan summation algorithm\n        y = vals_64 - compensation\n        t = sum_val + y\n        compensation = (t - sum_val) - y\n        sum_val = t\n\n    tl.store(out_ptr, sum_val.to(tl.float32))\n</code></pre></p> </li> <li> <p>Overflow/Underflow Protection: <pre><code>def safe_reduction_with_scaling(x, axis=None):\n    \"\"\"\u9632\u6b62overflow\u7684\u5b89\u5168reduction\"\"\"\n    # \u52a8\u6001\u8303\u56f4\u68c0\u67e5\n    if x.dtype in [torch.float16, torch.bfloat16]:\n        # \u68c0\u67e5\u6570\u503c\u8303\u56f4\uff0c\u5fc5\u8981\u65f6\u8fdb\u884cscaling\n        abs_max = torch.max(torch.abs(x))\n        if abs_max &gt; 1e4:  # \u63a5\u8fd1FP16\u4e0a\u965065504\n            scale_factor = 1e4 / abs_max\n            scaled_x = x * scale_factor\n            result = reduce_sum_v3(scaled_x, axis) / scale_factor\n            return result\n\n    return reduce_sum_v3(x, axis)\n</code></pre></p> </li> </ol>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_4","title":"\u6027\u80fd\u5206\u6790\u4e0e\u7ed3\u679c","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_5","title":"\u6d4b\u8bd5\u73af\u5883","text":"<ul> <li>GPU: NVIDIA A100-SXM4-40GB</li> <li>\u5185\u5b58: 39.4 GB</li> <li>\u7406\u8bba\u5e26\u5bbd: 1555 GB/s</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_6","title":"\u6027\u80fd\u5bf9\u6bd4","text":"<p>\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u6211\u4eec\u7684\u4f18\u5316\u7248\u672c\u76f8\u6bd4PyTorch\u6709\u663e\u8457\u63d0\u5347\uff1a</p> \u64cd\u4f5c \u5f20\u91cf\u5927\u5c0f Genesis v1 Genesis v2 Genesis v3 PyTorch \u6700\u4f73\u6027\u80fd sum 256\u00d7256 0.24x 0.58x 2.12x 1.0x \ud83d\udfe2 v3 sum_axis0 256\u00d7256 0.31x 0.45x 1.87x 1.0x \ud83d\udfe2 v3 max 1024\u00d71024 0.16x 0.16x 0.16x 1.0x \ud83d\udd34 \u5f85\u4f18\u5316","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_7","title":"\u4f18\u5316\u6548\u679c\u5206\u6790","text":"<p>\u6210\u529f\u6848\u4f8b - sum\u64cd\u4f5c\uff1a - v3\u7248\u672c\u5728256\u00d7256\u5f20\u91cf\u4e0a\u8fbe\u52302.12x speedup - \u4e24\u9636\u6bb5reduction\u7b56\u7565\u663e\u8457\u6539\u5584\u4e86\u6027\u80fd - \u4e13\u7528inner/non-inner\u7ef4\u5ea6kernel\u8d77\u5230\u5173\u952e\u4f5c\u7528</p> <p>\u5f85\u6539\u8fdb - max\u64cd\u4f5c\uff1a - \u5f53\u524d\u6240\u6709\u7248\u672c\u6027\u80fd\u76f8\u4f3c\uff0c\u672a\u8fbe\u5230\u9884\u671f - \u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316atomic\u64cd\u4f5c - \u8003\u8651\u4f7f\u7528\u66f4\u9ad8\u6548\u7684\u6bd4\u8f83\u7b56\u7565</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_8","title":"\u6280\u672f\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#1","title":"1. \u6570\u503c\u7a33\u5b9a\u6027","text":"<p>\u6311\u6218: float16\u7b49\u4f4e\u7cbe\u5ea6\u7c7b\u578b\u5bb9\u6613\u51fa\u73b0\u6570\u503c\u6ea2\u51fa \u89e3\u51b3\u65b9\u6848:  <pre><code># \u8ba1\u7b97\u65f6\u63d0\u5347\u7cbe\u5ea6\uff0c\u8f93\u51fa\u65f6\u8f6c\u56de\u539f\u7cbe\u5ea6\ncompute_vals = input_vals.to(tl.float32)\nresult = reduction_op(compute_vals)\noutput = result.to(original_dtype)\n</code></pre></p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#2","title":"2. \u5185\u5b58\u5408\u5e76\u8bbf\u95ee","text":"<p>\u6311\u6218: \u975e\u8fde\u7eed\u5185\u5b58\u8bbf\u95ee\u5bfc\u81f4\u5e26\u5bbd\u5229\u7528\u7387\u4f4e \u89e3\u51b3\u65b9\u6848:  <pre><code># \u91cd\u6392\u5f20\u91cf\u4f7freduction\u7ef4\u5ea6\u6210\u4e3a\u5185\u7ef4\u5ea6\nif axis != ndim - 1:\n    new_order = tuple(i for i in range(ndim) if i != axis) + (axis,)\n    x = x.permute(new_order)\n</code></pre></p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#3","title":"3. \u7ebf\u7a0b\u5757\u5927\u5c0f\u4f18\u5316","text":"<p>\u6311\u6218: \u4e0d\u540c\u5f20\u91cf\u5927\u5c0f\u9700\u8981\u4e0d\u540c\u7684\u7ebf\u7a0b\u5757\u914d\u7f6e \u89e3\u51b3\u65b9\u6848:  <pre><code># \u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u914d\u7f6e\nif n &lt;= 256:\n    tile_size = next_power_of_2(n)\n    one_tile_per_cta = True\nelse:\n    tile_size = min(512, next_power_of_2(min(n, 512)))\n    one_tile_per_cta = (tile_size &gt;= n)\n</code></pre></p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_9","title":"\u672a\u6765\u4f18\u5316\u65b9\u5411","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#1_1","title":"1. \u66f4\u9ad8\u7ea7\u7684\u5757\u8c03\u5ea6\u7b56\u7565","text":"<ul> <li>\u52a8\u6001\u8d1f\u8f7d\u5747\u8861</li> <li>\u57fa\u4e8eGPU\u5229\u7528\u7387\u7684\u81ea\u9002\u5e94\u8c03\u6574</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#2_1","title":"2. \u6df7\u5408\u7cbe\u5ea6\u4f18\u5316","text":"<ul> <li>\u667a\u80fd\u9009\u62e9\u8ba1\u7b97\u7cbe\u5ea6</li> <li>\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u7c7b\u578b\u8f6c\u6362\u5f00\u9500</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#3_1","title":"3. \u7279\u6b8a\u5f62\u72b6\u4f18\u5316","text":"<ul> <li>\u9488\u5bf9\u5e38\u89c1\u795e\u7ecf\u7f51\u7edc\u5c42\u5f62\u72b6\u7684\u4e13\u7528\u4f18\u5316</li> <li>Attention\u673a\u5236\u4e2d\u7684reduction\u6a21\u5f0f\u4f18\u5316</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#4","title":"4. \u8de8\u64cd\u4f5c\u878d\u5408","text":"<ul> <li>\u5c06reduction\u4e0e\u5176\u4ed6\u64cd\u4f5c\u878d\u5408</li> <li>\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u538b\u529b</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_10","title":"\u603b\u7ed3","text":"<p>\u5728Genesis\u6846\u67b6\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fdreduction\u64cd\u4f5c\u662f\u4e00\u4e2a\u590d\u6742\u7684\u5de5\u7a0b\u6311\u6218\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3GPU\u67b6\u6784\u3001\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u548c\u6570\u503c\u8ba1\u7b97\u539f\u7406\u3002\u901a\u8fc7\u501f\u9274Flag-Gems\u7b49\u5148\u8fdb\u9879\u76ee\u7684\u8bbe\u8ba1\u601d\u60f3\uff0c\u7ed3\u5408\u6211\u4eec\u7684\u521b\u65b0\u4f18\u5316\u7b56\u7565\uff0c\u6211\u4eec\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8d8aPyTorch\u7684\u6027\u80fd\u3002</p> <p>\u5173\u952e\u7684\u6210\u529f\u56e0\u7d20\u5305\u62ec\uff1a 1. **\u4e24\u9636\u6bb5reduction\u7b56\u7565**\u51cf\u5c11\u4e86\u540c\u6b65\u5f00\u9500 2. **\u81ea\u9002\u5e94\u5757\u5927\u5c0f\u9009\u62e9**\u63d0\u5347\u4e86GPU\u5229\u7528\u7387 3. **\u7ef4\u5ea6\u7279\u5316\u4f18\u5316**\u6539\u5584\u4e86\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f 4. **\u7248\u672c\u63a7\u5236\u7cfb\u7edf**\u652f\u6301\u6e10\u8fdb\u5f0f\u4f18\u5316</p> <p>\u5f53\u7136\uff0coptimization is never done\u3002\u6211\u4eec\u5c06\u7ee7\u7eed\u6df1\u5165\u7814\u7a76GPU\u8ba1\u7b97\u6a21\u5f0f\uff0c\u63a2\u7d22\u66f4\u591a\u521b\u65b0\u7684\u4f18\u5316\u6280\u672f\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u793e\u533a\u8d21\u732e\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u5f15\u64ce\u3002</p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#references","title":"References\u53ca\u6269\u5c55\u9605\u8bfb","text":"","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_11","title":"\u5b66\u672f\u8bba\u6587","text":"<ol> <li>Harris, M. et al. (2007). \"Optimizing Parallel Reduction in CUDA.\" NVIDIA Developer Technology.</li> <li>Bell, N. &amp; Hoberock, J. (2012). \"Thrust: A Productivity-Oriented Library for CUDA.\" GPU Computing Gems.</li> <li>Merrill, D. &amp; Garland, M. (2016). \"CUB: A Library of Reusable CUDA Parallel Primitives.\" CUDA Toolkit Documentation.</li> <li>Tillet, P. et al. (2019). \"Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations.\" MAPL 2019.</li> </ol>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_12","title":"\u5f00\u6e90\u9879\u76ee","text":"<ul> <li>Flag-Gems: https://github.com/FlagOpen/FlagGems - Triton-based PyTorch operator library</li> <li>CUB Library: https://github.com/NVIDIA/cub - CUDA parallel primitives</li> <li>Triton: https://github.com/openai/triton - GPU kernel programming language</li> <li>Genesis: https://github.com/genesis-ai/genesis - Our deep learning framework</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_13","title":"\u6280\u672f\u6587\u6863","text":"<ul> <li>NVIDIA CUDA C++ Programming Guide: Memory Coalescing Best Practices</li> <li>NVIDIA Ampere Architecture Whitepaper: Tensor Core Operations</li> <li>PyTorch Internals: Understanding Autograd and Operator Implementation</li> <li>Triton Documentation: Writing High-Performance GPU Kernels</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_14","title":"\u6027\u80fd\u5206\u6790\u5de5\u5177","text":"<ul> <li>NVIDIA Nsight Compute: GPU kernel profiling and optimization</li> <li>NVIDIA Nsight Systems: System-wide performance analysis  </li> <li>PyTorch Profiler: Python-level performance monitoring</li> <li>Triton Profiler: Kernel-level performance characterization</li> </ul>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"2025/08/20/reduction-ops-optimization/#_15","title":"\u4f5c\u8005\u53ca\u8d21\u732e\u8005","text":"<p>\u4e3b\u8981\u4f5c\u8005: Genesis Team - AI System Optimization Group</p> <p>\u7279\u522b\u9e23\u8c22: - OpenAI Triton Team - \u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684GPU kernel\u7f16\u7a0b\u5de5\u5177 - FlagOpen Community - Flag-Gems\u9879\u76ee\u7684\u6280\u672f\u542f\u53d1\u548c\u5f00\u6e90\u7cbe\u795e - NVIDIA Developer Community - CUDA\u4f18\u5316\u6700\u4f73\u5b9e\u8df5\u548c\u6280\u672f\u652f\u6301 - PyTorch Contributors - \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6280\u672f\u53c2\u8003\u548c\u57fa\u51c6\u5bf9\u6bd4</p> <p>\u8054\u7cfb\u65b9\u5f0f: - GitHub Issues: https://github.com/genesis-ai/genesis/issues - Technical Discussion: genesis-dev@example.com - Community Forum: https://forum.genesis-ai.org</p> <p>\u672c\u6587\u57fa\u4e8eGenesis Framework v0.2\u7684reduction operations\u5b9e\u73b0\u3002\u6587\u4e2d\u6240\u6709\u6027\u80fd\u6570\u636e\u57fa\u4e8eNVIDIA A100 GPU\u6d4b\u8bd5\u83b7\u5f97\uff0c\u5b9e\u9645\u6548\u679c\u53ef\u80fd\u56e0\u786c\u4ef6\u914d\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u800c\u5f02\u3002\u5982\u679c\u60a8\u5728\u7814\u7a76\u6216\u5de5\u4e1a\u5e94\u7528\u4e2d\u4f7f\u7528\u4e86\u672c\u6587\u7684\u6280\u672f\u65b9\u6cd5\uff0c\u6b22\u8fce\u5f15\u7528\u5e76\u4e0e\u6211\u4eec\u5206\u4eab\u60a8\u7684\u7ecf\u9a8c\u3002</p> <p>\u5f15\u7528\u683c\u5f0f (BibTeX): <pre><code>@article{genesis2025reduction,\n  title={Genesis\u6846\u67b6\u4e2dReduction\u64cd\u4f5c\u7684\u4f18\u5316\u4e4b\u8def\uff1a\u4ece\u539f\u7406\u5230\u5b9e\u8df5},\n  author={Genesis Team},\n  journal={Genesis AI Blog},\n  year={2025},\n  month={August},\n  url={https://blog.genesis-ai.org/reduction-optimization}\n}\n</code></pre></p>","tags":["reduction","triton","cuda","performance-tuning"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/optimization/","title":"Optimization","text":""},{"location":"category/gpu/","title":"GPU","text":""},{"location":"category/performance/","title":"Performance","text":""}]}