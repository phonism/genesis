# Automatic Differentiation System

Genesis's automatic differentiation system is the core of the framework, responsible for building computation graphs, executing forward propagation and backpropagation. The system is designed to be simple yet efficient, supporting complex neural network training.

## ğŸ¯ System Overview

The automatic differentiation system is based on dynamic computation graph implementation, mainly including three core components:

- **Tensor** - Tensors carrying gradient information
- **Function** - Abstract base class for differentiable operations
- **Context** - Context that saves intermediate results during forward propagation

## ğŸ—ï¸ Core Architecture

```mermaid
graph TB
    subgraph "è®¡ç®—å›¾èŠ‚ç‚¹"
        A[Tensor] --> B[data NDArray]
        A --> C[grad Tensor]
        A --> D[creator Function]
        A --> E[requires_grad bool]
    end
    
    subgraph "æ“ä½œèŠ‚ç‚¹"
        F[Function] --> G[forward]
        F --> H[backward]
        F --> I[Context]
        I --> J[saved_tensors]
    end
    
    subgraph "æ‰§è¡Œæµç¨‹"
        K[å‰å‘ä¼ æ’­] --> L[æ„å»ºè®¡ç®—å›¾]
        L --> M[ä¿å­˜ä¸­é—´ç»“æœ]
        M --> N[åå‘ä¼ æ’­]
        N --> O[æ¢¯åº¦è®¡ç®—]
        O --> P[æ¢¯åº¦ç´¯ç§¯]
    end
    
    A --> F
    F --> A
    
    style A fill:#e1f5fe
    style F fill:#f3e5f5
    style I fill:#e8f5e8
```

## ğŸ§® Tensorç±»è¯¦è§£

### æ ¸å¿ƒå±æ€§

```python
class Tensor:
    grad: "Tensor"          # æ¢¯åº¦å¼ é‡
    creator: Function       # åˆ›å»ºæ­¤å¼ é‡çš„æ“ä½œ
    inputs: List["Tensor"]  # è¾“å…¥å¼ é‡åˆ—è¡¨
    data: NDArray          # åº•å±‚æ•°æ®å­˜å‚¨
    requires_grad: bool    # æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦
    hooks: List[Callable]  # æ¢¯åº¦é’©å­å‡½æ•°
```

### å…³é”®æ–¹æ³•

#### 1. å¼ é‡åˆ›å»º

```python
# ä»æ•°ç»„åˆ›å»ºå¼ é‡
def __init__(self, array, *, device=None, dtype=None, requires_grad=True):
    if dtype is not None:
        dtype = get_dtype(dtype)  # è½¬æ¢ä¸ºDTypeå¯¹è±¡
    
    # å¤„ç†ä¸åŒè¾“å…¥ç±»å‹
    if isinstance(array, Tensor):
        # ä»ç°æœ‰å¼ é‡åˆ›å»º
        data = array.data if same_device_dtype else convert_data
    elif isinstance(array, NDArray):
        # ä»NDArrayåˆ›å»º
        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)
    else:
        # ä»Pythonå¯¹è±¡åˆ›å»º
        device = device if device else default_device()
        data = Tensor._array_from_numpy(array, device=device, dtype=dtype)
    
    self.init([], data=data, requires_grad=requires_grad)
```

#### 2. åå‘ä¼ æ’­

```python
def backward(self, out_grad=None):
    # è®¾ç½®è¾“å‡ºæ¢¯åº¦
    out_grad = out_grad if out_grad else init.ones(*self.shape, dtype=self.dtype, device=self.device)
    
    # åˆå§‹åŒ–æ¢¯åº¦ç´¯ç§¯å­—å…¸
    node_to_output_grads_list: Dict[Tensor, List[Tensor]] = {}
    node_to_output_grads_list[self] = [out_grad]
    
    # æ‹“æ‰‘æ’åºè·å–è®¡ç®—é¡ºåº
    topo_order = topo_sort(self)
    
    # é€†æ‹“æ‰‘åºéå†è®¡ç®—æ¢¯åº¦
    for node in reversed(topo_order):
        if not node.requires_grad:
            continue
            
        # ç´¯ç§¯å½“å‰èŠ‚ç‚¹çš„æ¢¯åº¦
        if node.grad is None:
            node.grad = reduce(operator.add, node_to_output_grads_list[node])
            # ç¡®ä¿æ¢¯åº¦è¿ç»­æ€§ï¼ˆè§£å†³å¹¿æ’­å¼ é‡é—®é¢˜ï¼‰
            if hasattr(node.grad, 'data') and hasattr(node.grad.data, 'data'):
                cuda_tensor = node.grad.data.data
                if hasattr(cuda_tensor, 'is_contiguous') and not cuda_tensor.is_contiguous():
                    node.grad.data.data = cuda_tensor.contiguous()
        else:
            node.grad += reduce(operator.add, node_to_output_grads_list[node])
        
        # åº”ç”¨æ¢¯åº¦é’©å­
        node.apply_hooks(node.grad)
        
        # è®¡ç®—è¾“å…¥èŠ‚ç‚¹çš„æ¢¯åº¦
        if node.creator is not None:
            # å¤„ç†æ··åˆç²¾åº¦
            grad = node.grad.half() if check_dtype(node.creator.ctx.saved_tensors, genesis.float16) else node.grad
            
            # è°ƒç”¨å¯¹åº”æ“ä½œçš„åå‘ä¼ æ’­
            if node.creator.is_tuple_result:
                backward_grad = node.creator.backward(node.creator.ctx, grad, node.idx)
            else:
                backward_grad = node.creator.backward(node.creator.ctx, grad)
            
            # åˆ†å‘æ¢¯åº¦åˆ°è¾“å…¥èŠ‚ç‚¹
            for i, input_node in enumerate(node.creator.inputs):
                if input_node.requires_grad:
                    if input_node not in node_to_output_grads_list:
                        node_to_output_grads_list[input_node] = []
                    node_to_output_grads_list[input_node].append(backward_grad[i].float())
```

#### 3. æ‹“æ‰‘æ’åº

```python
def topo_sort(node):
    """æ·±åº¦ä¼˜å…ˆæœç´¢å®ç°æ‹“æ‰‘æ’åº"""
    visited = set()
    topo_order = []

    def dfs(n):
        if n in visited:
            return
        visited.add(n)
        
        # é€’å½’è®¿é—®è¾“å…¥èŠ‚ç‚¹
        if n.creator is not None:
            for input_node in n.creator.inputs:
                if isinstance(input_node, Tensor):
                    dfs(input_node)
        
        topo_order.append(n)
    
    dfs(node)
    return topo_order
```

## âš™ï¸ FunctionåŸºç±»

Functionæ˜¯æ‰€æœ‰å¯å¾®åˆ†æ“ä½œçš„åŸºç±»ï¼Œå®šä¹‰äº†å‰å‘å’Œåå‘ä¼ æ’­çš„æ¥å£ã€‚

### åŸºæœ¬ç»“æ„

```python
class Function:
    @staticmethod
    def forward(ctx: Context, *args) -> Union[Tensor, Tuple[Tensor, ...]]:
        """å‰å‘ä¼ æ’­å®ç°"""
        raise NotImplementedError
    
    @staticmethod  
    def backward(ctx: Context, grad_output, out_idx=None) -> Tuple[Tensor, ...]:
        """åå‘ä¼ æ’­å®ç°"""
        raise NotImplementedError
    
    @classmethod
    def apply(cls, *args, **kwargs):
        """ç»Ÿä¸€çš„è°ƒç”¨æ¥å£"""
        # å¤„ç†æ··åˆç²¾åº¦
        instance = cls()
        instance.ctx = Context()
        
        # æ‰§è¡Œå‰å‘ä¼ æ’­
        if genesis.enable_autocast:
            result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))
        else:
            result = cls.forward(instance.ctx, *args, **kwargs)
        
        # è®¾ç½®è®¡ç®—å›¾è¿æ¥
        instance.is_tuple_result = isinstance(result, tuple)
        
        if instance.is_tuple_result:
            for idx, res in enumerate(result):
                if isinstance(res, Tensor) and res.requires_grad:
                    res.set_creator(instance, idx)
        elif isinstance(result, Tensor) and result.requires_grad:
            result.set_creator(instance)
        
        # è®°å½•è¾“å…¥å¼ é‡
        instance.inputs = []
        for t in args:
            if isinstance(t, Tensor):
                instance.inputs.append(t)
            elif isinstance(t, list) and all(isinstance(item, Tensor) for item in t):
                instance.inputs.extend(t)
        
        return result
```

### å®é™…æ“ä½œç¤ºä¾‹

#### çŸ©é˜µä¹˜æ³•

```python
class MatMul(Function):
    @staticmethod
    def forward(ctx, a, b):
        # ä¿å­˜è¾“å…¥ç”¨äºåå‘ä¼ æ’­
        ctx.save_for_backward(a, b)
        return a @ b  # è°ƒç”¨åº•å±‚NDArrayçš„çŸ©é˜µä¹˜æ³•
    
    @staticmethod
    def backward(ctx, grad_output):
        a, b = ctx.saved_tensors
        # è®¡ç®—è¾“å…¥æ¢¯åº¦
        grad_a = grad_output @ b.T
        grad_b = a.T @ grad_output
        return grad_a, grad_b
```

#### åŠ æ³•ï¼ˆæ”¯æŒå¹¿æ’­ï¼‰

```python
class Add(Function):
    @staticmethod
    def forward(ctx, a, b):
        ctx.a_shape = a.shape
        ctx.b_shape = b.shape
        return a + b
    
    @staticmethod
    def backward(ctx, grad_output):
        # å¤„ç†å¹¿æ’­çš„æ¢¯åº¦
        grad_a = grad_output
        grad_b = grad_output
        
        # å¯¹è¢«å¹¿æ’­çš„ç»´åº¦æ±‚å’Œ
        for i, (da, db) in enumerate(zip(ctx.a_shape, ctx.b_shape)):
            if da == 1 and db > 1:
                grad_a = grad_a.sum(axis=i, keepdims=True)
            elif db == 1 and da > 1:
                grad_b = grad_b.sum(axis=i, keepdims=True)
        
        return grad_a, grad_b
```

## ğŸ“ Contextç±»

Contextç±»ç”¨äºåœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚

```python
class Context:
    def __init__(self):
        self.saved_tensors = []
    
    def save_for_backward(self, *tensors):
        """ä¿å­˜å¼ é‡ç”¨äºåå‘ä¼ æ’­"""
        self.saved_tensors.extend(tensors)
    
    @property
    def saved_tensors(self):
        return self._saved_tensors
    
    @saved_tensors.setter  
    def saved_tensors(self, tensors):
        self._saved_tensors = tensors
```

## ğŸ”„ æ··åˆç²¾åº¦æ”¯æŒ

è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿå†…ç½®æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒï¼š

```python
# å…¨å±€å¼€å…³
genesis.enable_autocast = True

# è‡ªåŠ¨ç±»å‹è½¬æ¢
def _cast(value, dtype):
    if isinstance(value, Tensor) and value.is_floating_point():
        if dtype == genesis.float16:
            return value.half()
        else:
            return value.float()
    return value

# åœ¨Function.applyä¸­åº”ç”¨
if genesis.enable_autocast:
    result = cls.forward(instance.ctx, *_cast(args, genesis.float32), **_cast(kwargs, genesis.float32))
```

## ğŸª æ¢¯åº¦é’©å­ç³»ç»Ÿ

æ”¯æŒåœ¨æ¢¯åº¦è®¡ç®—æ—¶æ‰§è¡Œè‡ªå®šä¹‰å‡½æ•°ï¼š

```python
class Tensor:
    def register_hook(self, hook):
        """æ³¨å†Œæ¢¯åº¦é’©å­"""
        self.hooks.append(hook)
    
    def apply_hooks(self, grad):
        """åº”ç”¨æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            hook(grad)

# ä½¿ç”¨ç¤ºä¾‹
def grad_clipping_hook(grad):
    """æ¢¯åº¦è£å‰ªé’©å­"""
    grad.clamp_(-1.0, 1.0)

tensor.register_hook(grad_clipping_hook)
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ç®¡ç†ä¼˜åŒ–

- **è§†å›¾æ“ä½œ**ï¼šreshapeã€transposeç­‰æ“ä½œåˆ›å»ºè§†å›¾è€Œéæ‹·è´æ•°æ®
- **å°±åœ°æ“ä½œ**ï¼šæ”¯æŒ`+=`ã€`*=`ç­‰å°±åœ°æ›´æ–°æ“ä½œ
- **æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–**ï¼šæ™ºèƒ½çš„æ¢¯åº¦ç´¯ç§¯ç­–ç•¥

### 2. è®¡ç®—å›¾ä¼˜åŒ–

- **æƒ°æ€§æ„å»º**ï¼šåªæœ‰åœ¨éœ€è¦æ¢¯åº¦æ—¶æ‰æ„å»ºè®¡ç®—å›¾
- **æ™ºèƒ½é‡Šæ”¾**ï¼šè‡ªåŠ¨é‡Šæ”¾ä¸å†éœ€è¦çš„ä¸­é—´ç»“æœ
- **æ‹“æ‰‘æ’åºç¼“å­˜**ï¼šç¼“å­˜æ‹“æ‰‘æ’åºç»“æœ

### 3. è®¾å¤‡é—´ä¼˜åŒ–

- **è‡ªåŠ¨è®¾å¤‡æ¨æ–­**ï¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„è®¡ç®—è®¾å¤‡
- **å¼‚æ­¥æ‰§è¡Œ**ï¼šæ”¯æŒGPUå¼‚æ­¥è®¡ç®—
- **å†…å­˜é¢„åˆ†é…**ï¼šå‡å°‘åŠ¨æ€å†…å­˜åˆ†é…

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```python
import genesis

# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
x = genesis.randn(3, 4, requires_grad=True)
y = genesis.randn(4, 2, requires_grad=True)

# å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨æ„å»ºè®¡ç®—å›¾ï¼‰
z = x @ y
loss = z.sum()

# åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ‰€æœ‰æ¢¯åº¦ï¼‰
loss.backward()

print(f"xçš„æ¢¯åº¦: {x.grad}")  # è¾“å‡ºxçš„æ¢¯åº¦
print(f"yçš„æ¢¯åº¦: {y.grad}")  # è¾“å‡ºyçš„æ¢¯åº¦
```

### è‡ªå®šä¹‰æ“ä½œ

```python
class CustomFunction(genesis.autograd.Function):
    @staticmethod
    def forward(ctx, input_tensor):
        # è‡ªå®šä¹‰å‰å‘è®¡ç®—
        ctx.save_for_backward(input_tensor)
        result = input_tensor ** 2 + 2 * input_tensor + 1
        return result
    
    @staticmethod
    def backward(ctx, grad_output):
        input_tensor, = ctx.saved_tensors
        # è‡ªå®šä¹‰æ¢¯åº¦è®¡ç®—ï¼šd/dx(x^2 + 2x + 1) = 2x + 2
        grad_input = grad_output * (2 * input_tensor + 2)
        return grad_input

# ä½¿ç”¨è‡ªå®šä¹‰æ“ä½œ
x = genesis.randn(3, 4, requires_grad=True)
y = CustomFunction.apply(x)
y.sum().backward()
```

### æ¢¯åº¦é’©å­

```python
# æ¢¯åº¦ç›‘æ§é’©å­
def monitor_grad(grad):
    print(f"æ¢¯åº¦ç»Ÿè®¡: å‡å€¼={grad.mean():.4f}, æ ‡å‡†å·®={grad.std():.4f}")

# æ¢¯åº¦è£å‰ªé’©å­
def clip_grad(grad):
    grad.data.clamp_(-1.0, 1.0)

x = genesis.randn(10, requires_grad=True)
x.register_hook(monitor_grad)
x.register_hook(clip_grad)

# æ‰§è¡Œä¸€äº›è®¡ç®—
y = (x ** 3).sum()
y.backward()  # ä¼šè§¦å‘é’©å­å‡½æ•°
```

Genesisçš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿè®¾è®¡ç®€æ´è€Œå¼ºå¤§ï¼Œä¸ºæ·±åº¦å­¦ä¹ æä¾›äº†å¯é çš„æ¢¯åº¦è®¡ç®—åŸºç¡€ï¼ŒåŒæ—¶ä¿æŒäº†è‰¯å¥½çš„æ€§èƒ½å’Œå¯æ‰©å±•æ€§ã€‚