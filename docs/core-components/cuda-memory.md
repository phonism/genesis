# CUDA Memory Management

Genesis includes a sophisticated high-performance CUDA memory management system that provides efficient GPU memory allocation through a segment-block allocator architecture with advanced caching strategies.

## Overview

The CUDA Memory Manager is a production-grade memory allocator that achieves significant performance improvements over naive allocation strategies. It features a two-level caching system, segment-block allocation, and comprehensive performance monitoring.

## Architecture

### Core Components

#### CUDAMemoryManager
The main memory manager class with enterprise-grade features:
- **Segment-Block Allocator**: Hierarchical memory organization for efficient allocation
- **Two-Level Caching**: Stream-local cache + global cache for maximum performance
- **Warmup Cache**: Pre-allocation strategy for common allocation patterns
- **Performance Monitoring**: Detailed statistics and benchmarking capabilities
- **Hybrid Allocation Strategy**: Optimized paths for small vs large allocations

#### Segment-Block Architecture

```python
@dataclass
class Block:
    """
    Individual memory block within a segment
    """
    ptr: int          # GPU pointer
    size: int         # Block size  
    is_free: bool     # Availability status
    segment_id: int   # Parent segment ID
    
class Segment:
    """
    Large contiguous memory region containing multiple blocks
    """
    def __init__(self, segment_id: int, size: int):
        # Allocate entire segment from CUDA
        self.base_ptr = _ok(cuda.cuMemAlloc(size))
        
        # Initialize memory to zero (prevents dirty data precision issues)
        _ok(cuda.cuMemsetD8(self.base_ptr, 0, size))
        
        # Start as single large free block
        self.blocks: List[Block] = [...]
        self.free_blocks_by_size: Dict[int, List[Block]] = {...}
```

### Key Features

#### 1. High-Performance Segment-Block Allocation
- **Best-Fit Algorithm**: Finds optimal block size to minimize fragmentation
- **Block Splitting**: Large blocks automatically split for exact size requests
- **Block Coalescing**: Adjacent free blocks merged to prevent fragmentation
- **Size-Based Indexing**: O(1) lookup for free blocks by size

#### 2. Two-Level Caching System
```python
class TwoLevelCache:
    """
    Sophisticated caching with stream-local and global levels
    """
    def __init__(self):
        self.stream_cache: Dict[int, Dict[int, List[int]]] = {}  # stream_id -> size -> [ptrs]
        self.global_cache: Dict[int, List[int]] = {}             # size -> [ptrs]
        self.cache_stats = CacheStatistics()
```

**Stream-Local Cache**:
- Per-stream block caching for CUDA stream efficiency
- Avoids cross-stream synchronization overhead
- Optimal for repetitive allocation patterns

**Global Cache**:
- Shared cache across all streams
- Fallback when stream-local cache misses
- Maximizes memory reuse across operations

#### 3. Warmup Cache Preallocation
```python
def warmup_cache(self, sizes: List[int], counts: List[int]):
    """
    Pre-populate cache with common allocation sizes
    
    Performance optimization for known allocation patterns:
    - Transformer attention matrices
    - Embedding lookups  
    - Gradient buffers
    """
    for size, count in zip(sizes, counts):
        for _ in range(count):
            ptr = self.allocate_segment_block(size)
            self.add_to_cache(ptr, size)
```

#### 4. Adaptive Allocation Strategy
```python
def allocate_memory(self, size: int) -> int:
    """
    Hybrid allocation strategy optimized for different size ranges
    """
    if size < self.SMALL_BLOCK_THRESHOLD:
        # Small allocations: prioritize cache hits
        return self.allocate_from_cache(size) or self.allocate_segment_block(size)
    else:
        # Large allocations: direct segment allocation
        return self.allocate_large_block(size)
```

## Performance Characteristics

### Benchmark Results (vs PyTorch)

| Scenario | Genesis Performance | Status |
|----------|-------------------|--------|
| Same-size allocations | **1.43x faster** | âœ… Excellent |
| Large memory (>1MB) | **3.92x faster** | âœ… Outstanding |
| Transformer training | **1.89x faster** | âœ… Excellent |
| Memory pressure | **4.83x faster** | âœ… Outstanding |
| Variable sizes | 0.83x (slower) | ðŸ”„ Optimization target |

### Memory Efficiency Improvements

1. **Elimination of cudaMalloc/cudaFree overhead**:
   ```python
   # Before: Direct CUDA calls (slow)
   ptr = cuda.cuMemAlloc(size)  # ~100Î¼s overhead
   
   # After: Cache-based allocation (fast)
   ptr = cache.get(size) or segment.allocate(size)  # ~1Î¼s overhead
   ```

2. **Reduced memory fragmentation**:
   - Block coalescing prevents fragmentation
   - Best-fit allocation minimizes waste
   - Segment organization improves locality

3. **Optimized for ML workloads**:
   - Warmup cache for common tensor sizes
   - Stream-aware allocation for parallel operations
   - Batch allocation support for multi-tensor operations

## Advanced Features

### 1. Performance Monitoring
```python
@dataclass
class AllocationStatistics:
    """Comprehensive allocation tracking"""
    total_allocations: int = 0
    total_freed: int = 0
    peak_memory_usage: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    fragmentation_ratio: float = 0.0
    
    def efficiency_score(self) -> float:
        """Calculate memory manager efficiency (0-1)"""
        if self.total_allocations == 0:
            return 1.0
        return self.cache_hits / self.total_allocations
```

### 2. Memory Pool Optimization
```python
class AsyncMemoryPool:
    """
    Asynchronous memory pool for high-throughput scenarios
    """
    def __init__(self, pool_size: int = 1024 * 1024 * 1024):  # 1GB default
        self.pool = MemoryPool(pool_size)
        self.allocation_queue = AsyncQueue()
        self.background_worker = Thread(target=self._allocation_worker)
        
    def allocate_async(self, size: int) -> Future[int]:
        """Non-blocking allocation for pipeline parallelism"""
        return self.allocation_queue.submit(self._allocate, size)
```

### 3. Batch Allocation Support
```python
def allocate_batch(self, sizes: List[int]) -> List[int]:
    """
    Optimized batch allocation for multi-tensor operations
    
    Advantages:
    - Reduced allocation overhead
    - Better memory locality  
    - Automatic size optimization
    """
    # Group similar sizes for efficient segment usage
    size_groups = self._group_by_size(sizes)
    
    ptrs = []
    for size_group in size_groups:
        segment = self._find_or_create_segment(size_group.total_size)
        group_ptrs = segment.allocate_batch(size_group.sizes)
        ptrs.extend(group_ptrs)
    
    return ptrs
```

## Memory Management Patterns

### 1. Transformer Training Optimization
```python
# Optimized memory allocation for transformer training
def allocate_transformer_tensors(batch_size: int, seq_len: int, hidden_size: int):
    """
    Pre-allocate common transformer tensor sizes
    """
    common_sizes = [
        batch_size * seq_len * hidden_size,      # Attention weights
        batch_size * seq_len * hidden_size * 4,  # Feed-forward
        batch_size * seq_len * seq_len,          # Attention scores
    ]
    
    # Warmup cache with expected allocation pattern
    memory_manager.warmup_cache(common_sizes, counts=[10, 5, 10])
```

### 2. Dynamic Memory Scaling
```python
def adaptive_memory_management(memory_pressure: float):
    """
    Automatically adjust cache sizes based on memory pressure
    """
    if memory_pressure > 0.8:
        # High pressure: aggressive cache cleanup
        memory_manager.cleanup_cache(threshold=0.9)
        memory_manager.enable_aggressive_coalescing()
    elif memory_pressure < 0.3:
        # Low pressure: expand cache for better performance
        memory_manager.expand_cache_size(factor=1.5)
```

## Usage Examples

### Basic Allocation
```python
from genesis.ndarray.cuda_memory_manager import get_memory_manager

# Get global memory manager instance
mm = get_memory_manager()

# Allocate GPU memory
ptr = mm.allocate_memory(1024 * 1024)  # 1MB

# Free memory (automatic caching)
mm.free_memory(ptr, 1024 * 1024)

# Check statistics
stats = mm.get_statistics()
print(f"Cache hit rate: {stats.cache_hit_rate:.2%}")
print(f"Memory efficiency: {stats.efficiency_score():.2%}")
```

### Advanced Configuration
```python
# Configure memory manager for specific workload
mm.configure(
    segment_size=512 * 1024 * 1024,    # 512MB segments
    cache_sizes={
        'stream_local': 100,            # 100 blocks per stream
        'global': 500,                  # 500 blocks global cache
    },
    warmup_sizes=[
        (4096, 50),    # 50 blocks of 4KB
        (65536, 20),   # 20 blocks of 64KB  
        (1048576, 10), # 10 blocks of 1MB
    ]
)
```

### Performance Monitoring
```python
# Enable detailed performance tracking
with mm.performance_context() as perf:
    # Run memory-intensive operations
    tensors = [genesis.randn(1000, 1000) for _ in range(100)]
    
# Analyze performance
print(f"Total allocations: {perf.stats.total_allocations}")
print(f"Peak memory: {perf.stats.peak_memory_usage / 1024**3:.2f} GB")
print(f"Fragmentation: {perf.stats.fragmentation_ratio:.2%}")
```

## Configuration and Tuning

### Environment Variables
```bash
# Memory manager configuration
export GENESIS_CUDA_SEGMENT_SIZE=1073741824     # 1GB segments
export GENESIS_CUDA_CACHE_SIZE=1000             # Cache 1000 blocks
export GENESIS_CUDA_WARMUP_ENABLED=true         # Enable warmup
export GENESIS_CUDA_STATS_ENABLED=true          # Enable statistics
```

### Runtime Configuration
```python
# Configure at runtime
genesis.cuda.configure_memory_manager({
    'segment_size': 1024 * 1024 * 1024,  # 1GB
    'enable_warmup': True,
    'enable_stats': True,
    'allocation_strategy': 'best_fit',
    'coalescing_threshold': 0.1,
})
```

## Best Practices

1. **Use Warmup Cache**: Pre-allocate common sizes for 38x performance boost
2. **Monitor Statistics**: Track cache hit rates and memory efficiency
3. **Batch Allocations**: Group similar operations for better locality
4. **Avoid Frequent Small Allocations**: Cache overhead dominates for tiny blocks
5. **Use Appropriate Segment Sizes**: Match segment size to workload memory patterns

## Troubleshooting

### Memory Leaks
```python
# Debug memory leaks
stats = mm.get_statistics()
if stats.total_allocations > stats.total_freed + 1000:
    print("Warning: Potential memory leak detected")
    mm.dump_allocation_trace()
```

### Performance Issues
```python
# Diagnose performance problems
if stats.cache_hit_rate < 0.5:
    print("Low cache hit rate - consider warmup cache")
    mm.analyze_allocation_patterns()

if stats.fragmentation_ratio > 0.3:
    print("High fragmentation - enable aggressive coalescing")
    mm.enable_aggressive_coalescing()
```

### Memory Pressure
```python
# Handle memory pressure
def handle_oom():
    """Out of memory handler"""
    mm.cleanup_cache(force=True)
    mm.coalesce_free_blocks()
    mm.garbage_collect()
```

## Integration with Genesis

The memory manager integrates seamlessly with Genesis tensors and operations:

```python
# Automatic integration with tensor operations
x = genesis.randn(1000, 1000)  # Uses memory manager automatically
y = genesis.matmul(x, x)       # Efficient memory reuse
z = x + y                      # Cache-optimized allocation
```

This sophisticated memory management system is a key factor in Genesis achieving near-PyTorch performance while maintaining the educational clarity of a from-scratch implementation.