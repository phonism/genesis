# é«˜çº§è®­ç»ƒç‰¹æ€§

Genesisæä¾›äº†å¤šä¸ªé«˜çº§ç‰¹æ€§æ¥æå‡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚

## ğŸš€ æ··åˆç²¾åº¦è®­ç»ƒ (AMP)

è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰å…è®¸ä½ åœ¨é€‚å½“çš„åœ°æ–¹ä½¿ç”¨FP16/BF16è®¡ç®—æ¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶é™ä½å†…å­˜ä½¿ç”¨ï¼Œå¹¶é€šè¿‡ç»´æŒFP32ä¸»æƒé‡æ¥ä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚

### åŸºæœ¬ç”¨æ³•

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim
from genesis.amp import autocast, GradScaler

# åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
model = nn.Linear(1024, 512)
optimizer = optim.Adam(model.parameters())

# ä¸ºæ··åˆç²¾åº¦åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨
scaler = GradScaler()

# ä½¿ç”¨AMPçš„è®­ç»ƒå¾ªç¯
for data, target in dataloader:
    optimizer.zero_grad()
    
    # ä½¿ç”¨autocastè¿›è¡Œè‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        output = model(data)
        loss = criterion(output, target)
    
    # ç¼©æ”¾æŸå¤±å¹¶è¿›è¡Œåå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    # åç¼©æ”¾å¹¶æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤
    scaler.step(optimizer)
    scaler.update()
```

### æ”¯æŒçš„æ•°æ®ç±»å‹

Genesisæ”¯æŒå¤šç§ç²¾åº¦æ ¼å¼ï¼š

- **float16 (FP16)**: åŠç²¾åº¦ï¼Œåœ¨å¤§å¤šæ•°GPUä¸Šæœ€å¿«
- **bfloat16 (BF16)**: è„‘æµ®ç‚¹æ•°ï¼Œæ¯”FP16æœ‰æ›´å¥½çš„æ•°å€¼èŒƒå›´
- **float32 (FP32)**: å•ç²¾åº¦ï¼Œä¸»æƒé‡çš„é»˜è®¤ç±»å‹

### ä¼˜åŠ¿

- **é€Ÿåº¦**: åœ¨ç°ä»£GPUä¸Šè®­ç»ƒé€Ÿåº¦æå‡é«˜è¾¾2å€
- **å†…å­˜**: å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œå…è®¸æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
- **ç²¾åº¦**: é€šè¿‡æŸå¤±ç¼©æ”¾ä¿æŒæ¨¡å‹ç²¾åº¦

## âœ‚ï¸ æ¢¯åº¦è£å‰ª

æ¢¯åº¦è£å‰ªæœ‰åŠ©äºé˜²æ­¢æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦çˆ†ç‚¸ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œç‰¹åˆ«æ˜¯å¯¹äºRNNå’ŒTransformerã€‚

### æ¢¯åº¦èŒƒæ•°è£å‰ª

å½“æ¢¯åº¦çš„L2èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶è¿›è¡Œè£å‰ªï¼š

```python
import genesis.nn.utils as nn_utils

# è®­ç»ƒè¿‡ç¨‹ä¸­
loss.backward()

# æŒ‰èŒƒæ•°è£å‰ªæ¢¯åº¦ï¼ˆå¤§å¤šæ•°æƒ…å†µæ¨èï¼‰
nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

optimizer.step()
```

### æ¢¯åº¦å€¼è£å‰ª

å°†æ¢¯åº¦å€¼è£å‰ªåˆ°ç‰¹å®šèŒƒå›´ï¼š

```python
# æŒ‰å€¼è£å‰ªæ¢¯åº¦
nn_utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

### ä½•æ—¶ä½¿ç”¨

- **æ¢¯åº¦èŒƒæ•°è£å‰ª**: æ¨èç”¨äºRNNã€LSTMå’ŒTransformer
- **æ¢¯åº¦å€¼è£å‰ª**: å½“éœ€è¦å¯¹æ¢¯åº¦å€¼è¿›è¡Œç¡¬é™åˆ¶æ—¶æœ‰ç”¨
- **å…¸å‹å€¼**: å¤§å¤šæ•°æ¨¡å‹çš„max_normåœ¨0.5åˆ°5.0ä¹‹é—´

## ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦å™¨

å­¦ä¹ ç‡è°ƒåº¦å™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡ï¼Œä»¥æ”¹å–„æ”¶æ•›æ€§å’Œæœ€ç»ˆæ¨¡å‹æ€§èƒ½ã€‚

### StepLR

æ¯step_sizeä¸ªepochå°†å­¦ä¹ ç‡è¡°å‡gammaå€ï¼š

```python
from genesis.optim.lr_scheduler import StepLR

optimizer = optim.Adam(model.parameters(), lr=0.1)
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

for epoch in range(100):
    train(...)
    scheduler.step()  # æ¯30ä¸ªepochè¡°å‡å­¦ä¹ ç‡
```

### ExponentialLR

æŒ‡æ•°è¡°å‡å­¦ä¹ ç‡ï¼š

```python
from genesis.optim.lr_scheduler import ExponentialLR

scheduler = ExponentialLR(optimizer, gamma=0.95)

for epoch in range(100):
    train(...)
    scheduler.step()  # æ¯ä¸ªepochå­¦ä¹ ç‡ = å­¦ä¹ ç‡ * 0.95
```

### CosineAnnealingLR

ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦ï¼š

```python
from genesis.optim.lr_scheduler import CosineAnnealingLR

# T_max: æœ€å¤§è¿­ä»£æ¬¡æ•°
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)

for epoch in range(100):
    train(...)
    scheduler.step()
```

### è‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

ä½ ä¹Ÿå¯ä»¥å®ç°è‡ªå®šä¹‰è°ƒåº¦ï¼š

```python
def custom_lr_lambda(epoch):
    # å‰10ä¸ªepoché¢„çƒ­ï¼Œç„¶åè¡°å‡
    if epoch < 10:
        return epoch / 10
    else:
        return 0.95 ** (epoch - 10)

scheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)
```

## ğŸ’¾ æ£€æŸ¥ç‚¹

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å’Œæ¢å¤æ¨¡å‹çŠ¶æ€ï¼Œä»¥å®ç°å®¹é”™å’Œæ¨¡å‹éƒ¨ç½²ã€‚

### ä¿å­˜æ£€æŸ¥ç‚¹

```python
import genesis

# ä¿å­˜æ¨¡å‹çŠ¶æ€
genesis.save_checkpoint({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'best_accuracy': best_acc
}, 'checkpoint_epoch_10.pth')
```

### åŠ è½½æ£€æŸ¥ç‚¹

```python
# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = genesis.load_checkpoint('checkpoint_epoch_10.pth')

# æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

### æœ€ä½³å®è·µ

1. **å®šæœŸä¿å­˜**: æ¯Nä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
2. **æœ€ä½³æ¨¡å‹è·Ÿè¸ª**: ä¿ç•™æ€§èƒ½æœ€å¥½çš„æ¨¡å‹
3. **å…ƒæ•°æ®å­˜å‚¨**: åŒ…å«è®­ç»ƒé…ç½®å’ŒæŒ‡æ ‡

```python
# ç¤ºä¾‹ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æœ€ä½³æ¨¡å‹
best_loss = float('inf')

for epoch in range(num_epochs):
    val_loss = validate(model, val_loader)
    
    if val_loss < best_loss:
        best_loss = val_loss
        genesis.save_checkpoint({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'best_loss': best_loss
        }, 'best_model.pth')
```

## ğŸ”§ å®Œæ•´è®­ç»ƒç¤ºä¾‹

ä»¥ä¸‹æ˜¯ç»“åˆæ‰€æœ‰é«˜çº§ç‰¹æ€§çš„å®Œæ•´ç¤ºä¾‹ï¼š

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim
from genesis.amp import autocast, GradScaler
from genesis.optim.lr_scheduler import CosineAnnealingLR
import genesis.nn.utils as nn_utils

# æ¨¡å‹è®¾ç½®
model = YourModel()
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = CosineAnnealingLR(optimizer, T_max=100)
scaler = GradScaler()

# è®­ç»ƒé…ç½®
max_grad_norm = 1.0
checkpoint_interval = 10

for epoch in range(num_epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast():
            output = model(data)
            loss = criterion(output, target)
        
        # ç¼©æ”¾çš„åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        # æ¢¯åº¦è£å‰ª
        scaler.unscale_(optimizer)
        nn_utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # å¸¦ç¼©æ”¾çš„ä¼˜åŒ–å™¨æ­¥éª¤
        scaler.step(optimizer)
        scaler.update()
    
    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    if epoch % checkpoint_interval == 0:
        genesis.save_checkpoint({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
        }, f'checkpoint_epoch_{epoch}.pth')
```

## ğŸ“Š æ€§èƒ½æç¤º

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯è·å¾—æ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
- ä¸ºéå¸¸æ·±çš„æ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå‡å°‘å†…å­˜ä½¿ç”¨

### é€Ÿåº¦ä¼˜åŒ–
- ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹ï¼ˆFP16ç”¨äºé€Ÿåº¦ï¼ŒBF16ç”¨äºç¨³å®šæ€§ï¼‰
- è°ƒæ•´æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- åˆ†æè®­ç»ƒå¾ªç¯ä»¥è¯†åˆ«ç“¶é¢ˆ

### æ”¶æ•›æŠ€å·§
- ä»å­¦ä¹ ç‡æŸ¥æ‰¾å™¨å¼€å§‹è¯†åˆ«æœ€ä¼˜å­¦ä¹ ç‡
- å¯¹å¤§æ‰¹æ¬¡è®­ç»ƒä½¿ç”¨é¢„çƒ­
- ç›‘æ§æ¢¯åº¦èŒƒæ•°ä»¥æ—©æœŸæ£€æµ‹ä¸ç¨³å®šæ€§

## ğŸ”— ç›¸å…³ä¸»é¢˜

- [åŸºç¡€è®­ç»ƒæ•™ç¨‹](../tutorials/basic-training.md)
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](../tutorials/performance-tuning.md)
- [æ¨¡å‹æ¶æ„æŒ‡å—](../core-components/index.md)
- [ä¼˜åŒ–å™¨æ–‡æ¡£](../api/optim/optimizers.md)