# æ“ä½œç³»ç»Ÿæ¦‚è¿°

Genesis v2.0å…·æœ‰ç»Ÿä¸€çš„æ“ä½œåˆ†å‘ç³»ç»Ÿï¼Œåœ¨ä¿æŒä¸€è‡´APIçš„åŒæ—¶å°†å¼ é‡æ“ä½œè·¯ç”±åˆ°é€‚å½“çš„åç«¯å®ç°ã€‚

## ğŸ—ï¸ æ¶æ„

æ“ä½œç³»ç»Ÿåœ¨é¢å‘ç”¨æˆ·çš„æ“ä½œå’Œç‰¹å®šåç«¯å®ç°ä¹‹é—´æä¾›æ¸…æ™°çš„æŠ½è±¡å±‚ï¼š

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ“ä½œ"
        A[genesis.add] --> B[æ“ä½œè°ƒç”¨]
        C[genesis.matmul] --> B
        D[genesis.softmax] --> B
    end

    subgraph "åˆ†å‘å±‚"
        B --> E[ops/dispatcher.py]
        E --> F[è®¾å¤‡æ£€æµ‹]
        E --> G[åç«¯é€‰æ‹©]
        E --> H[æ“ä½œè·¯ç”±]
    end

    subgraph "åç«¯æ“ä½œ"
        H --> I[ops/cpu/basic.py]
        H --> J[ops/cuda/basic.py]
        H --> K[ops/cpu/reduction.py]
        H --> L[ops/cuda/reduction.py]
    end

    subgraph "å®ç°"
        I --> M[PyTorchæ“ä½œ]
        J --> N[Tritonå†…æ ¸]
        J --> O[CUDAå†…æ ¸]
    end

    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style H fill:#fff3e0
    style I fill:#e8f5e9
    style J fill:#ffeb3b
```

## ğŸ¯ å…³é”®ç»„ä»¶

### ä¸­å¤®åˆ†å‘å™¨
`ops/dispatcher.py`æ¨¡å—ä½œä¸ºä¸­å¤®è·¯ç”±ä¸­å¿ƒï¼š
- **æ“ä½œæ³¨å†Œ**ï¼šå°†æ“ä½œåç§°æ˜ å°„åˆ°å®ç°
- **è®¾å¤‡æ£€æµ‹**ï¼šè‡ªåŠ¨ç¡®å®šç›®æ ‡è®¾å¤‡
- **åç«¯é€‰æ‹©**ï¼šè·¯ç”±åˆ°é€‚å½“çš„åç«¯
- **ç±»å‹éªŒè¯**ï¼šç¡®ä¿ç±»å‹å…¼å®¹æ€§

### æ“ä½œç±»åˆ«

#### åŸºç¡€æ“ä½œ
- **ä½ç½®**ï¼š`ops/{backend}/basic.py`
- **æ“ä½œ**ï¼šaddã€subtractã€multiplyã€divideã€powerã€abs
- **ç‰¹æ€§**ï¼šå¹¿æ’­æ”¯æŒã€åŸåœ°å˜ä½“

#### è§„çº¦æ“ä½œ
- **ä½ç½®**ï¼š`ops/{backend}/reduction.py`
- **æ“ä½œ**ï¼šsumã€meanã€maxã€minã€argmaxã€argmin
- **ç‰¹æ€§**ï¼šå¤šç»´è§„çº¦ã€keepdimæ”¯æŒ

#### çŸ©é˜µæ“ä½œ
- **ä½ç½®**ï¼š`ops/{backend}/matrix.py`
- **æ“ä½œ**ï¼šmatmulã€transposeã€reshapeã€flatten
- **ç‰¹æ€§**ï¼šæ‰¹å¤„ç†æ“ä½œã€å†…å­˜é«˜æ•ˆå®ç°

#### æ¿€æ´»å‡½æ•°
- **ä½ç½®**ï¼š`ops/{backend}/activation.py`
- **æ“ä½œ**ï¼šreluã€sigmoidã€tanhã€softmaxã€gelu
- **ç‰¹æ€§**ï¼šåŸåœ°è®¡ç®—ã€æ¢¯åº¦å‹å¥½å®ç°

## ğŸš€ åˆ†å‘æœºåˆ¶

### æ“ä½œæ³¨å†Œ
```python
# ops/dispatcher.py
class OperationDispatcher:
    """ä¸­å¤®æ“ä½œåˆ†å‘å™¨ã€‚"""

    def __init__(self):
        self._operations = {}
        self._register_default_operations()

    def register(self, name, cpu_impl, cuda_impl=None):
        """æ³¨å†Œæ“ä½œå®ç°ã€‚"""
        self._operations[name] = {
            'cpu': cpu_impl,
            'cuda': cuda_impl or cpu_impl
        }

    def dispatch(self, op_name, *args, **kwargs):
        """å°†æ“ä½œåˆ†å‘åˆ°é€‚å½“çš„åç«¯ã€‚"""
        # ä»å‚æ•°ç¡®å®šè®¾å¤‡
        device = self._infer_device(*args)

        # é€‰æ‹©å®ç°
        impl = self._operations[op_name][device.type]

        # æ‰§è¡Œæ“ä½œ
        return impl(*args, **kwargs)
```

### è‡ªåŠ¨è®¾å¤‡æ¨æ–­
```python
def _infer_device(*tensors):
    """ä»å¼ é‡å‚æ•°è‡ªåŠ¨æ¨æ–­ç›®æ ‡è®¾å¤‡ã€‚"""
    devices = set()

    for tensor in tensors:
        if hasattr(tensor, 'device'):
            devices.add(tensor.device)

    if len(devices) == 0:
        return genesis.device('cpu')  # é»˜è®¤
    elif len(devices) == 1:
        return devices.pop()
    else:
        raise RuntimeError(f"ä¸æ”¯æŒæ··åˆè®¾å¤‡ï¼š{devices}")
```

## ğŸ’» åç«¯å®ç°

### CPUæ“ä½œ
CPUæ“ä½œåˆ©ç”¨PyTorchçš„ä¼˜åŒ–å®ç°ï¼š

```python
# ops/cpu/basic.py
def cpu_add(a, b, out=None):
    """åŠ æ³•çš„CPUå®ç°ã€‚"""
    result = torch.add(a.data, b.data)

    if out is not None:
        out.data.copy_(result)
        return out
    else:
        return genesis.tensor(result, device=a.device)

def cpu_matmul(a, b):
    """çŸ©é˜µä¹˜æ³•çš„CPUå®ç°ã€‚"""
    result = torch.matmul(a.data, b.data)
    return genesis.tensor(result, device=a.device)
```

### CUDAæ“ä½œ
CUDAæ“ä½œä½¿ç”¨è‡ªå®šä¹‰Tritonå†…æ ¸ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼š

```python
# ops/cuda/basic.py
import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    """ç”¨äºé€å…ƒç´ åŠ æ³•çš„Tritonå†…æ ¸ã€‚"""
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)

def cuda_add(a, b):
    """ä½¿ç”¨Tritonå†…æ ¸çš„CUDAå®ç°ã€‚"""
    output = genesis.empty_like(a)
    n_elements = a.numel()

    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    add_kernel[grid](a.data_ptr(), b.data_ptr(), output.data_ptr(), n_elements)

    return output
```

## ğŸ”§ é…ç½®å’Œæ‰©å±•

### æ³¨å†Œè‡ªå®šä¹‰æ“ä½œ
```python
import genesis

# å®šä¹‰è‡ªå®šä¹‰æ“ä½œ
def my_custom_op_cpu(x):
    """è‡ªå®šä¹‰CPUæ“ä½œã€‚"""
    return x * 2 + 1

def my_custom_op_cuda(x):
    """è‡ªå®šä¹‰CUDAæ“ä½œã€‚"""
    # è‡ªå®šä¹‰CUDAå®ç°
    pass

# å‘åˆ†å‘å™¨æ³¨å†Œ
genesis.ops.register_operation(
    'my_custom_op',
    cpu_impl=my_custom_op_cpu,
    cuda_impl=my_custom_op_cuda
)

# ä½¿ç”¨æ“ä½œ
result = genesis.ops.my_custom_op(tensor)
```

### æ“ä½œå…ƒæ•°æ®
```python
# å‘æ“ä½œæ·»åŠ å…ƒæ•°æ®
genesis.ops.set_operation_metadata('matmul', {
    'requires_grad': True,
    'supports_autograd': True,
    'memory_efficient': True,
    'fused_variants': ['matmul_add', 'matmul_relu']
})
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### æ“ä½œèåˆ
åˆ†å‘å™¨æ”¯æŒæ“ä½œèåˆä»¥æé«˜æ€§èƒ½ï¼š

```python
# è‡ªåŠ¨èåˆæ£€æµ‹
def detect_fusion_opportunities(operations):
    """æ£€æµ‹å¯ä»¥èåˆçš„æ“ä½œã€‚"""
    fusion_patterns = [
        ('matmul', 'add'),      # çŸ©é˜µä¹˜æ³• + åç½®
        ('conv2d', 'relu'),     # å·ç§¯ + æ¿€æ´»
        ('add', 'relu'),        # åŠ æ³• + æ¿€æ´»
    ]

    for pattern in fusion_patterns:
        if matches_pattern(operations, pattern):
            return create_fused_operation(pattern)

    return None

# èåˆæ“ä½œå®ç°
@triton.jit
def fused_matmul_add_kernel(a_ptr, b_ptr, bias_ptr, output_ptr, ...):
    """èåˆçš„çŸ©é˜µä¹˜æ³•å’ŒåŠ æ³•ã€‚"""
    # åœ¨å•ä¸ªå†…æ ¸ä¸­è®¡ç®—matmulå’Œæ·»åŠ åç½®
    pass
```

### å†…æ ¸ç¼“å­˜
```python
# å†…æ ¸ç¼–è¯‘ç¼“å­˜
class KernelCache:
    """ç¼“å­˜ç¼–è¯‘çš„å†…æ ¸ä»¥ä¾›é‡ç”¨ã€‚"""

    def __init__(self):
        self._cache = {}

    def get_kernel(self, op_name, input_shapes, dtypes):
        """è·å–ç¼“å­˜çš„å†…æ ¸æˆ–ç¼–è¯‘æ–°å†…æ ¸ã€‚"""
        cache_key = (op_name, input_shapes, dtypes)

        if cache_key not in self._cache:
            kernel = self._compile_kernel(op_name, input_shapes, dtypes)
            self._cache[cache_key] = kernel

        return self._cache[cache_key]
```

## ğŸ” è°ƒè¯•å’Œåˆ†æ

### æ“ä½œè·Ÿè¸ª
```python
# å¯ç”¨æ“ä½œè·Ÿè¸ª
genesis.ops.enable_tracing(True)

# æ“ä½œç°åœ¨å°†è¢«è·Ÿè¸ª
x = genesis.tensor([1, 2, 3])
y = genesis.tensor([4, 5, 6])
z = x + y  # è·Ÿè¸ªï¼š"add: cpu, shapes=[(3,), (3,)], time=0.05ms"

# è·å–è·Ÿè¸ªæ‘˜è¦
trace = genesis.ops.get_trace()
print(trace.summary())
```

### æ€§èƒ½åˆ†æ
```python
# åˆ†ææ“ä½œ
with genesis.ops.profile() as prof:
    # è¿™é‡Œæ˜¯ä½ çš„æ“ä½œ
    result = genesis.matmul(a, b)
    result = genesis.relu(result)

# åˆ†æç»“æœ
prof.print_stats()  # æ˜¾ç¤ºæŒ‰æ“ä½œçš„æ—¶é—´åˆ†è§£
prof.export_chrome_trace("ops_profile.json")  # Chromeåˆ†æå™¨æ ¼å¼
```

## ğŸ”— å‚è§

- [æ“ä½œåˆ†å‘å™¨](dispatcher.md)
- [CPUæ“ä½œ](cpu-ops.md)
- [CUDAæ“ä½œ](cuda-ops.md)
- [åç«¯ç³»ç»Ÿ](../backends/index.md)
- [æ€§èƒ½æŒ‡å—](../performance/optimization-guide.md)