# CUDAæ“ä½œ

CUDAæ“ä½œå®ç°æä¾›äº†ä½¿ç”¨Tritonå’Œè‡ªå®šä¹‰CUDAå†…æ ¸çš„é«˜æ€§èƒ½GPUæ“ä½œã€‚

## ğŸ“‹ æ¦‚è¿°

CUDAæ“ä½œé€šè¿‡è‡ªå®šä¹‰å†…æ ¸ä¼˜åŒ–ï¼Œå®ç°æœ€ä½³GPUæ€§èƒ½ã€‚

## ğŸ¯ Tritonå†…æ ¸

### é€å…ƒç´ æ“ä½œ
```python
@triton.jit
def elementwise_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
```

### è§„çº¦å†…æ ¸
```python
@triton.jit
def reduction_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    data = tl.load(input_ptr + offsets, mask=mask, other=0.0)
    result = tl.sum(data)
    tl.store(output_ptr + pid, result)
```

## ğŸš€ ä¼˜åŒ–ç‰¹æ€§

- è‡ªåŠ¨è°ƒä¼˜
- å†…æ ¸èåˆ
- å…±äº«å†…å­˜åˆ©ç”¨
- çº¿ç¨‹å—ä¼˜åŒ–

## ğŸ”— å‚è§

- [æ“ä½œç³»ç»Ÿæ¦‚è¿°](index.md)
- [CPUæ“ä½œ](cpu-ops.md)