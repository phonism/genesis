# æ“ä½œåˆ†å‘å™¨

æ“ä½œåˆ†å‘å™¨æ˜¯Genesis v2.0çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å°†å¼ é‡æ“ä½œè·¯ç”±åˆ°é€‚å½“çš„åç«¯å®ç°ã€‚

## ğŸ“‹ æ¦‚è¿°

åˆ†å‘å™¨æä¾›ï¼š
- é›†ä¸­çš„æ“ä½œè·¯ç”±
- è‡ªåŠ¨åç«¯é€‰æ‹©
- æ“ä½œæ³¨å†Œå’Œç®¡ç†
- æ€§èƒ½ä¼˜åŒ–æœºä¼š

## ğŸ—ï¸ æ¶æ„

```mermaid
graph TB
    subgraph "åˆ†å‘å™¨ç»„ä»¶"
        A[OperationDispatcher] --> B[æ“ä½œæ³¨å†Œè¡¨]
        A --> C[è®¾å¤‡æ¨æ–­]
        A --> D[åç«¯é€‰æ‹©å™¨]
        A --> E[æ‰§è¡Œå¼•æ“]
    end

    subgraph "æ“ä½œæµç¨‹"
        F[ç”¨æˆ·è°ƒç”¨] --> G[åˆ†å‘å™¨]
        G --> H[è®¾å¤‡æ£€æµ‹]
        H --> I[é€‰æ‹©å®ç°]
        I --> J[æ‰§è¡Œæ“ä½œ]
        J --> K[è¿”å›ç»“æœ]
    end

    style A fill:#e1f5fe
    style G fill:#f3e5f5
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### æ“ä½œåˆ†å‘å™¨ç±»
```python
class OperationDispatcher:
    """ä¸­å¤®æ“ä½œåˆ†å‘ç³»ç»Ÿã€‚"""

    def __init__(self):
        self._operations = {}
        self._metadata = {}
        self._cache = {}

    def register(self, name, implementations):
        """æ³¨å†Œæ–°æ“ä½œã€‚"""
        self._operations[name] = implementations

    def dispatch(self, op_name, *args, **kwargs):
        """åˆ†å‘æ“ä½œåˆ°åç«¯ã€‚"""
        # 1. éªŒè¯æ“ä½œå­˜åœ¨
        if op_name not in self._operations:
            raise ValueError(f"æœªçŸ¥æ“ä½œï¼š{op_name}")

        # 2. æ¨æ–­è®¾å¤‡
        device = self._infer_device(args)

        # 3. é€‰æ‹©å®ç°
        impl = self._select_implementation(op_name, device)

        # 4. æ‰§è¡Œæ“ä½œ
        return impl(*args, **kwargs)
```

### è®¾å¤‡æ¨æ–­
```python
def _infer_device(self, args):
    """ä»å‚æ•°æ¨æ–­ç›®æ ‡è®¾å¤‡ã€‚"""
    devices = []

    for arg in args:
        if hasattr(arg, 'device'):
            devices.append(arg.device)

    if not devices:
        # ä½¿ç”¨é»˜è®¤è®¾å¤‡
        return genesis.get_default_device()

    # æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
    unique_devices = set(str(d) for d in devices)
    if len(unique_devices) > 1:
        # è®¾å¤‡æå‡è§„åˆ™
        if 'cuda' in str(unique_devices):
            return genesis.device('cuda')
        else:
            raise RuntimeError(f"è®¾å¤‡å†²çªï¼š{unique_devices}")

    return devices[0]
```

## ğŸ’¡ æ“ä½œæ³¨å†Œ

### åŸºæœ¬æ³¨å†Œ
```python
# æ³¨å†Œç®€å•æ“ä½œ
dispatcher = OperationDispatcher()

dispatcher.register('add', {
    'cpu': cpu_add_impl,
    'cuda': cuda_add_impl
})

# ä½¿ç”¨æ³¨å†Œçš„æ“ä½œ
result = dispatcher.dispatch('add', x, y)
```

### å¸¦å…ƒæ•°æ®çš„æ³¨å†Œ
```python
# æ³¨å†Œå¸¦é¢å¤–ä¿¡æ¯çš„æ“ä½œ
dispatcher.register_with_metadata('matmul', {
    'implementations': {
        'cpu': cpu_matmul,
        'cuda': cuda_matmul
    },
    'supports_autograd': True,
    'memory_intensive': True,
    'fusion_candidates': ['add', 'relu']
})
```

### åŠ¨æ€æ³¨å†Œ
```python
def register_dynamic_operation(name, generator):
    """åŠ¨æ€ç”Ÿæˆæ“ä½œå®ç°ã€‚"""

    def dynamic_dispatcher(*args, **kwargs):
        # åŸºäºè¾“å…¥åŠ¨æ€ç”Ÿæˆå®ç°
        impl = generator(args, kwargs)
        return impl(*args, **kwargs)

    dispatcher.register(name, {
        'cpu': dynamic_dispatcher,
        'cuda': dynamic_dispatcher
    })
```

## ğŸš€ ä¼˜åŒ–ç­–ç•¥

### æ“ä½œç¼“å­˜
```python
class CachedDispatcher(OperationDispatcher):
    """å¸¦ç»“æœç¼“å­˜çš„åˆ†å‘å™¨ã€‚"""

    def dispatch(self, op_name, *args, **kwargs):
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(op_name, args)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._cache:
            return self._cache[cache_key]

        # æ‰§è¡Œå¹¶ç¼“å­˜
        result = super().dispatch(op_name, *args, **kwargs)
        self._cache[cache_key] = result

        return result

    def _generate_cache_key(self, op_name, args):
        """ä¸ºæ“ä½œç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®ã€‚"""
        # åŸºäºæ“ä½œå’Œè¾“å…¥å½¢çŠ¶/ç±»å‹çš„é”®
        shapes = tuple(arg.shape for arg in args if hasattr(arg, 'shape'))
        dtypes = tuple(arg.dtype for arg in args if hasattr(arg, 'dtype'))
        return (op_name, shapes, dtypes)
```

### æ“ä½œèåˆ
```python
class FusionDispatcher(OperationDispatcher):
    """æ”¯æŒæ“ä½œèåˆçš„åˆ†å‘å™¨ã€‚"""

    def __init__(self):
        super().__init__()
        self._fusion_patterns = []

    def register_fusion_pattern(self, pattern, fused_impl):
        """æ³¨å†Œèåˆæ¨¡å¼ã€‚"""
        self._fusion_patterns.append({
            'pattern': pattern,
            'implementation': fused_impl
        })

    def dispatch_sequence(self, operations):
        """åˆ†å‘æ“ä½œåºåˆ—ï¼Œå¯èƒ½è¿›è¡Œèåˆã€‚"""
        # æ£€æŸ¥èåˆæœºä¼š
        for fusion in self._fusion_patterns:
            if self._matches_pattern(operations, fusion['pattern']):
                return fusion['implementation'](*operations)

        # æ— èåˆï¼Œé¡ºåºæ‰§è¡Œ
        results = []
        for op in operations:
            results.append(self.dispatch(op.name, *op.args))
        return results
```

### æ‰¹é‡åˆ†å‘
```python
def batch_dispatch(self, operations):
    """æ‰¹é‡åˆ†å‘å¤šä¸ªæ“ä½œã€‚"""
    # æŒ‰è®¾å¤‡åˆ†ç»„æ“ä½œ
    device_groups = {}
    for op in operations:
        device = self._infer_device(op.args)
        if device not in device_groups:
            device_groups[device] = []
        device_groups[device].append(op)

    # å¹¶è¡Œæ‰§è¡Œæ¯ä¸ªè®¾å¤‡ç»„
    results = {}
    for device, ops in device_groups.items():
        if device.is_cuda:
            # GPUæ“ä½œå¯ä»¥å¼‚æ­¥æ‰§è¡Œ
            stream = genesis.cuda.Stream()
            with genesis.cuda.stream(stream):
                for op in ops:
                    results[op] = self.dispatch(op.name, *op.args)
        else:
            # CPUæ“ä½œé¡ºåºæ‰§è¡Œ
            for op in ops:
                results[op] = self.dispatch(op.name, *op.args)

    return results
```

## ğŸ”§ é…ç½®é€‰é¡¹

### å…¨å±€é…ç½®
```python
# é…ç½®åˆ†å‘å™¨è¡Œä¸º
genesis.ops.dispatcher.set_config({
    'enable_fusion': True,
    'cache_size': 1000,
    'profile_operations': False,
    'strict_device_checking': True
})
```

### æ“ä½œç‰¹å®šé…ç½®
```python
# ä¸ºç‰¹å®šæ“ä½œè®¾ç½®é…ç½®
genesis.ops.dispatcher.configure_operation('matmul', {
    'use_cublas': True,
    'transpose_threshold': 1024,
    'block_size': 256
})
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æ“ä½œç»Ÿè®¡
```python
class ProfilingDispatcher(OperationDispatcher):
    """å¸¦æ€§èƒ½åˆ†æçš„åˆ†å‘å™¨ã€‚"""

    def __init__(self):
        super().__init__()
        self._stats = {}

    def dispatch(self, op_name, *args, **kwargs):
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.perf_counter()

        # æ‰§è¡Œæ“ä½œ
        result = super().dispatch(op_name, *args, **kwargs)

        # è®°å½•ç»Ÿè®¡
        elapsed = time.perf_counter() - start_time
        if op_name not in self._stats:
            self._stats[op_name] = {
                'count': 0,
                'total_time': 0,
                'max_time': 0,
                'min_time': float('inf')
            }

        stats = self._stats[op_name]
        stats['count'] += 1
        stats['total_time'] += elapsed
        stats['max_time'] = max(stats['max_time'], elapsed)
        stats['min_time'] = min(stats['min_time'], elapsed)

        return result

    def print_stats(self):
        """æ‰“å°æ“ä½œç»Ÿè®¡ã€‚"""
        for op_name, stats in self._stats.items():
            avg_time = stats['total_time'] / stats['count']
            print(f"{op_name}:")
            print(f"  è°ƒç”¨æ¬¡æ•°ï¼š{stats['count']}")
            print(f"  å¹³å‡æ—¶é—´ï¼š{avg_time*1000:.3f} ms")
            print(f"  æœ€å¤§æ—¶é—´ï¼š{stats['max_time']*1000:.3f} ms")
            print(f"  æœ€å°æ—¶é—´ï¼š{stats['min_time']*1000:.3f} ms")
```

### ç“¶é¢ˆæ£€æµ‹
```python
def detect_bottlenecks(self):
    """æ£€æµ‹æ€§èƒ½ç“¶é¢ˆã€‚"""
    bottlenecks = []

    for op_name, stats in self._stats.items():
        avg_time = stats['total_time'] / stats['count']

        # æ£€æŸ¥æ…¢æ“ä½œ
        if avg_time > 0.1:  # 100msé˜ˆå€¼
            bottlenecks.append({
                'operation': op_name,
                'avg_time': avg_time,
                'suggestion': 'è€ƒè™‘ä¼˜åŒ–æˆ–èåˆ'
            })

        # æ£€æŸ¥é¢‘ç¹æ“ä½œ
        if stats['count'] > 1000:
            bottlenecks.append({
                'operation': op_name,
                'count': stats['count'],
                'suggestion': 'è€ƒè™‘ç¼“å­˜ç»“æœ'
            })

    return bottlenecks
```

## ğŸ” è°ƒè¯•åŠŸèƒ½

### æ“ä½œæ—¥å¿—
```python
class DebugDispatcher(OperationDispatcher):
    """å¸¦è°ƒè¯•æ—¥å¿—çš„åˆ†å‘å™¨ã€‚"""

    def dispatch(self, op_name, *args, **kwargs):
        # è®°å½•è¾“å…¥
        print(f"[DISPATCH] æ“ä½œï¼š{op_name}")
        for i, arg in enumerate(args):
            if hasattr(arg, 'shape'):
                print(f"  å‚æ•°{i}ï¼šshape={arg.shape}, dtype={arg.dtype}")

        # æ‰§è¡Œæ“ä½œ
        result = super().dispatch(op_name, *args, **kwargs)

        # è®°å½•è¾“å‡º
        if hasattr(result, 'shape'):
            print(f"  ç»“æœï¼šshape={result.shape}, dtype={result.dtype}")

        return result
```

### éªŒè¯æ¨¡å¼
```python
def enable_validation_mode(self):
    """å¯ç”¨æ“ä½œéªŒè¯ã€‚"""
    self._validation_enabled = True

    def validated_dispatch(op_name, *args, **kwargs):
        # éªŒè¯è¾“å…¥
        self._validate_inputs(op_name, args)

        # æ‰§è¡Œæ“ä½œ
        result = self._original_dispatch(op_name, *args, **kwargs)

        # éªŒè¯è¾“å‡º
        self._validate_output(op_name, result)

        return result

    self._original_dispatch = self.dispatch
    self.dispatch = validated_dispatch
```

## ğŸ”— å‚è§

- [æ“ä½œç³»ç»Ÿæ¦‚è¿°](index.md)
- [CPUæ“ä½œ](cpu-ops.md)
- [CUDAæ“ä½œ](cuda-ops.md)
- [æ€§èƒ½ä¼˜åŒ–](../performance/optimization-guide.md)