# æ¶æ„æ¦‚è§ˆ

Genesisæ·±åº¦å­¦ä¹ æ¡†æ¶é‡‡ç”¨äº†åˆ†å±‚æ¨¡å—åŒ–æ¶æ„è®¾è®¡ï¼Œåœ¨ä¿æŒä»£ç æ¸…æ™°æ€§çš„åŒæ—¶å®ç°é«˜æ€§èƒ½è®¡ç®—èƒ½åŠ›ã€‚

## ğŸ¯ è®¾è®¡åŸåˆ™

1. **æ¸…æ™°çš„å±‚æ¬¡åˆ†ç¦»**: æ¯ä¸€å±‚éƒ½æœ‰å•ä¸€ã€æ˜ç¡®çš„èŒè´£
2. **æ— æ³„æ¼çš„æŠ½è±¡**: ä¸Šå±‚ä¸äº†è§£ä¸‹å±‚å®ç°ç»†èŠ‚
3. **è®¾å¤‡æ— å…³**: è®¡ç®—é€»è¾‘ä¸è®¾å¤‡ç‰¹å®šå®ç°åˆ†ç¦»
4. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°æ“ä½œæˆ–åç«¯å®ç°

## ğŸ”„ æœ€æ–°æ¶æ„æ”¹è¿›

### æ¨¡å—åŒ–ä¼˜åŒ–
- **CUDAæ“ä½œæ•´åˆ**: å°†ç´¢å¼•æ“ä½œåˆå¹¶è‡³`cuda_indexing_ops.py`ä»¥æé«˜å¯ç»´æŠ¤æ€§
- **å†…å­˜ç®¡ç†ä¼˜åŒ–**: æ”¹è¿›CUDAå†…å­˜åˆ†é…æ¨¡å¼ï¼Œé™ä½å¼€é”€
- **ä»£ç ç»“æ„ç²¾ç®€**: ç§»é™¤å†—ä½™æ¨¡å—ï¼Œä¼˜åŒ–ç»„ä»¶å…³ç³»

### æ€§èƒ½å¢å¼º
- **å†…æ ¸ç¼–è¯‘ä¼˜åŒ–**: æ”¹è¿›Tritonå†…æ ¸åˆå§‹åŒ–å’Œç¼–è¯‘æµç¨‹
- **å¯åŠ¨æ—¶é—´æ”¹è¿›**: ä¼˜åŒ–æ¡†æ¶åˆå§‹åŒ–å’Œé¢„çƒ­è¿‡ç¨‹
- **èµ„æºåˆ©ç”¨ä¼˜åŒ–**: å¢å¼ºGPUå†…å­˜ä½¿ç”¨æ¨¡å¼å’Œåˆ†é…ç­–ç•¥

## ğŸ—ï¸ åˆ†å±‚æ¶æ„

### å››å±‚è®¾è®¡

1. **å¼ é‡å±‚** (ç”¨æˆ·æ¥å£ + è‡ªåŠ¨å¾®åˆ†)
   - é¢å‘ç”¨æˆ·çš„API
   - è‡ªåŠ¨å¾®åˆ†
   - è®¡ç®—å›¾ç®¡ç†

2. **å‡½æ•°å±‚** (æ¢¯åº¦å®šä¹‰)
   - å‰å‘è®¡ç®—é€»è¾‘
   - åå‘æ¢¯åº¦è§„åˆ™
   - è¿æ¥å¼ é‡å±‚å’ŒNDArrayå±‚

3. **NDArrayå±‚** (è®¾å¤‡æŠ½è±¡)
   - è®¾å¤‡æ— å…³çš„è®¡ç®—æ¥å£
   - CPU/GPUç»Ÿä¸€æ“ä½œ
   - è®¾å¤‡ç®¡ç†å’Œåˆ‡æ¢

4. **åç«¯å±‚** (å®é™…è®¡ç®—)
   - CPU: PyTorchå¼ é‡
   - GPU: CUDAStorageä¸Tritonå†…æ ¸

## ğŸ”„ è®¡ç®—æµç¨‹

```mermaid
graph TB
    subgraph "ç¬¬1å±‚: ç”¨æˆ·æ¥å£"
        User["ç”¨æˆ·ä»£ç <br/>c = a + b"]
    end
    
    subgraph "ç¬¬2å±‚: å¼ é‡ (è‡ªåŠ¨å¾®åˆ†)"
        Tensor["Tensor.__add__()<br/>â€¢ ç®¡ç†æ¢¯åº¦<br/>â€¢ æ„å»ºè®¡ç®—å›¾"]
    end
    
    subgraph "ç¬¬3å±‚: å‡½æ•°å±‚"
        Func["nn.functional.Add<br/>â€¢ forward(): å®šä¹‰è®¡ç®—<br/>â€¢ backward(): å®šä¹‰æ¢¯åº¦"]
    end
    
    subgraph "ç¬¬4å±‚: NDArray"
        NDArray["NDArray.__add__()<br/>â€¢ è®¾å¤‡æ— å…³æ¥å£<br/>â€¢ åˆ†å‘åˆ°åç«¯"]
    end
    
    subgraph "ç¬¬5å±‚: åç«¯"
        CPU["CPUåç«¯<br/>PyTorchå¼ é‡"]
        GPU["GPUåç«¯<br/>CUDAStorage + Triton"]
    end
    
    User --> Tensor
    Tensor -->|"è°ƒç”¨"| Func
    Func -->|"a.data + b.data"| NDArray
    NDArray -->|"device.add()"| CPU
    NDArray -->|"device.add()"| GPU
    
    style User fill:#e1f5fe
    style Tensor fill:#f3e5f5
    style Func fill:#fff3e0
    style NDArray fill:#e8f5e8
    style CPU fill:#fce4ec
    style GPU fill:#e3f2fd
```

### ç¤ºä¾‹: åŠ æ³•æ“ä½œ

```python
# ç”¨æˆ·ä»£ç 
c = a + b  # a, b æ˜¯å¼ é‡

# ç¬¬1å±‚: å¼ é‡
def __add__(self, other):
    return genesis.nn.functional.add(self, other)

# ç¬¬2å±‚: å‡½æ•°å±‚
class Add(Function):
    @staticmethod
    def forward(ctx, a, b):
        ctx.save_for_backward(a, b)
        # åªä½¿ç”¨NDArrayæ¥å£ï¼Œä¸æ¶‰åŠåç«¯ç»†èŠ‚
        return Tensor(a.data + b.data)
    
    @staticmethod
    def backward(ctx, grad_output):
        # æ¢¯åº¦è§„åˆ™
        return grad_output, grad_output

# ç¬¬3å±‚: NDArray
def __add__(self, other):
    # åˆ†å‘åˆ°è®¾å¤‡ç‰¹å®šå®ç°
    return self.device.add(self, other)

# ç¬¬4å±‚: åç«¯ (GPUç¤ºä¾‹)
def add(x, y):
    # å®é™…çš„Tritonå†…æ ¸æ‰§è¡Œ
    output = empty(x.shape)
    add_kernel[grid](x.ptr, y.ptr, output.ptr, n_elements)
    return output
```

## ğŸ”‘ å…³é”®è®¾è®¡åŸåˆ™

### 1. æ¸…æ™°çš„æŠ½è±¡å±‚æ¬¡

**åŸåˆ™**: æ¯ä¸€å±‚åªäº†è§£ç›´æ¥ä¸‹å±‚çš„ä¿¡æ¯ã€‚

- **å¼ é‡** â†’ äº†è§£ **nn.functional** (ç”¨äºæ“ä½œ)
- **nn.functional** â†’ äº†è§£ **NDArray** (ç”¨äºè®¡ç®—)
- **NDArray** â†’ äº†è§£ **åç«¯** (ç”¨äºè®¾å¤‡ç‰¹å®šæ“ä½œ)
- **åç«¯** â†’ å®ç°å®é™…è®¡ç®—

**åæ¨¡å¼** (æˆ‘ä»¬æ­£åœ¨ä¿®å¤çš„):
```python
# é”™è¯¯: nn.functionalä¸åº”è¯¥äº†è§£CUDAStorage
if hasattr(tensor.data.data, 'to_numpy'):  # è§¦åŠè¿‡æ·±å±‚æ¬¡!
    # CUDAStorageç‰¹å®šä»£ç 
```

**æ­£ç¡®æ¨¡å¼**:
```python
# æ­£ç¡®: nn.functionalåªä½¿ç”¨NDArrayæ¥å£
result = a.data + b.data  # æ¸…æ™°çš„æŠ½è±¡
```

### 2. å•ä¸€èŒè´£

- **å¼ é‡**: è‡ªåŠ¨å¾®åˆ†å’Œæ¢¯åº¦ç®¡ç†
- **nn.functional**: å®šä¹‰å‰å‘/åå‘è®¡ç®—è§„åˆ™
- **NDArray**: è®¾å¤‡æŠ½è±¡å’Œç»Ÿä¸€æ“ä½œ
- **åç«¯**: å®é™…è®¡ç®—å®ç°

### 3. åŒåç«¯æ¶æ„

- **CPUåç«¯**: åˆ©ç”¨PyTorchæˆç†Ÿçš„CPUå¼ é‡å®ç°
- **GPUåç«¯**: å®Œå…¨ç‹¬ç«‹çš„CUDAå®ç°ï¼Œä½¿ç”¨CUDAStorage

## ğŸ“Š ç»„ä»¶èŒè´£

### ç¬¬1å±‚: å¼ é‡ (`autograd.py`)

```python
class Tensor:
    data: NDArray          # åº•å±‚æ•°æ® (å§”æ‰˜è®¡ç®—)
    requires_grad: bool    # æ˜¯å¦éœ€è¦æ¢¯åº¦
    grad: Tensor          # ç´¯ç§¯æ¢¯åº¦
    creator: Function     # åˆ›å»ºæ­¤å¼ é‡çš„æ“ä½œ
    
    # é¢å‘ç”¨æˆ·çš„æ“ä½œ
    def __add__(self, other):
        return nn.functional.add(self, other)  # å§”æ‰˜ç»™å‡½æ•°å±‚
```

**èŒè´£**:
- ç®¡ç†è®¡ç®—å›¾
- å­˜å‚¨å’Œç´¯ç§¯æ¢¯åº¦
- æä¾›ç”¨æˆ·å‹å¥½çš„API
- å§”æ‰˜å®é™…è®¡ç®—ç»™nn.functional

### ç¬¬2å±‚: nn.functional (`nn/functional.py`)

```python
class Add(Function):
    @staticmethod
    def forward(ctx, a, b):
        ctx.save_for_backward(a, b)
        # åªä½¿ç”¨NDArrayæ¥å£
        return Tensor(a.data + b.data)
    
    @staticmethod
    def backward(ctx, grad_output):
        # å®šä¹‰æ¢¯åº¦è§„åˆ™
        return grad_output, grad_output
```

**èŒè´£**:
- å®šä¹‰å‰å‘è®¡ç®—é€»è¾‘
- å®šä¹‰åå‘æ¢¯åº¦è§„åˆ™
- ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€ä¿¡æ¯
- **ä¸**è´Ÿè´£å®é™…è®¡ç®—å®ç°

### ç¬¬3å±‚: NDArray (`ndarray/ndarray.py`)

```python
class NDArray:
    device: Device        # CPUæˆ–CUDA
    data: Union[torch.Tensor, CUDAStorage]  # å®é™…æ•°æ®
    
    def __add__(self, other):
        # åˆ†å‘åˆ°è®¾å¤‡ç‰¹å®šå®ç°
        return self.device.add(self, other)
```

**èŒè´£**:
- æä¾›è®¾å¤‡æ— å…³çš„è®¡ç®—æ¥å£
- å¤„ç†è®¾å¤‡åˆ‡æ¢ (CPU â†” GPU)
- åˆ†å‘æ“ä½œåˆ°æ­£ç¡®çš„åç«¯
- æ•°æ®æ ¼å¼è½¬æ¢ (numpyç­‰)

### ç¬¬4å±‚: åç«¯ (`ndarray_ops_cpu.py`, `ndarray_ops_gpu.py`)

```python
# GPUåç«¯ç¤ºä¾‹
def add(x, y):
    output = empty(x.shape)
    add_kernel[grid](x.ptr, y.ptr, output.ptr, n_elements)
    return output
```

**èŒè´£**:
- å®é™…è®¡ç®—å®ç°
- å†…å­˜ç®¡ç†
- è®¾å¤‡ç‰¹å®šä¼˜åŒ–
- å†…æ ¸æ‰§è¡Œ

## ğŸ”„ æ¢¯åº¦æµç¤ºä¾‹

è®©æˆ‘ä»¬è¿½è¸ªä¸€æ¬¡å®Œæ•´çš„å‰å‘å’Œåå‘ä¼ æ’­ï¼š

```python
# ç”¨æˆ·ä»£ç 
a = Tensor([1, 2, 3], requires_grad=True)
b = Tensor([4, 5, 6], requires_grad=True)
c = a + b
c.backward(Tensor([1, 1, 1]))
```

### å‰å‘ä¼ æ’­

```mermaid
sequenceDiagram
    participant User
    participant Tensor
    participant Functional
    participant NDArray
    participant Backend
    
    User->>Tensor: c = a + b
    Tensor->>Functional: functional.add(a, b)
    Functional->>Functional: ctx.save_for_backward(a, b)
    Functional->>NDArray: a.data + b.data
    NDArray->>Backend: device.add(x, y)
    Backend-->>NDArray: result_data
    NDArray-->>Functional: result_ndarray
    Functional-->>Tensor: Tensor(result_ndarray)
    Tensor-->>User: c
```

### åå‘ä¼ æ’­

```mermaid
sequenceDiagram
    participant User
    participant Tensor
    participant Functional
    
    User->>Tensor: c.backward(grad)
    Tensor->>Functional: Add.backward(ctx, grad)
    Functional->>Functional: è®¡ç®—æ¢¯åº¦
    Functional-->>Tensor: grad_a, grad_b
    Tensor->>Tensor: a.grad += grad_a
    Tensor->>Tensor: b.grad += grad_b
```

## âš ï¸ å½“å‰æ­£åœ¨ä¿®å¤çš„é—®é¢˜

### é—®é¢˜: nn.functionalä¸­çš„æŠ½è±¡æ³„æ¼

å½“å‰ï¼Œ`nn.functional`æœ‰å¦‚ä¸‹ä»£ç :
```python
# é”™è¯¯: è§¦åŠå®ç°ç»†èŠ‚
if hasattr(t.data.data, 'to_numpy'):  # æ£€æŸ¥CUDAStorage
    # ç‰¹æ®ŠGPUå¤„ç†
```

### è§£å†³æ–¹æ¡ˆ: æ¸…æ™°æŠ½è±¡

æˆ‘ä»¬æ­£åœ¨é‡æ„ä¸º:
```python
# æ­£ç¡®: åªä½¿ç”¨NDArrayæ¥å£
result = a.data + b.data  # NDArrayå¤„ç†è®¾å¤‡ç»†èŠ‚
```

è¿™ç¡®ä¿:
1. nn.functionalä¸äº†è§£CUDAStorage
2. æ¯ä¸€å±‚åªäº†è§£å…¶ç›´æ¥é‚»å±…
3. æ˜“äºæ·»åŠ æ–°åç«¯è€Œä¸æ›´æ”¹ä¸Šå±‚

**å…³é”®ç‰¹æ€§**ï¼š
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒçš„è‡ªåŠ¨ç±»å‹è½¬æ¢
- çµæ´»çš„è®¡ç®—å›¾æ„å»ºå’Œéå†
- å†…ç½®çš„æ¢¯åº¦ç´¯ç§¯å’Œæ¸…é›¶æœºåˆ¶

### å¼ é‡åç«¯ç³»ç»Ÿ

#### CPUåç«¯ (`ndarray_ops_cpu.py`)
```python
# ç›´æ¥ä½¿ç”¨PyTorchæ“ä½œ
def add(x, y):
    return x + y

def matmul(x, y):
    return torch.matmul(x, y)
```

#### GPUåç«¯ (`ndarray_ops_gpu.py`)
```python
# ä½¿ç”¨Tritonå®ç°çš„GPUå†…æ ¸
@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
```

#### CUDAå†…å­˜ç®¡ç† (`cuda_storage.py`)
```python
class CUDAStorage:
    """çº¯CUDAå®ç°çš„å­˜å‚¨ï¼Œä¸ä¾èµ–PyTorch"""
    def __init__(self, shape, dtype):
        self._cuda_device, self._cuda_context = _ensure_cuda_initialized()
        self._allocate_memory(shape, dtype)
        
    def _allocate_memory(self, shape, dtype):
        # ä½¿ç”¨CUDA Python APIç›´æ¥åˆ†é…GPUå†…å­˜
        size_bytes = prod(shape) * dtype.itemsize
        result = cuda.cuMemAlloc(size_bytes)
        self._data_ptr = check_cuda_error(result)
```

### ç¥ç»ç½‘ç»œæ¨¡å— (`nn/modules/`)

Genesisé‡‡ç”¨ä¸PyTorchç±»ä¼¼çš„æ¨¡å—åŒ–æ¶æ„ï¼Œä¾¿äºä»£ç ç»„ç»‡ï¼š

```
nn/modules/
â”œâ”€â”€ module.py          # ModuleåŸºç±»å’ŒParameterç±»
â”œâ”€â”€ linear.py          # Linearã€Flattenå±‚
â”œâ”€â”€ activation.py      # ReLUã€Softmaxã€SiLUæ¿€æ´»å‡½æ•°
â”œâ”€â”€ normalization.py   # BatchNormã€LayerNormã€RMSNorm
â”œâ”€â”€ loss.py           # CrossEntropyLossã€MSELossã€BCELoss
â”œâ”€â”€ container.py      # Sequentialã€ModuleListå®¹å™¨
â”œâ”€â”€ dropout.py        # Dropoutæ­£åˆ™åŒ–
â”œâ”€â”€ sparse.py         # Embeddingã€RotaryEmbedding
â””â”€â”€ transformer.py    # MultiheadAttentionã€FeedForwardSwiGLU
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
# modules/module.py
class Module:
    """ç¥ç»ç½‘ç»œæ¨¡å—åŸºç±»"""
    def parameters(self) -> List[Tensor]:
        # é€’å½’æ”¶é›†æ‰€æœ‰å‚æ•°
        return _unpack_params(self.__dict__)
    
    def forward(self, *args, **kwargs):
        # å­ç±»å®ç°å…·ä½“çš„å‰å‘ä¼ æ’­é€»è¾‘
        raise NotImplementedError

# modules/linear.py  
class Linear(Module):
    """å…¨è¿æ¥å±‚å®ç°"""
    def __init__(self, in_features, out_features):
        self.weight = Parameter(genesis.randn(out_features, in_features))
        self.bias = Parameter(genesis.zeros(out_features))

# modules/loss.py
class CrossEntropyLoss(Module):
    """åˆ†ç±»ä»»åŠ¡çš„äº¤å‰ç†µæŸå¤±"""
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        log_prob = F.log_softmax(input, dim=1)
        # ... å®ç°ç»†èŠ‚
```

## ğŸ”§ å…³é”®æŠ€æœ¯å®ç°

### 1. å†…å­˜ç®¡ç†ç­–ç•¥

**CPUå†…å­˜ç®¡ç†**ï¼š
- ä¾èµ–PyTorchçš„å†…å­˜æ± å’Œåƒåœ¾å›æ”¶
- è‡ªåŠ¨å¤„ç†å†…å­˜å¯¹é½å’Œç¼“å­˜ä¼˜åŒ–

**GPUå†…å­˜ç®¡ç†**ï¼š
```python
class CUDAStorage:
    def __init__(self, shape, dtype, base=None):
        if base is not None:
            # è§†å›¾å­˜å‚¨ï¼šå…±äº«å†…å­˜ä½†ä¿æŒå¯¹åŸå­˜å‚¨çš„å¼•ç”¨
            self.base = base
            self._data_ptr = base._data_ptr + offset
        else:
            # æ–°å­˜å‚¨ï¼šåˆ†é…ç‹¬ç«‹å†…å­˜
            self.base = None
            self._data_ptr = cuda.cuMemAlloc(size_bytes)
    
    def __del__(self):
        # åªæœ‰åŸºç¡€å­˜å‚¨æ‰é‡Šæ”¾å†…å­˜
        if self.base is None and self._data_ptr:
            cuda.cuMemFree(self._data_ptr)
```

### 2. è®¾å¤‡æŠ½è±¡

```python
class Device:
    def __init__(self, name: str, mod: Any, device_id: Optional[int] = None):
        self.name = name        # "cpu" æˆ– "cuda"
        self.mod = mod          # å¯¹åº”çš„æ“ä½œæ¨¡å—
        self.device_id = device_id  # GPUè®¾å¤‡ID
        
    def randn(self, *shape, dtype=genesis.float32):
        if self.name == "cuda":
            return NDArray(CUDAStorage(shape, dtype), device=self)
        else:
            return NDArray(torch.randn(*shape), device=self)
```

### 3. ç±»å‹ç³»ç»Ÿ

```python
# dtypes.py - ç»Ÿä¸€çš„æ•°æ®ç±»å‹ç³»ç»Ÿ
class DType:
    def __init__(self, name: str, torch_dtype, numpy_dtype, itemsize: int):
        self.name = name
        self.torch_dtype = torch_dtype
        self.numpy_dtype = numpy_dtype  
        self.itemsize = itemsize

# æ”¯æŒçš„æ•°æ®ç±»å‹
float32 = DType("float32", torch.float32, np.float32, 4)
float16 = DType("float16", torch.float16, np.float16, 2)
bfloat16 = DType("bfloat16", torch.bfloat16, np.dtype('uint16'), 2)
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. Tritonå†…æ ¸ä¼˜åŒ–

**Softmaxå®ç°**ï¼š
```python
@triton.jit
def softmax_kernel(input_ptr, output_ptr, input_row_stride, output_row_stride, 
                  n_cols, BLOCK_SIZE: tl.constexpr):
    # é«˜æ•ˆçš„å¹¶è¡Œsoftmaxå®ç°
    row_idx = tl.program_id(0)
    row_start_ptr = input_ptr + row_idx * input_row_stride
    col_offsets = tl.arange(0, BLOCK_SIZE)
    input_ptrs = row_start_ptr + col_offsets
    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))
    
    # æ•°å€¼ç¨³å®šçš„softmax
    row_minus_max = row - tl.max(row, axis=0)
    numerator = tl.exp(row_minus_max)
    denominator = tl.sum(numerator, axis=0)
    softmax_output = numerator / denominator
    
    output_row_start_ptr = output_ptr + row_idx * output_row_stride
    output_ptrs = output_row_start_ptr + col_offsets
    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
# amp.py - è‡ªåŠ¨æ··åˆç²¾åº¦
enable_autocast = False

def _cast(value, dtype):
    """è‡ªåŠ¨ç±»å‹è½¬æ¢"""
    if isinstance(value, Tensor) and value.is_floating_point():
        if dtype == genesis.float16:
            return value.half()
        else:
            return value.float()
    return value
```

## ğŸ” æ¶æ„ä¼˜åŠ¿

### æ•™è‚²ä»·å€¼
1. **æ¸è¿›å¼å¤æ‚åº¦**ï¼šä»ç®€å•çš„CPUå®ç°åˆ°å¤æ‚çš„GPUä¼˜åŒ–
2. **å®Œæ•´å®ç°å±•ç¤º**ï¼šå±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ¡†æ¶çš„å®Œæ•´æ„å»ºè¿‡ç¨‹  
3. **æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ**ï¼šæ¯ä¸ªç»„ä»¶èŒè´£æ˜ç¡®ï¼Œä¾¿äºç†è§£

### å·¥ç¨‹å®è·µ
1. **åŒåç«¯è®¾è®¡**ï¼šCPUç¨³å®šæ€§ + GPUé«˜æ€§èƒ½
2. **å†…å­˜å®‰å…¨**ï¼šRAIIæ¨¡å¼çš„å†…å­˜ç®¡ç†ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
3. **ç±»å‹å®‰å…¨**ï¼šç»Ÿä¸€çš„ç±»å‹ç³»ç»Ÿï¼Œé¿å…ç±»å‹é”™è¯¯

### æ€§èƒ½ç‰¹æ€§
1. **Tritonä¼˜åŒ–**ï¼šç°ä»£GPUå†…æ ¸ç¼–å†™æ–¹å¼
2. **é›¶æ‹·è´è§†å›¾**ï¼šé«˜æ•ˆçš„å¼ é‡è§†å›¾æ“ä½œ
3. **å¹¶è¡Œè®¡ç®—**ï¼šå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›

## ğŸ¯ è®¾è®¡æƒè¡¡

### CPU vs GPU å®ç°é€‰æ‹©
- **CPU**ï¼šä½¿ç”¨PyTorchç¡®ä¿ç¨³å®šæ€§å’Œå…¼å®¹æ€§
- **GPU**ï¼šç‹¬ç«‹å®ç°å±•ç¤ºå®Œæ•´çš„GPUç¼–ç¨‹æ ˆ

### ç®€æ´æ€§ vs æ€§èƒ½
- ä¿æŒAPIç®€æ´çš„åŒæ—¶ï¼Œåº•å±‚å®ç°é«˜åº¦ä¼˜åŒ–
- é€šè¿‡åˆ†å±‚æ¶æ„å°†å¤æ‚æ€§éš”ç¦»åœ¨åº•å±‚

### æ•™è‚² vs ç”Ÿäº§
- ä»£ç æ³¨é‡å¯è¯»æ€§å’Œæ•™è‚²ä»·å€¼
- æ€§èƒ½ä»ç„¶è¾¾åˆ°å®ç”¨çº§åˆ«

è¿™ç§æ¶æ„è®¾è®¡ä½¿å¾—Genesisæ—¢æ˜¯ä¸€ä¸ªä¼˜ç§€çš„å­¦ä¹ èµ„æºï¼Œä¹Ÿæ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚