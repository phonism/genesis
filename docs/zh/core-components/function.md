# å‡½æ•°ç³»ç»Ÿ

Genesisçš„å‡½æ•°ç³»ç»Ÿä¸ºè‡ªåŠ¨å¾®åˆ†æä¾›åŸºç¡€ï¼Œå®šä¹‰äº†æ“ä½œåœ¨å‰å‘ä¼ æ’­ä¸­å¦‚ä½•æ‰§è¡Œä»¥åŠåœ¨åå‘ä¼ æ’­ä¸­å¦‚ä½•è®¡ç®—æ¢¯åº¦ã€‚

## ğŸ“‹ æ¦‚è¿°

å‡½æ•°ç³»ç»Ÿå›´ç»•`Function`åŸºç±»æ„å»ºï¼Œå°è£…äº†ï¼š
- å‰å‘è®¡ç®—é€»è¾‘
- åå‘æ¢¯åº¦è®¡ç®—
- ç”¨äºå­˜å‚¨ä¸­é—´å€¼çš„ä¸Šä¸‹æ–‡ç®¡ç†
- ä¸è‡ªåŠ¨å¾®åˆ†å¼•æ“çš„é›†æˆ

## ğŸ—ï¸ æ¶æ„

```mermaid
graph TB
    subgraph "å‡½æ•°ç³»ç»Ÿ"
        A[FunctionåŸºç±»] --> B[apply()æ–¹æ³•]
        A --> C[forward()æ–¹æ³•]
        A --> D[backward()æ–¹æ³•]
        E[Context] --> F[save_for_backward()]
        E --> G[saved_tensors]
    end

    subgraph "è‡ªåŠ¨å¾®åˆ†é›†æˆ"
        B --> H[è®¡ç®—å›¾]
        H --> I[æ¢¯åº¦æµ]
        I --> J[åå‘ä¼ æ’­]
    end

    subgraph "å†…ç½®å‡½æ•°"
        K[AddFunction] --> A
        L[MulFunction] --> A
        M[MatMulFunction] --> A
        N[ReluFunction] --> A
    end

    style A fill:#e1f5fe
    style E fill:#f3e5f5
    style H fill:#e8f5e8
```

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### FunctionåŸºç±»
`Function`ç±»ä¸ºæ‰€æœ‰æ“ä½œæä¾›æ¥å£ï¼š

```python
class Function:
    """æ‰€æœ‰è‡ªåŠ¨å¾®åˆ†å‡½æ•°çš„åŸºç±»ã€‚"""

    @staticmethod
    def apply(*args):
        """åº”ç”¨å…·æœ‰è‡ªåŠ¨å¾®åˆ†æ”¯æŒçš„å‡½æ•°ã€‚"""
        ctx = Context()

        # å‰å‘ä¼ æ’­
        result = cls.forward(ctx, *args)

        # å¦‚æœä»»ä½•è¾“å…¥éœ€è¦æ¢¯åº¦ï¼Œè®¾ç½®åå‘ä¼ æ’­
        if any(tensor.requires_grad for tensor in args if isinstance(tensor, Tensor)):
            result.set_creator(ctx, cls.backward)

        return result

    @staticmethod
    def forward(ctx, *args):
        """è®¡ç®—å‰å‘ä¼ æ’­ã€‚å¿…é¡»ç”±å­ç±»å®ç°ã€‚"""
        raise NotImplementedError

    @staticmethod
    def backward(ctx, *grad_outputs):
        """è®¡ç®—åå‘ä¼ æ’­ã€‚å¿…é¡»ç”±å­ç±»å®ç°ã€‚"""
        raise NotImplementedError
```

### ä¸Šä¸‹æ–‡ç®¡ç†
`Context`ç±»ç®¡ç†åå‘è®¡ç®—æ‰€éœ€çš„ä¿¡æ¯ï¼š

```python
class Context:
    """ç”¨äºå­˜å‚¨åå‘ä¼ æ’­æœŸé—´æ‰€éœ€ä¿¡æ¯çš„ä¸Šä¸‹æ–‡ã€‚"""

    def __init__(self):
        self.saved_tensors = []
        self.saved_variables = {}

    def save_for_backward(self, *tensors):
        """ä¿å­˜å¼ é‡ä»¥ä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚"""
        self.saved_tensors.extend(tensors)

    def save_variable(self, name, value):
        """ä¿å­˜å˜é‡ä»¥ä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚"""
        self.saved_variables[name] = value
```

## ğŸ’» å®ç°ç¤ºä¾‹

### åŸºæœ¬ç®—æœ¯å‡½æ•°
```python
class AddFunction(Function):
    """æ”¯æŒæ¢¯åº¦çš„åŠ æ³•å‡½æ•°ã€‚"""

    @staticmethod
    def forward(ctx, a, b):
        """å‰å‘ä¼ æ’­ï¼šè®¡ç®—a + bã€‚"""
        # åŠ æ³•ä¸éœ€è¦ä¿å­˜è¾“å…¥
        return genesis.ops.add(a, b)

    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­ï¼šæ¢¯åº¦ä¸å˜æµåŠ¨ã€‚"""
        return grad_output, grad_output

# ä½¿ç”¨
add = AddFunction.apply
c = add(a, b)  # ç­‰ä»·äºæ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„ a + b
```

### çŸ©é˜µä¹˜æ³•å‡½æ•°
```python
class MatMulFunction(Function):
    """æ”¯æŒæ¢¯åº¦çš„çŸ©é˜µä¹˜æ³•ã€‚"""

    @staticmethod
    def forward(ctx, a, b):
        """å‰å‘ä¼ æ’­ï¼šè®¡ç®— a @ bã€‚"""
        ctx.save_for_backward(a, b)
        return genesis.ops.matmul(a, b)

    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­ï¼šä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦ã€‚"""
        a, b = ctx.saved_tensors

        grad_a = genesis.ops.matmul(grad_output, b.transpose(-2, -1))
        grad_b = genesis.ops.matmul(a.transpose(-2, -1), grad_output)

        return grad_a, grad_b

# ä½¿ç”¨
matmul = MatMulFunction.apply
c = matmul(a, b)  # ç­‰ä»·äºæ”¯æŒè‡ªåŠ¨å¾®åˆ†çš„ a @ b
```

### å¸¦ä¸Šä¸‹æ–‡çš„æ¿€æ´»å‡½æ•°
```python
class ReluFunction(Function):
    """æ”¯æŒæ¢¯åº¦çš„ReLUæ¿€æ´»ã€‚"""

    @staticmethod
    def forward(ctx, input):
        """å‰å‘ä¼ æ’­ï¼šè®¡ç®— max(0, input)ã€‚"""
        output = genesis.ops.maximum(input, 0)
        ctx.save_for_backward(input)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        """åå‘ä¼ æ’­ï¼šè´Ÿè¾“å…¥çš„æ¢¯åº¦ä¸º0ã€‚"""
        input, = ctx.saved_tensors
        mask = input > 0
        return grad_output * mask

# ä½¿ç”¨
relu = ReluFunction.apply
activated = relu(x)
```

## ğŸš€ é«˜çº§ç‰¹æ€§

### åŸåœ°æ“ä½œ
```python
class AddInplaceFunction(Function):
    """åŸåœ°åŠ æ³•å‡½æ•°ã€‚"""

    @staticmethod
    def forward(ctx, a, b):
        """å‰å‘ä¼ æ’­ï¼šåŸåœ°ä¿®æ”¹aã€‚"""
        ctx.save_variable('original_a', a.clone())
        a.add_(b)
        return a

    @staticmethod
    def backward(ctx, grad_output):
        """åŸåœ°æ“ä½œçš„åå‘ä¼ æ’­ã€‚"""
        return grad_output, grad_output
```

### å¤šè¾“å‡ºå‡½æ•°
```python
class SplitFunction(Function):
    """è¿”å›å¤šä¸ªè¾“å‡ºçš„å‡½æ•°ã€‚"""

    @staticmethod
    def forward(ctx, input, split_sizes):
        """å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ã€‚"""
        ctx.save_variable('split_sizes', split_sizes)
        return genesis.ops.split(input, split_sizes)

    @staticmethod
    def backward(ctx, *grad_outputs):
        """ä»å¤šä¸ªè¾“å‡ºè¿æ¥æ¢¯åº¦ã€‚"""
        grad_input = genesis.ops.cat(grad_outputs, dim=0)
        return grad_input, None  # split_sizesæ²¡æœ‰æ¢¯åº¦
```

### è‡ªå®šä¹‰ä¸Šä¸‹æ–‡å˜é‡
```python
class ScaleFunction(Function):
    """é€šè¿‡å¸¸æ•°å› å­ç¼©æ”¾å¼ é‡ã€‚"""

    @staticmethod
    def forward(ctx, input, scale_factor):
        """é€šè¿‡å¸¸æ•°å› å­ç¼©æ”¾è¾“å…¥ã€‚"""
        ctx.save_variable('scale_factor', scale_factor)
        return input * scale_factor

    @staticmethod
    def backward(ctx, grad_output):
        """é€šè¿‡ç›¸åŒå› å­ç¼©æ”¾æ¢¯åº¦ã€‚"""
        scale_factor = ctx.saved_variables['scale_factor']
        return grad_output * scale_factor, None
```

## ğŸ”§ ä¸æ“ä½œé›†æˆ

### å‘è°ƒåº¦å™¨æ³¨å†Œå‡½æ•°
```python
# å‘æ“ä½œè°ƒåº¦å™¨æ³¨å†Œå‡½æ•°
genesis.ops.register_function('add', AddFunction.apply)
genesis.ops.register_function('matmul', MatMulFunction.apply)
genesis.ops.register_function('relu', ReluFunction.apply)

# ç°åœ¨æ“ä½œè‡ªåŠ¨ä½¿ç”¨æ³¨å†Œçš„å‡½æ•°
x = genesis.tensor([1, 2, 3], requires_grad=True)
y = genesis.tensor([4, 5, 6], requires_grad=True)
z = x + y  # è‡ªåŠ¨ä½¿ç”¨AddFunction
```

### è‡ªå®šä¹‰æ“ä½œå®šä¹‰
```python
def custom_operation(input, param):
    """ä½¿ç”¨Functionå®šä¹‰è‡ªå®šä¹‰æ“ä½œã€‚"""
    return CustomFunction.apply(input, param)

# æ³¨å†Œä¸ºæ“ä½œ
genesis.ops.register_operation('custom_op', custom_operation)

# åƒä»»ä½•å…¶ä»–æ“ä½œä¸€æ ·ä½¿ç”¨
result = genesis.custom_op(tensor, param)
```

## ğŸ“Š æ€§èƒ½è€ƒè™‘

### å†…å­˜æ•ˆç‡
```python
class EfficientFunction(Function):
    """å†…å­˜é«˜æ•ˆçš„å‡½æ•°å®ç°ã€‚"""

    @staticmethod
    def forward(ctx, input):
        # åªä¿å­˜åå‘æ‰€éœ€çš„å†…å®¹
        ctx.save_for_backward(input.detach())  # åˆ†ç¦»ä»¥é¿å…é€’å½’æ¢¯åº¦

        # é«˜æ•ˆè®¡ç®—ç»“æœ
        result = efficient_computation(input)
        return result

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        # é«˜æ•ˆè®¡ç®—æ¢¯åº¦
        return efficient_gradient_computation(input, grad_output)
```

### æ•°å€¼ç¨³å®šæ€§
```python
class StableFunction(Function):
    """æ•°å€¼ç¨³å®šçš„å‡½æ•°å®ç°ã€‚"""

    @staticmethod
    def forward(ctx, input):
        # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„è®¡ç®—
        output = stable_computation(input)
        ctx.save_for_backward(input, output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, output = ctx.saved_tensors
        # ä½¿ç”¨ç¨³å®šçš„æ¢¯åº¦è®¡ç®—
        return stable_gradient(input, output, grad_output)
```

## ğŸ” è°ƒè¯•å’Œæµ‹è¯•

### å‡½æ•°æµ‹è¯•
```python
def test_function_gradients():
    """æµ‹è¯•å‡½æ•°æ¢¯åº¦è®¡ç®—ã€‚"""
    x = genesis.tensor([1.0, 2.0, 3.0], requires_grad=True)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    y = CustomFunction.apply(x)

    # æµ‹è¯•åå‘ä¼ æ’­
    y.backward(genesis.tensor([1.0, 1.0, 1.0]))

    # æ£€æŸ¥æ¢¯åº¦
    assert x.grad is not None
    print(f"æ¢¯åº¦ï¼š{x.grad}")

# æ•°å€¼æ¢¯åº¦æ£€æŸ¥
def numerical_gradient_check(func, input, eps=1e-5):
    """ä½¿ç”¨æ•°å€¼å¾®åˆ†æ£€æŸ¥æ¢¯åº¦ã€‚"""
    # æ•°å€¼æ¢¯åº¦æ£€æŸ¥çš„å®ç°
    pass
```

### è°ƒè¯•ä¸Šä¸‹æ–‡
```python
class DebugFunction(Function):
    """å¸¦è°ƒè¯•ä¿¡æ¯çš„å‡½æ•°ã€‚"""

    @staticmethod
    def forward(ctx, input):
        print(f"å‰å‘ï¼šè¾“å…¥å½¢çŠ¶ = {input.shape}")
        ctx.save_for_backward(input)
        result = computation(input)
        print(f"å‰å‘ï¼šè¾“å‡ºå½¢çŠ¶ = {result.shape}")
        return result

    @staticmethod
    def backward(ctx, grad_output):
        print(f"åå‘ï¼šgrad_outputå½¢çŠ¶ = {grad_output.shape}")
        input, = ctx.saved_tensors
        grad_input = gradient_computation(input, grad_output)
        print(f"åå‘ï¼šgrad_inputå½¢çŠ¶ = {grad_input.shape}")
        return grad_input
```

## ğŸ”— å‚è§

- [å¼ é‡ç³»ç»Ÿ](tensor.md) - å¼ é‡ç±»å’Œè‡ªåŠ¨å¾®åˆ†é›†æˆ
- [æ ¸å¿ƒç»„ä»¶æ¦‚è¿°](index.md) - æ•´ä½“ç³»ç»Ÿæ¶æ„
- [è‡ªåŠ¨å¾®åˆ†](autograd.md) - è¯¦ç»†çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿ
- [è‡ªå®šä¹‰æ“ä½œæŒ‡å—](../tutorials/custom-ops.md) - åˆ›å»ºè‡ªå®šä¹‰æ“ä½œ