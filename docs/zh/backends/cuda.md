# CUDAåç«¯

CUDAåç«¯é€šè¿‡è‡ªå®šä¹‰CUDAå†…æ ¸å’ŒTritonå®ç°æä¾›é«˜æ€§èƒ½GPUæ“ä½œã€‚

## ğŸ“‹ æ¦‚è¿°

CUDAåç«¯ç”±å‡ ä¸ªä¸“é—¨ç»„ä»¶ç»„æˆï¼š
- `backends/cuda.py` - ä¸»CUDAå­˜å‚¨å®ç°
- `backends/cuda_memory.py` - é«˜çº§å†…å­˜ç®¡ç†
- `backends/cuda_kernels.py` - ä¼˜åŒ–çš„CUDAå†…æ ¸

## ğŸ—ï¸ æ¶æ„

```mermaid
graph TB
    subgraph "CUDAåç«¯"
        A[cuda.py] --> B[CUDAStorage]
        C[cuda_memory.py] --> D[MemoryPool]
        E[cuda_kernels.py] --> F[è‡ªå®šä¹‰å†…æ ¸]
    end

    subgraph "å†…å­˜ç®¡ç†"
        D --> G[å—åˆ†é…å™¨]
        D --> H[å†…å­˜ç»Ÿè®¡]
        D --> I[åƒåœ¾å›æ”¶]
    end

    subgraph "å†…æ ¸ç³»ç»Ÿ"
        F --> J[Tritonå†…æ ¸]
        F --> K[CUDA C++å†…æ ¸]
        F --> L[å†…æ ¸ç¼“å­˜]
    end

    style A fill:#76ff03
    style C fill:#ff5722
    style E fill:#2196f3
```

## ğŸ¯ å…³é”®ç‰¹æ€§

### é«˜çº§å†…å­˜ç®¡ç†
- **å†…å­˜æ± åŒ–**ï¼šå‡å°‘åˆ†é…å¼€é”€
- **æ™ºèƒ½ç¼“å­˜**ï¼šé«˜æ•ˆé‡ç”¨å†…å­˜å—
- **ç¢ç‰‡å¤„ç†**ï¼šæœ€å°åŒ–å†…å­˜ç¢ç‰‡
- **ç»Ÿè®¡è·Ÿè¸ª**ï¼šè¯¦ç»†çš„å†…å­˜ä½¿ç”¨åˆ†æ

### è‡ªå®šä¹‰å†…æ ¸å®ç°
- **Tritonå†…æ ¸**ï¼šé«˜æ€§èƒ½GPUå†…æ ¸
- **å†…æ ¸èåˆ**ï¼šç»„åˆå¤šä¸ªæ“ä½œ
- **è‡ªåŠ¨è°ƒä¼˜**ï¼šæœ€ä½³å—å¤§å°é€‰æ‹©
- **æ‡’ç¼–è¯‘**ï¼šå†…æ ¸åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶ç¼–è¯‘

### å¯é æ€§ç‰¹æ€§
- **æ‡’åˆå§‹åŒ–**ï¼šé¿å…å¯¼å…¥æ—¶çš„CUDAé”™è¯¯
- **é”™è¯¯å¤„ç†**ï¼šå¤±è´¥æ—¶ä¼˜é›…å›é€€
- **å†…å­˜æ¸…ç†**ï¼šè‡ªåŠ¨èµ„æºç®¡ç†

## ğŸƒâ€â™‚ï¸ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜æ± 
```python
class CUDAMemoryPool:
    """é«˜æ€§èƒ½CUDAå†…å­˜æ± ã€‚"""

    def allocate(self, size):
        """åˆ†é…å†…å­˜å—ã€‚"""
        # å°è¯•é‡ç”¨ç°æœ‰å—
        block = self._find_free_block(size)
        if block:
            return block

        # åˆ†é…æ–°å—
        return self._allocate_new_block(size)

    def deallocate(self, ptr):
        """å°†å—è¿”å›åˆ°æ± ã€‚"""
        self._free_blocks.add(ptr)
```

### å†…æ ¸ä¼˜åŒ–
å¸¦è‡ªåŠ¨è°ƒä¼˜çš„Tritonå†…æ ¸ï¼š
```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 128}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 256}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 512}, num_warps=16),
    ],
    key=['n_elements'],
)
@triton.jit
def elementwise_add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    """ä¼˜åŒ–çš„é€å…ƒç´ åŠ æ³•å†…æ ¸ã€‚"""
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
```

## ğŸ’» å®ç°ç»†èŠ‚

### CUDAå­˜å‚¨
```python
class CUDAStorage:
    """å…·æœ‰é«˜çº§ç‰¹æ€§çš„CUDAå¼ é‡å­˜å‚¨ã€‚"""

    def __init__(self, shape, dtype, device_id=0):
        self.device_id = device_id
        self.shape = shape
        self.dtype = dtype
        self._data_ptr = None
        self._initialize_lazy()

    def _initialize_lazy(self):
        """æ‡’CUDAåˆå§‹åŒ–ã€‚"""
        if not torch.cuda.is_available():
            raise RuntimeError("CUDAä¸å¯ç”¨")

        torch.cuda.set_device(self.device_id)
        size = self._compute_size()
        self._data_ptr = CUDAMemoryPool.get_instance().allocate(size)
```

### å†…å­˜ç»Ÿè®¡
```python
def get_memory_stats():
    """è·å–è¯¦ç»†çš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡ã€‚"""
    pool = CUDAMemoryPool.get_instance()
    return {
        'allocated': pool.allocated_bytes,
        'cached': pool.cached_bytes,
        'reserved': pool.reserved_bytes,
        'free': pool.free_bytes,
        'fragmentation': pool.fragmentation_ratio,
        'peak_allocated': pool.peak_allocated_bytes,
    }
```

## ğŸ”§ é…ç½®

### ç¯å¢ƒå˜é‡
```bash
# æŒ‡å®šGPUè®¾å¤‡
export CUDA_VISIBLE_DEVICES=0

# å†…å­˜æ± è®¾ç½®
export GENESIS_CUDA_MEMORY_FRACTION=0.8
export GENESIS_CUDA_CACHE_SIZE=1GB

# å†…æ ¸ç¼–è¯‘ç¼“å­˜
export GENESIS_KERNEL_CACHE_DIR=/tmp/genesis_kernels
```

### è¿è¡Œæ—¶é…ç½®
```python
import genesis

# é…ç½®CUDAåç«¯
genesis.cuda.set_memory_fraction(0.9)
genesis.cuda.set_cache_size("2GB")
genesis.cuda.enable_lazy_init(True)

# åˆ›å»ºCUDAå¼ é‡
device = genesis.device("cuda:0")
x = genesis.tensor([1, 2, 3], device=device)
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

ä¸PyTorch CUDAçš„æ¯”è¾ƒï¼š

| æ“ä½œ | å¤§å° | Genesis CUDA | PyTorch CUDA | åŠ é€Ÿæ¯” |
|------|------|--------------|--------------|--------|
| åŠ æ³• | 1M | 0.15ms | 0.45ms | 3.0x |
| çŸ©ä¹˜ | 1024Â² | 0.8ms | 1.2ms | 1.5x |
| Softmax | 10K | 0.25ms | 0.35ms | 1.4x |
| è§„çº¦ | 1M | 0.12ms | 0.18ms | 1.5x |

### å†…å­˜æ€§èƒ½
```python
# å†…å­˜ä½¿ç”¨æ¯”è¾ƒ
genesis_tensor = genesis.tensor(data, device="cuda")
torch_tensor = torch.tensor(data, device="cuda")

print(f"Genesiså†…å­˜ï¼š{genesis.cuda.memory_allocated()}")
print(f"PyTorchå†…å­˜ï¼š{torch.cuda.memory_allocated()}")
```

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### å†…å­˜ç›‘æ§
```python
# ç›‘æ§å†…å­˜ä½¿ç”¨
def monitor_cuda_memory():
    stats = genesis.cuda.memory_stats()
    print(f"å·²åˆ†é…ï¼š{stats['allocated'] / 1e9:.2f} GB")
    print(f"ç¼“å­˜ï¼š{stats['cached'] / 1e9:.2f} GB")
    print(f"ç¢ç‰‡ç‡ï¼š{stats['fragmentation']:.2%}")

# è®¾ç½®ç›‘æ§
genesis.cuda.set_memory_callback(monitor_cuda_memory)
```

### å†…æ ¸æ€§èƒ½åˆ†æ
```python
# å¯ç”¨å†…æ ¸æ€§èƒ½åˆ†æ
genesis.cuda.enable_profiling(True)

# æ“ä½œç°åœ¨å°†æ‰“å°æ—¶é—´ä¿¡æ¯
x = genesis.tensor([[1, 2], [3, 4]], device="cuda")
y = genesis.matmul(x, x)  # æ‰“å°ï¼š"matmul_kernel: 0.15ms"
```

## âš ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### CUDAå†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆ1ï¼šå‡å°‘å†…å­˜ä½¿ç”¨
genesis.cuda.empty_cache()

# è§£å†³æ–¹æ¡ˆ2ï¼šè°ƒæ•´å†…å­˜åˆ†æ•°
genesis.cuda.set_memory_fraction(0.7)

# è§£å†³æ–¹æ¡ˆ3ï¼šå¯ç”¨å†…å­˜è°ƒè¯•
genesis.cuda.enable_memory_debugging(True)
```

#### å†…æ ¸ç¼–è¯‘é”™è¯¯
```python
# æ¸…é™¤å†…æ ¸ç¼“å­˜
genesis.cuda.clear_kernel_cache()

# æš‚æ—¶ç¦ç”¨å†…æ ¸èåˆ
genesis.cuda.set_kernel_fusion(False)
```

#### æ€§èƒ½ç¼“æ…¢
```python
# é¢„çƒ­å†…æ ¸
genesis.cuda.warm_up_kernels()

# æ£€æŸ¥æ­£ç¡®çš„è®¾å¤‡æ”¾ç½®
print(f"å¼ é‡è®¾å¤‡ï¼š{x.device}")
print(f"å½“å‰è®¾å¤‡ï¼š{genesis.cuda.current_device()}")
```

## ğŸ”— å‚è§

- [åç«¯ç³»ç»Ÿæ¦‚è¿°](index.md)
- [CPUåç«¯](cpu.md)
- [å†…å­˜ç®¡ç†](memory.md)
- [CUDAæ“ä½œ](../ops/cuda-ops.md)