# å†…å­˜ç®¡ç†ç³»ç»Ÿ

Genesisæä¾›é«˜çº§å†…å­˜ç®¡ç†åŠŸèƒ½ï¼Œå®ç°é«˜æ•ˆçš„GPUå’ŒCPUå†…å­˜åˆ©ç”¨ã€‚

## ğŸ“‹ æ¦‚è¿°

å†…å­˜ç®¡ç†ç³»ç»Ÿæ—¨åœ¨ï¼š
- é€šè¿‡æ± åŒ–æœ€å°åŒ–åˆ†é…å¼€é”€
- å‡å°‘å†…å­˜ç¢ç‰‡
- æä¾›è¯¦ç»†çš„å†…å­˜ç»Ÿè®¡
- å¯ç”¨é«˜æ•ˆçš„å†…å­˜é‡ç”¨

## ğŸ—ï¸ æ¶æ„

```mermaid
graph TB
    subgraph "å†…å­˜ç®¡ç†å±‚"
        A[å†…å­˜æ± ] --> B[å—åˆ†é…å™¨]
        C[å†…å­˜ç»Ÿè®¡] --> D[ä½¿ç”¨è·Ÿè¸ª]
        E[åƒåœ¾æ”¶é›†å™¨] --> F[æ¸…ç†é€»è¾‘]
    end

    subgraph "è®¾å¤‡å†…å­˜"
        B --> G[CUDAå†…å­˜]
        B --> H[CPUå†…å­˜]
        G --> I[è®¾å¤‡åˆ†é…]
        H --> J[ä¸»æœºåˆ†é…]
    end

    subgraph "ç¼“å­˜ç®¡ç†"
        K[å†…å­˜ç¼“å­˜] --> L[ç©ºé—²å—]
        K --> M[ä¿ç•™å—]
        K --> N[æ´»åŠ¨å—]
    end

    style A fill:#4caf50
    style C fill:#ff9800
    style E fill:#f44336
```

## ğŸ¯ å…³é”®ç»„ä»¶

### å†…å­˜æ± 
ç®¡ç†å†…å­˜åˆ†é…å’Œé‡Šæ”¾çš„ä¸­å¤®ç»„ä»¶ï¼š

```python
class MemoryPool:
    """ç”¨äºé«˜æ•ˆåˆ†é…çš„ç»Ÿä¸€å†…å­˜æ± ã€‚"""

    def __init__(self, device_type):
        self.device_type = device_type
        self.free_blocks = {}  # å¤§å° -> å—åˆ—è¡¨
        self.allocated_blocks = {}  # æŒ‡é’ˆ -> å—ä¿¡æ¯
        self.total_allocated = 0
        self.peak_allocated = 0

    def allocate(self, size):
        """åˆ†é…ç»™å®šå¤§å°çš„å†…å­˜å—ã€‚"""
        # å°è¯•é‡ç”¨ç°æœ‰å—
        block = self._find_suitable_block(size)
        if block:
            return self._reuse_block(block, size)

        # åˆ†é…æ–°å—
        return self._allocate_new_block(size)

    def deallocate(self, ptr):
        """å°†å†…å­˜å—è¿”å›åˆ°æ± ã€‚"""
        block = self.allocated_blocks.pop(ptr)
        self._add_to_free_blocks(block)
```

### å†…å­˜ç»Ÿè®¡
å…¨é¢è·Ÿè¸ªå†…å­˜ä½¿ç”¨ï¼š

```python
class MemoryStats:
    """è¯¦ç»†çš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡ã€‚"""

    def __init__(self):
        self.allocated_bytes = 0
        self.reserved_bytes = 0
        self.active_bytes = 0
        self.inactive_bytes = 0
        self.peak_allocated = 0
        self.num_allocs = 0
        self.num_frees = 0

    def update_allocation(self, size):
        """æ›´æ–°æ–°åˆ†é…çš„ç»Ÿè®¡ã€‚"""
        self.allocated_bytes += size
        self.active_bytes += size
        self.peak_allocated = max(self.peak_allocated, self.allocated_bytes)
        self.num_allocs += 1

    def fragmentation_ratio(self):
        """è®¡ç®—å†…å­˜ç¢ç‰‡ã€‚"""
        if self.reserved_bytes == 0:
            return 0.0
        return (self.reserved_bytes - self.allocated_bytes) / self.reserved_bytes
```

## ğŸš€ CUDAå†…å­˜ç®¡ç†

### é«˜çº§æ± åŒ–ç­–ç•¥
```python
class CUDAMemoryPool(MemoryPool):
    """å…·æœ‰é«˜çº§ç‰¹æ€§çš„CUDAç‰¹å®šå†…å­˜æ± ã€‚"""

    def __init__(self, device_id=0):
        super().__init__("cuda")
        self.device_id = device_id
        self.memory_fraction = 0.8  # ä½¿ç”¨80%çš„å¯ç”¨å†…å­˜
        self._initialize_pool()

    def _initialize_pool(self):
        """åˆå§‹åŒ–CUDAå†…å­˜æ± ã€‚"""
        torch.cuda.set_device(self.device_id)

        # è·å–å¯ç”¨å†…å­˜
        total_memory = torch.cuda.get_device_properties(self.device_id).total_memory
        available_memory = int(total_memory * self.memory_fraction)

        # é¢„åˆ†é…å¤§å—
        self._preallocate_blocks(available_memory)

    def _preallocate_blocks(self, total_size):
        """é¢„åˆ†é…å„ç§å¤§å°çš„å†…å­˜å—ã€‚"""
        block_sizes = [1024, 4096, 16384, 65536, 262144, 1048576]  # 2çš„å¹‚

        for block_size in block_sizes:
            num_blocks = max(1, total_size // (block_size * len(block_sizes)))
            for _ in range(num_blocks):
                ptr = torch.cuda.caching_allocator_alloc(block_size)
                self._add_to_free_blocks(Block(ptr, block_size))
```

### å†…å­˜ä¼˜åŒ–ç‰¹æ€§

#### æ™ºèƒ½ç¼“å­˜
```python
def smart_cache_management(self):
    """åŸºäºä½¿ç”¨æ¨¡å¼çš„æ™ºèƒ½å†…å­˜ç¼“å­˜ã€‚"""
    # åˆ†æåˆ†é…æ¨¡å¼
    frequent_sizes = self._analyze_allocation_patterns()

    # åŸºäºæ¨¡å¼è°ƒæ•´ç¼“å­˜å¤§å°
    for size in frequent_sizes:
        self._increase_cache_for_size(size)

    # æ¸…ç†å¾ˆå°‘ä½¿ç”¨çš„å—
    self._cleanup_unused_blocks()
```

#### å†…å­˜å‹å®
```python
def compact_memory(self):
    """é€šè¿‡å‹å®å‡å°‘å†…å­˜ç¢ç‰‡ã€‚"""
    # æŸ¥æ‰¾ç¢ç‰‡åŒºåŸŸ
    fragmented_blocks = self._find_fragmented_blocks()

    # å‹å®ç›¸é‚»çš„ç©ºé—²å—
    for block_group in fragmented_blocks:
        merged_block = self._merge_blocks(block_group)
        self._add_to_free_blocks(merged_block)
```

## ğŸ’» CPUå†…å­˜ç®¡ç†

### é«˜æ•ˆä¸»æœºå†…å­˜
```python
class CPUMemoryPool(MemoryPool):
    """æ”¯æŒé’‰ä½å†…å­˜çš„CPUå†…å­˜æ± ã€‚"""

    def __init__(self):
        super().__init__("cpu")
        self.use_pinned_memory = False
        self.pinned_blocks = set()

    def allocate_pinned(self, size):
        """åˆ†é…é’‰ä½å†…å­˜ä»¥åŠ å¿«GPUä¼ è¾“ã€‚"""
        ptr = torch.cuda.cudaHostAlloc(size, torch.cuda.cudaHostAllocDefault)
        block = Block(ptr, size, pinned=True)
        self.pinned_blocks.add(ptr)
        return block

    def is_pinned(self, ptr):
        """æ£€æŸ¥å†…å­˜å—æ˜¯å¦è¢«é’‰ä½ã€‚"""
        return ptr in self.pinned_blocks
```

## ğŸ”§ é…ç½®å’Œä½¿ç”¨

### åŸºæœ¬é…ç½®
```python
import genesis

# é…ç½®CUDAå†…å­˜
genesis.cuda.set_memory_fraction(0.9)  # ä½¿ç”¨90%çš„GPUå†…å­˜
genesis.cuda.set_cache_size("2GB")     # è®¾ç½®ç¼“å­˜å¤§å°

# é…ç½®CPUå†…å­˜
genesis.cpu.enable_pinned_memory(True) # å¯ç”¨é’‰ä½å†…å­˜

# è·å–å½“å‰å†…å­˜ç»Ÿè®¡
stats = genesis.memory_stats()
print(f"å·²åˆ†é…ï¼š{stats.allocated_bytes / 1e9:.2f} GB")
print(f"ç¼“å­˜ï¼š{stats.cached_bytes / 1e9:.2f} GB")
```

### é«˜çº§å†…å­˜æ§åˆ¶
```python
# æ‰‹åŠ¨å†…å­˜ç®¡ç†
def optimize_memory_usage():
    # æ¸…é™¤æœªä½¿ç”¨çš„ç¼“å­˜
    genesis.empty_cache()

    # è§¦å‘åƒåœ¾æ”¶é›†
    genesis.memory_manager.collect_garbage()

    # å‹å®ç¢ç‰‡å†…å­˜
    genesis.memory_manager.compact()

# å†…å­˜ç›‘æ§
def monitor_memory():
    stats = genesis.memory_stats()
    fragmentation = stats.fragmentation_ratio()

    if fragmentation > 0.3:  # 30%ç¢ç‰‡é˜ˆå€¼
        print("æ£€æµ‹åˆ°é«˜ç¢ç‰‡ï¼Œæ­£åœ¨å‹å®...")
        genesis.memory_manager.compact()
```

## ğŸ“Š å†…å­˜åˆ†æ

### å†…ç½®åˆ†æå™¨
```python
# å¯ç”¨å†…å­˜åˆ†æ
genesis.enable_memory_profiling(True)

# å†…å­˜æ“ä½œç°åœ¨å°†è¢«è·Ÿè¸ª
x = genesis.tensor([1, 2, 3], device="cuda")  # åˆ†é…è¢«è·Ÿè¸ª
y = x + 1                                     # ä¸´æ—¶åˆ†é…è¢«è·Ÿè¸ª
del x                                         # é‡Šæ”¾è¢«è·Ÿè¸ª

# è·å–åˆ†ææŠ¥å‘Š
report = genesis.memory_profiler.get_report()
print(report.summary())
```

### å†…å­˜æ—¶é—´çº¿
```python
# è®°å½•å†…å­˜æ—¶é—´çº¿
with genesis.memory_profiler.record():
    # è¿™é‡Œæ˜¯ä½ çš„ä»£ç 
    model = create_model()
    data = load_data()
    output = model(data)

# åˆ†ææ—¶é—´çº¿
timeline = genesis.memory_profiler.get_timeline()
timeline.plot()  # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨éšæ—¶é—´å˜åŒ–
```

## âš¡ æ€§èƒ½æŠ€å·§

### å†…å­˜ä¼˜åŒ–æœ€ä½³å®è·µ

1. **é¢„åˆ†é…å¤§å¼ é‡**
   ```python
   # å¥½ï¼šé¢„åˆ†é…
   buffer = genesis.empty((1000000,), device="cuda")

   # é¿å…ï¼šé¢‘ç¹çš„å°åˆ†é…
   for i in range(1000):
       x = genesis.tensor([i], device="cuda")
   ```

2. **é‡ç”¨å†…å­˜ç¼“å†²åŒº**
   ```python
   # é‡ç”¨ç¼“å†²åŒº
   result_buffer = genesis.empty((batch_size, num_features))
   for batch in dataloader:
       genesis.matmul(batch.input, weights, out=result_buffer)
   ```

3. **ä½¿ç”¨åŸåœ°æ“ä½œ**
   ```python
   # åŸåœ°æ“ä½œèŠ‚çœå†…å­˜
   x.add_(y)      # è€Œä¸æ˜¯ x = x + y
   x.mul_(0.5)    # è€Œä¸æ˜¯ x = x * 0.5
   ```

4. **æ‰‹åŠ¨å†…å­˜ç®¡ç†**
   ```python
   # åœ¨éœ€è¦æ—¶æ¸…é™¤ç¼“å­˜
   if memory_pressure_detected():
       genesis.empty_cache()
   ```

## ğŸ” è°ƒè¯•å†…å­˜é—®é¢˜

### å†…å­˜æ³„æ¼æ£€æµ‹
```python
# å¯ç”¨æ³„æ¼æ£€æµ‹
genesis.enable_memory_debugging(True)

# è¿è¡Œä½ çš„ä»£ç 
train_model()

# æ£€æŸ¥æ³„æ¼
leaks = genesis.check_memory_leaks()
if leaks:
    print("æ£€æµ‹åˆ°å†…å­˜æ³„æ¼ï¼š")
    for leak in leaks:
        print(f"  {leak.location}å¤„çš„{leak.size}å­—èŠ‚")
```

### å†…å­˜ä½¿ç”¨åˆ†æ
```python
def analyze_memory_usage():
    """å…¨é¢çš„å†…å­˜åˆ†æã€‚"""
    stats = genesis.detailed_memory_stats()

    print(f"æ€»åˆ†é…ï¼š{stats.total_allocated / 1e9:.2f} GB")
    print(f"å³°å€¼ä½¿ç”¨ï¼š{stats.peak_allocated / 1e9:.2f} GB")
    print(f"ç¢ç‰‡ç‡ï¼š{stats.fragmentation_ratio:.2%}")
    print(f"ç¼“å­˜å‘½ä¸­ç‡ï¼š{stats.cache_hit_rate:.2%}")

    # æ˜¾ç¤ºæœ€å¤§çš„åˆ†é…
    large_allocs = stats.get_large_allocations(min_size=100*1024*1024)  # 100MB+
    for alloc in large_allocs:
        print(f"å¤§åˆ†é…ï¼š{alloc.location}å¤„çš„{alloc.size / 1e6:.1f} MB")
```

## ğŸ”— å‚è§

- [CUDAåç«¯](cuda.md)
- [CPUåç«¯](cpu.md)
- [æ€§èƒ½ä¼˜åŒ–](../performance/optimization-guide.md)
- [è°ƒè¯•æŒ‡å—](../contributing/debugging.md)