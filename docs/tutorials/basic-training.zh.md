# åŸºç¡€è®­ç»ƒæ•™ç¨‹

æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œä½¿ç”¨Genesisæ·±åº¦å­¦ä¹ æ¡†æ¶æ„å»ºå’Œè®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å›¾åƒåˆ†ç±»é¡¹ç›®æ¥å­¦ä¹ Genesisçš„æ ¸å¿ƒæ¦‚å¿µå’Œç”¨æ³•ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š
- Genesisçš„åŸºæœ¬APIå’Œæ•°æ®ç»“æ„
- å¦‚ä½•å®šä¹‰å’Œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- è®­ç»ƒå¾ªç¯çš„æ„å»ºå’Œä¼˜åŒ–
- æ¨¡å‹è¯„ä¼°å’Œä¿å­˜

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£…ä¾èµ–

```bash
# ç¡®ä¿å·²å®‰è£…Genesis
pip install torch triton
git clone https://github.com/phonism/genesis.git
cd genesis
pip install -e .

# å®‰è£…é¢å¤–ä¾èµ–
pip install matplotlib torchvision tqdm
```

### éªŒè¯å®‰è£…

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim

print(f"Genesisç‰ˆæœ¬: {genesis.__version__}")
print(f"CUDAå¯ç”¨: {genesis.cuda.is_available()}")
```

## ğŸ“Š é¡¹ç›®ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ‰‹å†™æ•°å­—è¯†åˆ«ç³»ç»Ÿï¼Œä½¿ç”¨ç»å…¸çš„MNISTæ•°æ®é›†ã€‚

### 1. æ•°æ®å‡†å¤‡

```python
import genesis
import genesis.nn as nn
import genesis.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# æ•°æ®é¢„å¤„ç†
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# åŠ è½½MNISTæ•°æ®é›†
train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('data', train=False, transform=transform)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 64
train_loader = genesis.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = genesis.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
```

### 2. æ¨¡å‹å®šä¹‰

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼š

```python
class MNISTNet(nn.Module):
    """MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ç½‘ç»œ"""
    
    def __init__(self, num_classes=10):
        super(MNISTNet, self).__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
        # æ¿€æ´»å‡½æ•°å’ŒDropout
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x):
        # å·ç§¯å—1
        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -> 14x14
        
        # å·ç§¯å—2  
        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -> 7x7
        
        # å±•å¹³
        x = x.view(x.size(0), -1)  # [batch_size, 64*7*7]
        
        # å…¨è¿æ¥å±‚
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
device = genesis.device('cuda' if genesis.cuda.is_available() else 'cpu')
model = MNISTNet().to(device)

print("æ¨¡å‹ç»“æ„:")
print(model)
print(f"\\nå‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
```

### 3. è®­ç»ƒé…ç½®

```python
# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)

# è®­ç»ƒå‚æ•°
num_epochs = 10
print_every = 100  # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡

print(f"è®¾å¤‡: {device}")
print(f"æ‰¹é‡å¤§å°: {batch_size}")
print(f"è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
```

### 4. è®­ç»ƒå¾ªç¯

```python
def train_epoch(model, train_loader, criterion, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        data, target = data.to(device), target.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = genesis.max(output, dim=1)
        total += target.size(0)
        correct += (predicted == target).sum().item()
        
        # æ‰“å°è¿›åº¦
        if batch_idx % print_every == 0:
            print(f'Epoch {epoch+1}/{num_epochs}, '
                  f'Batch {batch_idx}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}, '
                  f'Acc: {100*correct/total:.2f}%')
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc

def validate(model, test_loader, criterion):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with genesis.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            
            test_loss += criterion(output, target).item()
            _, predicted = genesis.max(output, dim=1)
            total += target.size(0)
            correct += (predicted == target).sum().item()
    
    avg_loss = test_loss / len(test_loader)
    accuracy = 100 * correct / total
    
    return avg_loss, accuracy

# å¼€å§‹è®­ç»ƒ
print("å¼€å§‹è®­ç»ƒ...")
train_losses, train_accs = [], []
val_losses, val_accs = [], []

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, epoch)
    
    # éªŒè¯
    val_loss, val_acc = validate(model, test_loader, criterion)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()
    
    # è®°å½•ç»“æœ
    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)
    
    print(f"Epoch {epoch+1}/{num_epochs}:")
    print(f"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
    print(f"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
    print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
    print("-" * 50)

print("è®­ç»ƒå®Œæˆï¼")
```

### 5. ç»“æœå¯è§†åŒ–

```python
# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# æŸå¤±æ›²çº¿
ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
ax1.plot(val_losses, label='éªŒè¯æŸå¤±', color='red')
ax1.set_title('æŸå¤±æ›²çº¿')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True)

# å‡†ç¡®ç‡æ›²çº¿
ax2.plot(train_accs, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
ax2.plot(val_accs, label='éªŒè¯å‡†ç¡®ç‡', color='red')
ax2.set_title('å‡†ç¡®ç‡æ›²çº¿')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy (%)')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.show()

print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {val_accs[-1]:.2f}%")
```

### 6. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æ¨¡å‹
model_path = 'mnist_model.pth'
genesis.save_checkpoint({
    'epoch': num_epochs,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_loss': train_losses[-1],
    'val_loss': val_losses[-1],
    'val_acc': val_accs[-1]
}, model_path)

print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

# åŠ è½½æ¨¡å‹
def load_model(model_path, model_class, num_classes=10):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    checkpoint = genesis.load_checkpoint(model_path)
    
    model = model_class(num_classes)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']:.2f}%")
    return model

# æµ‹è¯•åŠ è½½
loaded_model = load_model(model_path, MNISTNet)
```

### 7. å•å¼ å›¾ç‰‡é¢„æµ‹

```python
def predict_single_image(model, image, class_names=None):
    """å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹"""
    model.eval()
    
    if class_names is None:
        class_names = [str(i) for i in range(10)]
    
    with genesis.no_grad():
        if image.dim() == 3:  # æ·»åŠ batchç»´åº¦
            image = image.unsqueeze(0)
        
        image = image.to(device)
        output = model(image)
        probabilities = genesis.softmax(output, dim=1)
        
        confidence, predicted = genesis.max(probabilities, dim=1)
        
    return predicted.item(), confidence.item()

# æµ‹è¯•é¢„æµ‹
test_iter = iter(test_loader)
images, labels = next(test_iter)

# é¢„æµ‹å‰5å¼ å›¾ç‰‡
fig, axes = plt.subplots(1, 5, figsize=(15, 3))
for i in range(5):
    image = images[i]
    true_label = labels[i].item()
    
    predicted, confidence = predict_single_image(model, image)
    
    # æ˜¾ç¤ºå›¾ç‰‡
    axes[i].imshow(image.squeeze(), cmap='gray')
    axes[i].set_title(f'çœŸå®: {true_label}\\né¢„æµ‹: {predicted}\\nç½®ä¿¡åº¦: {confidence:.3f}')
    axes[i].axis('off')

plt.tight_layout()
plt.show()
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

è®©æˆ‘ä»¬æ¯”è¾ƒGenesisä¸PyTorchçš„æ€§èƒ½ï¼š

```python
import time

def benchmark_training(model, train_loader, criterion, optimizer, device, num_batches=100):
    """è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    model.train()
    start_time = time.time()
    
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx >= num_batches:
            break
            
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
    
    elapsed_time = time.time() - start_time
    return elapsed_time

# è¿è¡ŒåŸºå‡†æµ‹è¯•
print("æ€§èƒ½åŸºå‡†æµ‹è¯• (100ä¸ªbatch):")
genesis_time = benchmark_training(model, train_loader, criterion, optimizer, device)
print(f"Genesisè®­ç»ƒæ—¶é—´: {genesis_time:.2f} ç§’")
print(f"å¹³å‡æ¯ä¸ªbatch: {genesis_time/100*1000:.1f} ms")
```

## ğŸ¯ å…³é”®æ¦‚å¿µæ€»ç»“

### 1. å¼ é‡æ“ä½œ
```python
# åˆ›å»ºå¼ é‡
x = genesis.randn(3, 4, requires_grad=True)
y = genesis.ones(3, 4)

# åŸºç¡€è¿ç®—
z = x + y
w = genesis.matmul(x, y.T)

# æ¢¯åº¦è®¡ç®—
z.sum().backward()
print(x.grad)  # xçš„æ¢¯åº¦
```

### 2. æ¨¡å‹å®šä¹‰æœ€ä½³å®è·µ
```python
class BestPracticeNet(nn.Module):
    def __init__(self):
        super().__init__()
        # ä½¿ç”¨nn.Sequentialç®€åŒ–å®šä¹‰
        self.features = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)
        )
        self.classifier = nn.Sequential(
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
```

### 3. è®­ç»ƒæŠ€å·§
```python
# æ¢¯åº¦è£å‰ª
genesis.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æƒé‡åˆå§‹åŒ–
def init_weights(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        genesis.nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            genesis.nn.init.zeros_(m.bias)

model.apply(init_weights)
```

## ğŸš€ ä¸‹ä¸€æ­¥

æ­å–œï¼ä½ å·²ç»å®Œæˆäº†ç¬¬ä¸€ä¸ªGenesisè®­ç»ƒé¡¹ç›®ã€‚æ¥ä¸‹æ¥å¯ä»¥æ¢ç´¢ï¼š

1. **[æ··åˆç²¾åº¦è®­ç»ƒ](amp-training.zh.md)** - åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜
2. **[è‡ªå®šä¹‰ç®—å­](custom-ops.zh.md)** - å®ç°ä¸“ç”¨çš„ç¥ç»ç½‘ç»œæ“ä½œ
3. **[æ€§èƒ½è°ƒä¼˜](performance-tuning.zh.md)** - ä¼˜åŒ–è®­ç»ƒæ€§èƒ½
4. **[åˆ†å¸ƒå¼è®­ç»ƒ](distributed-training.zh.md)** - å¤šGPUå¹¶è¡Œè®­ç»ƒ

## â“ å¸¸è§é—®é¢˜

**Q: è®­ç»ƒé€Ÿåº¦æ¯”é¢„æœŸæ…¢ï¼Ÿ**
A: æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†CUDAï¼Œç¡®ä¿æ•°æ®é¢„å¤„ç†ä¸æ˜¯ç“¶é¢ˆï¼Œè€ƒè™‘è°ƒæ•´batch_sizeã€‚

**Q: å†…å­˜ä¸è¶³é”™è¯¯ï¼Ÿ**
A: å‡å°batch_sizeï¼Œå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œæˆ–ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒã€‚

**Q: æ¨¡å‹ä¸æ”¶æ•›ï¼Ÿ**
A: æ£€æŸ¥å­¦ä¹ ç‡è®¾ç½®ï¼Œç¡®è®¤æ•°æ®é¢„å¤„ç†æ­£ç¡®ï¼Œå°è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•ã€‚

---

!!! success "å®Œæˆäº†åŸºç¡€æ•™ç¨‹ï¼"
    ä½ ç°åœ¨å·²ç»æŒæ¡äº†Genesisçš„æ ¸å¿ƒæ¦‚å¿µã€‚ç»§ç»­æ¢ç´¢æ›´é«˜çº§çš„ç‰¹æ€§å§ï¼

[ä¸‹ä¸€æ•™ç¨‹ï¼šè‡ªå®šä¹‰ç®—å­](custom-ops.zh.md){ .md-button .md-button--primary }
[è¿”å›æ•™ç¨‹ç›®å½•](index.zh.md){ .md-button }